```{r include=FALSE, eval=TRUE}
knitr::opts_chunk$set(eval = FALSE)
source("r/render.R")
```

# Appendix

## Prerequisites {#appendix-prerequisites}

### Installing R {#appendix-install-r}

From [r-project.org](https://r-project.org/), download and launch the R installer for your platform, Windows, Macs or Linux available.

```{r appendix-r-download, eval=TRUE, echo=FALSE, fig.width=4, fig.align='center', fig.cap='The R Project for Statistical Computing'}
render_image("images/appendix-download-r.png", "The R Project for Statistical Computing")
```

### Installing Java {#appendix-install-java}

From [java.com/download](https://java.com/download), download and launch the installer for your platform, Windows, Macs or Linux are also available.

```{r appendix-java-download, eval=TRUE, fig.width=4, fig.align='center', echo=FALSE, fig.cap='Java Download Page'}
render_image("images/appendix-download-java.png", "Java Download Page")
```

Starting with Spark 2.1, Java 8 is required; however, previous versions of Spark support Java 7. Regardless, we recommend installing Java Runtime Engine 8, or JRE 8 for short.

**Note:** For advanced readers that are already using the Java Development Kit, JDK for short. Please notice that JDK 9+ is currently unsupported so you will need to downgrade to JDK 8 by uninstalling JDK 9+ or by setting `JAVA_HOME` appropiately.

### Installing RStudio {#appendix-install-rstudio}

While installing RStudio is not strictly required to work with Spark with R, it will make you much more productive and therefore, I would recommend you take the time to install RStudio from [rstudio.com/download](https://www.rstudio.com/download), then download and launch the installer for your platform: Windows, Macs or Linux.

```{r appendix-rstudio-download, eval=TRUE, fig.width=4, fig.align='center', echo=FALSE, fig.cap='RStudio Downloads Page'}
render_image("images/appendix-rstudio.png", "RStudio Downloads Page")
```

After launching RStudio, you can use RStudio's console panel to execute the code provided in this chapter.

### Using RStudio {#appendix-using-rstudio}

If you are not familiar with RStudio, you should make note of the following panes:

- **Console**: A standalone R console you can use to execute all the code presented in this book.
- **Packages**: This pane allows you to install `sparklyr` with ease, check its version, navigate to the help contents, etc.
- **Connections**: This pane allows you to connecto to Spark, manage your active connection and view the available datasets.

```{r appendix-rstudio-overview, eval=TRUE, fig.width=4, fig.align='center', echo=FALSE, fig.cap='RStudio Overview'}
render_image("images/appendix-rstudio-overview.png", "RStudio Overview")
```

## Diagrams

### Worlds Store Capacity {#appendix-storage-capacity}

```{r eval=FALSE}
library(tidyverse)
read_csv("data/01-worlds-capacity-to-store-information.csv", skip = 8) %>%
  gather(key = storage, value = capacity, analog, digital) %>%
  mutate(year = X1, terabytes = capacity / 1e+12) %>%
  ggplot(aes(x = year, y = terabytes, group = storage)) +
    geom_line(aes(linetype = storage)) +
    geom_point(aes(shape = storage)) +
    scale_y_log10(
      breaks = scales::trans_breaks("log10", function(x) 10^x),
      labels = scales::trans_format("log10", scales::math_format(10^x))
    ) +
    theme_light() +
    theme(legend.position = "bottom")
```

### Daily downloads of CRAN packages {#appendix-cran-downloads}

```{r eval=FALSE}
downloads_csv <- "data/01-intro-r-cran-downloads.csv"
if (!file.exists(downloads_csv)) {
  downloads <- cranlogs::cran_downloads(from = "2014-01-01", to = "2019-01-01")
  readr::write_csv(downloads, downloads_csv)
}

cran_downloads <- readr::read_csv(downloads_csv)

ggplot(cran_downloads, aes(date, count)) + 
  geom_point(colour="black", pch = 21, size = 1) +
  scale_x_date() +
  xlab("") +
  ylab("") +
  theme_light()
```

### Google trends for mainframes, cloud computing and kubernetes {#appendix-cluster-trends}

Data downloaded from [https://trends.google.com/trends/explore?date=all&q=cloud%20computing,mainframe,kubernetes](https://trends.google.com/trends/explore?date=all&q=cloud%20computing,mainframe,kubernetes).

```{r eval=FALSE}
library(r2d3)

lines <- readLines("data/clusters-trends.csv")
lines <- gsub("<1", 0, lines)
writeLines(lines, "data/clusters-trends.csv")

read.csv("data/clusters-trends.csv", skip = 2) %>%
  mutate(year = as.Date(paste(Month, "-01", sep = ""))) %>%
    mutate(`On-Premise` = `mainframe...Worldwide.`,
           Cloud = `cloud.computing...Worldwide.`,
           Kubernetes = `kubernetes...Worldwide.`) %>%
    tidyr::gather(`On-Premise`, Cloud, Kubernetes, key = "trend", value = "popularity") %>%
    ggplot(aes(x=year, y=popularity, group=trend)) +
      geom_line(aes(linetype = trend, color = trend)) +
      scale_x_date(date_breaks = "2 year", date_labels = "%Y") +
      labs(title = "Cluster Computing Trends",
           subtitle = "Search popularity for on-premise (mainframe), cloud computing and kubernetes ") +
      scale_color_grey(start = 0.6, end = 0.2) +
      geom_hline(yintercept = 0, size = 1, colour = "#333333")
```

## Formatting {#appendix-ggplot2-theme}

The following `ggplot2` theme was use to format plots in this book:

```{r appendix-ggplot2-theme-code}
plot_style <- function() {
  font <- "Helvetica"

  ggplot2::theme_classic() +
  ggplot2::theme(
    plot.title = ggplot2::element_text(family = font,
                                       size=14,
                                       color = "#222222"),
    plot.subtitle = ggplot2::element_text(family=font,
                                          size=12,
                                          color = "#666666"),

    legend.position = "right",
    legend.background = ggplot2::element_blank(),
    legend.title = ggplot2::element_blank(),
    legend.key = ggplot2::element_blank(),
    legend.text = ggplot2::element_text(family=font,
                                        size=14,
                                        color="#222222"),

    axis.title.y = ggplot2::element_text(margin = ggplot2::margin(t = 0, r = 8, b = 0, l = 0),
                                size = 14,
                                color="#666666"),
    axis.title.x = ggplot2::element_text(margin = ggplot2::margin(t = -2, r = 0, b = 0, l = 0),
                                size = 14,
                                color = "#666666"),
    axis.text = ggplot2::element_text(family=font,
                                      size=14,
                                      color="#222222"),
    axis.text.x = ggplot2::element_text(margin = ggplot2::margin(5, b = 10)),
    axis.ticks = ggplot2::element_blank(),
    axis.line = ggplot2::element_blank(),

    panel.grid.minor = ggplot2::element_blank(),
    panel.grid.major.y = ggplot2::element_line(color = "#eeeeee"),
    panel.grid.major.x = ggplot2::element_line(color = "#ebebeb"),

    panel.background = ggplot2::element_blank(),

    strip.background = ggplot2::element_rect(fill = "white"),
    strip.text = ggplot2::element_text(size  = 20,  hjust = 0)
  )
}
```

Which you can then active with:

```{r appendix-ggplot2-theme-activate}
ggplot2::theme_set(plot_style())
```

## List of ML Functions {#ml-functionlist}

The following table exhibits the ML algorithms supported in sparklyr:

### Classification

Algorithm | Function
----------|---------
Decision Trees | ml_decision_tree_classifier()
Gradient-Boosted Trees | ml_gbt_classifier()
Linear Support Vector Machines | ml_linear_svc()
Logistic Regression | ml_logistic_regression()
Multilayer Perceptron | ml_multilayer_perceptron_classifier()
Naive-Bayes | ml_naive_bayes()
One vs Rest | ml_one_vs_rest()
Random Forests | ml_random_forest_classifier()

### Regression

Algorithm | Function
----------|---------
Accelerated Failure Time Survival Regression | ml_aft_survival_regression()
Decision Trees | ml_decision_tree_regressor()
Generalized Linear Regression | ml_generalized_linear_regression()
Gradient-Boosted Trees | ml_gbt_regressor()
Isotonic Regression | ml_isotonic_regression()
Linear Regression | ml_linear_regression()

### Clustering

Algorithm | Function
----------|---------
Bisecting K-Means Clustering | ml_bisecting_kmeans()
Gaussian Mixture Clustering | ml_gaussian_mixture()
K-Means Clustering | ml_kmeans()
Latent Dirichlet Allocation | ml_lda()

### Recommendation

Algorithm | Function
----------|---------
Alternating Least Squares Factorization | ml_als()

### Frequent Pattern Mining

Algorithm | Function
----------|---------
FPGrowth | ml_fpgrowth()

### Feature Transformers

Transformer | Function
------------|---------
Binarizer | ft_binarizer()
Bucketizer | ft_bucketizer()
Chi-Squared Feature Selector | ft_chisq_selector()
Vocabulary from Document Collections | ft_count_vectorizer()
Discrete Cosine Transform  | ft_discrete_cosine_transform()
Transformation using dplyr | ft_dplyr_transformer()
Hadamard Product | ft_elementwise_product()
Feature Hasher | ft_feature_hasher()
Term Frequencies using Hashing | export(ft_hashing_tf)
Inverse Document Frequency | ft_idf()
Imputation for Missing Values | export(ft_imputer)
Index to String | ft_index_to_string()
Feature Interaction Transform | ft_interaction()
Rescale to [-1, 1] Range | ft_max_abs_scaler()
Rescale to [min, max] Range | ft_min_max_scaler()
Locality Sensitive Hashing | ft_minhash_lsh()
Converts to n-grams | ft_ngram()
Normalize using the given P-Norm | ft_normalizer()
One-Hot Encoding | ft_one_hot_encoder()
Feature Expansion in Polynomial Space | ft_polynomial_expansion()
Maps to Binned Categorical Features | ft_quantile_discretizer()
SQL Transformation | ft_sql_transformer()
Standardizes Features using Corrected STD | ft_standard_scaler()
Filters out Stop Words | ft_stop_words_remover()
Map to Label Indices | ft_string_indexer()
Splits by White Spaces | ft_tokenizer()
Combine Vectors to Row Vector | ft_vector_assembler()
Indexing Categorical Feature | ft_vector_indexer()
Subarray of the Original Feature | ft_vector_slicer()
Transform Word into Code | ft_word2vec()


## Kafka

This instructions were put together using information from the official Kafka site. Specifically from the Quickstart page, and at the time of writting this book.  Newer versions of Kafka undoubtely will be available when reading this book.  The idea is to "timestamp" the versions used in the example in the Streaming chapter. 

1. Download Kafka

    ```
    wget http://apache.claz.org/kafka/2.2.0/kafka_2.12-2.2.0.tgz
    ```
    
1. Expand the tar file and enter the new folder

    ```
    tar -xzf kafka_2.12-2.2.0.tgz
    cd kafka_2.12-2.2.0
    ```
    
1. Start the Zookeeper service that comes with Kafka

    ```
    bin/zookeeper-server-start.sh config/zookeeper.properties
    ```
    
1. Start the Kafka service

    ```  
    bin/kafka-server-start.sh config/server.properties
    ```
    
Make sure to always start Zookeper first, and then Kafka. 

