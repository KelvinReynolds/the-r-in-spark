\mainmatter

```{r include=FALSE, eval=TRUE}
knitr::opts_chunk$set(eval = FALSE)
source("r/render.R")
library(sparklyr)
```

# Introduction {#intro}

This chapter covers the historical background that lead to the development of Apache Spark, introduces R in the context of Spark and [sparklyr](https://github.com/rstudio/sparklyr) as a project bridging Spark and R.

## Background {#intro-background}

As humans, we have been storing, retrieving, manipulating, and communicating information since the Sumerians in Mesopotamia developed writing in about 3000 BC. Based on the storage and processing technologies employed, it is possible to distinguish four distinct phases of development: pre-mechanical (3000 BC – 1450 AD), mechanical (1450–1840), electromechanical (1840–1940), and electronic (1940–present)[@intro-information-technology].

Mathematician George Stibitz used the word **digital** to describe fast electric pulses back in 1942[@intro-computing-concise-history] and, to this day, we describe information stored electronically as digital information. In contrast, **analog** information is defined as, not digital and it represents everything we have stored by any non-electronic means such as: hand written notes, books, newspapers and so on[@intro-webster2006merriam].

The world bank report on digital development provides an estimate of digital and analog information stored over the last decades[@intro-data-revolution]. This report found out that digital information surpassed analog information around 2003, at that time, there were aboput 10M terabytes of digital information, which is roughly about 10M computer drives today. However, a more relevant finding from this report was that our footprint of digital information is growing at [exponential rates](#appendix-storage-capacity):

```{r intro-store-capacity-code, echo=FALSE}
suppressMessages(library(tidyverse))
read_csv("data/01-worlds-capacity-to-store-information.csv", skip = 8) %>%
  gather(key = storage, value = capacity, analog, digital) %>%
  mutate(year = X1, terabytes = capacity / 10^12) %>%
  ggplot(aes(x = year, y = terabytes, group = storage)) +
    geom_line(aes(linetype = storage)) +
    geom_point(aes(shape = storage)) +
    scale_y_log10(
      breaks = scales::trans_breaks("log10", function(x) 10^x),
      labels = scales::trans_format("log10", scales::math_format(10^.x))
    ) +
    scale_y_log10(name="World's Information (terabytes)") +
    theme_light() +
    theme(legend.position = "bottom") +
    ggsave("images/intro-world-store-capacity.png", width = 10, height = 6)
```
```{r intro-store-capacity-render, eval=TRUE, echo=FALSE, fig.cap='World’s capacity to store information.', fig.align = 'center'}
render_image("images/intro-world-store-capacity.png", "World’s capacity to store information.")
```

With the ambition to provide tools capable of searching all this new digital information, many companies attempted to provide such functionality with what we know today as search engines, used when searching the web. Given the vast amount of digital information, managing information at this scale was a challenging problem. Search engines were unble to store all the web page information required to support web searches in a single computer. This meant that they had to split information across many machines, which was accomlished by splitting this data and storing it as many files across many machines. This approach became known as the Google File System from a research paper published in 2003 by Google[@intro-google-file-system].

### Hadoop {#intro-hadoop}

One year later, in 2004, Google published a new paper describing how to perform operations across the Google File System, this approach came to be known as **MapReduce**[@intro-google-map-reduce]. As you would expect, there are two operations in MapReduce: Map and Reduce. The **map operation** provides an arbitrary way to transform each file into a new file, usually defined by arbitrary code that scans the file and outputs a different file. The **reduce operation** combines two files into a new one. These two operations are sufficient to process data at the scale of the data available in the web.

For example, we can use MapReduce to count words in two different text files. The mapping operation splits each word in the original file and outputs a new word-counting file with a mapping of words and counts. The reduce operation can be defined to take two word-counting files and combine them by aggregating the totals for each word, this last file will contain a list of word counts across all the original files.

```{r eval=TRUE, echo=FALSE, fig.cap='Simple MapReduce Example.'}
render_nomnoml("
#spacing: 80
#direction: right
#zoom: 4
[Alice's File|I like apples]
[Bob's File|I like bananas]

[Alice's File]-Map[Alice's Words]
[Bob's File]-Map[Bob's Words]

[Alice's Words|I,1|like,1|apples,1]
[Bob's Words|I,1|like,1|bananas,1]

[Alice's Words]- Reduce[Word Counts]
[Bob's Words]- Reduce[Word Counts]

[Word Counts|I,2|like,2|apples,1|bananas,1]
", "images/intro-simple-map-reduce-example.png", "Simple MapReduce Example.")
```

Counting words is often the most basic MapReduce example, but it can be also used for much more sophisticated and interesting applications. For instance, MapReduce can be used to rank web pages in Google's **PageRank** algorithm, which assigns ranks to web pages based on the count of hyperlinks linking to a web page and the rank of the page linking to it.

After these papers were released by Google, a team in Yahoo worked on implementing the Google File System and MapReduce as a single open source project. This project was released in 2006 as **Hadoop** with the Google File System implemented as the Hadoop File System, or **HDFS** for short. The Hadoop project made distributed file-based computing accessible to a wider range of users and organizations which enabled them to make use of MapReduce beyond web data processing.

While Hadoop provided support to perform MapReduce operations over a distributed file system, it still required MapReduce operations to be written with code every time a data analysis was run. To improve over this tedious process, the **Hive** project released in 2008 by Facebook, brought Structured Query Language (**SQL**) support to Hadoop. This meant that data analysis could now be performed at large-scale without the need to write code for each MapReduce operation; instead, one could write generic data analysis statements in SQL that are much easier to understand and write.

### Spark {#intro-spark}

In 2009, **Apache Spark** starts as a research project at the UC Berkeley's AMPLab to improve over MapReduce. Specifically, by providing a richer set of verbs beyond MapReduce that facilitate optimizing code running in multiple machines and, by loading data in-memory which made operations much fasters than Hadoop's on-disk storage. One of the earliest results showed that running logistic regression, a data modeling technique that will be introduced under the [modeling](#modeling) chapter, allowed Spark to run 10x faster than Hadoop by making use of in-memory datasets[@intro-zaharia2010spark].


```{r intro-spark-logistic-regression, echo=FALSE}
iterations <- c(1, 1:6 * 5)
data.frame(
  iteration = rep(iterations, 2),
  cluster = c(rep("hadoop", length(iterations)), rep("spark", length(iterations))),
  seconds = c(iterations * 127, 174 + (iterations - 1) * 6)) %>%
  ggplot(aes(x=iteration, y=seconds, fill=cluster)) +
  geom_bar(stat="identity", position=position_dodge()) +
  labs(x = "Number of Iterations", y = "Running Time (s)") +
  theme_light() +
  ggsave("images/intro-hadoop-spark-logistic-regression.png", width = 10, height = 4)
```
```{r intro-spark-logistic-regression-render, eval=TRUE, echo=FALSE, fig.cap='Logistic regression performance in Hadoop and Spark.'}
render_image("images/intro-hadoop-spark-logistic-regression.png", "Logistic regression performance in Hadoop and Spark.")
```

While Spark is well known for its in-memory performance, Spark was designed to be a general execution engine that works both in-memory and on-disk. For instance, Spark holds [records in large-scale sorting](http://sortbenchmark.org/), where data was not loaded in-memory; but rather, Spark made use of improvements in network serialization, network shuffling and efficient use of the CPU's cache to dramatically improve performance. For comparison, one can [sort 100TB of data in 72min and 2100 computers using Hadoop, but only 206 computers in 23min using Spark](https://databricks.com/blog/2014/11/05/spark-officially-sets-a-new-record-in-large-scale-sorting.html), it's also the case that [Spark holds the record in the cloud sorting benchmark ](https://spark.apache.org/news/spark-wins-cloudsort-100tb-benchmark.html), which makes Spark the most cost effective solution for large-scale sorting.

|                  | Hadoop Record | Spark Record  |
|------------------|---------------|---------------|
| Data Size        | 102.5 TB      | 100 TB        |
| Elapsed Time     | 72 mins       | 23 mins       |
| Nodes            | 2100          | 206           |
| Cores            | 50400         | 6592          |
| Disk             | 3150 GB/s     | 618 GB/s      |
| Network          | 10Gbps        | 10Gbps        |
| Sort rate        | 1.42 TB/min   | 4.27 TB/min   |
| Sort rate / node | 0.67 GB/min   | 20.7 GB/min   |

In 2010, Spark was released as an open source project and then donated to the Apache Software Foundation in 2013. Spark is licensed under the [Apache 2.0](https://en.wikipedia.org/wiki/Apache_License), which allows you to freely use, modify, and distribute it. In 2015, Spark reaches more than 1000 contributors, making it one of the most active projects in the Apache Software Foundation.

This gives an overview of how Spark came to be, which we can now use to formally introduce Apache Spark as follows:

> "Apache Spark is a fast and general engine for large-scale data processing."
>
> --- [spark.apache.org](http://spark.apache.org/)

To help us understand this definition of Apache Spark, we will break it down as follows:

- **Data Processing**: Data processing is the collection and manipulation of items of data to produce meaningful information[@intro-data-processing].
- **General**: Spark optimizes and executes parallel generic code, as in, there are no restrictions as to what type of code one can write in Spark.
- **Large-Scale**: One can interpret this as **cluster**-scale, as in, a set of connected computers working together to accomplish specific goals.
- **Fast**: Spark is much faster than its predecessor by making efficient use of memory, network and CPUs to speed data processing algorithms in computing cluster.

Since Spark is _general_, you can use Spark to solve many problems, from calculating averages to approximating the value of Pi, [predicting customer churn](https://mapr.com/blog/churn-prediction-sparkml/), [aligning protein sequences](https://academic.oup.com/gigascience/article/7/8/giy098/5067872) or analyzing [high energy physics at CERN](https://db-blog.web.cern.ch/blog/luca-canali/2017-08-apache-spark-and-cern-open-data-example).

Describing Spark as _large_ _scale_ implies that a good use case for Spark is tackling problems that can be solved with multiple machines. For instance, when data does not fit in a single disk driver or does not fit into memory, Spark is a good candidate to consider.

Since Spark is _fast_, it is worth considering for problems that may not be large-scale, but where using multiple processors could speed up computation. For instance, sorting large datasets or CPU intensive models could also bennefit from running in Spark.

Therefore, Spark is good at tackling large-scale data processing problems, this usually known as **big data** ([data sets that are more voluminous and complex that traditional ones](https://en.wikipedia.org/wiki/big_data)), but also is good at tackling large-scale computation problems, known as **big compute** ([tools and approaches using a large amount of CPU and memory resources in a coordinated way](https://www.nimbix.net/glossary/big-compute/)). 

Big data and big compute problems are usually easy to spot, if the data does not fit into a single machine, you might have a big data problem; if the data fits into a single machine but a process over the data takes days, weeks or even months to compute, you might have a big compute problem.

However, there is also a third problem space where data nor compute are necessarily large-scale and yet, there are significant benefits from using Spark. For this third problem space, there are a few use cases this breaks to:

1. **Velocity**: One can have a dataset of 10GB in size and a process that takes 30min to run over this data, this is by no means big-compute nor big-data; however, if a data scientist is researching ways to improve accuracy for their models, reducing the runtime down to 3min it's a 10X improvement, this improvement can lead to significant advances and productivity gains by increasing the velocity at which one can analyze data.

2. **Variety**: One can have an efficient process to collect data from many sources into a single location, usually a database, this process could be already running efficiently and close to realtime. Such processes are known at ETL (Extract-Transform-Load); data is extracted from multiple sources, transformed to the required format and loaded in a single data store. While this has worked for years, the tradeoff from this system is that adding a new data source is expensive, the system is centralized and tightly controlled. Since making changes to this type of systems could cause the entire process to come to a halt, adding new data sources usually takes long to be implemented. Instead, one can store all data its natural format and process it as needed using cluster computing, this architecture is currently known as a [data lake](https://en.wikipedia.org/wiki/Data_lake).

Some people refer to some of these benefits as [the four 'V's of big data](http://www.theserverside.com/feature/Handling-the-four-Vs-of-big-data-volume-velocity-variety-and-veracity): Velocity, Variety, Volume and Veracity (which asserts that data can vary greatly in quality which require analysis methods to improve accuracy across a variety of sources). Others have gone as far as expending this to [five](https://en.wikipedia.org/wiki/Big_data) or even as [the 10 Vs of Big Data](https://tdwi.org/articles/2017/02/08/10-vs-of-big-data.aspx). Mnemonics set aside, cluster computing is being used today in more innovative ways and and is not uncommon to see organizations experimenting with new workflows and a variety of tasks that were traditionally uncommon for cluster computing. Much of the hype attributed to big data falls into this space where, strictly speaking, one is not handling big data but there are still beneffits from using tools designed for big data and big compute. My hope is that this book will help you understand the opportunities and limitations of cluster computing, and specifically, the opportunities and limitations from using Apache Spark with R.

### R {#intro-r}

R is a computing language with it's inception dating back to Bell Laboratories. R was not created at Bell Labs, but it's predecesor, the S computing language was. [Rick Becker explained in useR 2016](https://blog.revolutionanalytics.com/2016/07/rick-becker-s-talk.html) that at that time in Bell Labs, computing was done by calling subroutines written in the Fortran language which, apparently, were not pleasant to deal with. The S computing language was designed as an interface language to solve particular problems without having to worry about other languages, Fortran at that time. The creator of S, [John Chambers](https://en.wikipedia.org/wiki/John_Chambers_(statistician)), describes how S was designed to provide an interface through the following diagram:

```{r intro-r-diagram, eval=TRUE, echo=FALSE, fig.cap='Interface language diagram by John Chambers - Rick Becker useR 2016.', fig.align = 'center'}
render_image("images/intro-s-algorithm-interface.png", "Interface language diagram by John Chambers - Rick Becker useR 2016.")
```

R is a modern and free implementation of S, specifically:

> R is a programming language and free software environment for statistical computing and graphics.
>
> --- [The R Project for Statistical Computing](https://www.r-project.org/)

While working with data, I believe there are two strong arguments for using R:

- The **R Language** was designed by statisticians for statisticians, meaning, this is one of the few successful languages designed for non-programmers; so learning R will probably feel more natural. Additionally, since the R language was designed to be an interface to other tools and languages, R allows you to focus more on modeling and less on peculiarities of computer science and engineering.
- The **R Community** provides a rich package archive provided by CRAN ([The Comprehensive R Archive Network](https://cran.r-project.org/)) which allows you to install ready-to-use packages to perform many tasks, most notably, high-quality statistic models with many only available in R. In addition, the R community is a welcoming and active group of talented individuals motivated to help you succeed. Many packages provided by the R community make R, by far, the place to do statistical computing. To mention some of the popular packages: [dplyr](https://CRAN.R-project.org/package=dplyr) to manipulate data, [cluster](https://CRAN.R-project.org/package=cluster) to analyze clusters and [ggplot2](https://CRAN.R-project.org/package=ggplot2) to visualize data. We can quantify the [growth of the R community](#appendix-cran-downloads) by plotting daily downloads of R packages in CRAN.

```{r intro-cran-downloads-code, echo=FALSE}
downloads_csv <- "data/01-intro-r-cran-downloads.csv"
if (!file.exists(downloads_csv)) {
  downloads <- cranlogs::cran_downloads(from = "2014-01-01", to = "2018-01-01")
  readr::write_csv(downloads, downloads_csv)
}

cran_downloads <- readr::read_csv(downloads_csv)

ggplot(cran_downloads, aes(date, count / 10^6)) + 
  geom_point(colour="black", pch = 21, size = 1) +
  scale_x_date() +
  xlab("Year") +
  ylab("Daily Downloads (millions)") +
  theme_light() +
  ggsave("images/intro-daily-cran-downloads.png", width = 10, height = 6)
```
```{r intro-cran-downloads-render, eval=TRUE, echo=FALSE, fig.cap='Daily downloads of CRAN packages.', fig.align = 'center'}
render_image("images/intro-daily-cran-downloads.png", "Daily downloads of CRAN packages.")
```

Aside from statistics, R is also used in many other fields, the following ones are particularily relevant to this book:

- **Data Science**: An exciting discipline that allows you to turn raw data into understanding, insight, and knowledge[@intro-r-for-data-science]. In 1997, C.F. Jeff Wu advocated that statistics be renamed data science[@intro-statisticsisdatascience]; however, in 2001, William S. Cleveland introduced data science as an independent discipline extending the field of statistics to incorporate "advances in computing with data"[@intro-datascienceindependent].
- **Machine Learning**: Field of artificial intelligence that uses statistical techniques to give computer systems the ability to "learn" from data. Arthur Samuel coined the term "machine learning" in 1959 while designing programs to play checkers using alpha-beta prunning[@intro-samuel1959some]. Machine learning is employed in computing tasks where designing explicit algorithms is difficult; for example, email filtering, detection of network intruders, and computer vision.
- **Deep Learning**: A field of machine learning where models are vaguely inspired in biological nervous systems. In 2006, Geoffrey E. Hinton overcomed the vanishing-gradient-problem in deep neural networks by pre-training one layer at a time[@intro-hinton2006fast]. In 2012, convolutions and GPUs were introduced by Alex Krizhevsky  making them the best image classification model over ImageNet[@intro-krizhevsky2012imagenet].

While working in any of the previous fields, you might be faced with increasingly large datasets or increasingly complex computations that are slow to execute or at times, even impossible to complete in your personal computer. When faced with this challenge, the following techniques are usually effective:

- **Sampling:** A first approach to try is reduce the amount of data being handled, through sampling. However, data must be sampled properly by applying sound statistical principles. For instance, selecting the top results is not sufficient in order datasets; with simple random sampling, there might be underrepresented groups, which we could overcome with stratified sampling, which in turn adds complexity to properly select categories. It is out of the scope of this book to teach how to properly perform statistical sampling, but many online resources and literature is available on this subject.
- **Profiling:** One can try to understand why a computation is slow and make the necessary improvements. A profiler, is a tool capable of inspecting code execution to help identify bottlenecks. In R, the R profiler, the `profvis` R package[@intro-profvis] and RStudio Profiler feature[@intro-rstudio-profiler], allow you to easily to retrieve and visualize a profile. However, it's not always trivial to optimize code; for instance, the code might reside on a separate package or it might have already be already optimized.
- **Scaling Up:** Speeding up computation is usually possible by buying faster or more capable hardware, say, increasing ones machine memory, hard drive or procuring a machine with many more processors, this approach is known as "scaling up". It can be quite effective to achieve orders of magnitude improvements. However, there is usually a hard limit as to how much a single computer can scale up and even with significant CPUs, one needs to find frameworks that parallelize computation efficiently For the former concern, cloud providers (as explained in the [Clusters](#clusters) chapter) can mitigate this issue by allowing you to rent really capable computers on-demand; the latter is feasible but they are also frameworks supported while scaling out.
- **Scaling Out:** Finally, we can consider spreading computation and storage across multiple machines; this approach provides the highest degree of scalability since one can potentially use an arbitrary number of machines to perform a computation, this approach is commonly known as "scaling out". However, spreading computation effectively across many machines is a complex endeavour, specially without using specialized tools and frameworks. Instead, frameworks like Apache Spark, simplify the complexity of dealing with distributed computing systems while accelerating computation across evergrowing datasets.

This last point brings us closer to the purpose of this book, which is to bring the power of distributed computing systems provided by Apache Spark, to solve meaningful computation problems in Data Science and related fields, using R.

### sparklyr {#intro-sparklyr}

Back in 2016, there was a need in the R community to support Spark through an R package that would provide an interface compatible with other R packages, easy to use and available in CRAN. To this end, development of `sparklyr` started in 2016 by RStudio under [JJ Allaire](https://github.com/jjallaire), [Kevin Ushey](https://github.com/kevinushey) and [Javier Luraschi](https://github.com/javierluraschi), version [0.4 was released](https://blog.rstudio.com/2016/09/27/sparklyr-r-interface-for-apache-spark/) in summer during the _useR!_ conference, this first version added support for `dplyr`, `DBI`, modeling with `MLlib` and an extensible API that enabled extensions like [H2O](https://www.h2o.ai/)'s [rsparkling](https://github.com/h2oai/rsparkling/) package. Since then, many new features and improvements have been made available through [sparklyr 0.5](https://blog.rstudio.com/2017/01/24/sparklyr-0-5/), [0.6](https://blog.rstudio.com/2017/07/31/sparklyr-0-6/), [0.7](https://blog.rstudio.com/2018/01/29/sparklyr-0-7/), [0.8](https://blog.rstudio.com/2018/05/14/sparklyr-0-8/) and [0.9](https://blog.rstudio.com/2018/10/01/sparklyr-0-9/).

Officially,

> `sparklyr` is an R interface for Apache Spark.
>
> --- [github.com/rstudio/sparklyr](https://github.com/rstudio/sparklyr)

It's available in CRAN and works like any other CRAN package, meaning that: it's agnostic to Spark versions, it's easy to install, it serves the R community, it embraces other packages and practices from the R community and so on. It's hosted in GitHub under [github.com/rstudio/sparklyr](https://github.com/rstudio/sparklyr) and licensed under Apache 2.0 which is allows you to clone, modify and [contribute back](#contributing) to this project.

While thinking of who and why should use `sparklyr`, the following roles come to mind:

- **New Users**: For new users, `sparklyr` provides the easiest way to get started with Spark. My hope is that the first chapters of this book will get you up running with ease and set you up for long term success.
- **Data Scientists**: For data scientists that already use and love R, `sparklyr` integrates with many other R practices and packages like `dplyr`, `magrittr`, `broom`, `DBI`, `tibble` and many others that will make you feel at home while working with Spark. For those new to R and Spark, the combination of high-level workflows available in `sparklyr` and low-level extensibility mechanisms make it a productive environment to match the needs and skills of every data scientist.
- **Expert Users**: For those users that are already immersed in Spark and can write code natively in Scala, consider making your libraries available as an `sparklyr` [custom extension](#contributing-r-extension) to the R community, a diverse and skilled community that can put your contributions to good use while moving [open science](https://en.wikipedia.org/wiki/Open_science) forward.

This book is titled "The R in Spark" as a way to describe and teach that area of overlap between Spark and R. The R package that represents this overlap is `sparklyr`; however, the overlap goes beyond a package. It's an overlap of communities, expectations, future directions, packages and package extensions as well. Naming this book `sparklyr` or "Introduction to sparklyr" would have left behind a much more exciting opportunity, an opportunity to present this book as an intersection of the R and Spark communities. Both are solving very similar problems with a set of different skills and backgrounds; therefore, it is my hope that `sparklyr` can be a fertile ground for innovation, a welcoming place to newcomers, a productive place for experienced data scientists and an open community where cluster computing and modeling can come together.

## Getting Started {#starting}

From R, getting started with Spark using `sparklyr` and a local cluster is as easy as running:

```{r intro-connect}
spark_install()
sc <- spark_connect(master = "local")
```

To make sure we can all run the code above and understand it, this section will walk you through the prerequisites, installing `sparklyr` and Spark, connecting to a local Spark cluster and briefly explaining how to use Spark.

However, if a Spark cluster and R environment have been made available to you, you do not need to install the [prerequisites](#intro-prerequisites) nor [install Spark](#intro-installing-spark) yourself. Instead, you should ask for the Spark `master` parameter and connect as follows, this parameter will be formally introduced under the [clusters](#clusters) and [connections](#connections) chapters.

```{r intro-connect-master}
sc <- spark_connect(master = "<cluster-master>")
```

### Prerequisites {#intro-prerequisites}

R can run in many platforms and environments; therfore, whether you use Windows, Mac or Linux, the first step is to install R from the [r-project.org](https://r-project.org/), detailed instructions are provided in the [Installing R](#appendix-install-r) appendix.

Most people use programming languages with tools to make them more productive; for R, RStudio would be such tool. Strictly speaking, RStudio is an Integrated Development Environment or IDE for short, which also happens to support many platforms and environments. I strongly recommend you get RStudio installed if you haven't done so already, see details under the [Installing RStudio](#appendix-install-rstudio) appendix.

Since Spark is build in the Scala programming language which is run by the Java Virtual Machine, you also need to install Java 8 in your system. It is likely that your system already has Java installed, but you should still check the version and update if needed as described in the [Installing Java](#appendix-install-java) appendix.

### Installing sparklyr {#intro-install-sparklyr}

As many other R packages, `sparkylr` is available to be installed from [CRAN](https://CRAN.R-project.org/package=sparklyr) and can be easily installed as follows:

```{r intro-install-sparklyr, eval=FALSE}
install.packages("sparklyr")
```

The CRAN release of `sparklyr` contains the most stable version and it's the recommended version to use; however, to try out features being developed in `sparklyr`, you can install directly from GitHub using the `devtools` package. First, install the `devtools` package and then install `sparklyr` as follows:

```{r intro-install-sparklyr-devel, eval=FALSE}
install.packages("devtools")
devtools::install_github("rstudio/sparklyr")
```

The `sparklyr` GitHub repository contains all the latest features,issues and project updates; it's the place where `sparklyr` is actevely developed and a resource that is helpful while troubleshooting issues.

```{r intro-install-github, eval=TRUE, fig.width=4, fig.align='center', echo=FALSE, fig.cap='GitHub Repository for sparklyr.'}
render_image("images/intro-sparklyr-github.png", "GitHub Repository for sparklyr.")
```

### Installing Spark {#intro-installing-spark}

Start by loading `sparklyr`, 

```{r intro-install-spark-header, warning=FALSE, message=FALSE}
library(sparklyr)
```

This will makes all `sparklyr` functions available in R, which is really helpful; otherwise, we would have to run each `sparklyr` command prefixed with `sparklyr::`.

As mentioned, Spark can be easily installed by running `spark_install()`; this will install the latest version of Spark locally in your computer, go ahead and run `spark_install()`. Notice that this command requires internet connectivity to download Spark.

```{r intro-install-spark}
spark_install()
```

All the versions of Spark that are available for installation can be displayed with `spark_available_versions()`:

```{r intro-install-available, eval=TRUE}
spark_available_versions()
```

A specific version can be installed using the Spark version and, optionally, by also specifying the Hadoop version. For instance, to install Spark 1.6.3, we would run:

```{r intro-install-install-version}
spark_install(version = "1.6.3")
```

You can also check which versions are installed by running:

```{r intro-install-installed}
spark_installed_versions()
```
```
  spark hadoop                              dir
7 2.3.1    2.7 /spark/spark-2.3.1-bin-hadoop2.7
```

The path where Spark is installed is referenced as Spark's home, which is defined in R code and system configuration settings with the `SPARK_HOME` identifier. When using a local Spark cluster installed with `sparklyr`, this path is already known and no additional configuration needs to take place.

Finally, in order to uninstall an specific version of Spark you can run `spark_uninstall()` by specifying the Spark and Hadoop versions, for instance:

```{r intro-install-uninstall}
spark_uninstall(version = "2.3.1", hadoop = "2.7")
```

### Connecting to Spark {#intro-connect-to-spark}

It's important to mention that, so far, we've only installed a local Spark cluster. A local cluster is really helpful to get started, test code and troubleshoot with ease; further chapters will explain where to find, install and connect to real Spark clusters with many machines; but for the first few chapters, we will focus on using local clusters.

Threfore, to connect to this local cluster we simply run:

```{r intro-connect-local}
sc <- spark_connect(master = "local")
```

The `master` parameter identifies which is the "main" machine from the Spark cluster, this machine is often call the driver node. While working with real clusters using many machines, most machines will be worker machines and one will be the master. Since we only have a local cluster with only one machine, we will default to use `"local"` for now.

### Using Spark {#intro-sparklyr-hello-world}

Now that you are connected, we can run a few simple commands. For instance, let's start by loading some data into Apache Spark.

To accomplish this, lets first create a text file by running:

```{r intro-hello-write}
write("Hello World!", "hello.txt")
```

We can now read this text file back from Spark by running:

```{r intro-hello-read}
spark_read_text(sc, "hello", "hello.txt")
```
```
## # Source: spark<hello> [?? x 1]
##   line        
## * <chr>       
## 1 Hello World!
```

**Congrats!** You have successfully connected and loaded your first dataset into Spark.

Let's explain what's going on in `spark_read_text()`. The first parameter, `sc`, gives the function a reference to the active Spark Connection that was earlier created with `spark_connect()`. The second parameter names this dataset in Spark. The third parameter specifies a path to the file to load into Spark. Now, `spark_read_text()` returns a reference to the dataset in Spark which R automatically prints. Whenever a Spark dataset is printed, Spark will **collect** some of the records and display them for you. In this particular case, that dataset contains just one row for the line: `Hello World!`.

We will now use this simple example to present various useful tools in Spark we should get familiar with.

#### Web Interface {#intro-spark-web-interface}

Most of the Spark commands will get started from the R console; however, it is often the case that monitoring and analyzing execution is done through Spark's web interface. This interface is a web page provided by Spark which can be accessed by running:

```{r intro-spark-web}
spark_web(sc)
```

```{r intro-spark-web-shot, echo=FALSE}
invisible(webshot::webshot(
  "http://localhost:4040/",
  "images/intro-spark-web.png",
  cliprect = c(0, 0, 992, 600),
  zoom = 2
))
```
```{r intro-spark-render, eval=TRUE, fig.width=4, fig.align='center', echo=FALSE, fig.cap='Apache Spark Web Interface.'}
render_image("images/intro-spark-web.png", "Apache Spark Web Interface.")
```

As mentioned, printing the "hello" dataset collected a few records to be displayed in the R console. You can see in the Spark web interface that a job was started to collect this information back from Spark. You can also select the **storage** tab to see the "hello" dataset cached in-memory in Spark:

```{r intro-spark-web-storage, echo=FALSE}
invisible(webshot::webshot(
  "http://localhost:4040/storage/",
  "images/intro-spark-web-storage.png",
  cliprect = c(0, 0, 992, 300),
  zoom = 2
))
```
```{r intro-spark-web-storage-render, eval=TRUE, fig.width=4, fig.align='center', echo=FALSE, fig.cap='Apache Spark Web Interface - Storage Tab.'}
render_image("images/intro-spark-web-storage.png", "Apache Spark Web Interface - Storage Tab.")
```

The [caching](#tunning-caching) section in the [tunning](#tunning) chapter will cover this in detail, but as a start, it's worth noticing that this dataset is fully loaded into memory since the **fraction cached** is 100%, it is useful also to point out the **size in memory** column which tracks the total memory being used by this dataset.

#### Logs {#intro-logs}

Another common tool to use in Spark that you should familiarize with are the Spark logs. A log is just a text file where Spark will append information relevant to the execution of tasks in the cluster. For local clusters, we can retrieve all the logs by running:

```{r intro-logs}
spark_log(sc, n = 5)
```
```
18/10/09 19:41:46 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1499 bytes result sent to driver
18/10/09 19:41:46 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 32 ms on localhost (executor driver) (1/1)
18/10/09 19:41:46 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
18/10/09 19:41:46 INFO DAGScheduler: ResultStage 5 (collect at utils.scala:197) finished in 0.039 s
18/10/09 19:41:46 INFO DAGScheduler: Job 3 finished: collect at utils.scala:197, took 0.043086 s
```

Or we can retrieve specific log entries containing, say `sparklyr`, by using the `filter` parameter as follows:

```{r intro-logs-filter}
spark_log(sc, filter = "sparklyr", n = 5)
```
```
## 18/10/09 18:53:23 INFO SparkContext: Submitted application: sparklyr
## 18/10/09 18:53:23 INFO SparkContext: Added JAR file:/Library/Frameworks/R.framework/Versions/3.5/Resources/library/sparklyr/java/sparklyr-2.3-2.11.jar at spark://localhost:52930/jars/sparklyr-2.3-2.11.jar with timestamp 1539136403697
## 18/10/09 18:53:27 INFO Executor: Fetching spark://localhost:52930/jars/sparklyr-2.3-2.11.jar with timestamp 1539136403697
## 18/10/09 18:53:27 INFO Utils: Fetching spark://localhost:52930/jars/sparklyr-2.3-2.11.jar to /private/var/folders/ks/wm_bx4cn70s6h0r5vgqpsldm0000gn/T/spark-141b51a3-f277-4530-aa6a-69be176e0c0b/userFiles-3d94e32b-c65d-4081-a85e-d1e4716e0cef/fetchFileTemp1188493532217239876.tmp
## 18/10/09 18:53:27 INFO Executor: Adding file:/private/var/folders/ks/wm_bx4cn70s6h0r5vgqpsldm0000gn/T/spark-141b51a3-f277-4530-aa6a-69be176e0c0b/userFiles-3d94e32b-c65d-4081-a85e-d1e4716e0cef/sparklyr-2.3-2.11.jar to class loader
```

### Using RStudio {#intro-using-spark-from-rstudio}

Since it's very common to use RStudio with R, `sparklyr` provides RStudio extensions to help simplify your workflows and increase your productivity while using Spark in RStudio. If you are not familiar with RStudio, take a quick look at the [Using RStudio](#appendix-using-rstudio) appendix section. This, to mention that there are a couple 

First, instead of starging a new connections using `spark_connect()` from RStudio's R console, you can use the **new connection** action from the **connections pane** and then, select the Spark connection. You can then customize the versions and connect to Spark which will simply generate the right `spark_connect()` command and execute this in the R console for you.

```{r intro-rstudio-new-connection, eval=TRUE, fig.width=4, fig.align='center', echo=FALSE, fig.cap='RStudio New Spark Connection.'}
render_image("images/intro-rstudio-new-spark-connection.png", "RStudio New Spark Connection.")
```

Second, once connected to Spark, either by using the R console or through RStudio's connections pane, RStudio will display your datasets available in the connections pane. This is a useful way to track your existing datasets and provides an easy way to explore each of them.

```{r intro-rstudio-connections-pane, eval=TRUE, fig.width=4, fig.align='center', echo=FALSE, fig.cap='RStudio Connections Pane.'}
render_image("images/intro-rstudio-connections-pane.png", "RStudio Connections Pane.")
```

Additionally, an active connection provides the following custom actions:

- **Spark UI**: Opens the Spark web interface, a shortcut to `spark_ui(sc)`.
- **Log**: Opens the Spark web logs, a shortcut to `spark_log(sc)`.
- **SQL**: Opens a new SQL query, see `DBI` and SQL support in the data [analysis](#analysis) chapter.
- **Help**: Opens the reference documentation in a new web browser window.
- **Disconnect**: Disconnects from Spark, a shortcut to `spark_disconnect(sc)`.

The rest of this book will use plain R code, it is up to you to execute this code in the R console, RStudio, Jupyter Notebooks or any other tool that support executing R code since, the code provided in this book executes in any R environment.

### Disconnecting {#intro-disconnecting}

For local clusters and really, any cluster; once you are done processing data you should disconnect by running:

```{r intro-disconnect}
spark_disconnect(sc)
```

This will terminate the connection to the cluster but also terminate the cluster tasks as well. If multiple Spark connections are active, or if the conneciton instance `sc` is no longer available, you can also disconnect all your Spark connections by running:

```{r intro-disconnect-all}
spark_disconnect_all()
```

Notice that exiting R, RStudio or restarting your R session will also cause the Spark connection to terminate, which in turn terminates the Spark cluster and cached data that is not explicitly persisted.

### Resources {#intro-resources}

While we've put significant effort to simplify the onboarding process, it is now a good time to give you additional resources that can help you troubleshoot particular issues while getting started and, in general, introduce you to the broader Spark and R communities to help you get specific answers, discuss topics and get connected with many users actevely using Spark with R. We hope you will find the following resources helpful:

- **Documentation**: This should be your entry point to learn more about Spark when using R, the documentation is kept up to date with examples, reference functions and many more relevant resources, [spark.rstudio.com](https://spark.rstudio.com).
- **Blog**: To keep up to date with major `sparklyr` announcements, you can follow the RStudio blog, [blog.rstudio.com/tags/sparklyr](https://blog.rstudio.com/tags/sparklyr).
- **Community**: For general `sparklyr` questions, you can post then in the RStudio Community tagged as `sparklyr`, [community.rstudio.com/tags/sparklyr](https://community.rstudio.com/tags/sparklyr).
- **Stack Overflow**: For general Spark questions, Stack Overflow is the good start, [stackoverflow.com/questions/tagged/apache-spark](https://stackoverflow.com/questions/tagged/apache-spark); there are also many topics specifically about `sparklyr`, [stackoverflow.com/questions/tagged/sparklyr](https://stackoverflow.com/questions/tagged/sparklyr).
- **Github**: If you believe something needs to get fixed, open a GitHub issue or send us a pull request, [github.com/rstudio/sparklyr](https://github.com/rstudio/sparklyr).
- **Gitter**: For urgent issues or to keep in touch you can chat with us in Gitter, [gitter.im/rstudio/sparklyr](https://gitter.im/rstudio/sparklyr).

## Recap {#intro-recap}

This chapter presented Spark, R, `sparklyr` and walked you through installing R, Java, RStudio and `sparklyr` as the main tools required to use Spark from R. We covered installing local Spark clusters using `spark_install()`, connecting using `spark_connect()` and learned how to launch the web interface and logs using `spark_web(sc)` and `spark_log(sc)` respectively.

It is my hope that this chapter will help anyone interested in learning cluster computing using Spark and R getting started, ready to experiment on your own and ready to tackle actual data analysis and modeling tasks without any major blockers.
