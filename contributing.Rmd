# Contributing {#contributing}

There are many ways to contribute, from helping community members to opening GitHub issues, to providing new functionality for yourself, colleagues or the R and Spark community; this last chapter will focus on writting and sharing code that extends Spark in many useful and probably also, awesome, ways. Specifically, in this chapter you will learn what an extension is, when to build one, what tools are available, how to build an extension and when to consider contributing to `sparklyr` itself.

## Overview

While working with R and therefore, while working with R and Spark, you will write R code. In fact, you have already written R code throught most of the previous chaters in this book. Writting code can be as simple as [loading data from a text file](#starting-sparklyr-hello-world) to writting [distributed R](distributed) code. But for the sake of the argument, lets consider one of the first lines of code presented in this book:

```{r eval=FALSE}
spark_read_text(sc, "hello", "hello.txt")
```

When thinking of contributing back, the most important question you can ask about the code above, but really, about any piece of code you write is: Would this code be useful to someone else? For the code above, the answer is probably no, it's just too generic and can be easiy found online; however, a more realistic example would be to tailor something the code above for something that you actually care about, perhaps:

```{r eval=FALSE}
spark_read_text(sc, "stuff-that-matters", "/secret/path/which/was/hard/to/figure/out/")
```

The code above is quite similar to the original one, but assuming that you work with colleages, the answer to: Would this code be useful to someone else? Is now completely different: Yes, most likely! Which is surprising since this means that not all useful code needs to be very advanced or complicated; however, for it to be useful to others, it does need to be packaged, presented and shared in a usable format.

One first attempt would be to wrap this into a file `useful.R` and then write a function over it, as in:

```{r eval=FALSE}
load_useful_data <- function() {
  spark_read_text(sc, "stuff-that-matters", "/secret/path/which/was/hard/to/figure/out/")
}
```

Which is an improvement but it would require users to manually share this file over and over. Fortunately, this is a problem already solved in R quite well through **R Packages**.

An **R package** contains R code packaged in a format installable using the `install.packages()` function. `sparklyr` is an R package, but there are many other packages available in R and you can also create your own packages. For those of you new to creating R packages, I would encourage reading Hadley Wickam's book on packages: [R Packages: Organize, Test, Document, and Share Your Code](@wickham2015r). Creating an R package allows you to easily share your functions with others by sharing the package file in your organization.

Once a package is created, there are many ways to share this with colleagues or the world. For instance, for packages meant to be private, you can consider using [Drat](https://cran.r-project.org/web/packages/drat/vignettes/WhyDrat.html) or products like [RStudio Package Manager](https://www.rstudio.com/products/package-manager/). R packages meant for public consumption are made available to the R community in [**CRAN**](https://cran.r-project.org/), which stands for the Comprehensive R Archive Network.

These repositories of R packages make packages allow users to install packages through `install.packages("usefulness")` without having to worry where to download the package from and allows other packages to reuse your package in their packages as well.

While this was a very brief introduction to R packages in the context of Spark, it should be more or less obvious that you should be thinking of writting R packages while extending Spark from R. The rest of this chapter will present the tools and concepts require to extend functionality in `sparklyr`. There are three different ways in which `sparklyr` extensions can be written:

- **[R Extensions]**: These extensions make use of only R code and are the easiest one to get started with.
- **[Scala Extensions]**: These extensions make use of R code but also Scala code to get access to all the functionality available in Spark.
- **[Spark Extensions]**: These extensions make use of R code, Scala code and also Spark extensions on their own and while they could be seen as the most complex of all, they are also some of the most useful extensions we can write.

Then we can wrap those extensions into an [R Package] or consider the functionality to be added back into [sparklyr](contributing-sparklyr).

## R Extensions {#contributing-r-extension}

R extensions make use of three functions in `sparklyr`: `invoke_new()`, `invoke_static()` and `invoke()`. For the most part, that's all you need to extend Spark's functionality in R.

```{r eval=FALSE}
spark_context(sc) %>% 
  invoke("textFile", "my-file.txt", 1L) %>% 
  invoke("count")
```

## Scala Extensions

### Prerequisites {#scala-extension-prereq}

Changes in the scala sources require the Scala compilers to be installed. You can install the required compilers by running:

```{r eval=FALSE}
library(sparklyr)
download_scalac()
```

Which will download the correct compilers from [https://www.scala-lang.org/](https://www.scala-lang.org).

## Spark Extensions

## R Packages

### RStudio Projects

You can create an `sparklyr` extension with ease from RStudio. This feature requires RStudio 1.1 or newer and the `sparklyr` package to be installed. Then, from the `File` menu, `New Project...`, select `R Packag using sparklyr`:

![](images/09-extensions-rstudio-project.png)

### Troubleshooting

We can trace all the calls made to `invoke()`, `invoke_new()` and `invoke_static()` using the `sparklyr.invoke.trace` and `sparklyr.invoke.trace.callstack` options as follows:

```{r eval=FALSE}
config <- spark_config()
config$sparklyr.invoke.trace <- TRUE

spark_connect(master = "local", config = config)
```

## sparklyr {#contributing-sparklyr}

First of all, it's worth mentioning that, `sparklyr` is just another R package; a package which contains R code packaged in a format installable using the `install.packages()` function. Now, since Spark was built in the Scala programming language, `sparklyr` also contains Scala code to provide the functionality required to interoperate with Spark efficiently.

[CRAN](https://cran.r-project.org/), which stands for the Comprehensive R Archive Network, provides a repository of packages that can be easily installable and which are carefully reviewed before they are made available. You can read more about the release process under [releasing a package](http://r-pkgs.had.co.nz/release.html) and Hadley Wickam's book on packages: [R Packages: Organize, Test, Document, and Share Your Code](@wickham2015r). CRAN users are always encouraged to install the latest version of a package, which means that `sparklyr` also needs to support multiple versions of Spark.

Therefore, at a high level, `sparklyr` is composed of: R code and versioned Scala code:

```{r echo=FALSE, out.width='100%', out.height='220pt', fig.cap='Sparklyr Package Architecture', fig.align = 'center'}
nomnoml::nomnoml("
#spacing: 15
[sparklyr|
  [R Code]
  [Scala Code|
    [Spark 1.6.x]
    [Spark 2.x.0]
  ]
]
")
```

The `sparklyr` sources are built with `R CMD build` or from the RStudio's build menu, this topic extensevely covered in the [R Packages](@wickham2015r) book.

The Scala code is compiled into [JAVA Archive](https://docs.oracle.com/javase/8/docs/technotes/guides/jar/jarGuide.html) files which will eventually be executed by Spark through the [Java Virtual Machine](https://en.wikipedia.org/wiki/Java_virtual_machine). Compilation can be manually performed with the Scala compiler using `compile_package_jars()`, a `sparklyr` function that invokes the Scala compiler over a set of supported versions.

When connecting to Spark using `spark_connect()`, `sparklyr` submits the correct version of the `sparklyr` JAR to Spark which then Spark executes. We will refer to this submitted application as the **sparklyr backend** and the R code as the **sparklyr frontend**. [Fontend and backend](https://en.wikipedia.org/wiki/Front_and_back_ends) are common software engineer concepts related to separating the user interface, the R console in this case, with the data layer, Spark data proessing in this context. For most [connections], the backend is usually submitted by `sparklyr` to Spark using `spark-submit`, which is a command line tool well-known in Spark; however, for others connections, like Livy, the backend is submitted through Livy's HTTP interface.

Once the backend is submitted to Spark, the frontend communicates to the backend using socket connections, expect for Livy connections where this happens over HTTP:

```{r echo=FALSE, out.width='100%', out.height='220pt', fig.cap='Sparklyr Connection Architecture', fig.align = 'center'}
nomnoml::nomnoml("
#spacing: 15
[R|
  [sparklyr|
    [frontend]
  ]
]
[Spark|
  [sparklyr|
    [backend]
  ]
]
#.hidden: visual=hidden
[<hidden> Socket / HTTP Connection]
[R]-[Socket / HTTP Connection]
[Socket / HTTP Connection]-[Spark]
")
```

### Compiling

To compile `sparklyr`, make sure the prerequisites described in the [Scala Extensions Prerequisites](#scala-extension-prereq) section are fullfilled.  Then you can recompile all the jars by running `configure.R` in the root of the `sparklyr` sources. Once the jars are compiled, you can build the R package as described in the [R Extensions](#r-extension) section.

### Serialization

### Invocations

### R Packages

(dbi, dplyr, broom, etc)

### Connections

### Distributed R

## Recap

