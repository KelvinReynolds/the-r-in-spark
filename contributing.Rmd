```{r include=FALSE}
knitr::opts_chunk$set(eval = FALSE)
```

# Contributing {#contributing}

There are many ways to contribute, from helping community members to opening GitHub issues, to providing new functionality for yourself, colleagues or the R and Spark community; this last chapter will focus on writting and sharing code that extends Spark in many useful and probably also, awesome, ways. Specifically, in this chapter you will learn what an extension is, when to build one, what tools are available, how to build an extension and when to consider contributing to `sparklyr` itself.

## Overview

While working with R and therefore, while working with R and Spark, you will write R code. In fact, you have already written R code throught most of the previous chaters in this book. Writting code can be as simple as [loading data from a text file](#starting-sparklyr-hello-world) to writting [distributed R](distributed) code. But for the sake of the argument, lets consider one of the first lines of code presented in this book:

```{r contributing-read}
spark_read_text(sc, "hello", "hello.txt")
```

When thinking of contributing back, the most important question you can ask about the code above, but really, about any piece of code you write is: Would this code be useful to someone else? For the code above, the answer is probably no, it's just too generic and can be easiy found online; however, a more realistic example would be to tailor something the code above for something that you actually care about, perhaps:

```{r contributing-read-useful}
spark_read_text(sc, "stuff-that-matters", "/secret/path/which/was/hard/to/figure/out/")
```

The code above is quite similar to the original one, but assuming that you work with colleages, the answer to: Would this code be useful to someone else? Is now completely different: Yes, most likely! Which is surprising since this means that not all useful code needs to be very advanced or complicated; however, for it to be useful to others, it does need to be packaged, presented and shared in a usable format.

One first attempt would be to wrap this into a file `useful.R` and then write a function over it, as in:

```{r contributing-useful-function}
load_useful_data <- function() {
  spark_read_text(sc, "stuff-that-matters", "/secret/path/which/was/hard/to/figure/out/")
}
```

Which is an improvement but it would require users to manually share this file over and over. Fortunately, this is a problem already solved in R quite well through **R Packages**.

An **R package** contains R code packaged in a format installable using the `install.packages()` function. `sparklyr` is an R package, but there are many other packages available in R and you can also create your own packages. For those of you new to creating R packages, I would encourage reading Hadley Wickam's book on packages: [R Packages: Organize, Test, Document, and Share Your Code](@wickham2015r). Creating an R package allows you to easily share your functions with others by sharing the package file in your organization.

Once a package is created, there are many ways to share this with colleagues or the world. For instance, for packages meant to be private, you can consider using [Drat](https://cran.r-project.org/web/packages/drat/vignettes/WhyDrat.html) or products like [RStudio Package Manager](https://www.rstudio.com/products/package-manager/). R packages meant for public consumption are made available to the R community in [**CRAN**](https://cran.r-project.org/), which stands for the Comprehensive R Archive Network.

These repositories of R packages make packages allow users to install packages through `install.packages("usefulness")` without having to worry where to download the package from and allows other packages to reuse your package in their packages as well.

While this was a very brief introduction to R packages in the context of Spark, it should be more or less obvious that you should be thinking of writting R packages while extending Spark from R. The rest of this chapter will present the tools and concepts require to extend functionality in `sparklyr`. There are three different ways in which `sparklyr` extensions can be written:

- **[R Extensions]**: These extensions make use of only R code and are the easiest one to get started with.
- **[Scala Extensions]**: These extensions make use of R code but also Scala code to get access to all the functionality available in Spark.
- **[Spark Extensions]**: These extensions make use of R code, Scala code and also Spark extensions on their own and while they could be seen as the most complex of all, they are also some of the most useful extensions we can write.

Then we can wrap those extensions into an [R Package] or consider the functionality to be added back into [sparklyr](contributing-sparklyr).

## R Extensions {#contributing-r-extension}

R extensions make use of three functions in `sparklyr`: `invoke_new()`, `invoke_static()` and `invoke()`. For the most part, that's all you need to extend Spark's functionality in R.

```{r contributing-invoke}
spark_context(sc) %>% 
  invoke("textFile", "my-file.txt", 1L) %>% 
  invoke("count")
```

### Troubleshooting

We can trace all the calls made to `invoke()`, `invoke_new()` and `invoke_static()` using the `sparklyr.invoke.trace` and `sparklyr.invoke.trace.callstack` options as follows:

```{r contributing-trace}
config <- spark_config()
config$sparklyr.invoke.trace <- TRUE

spark_connect(master = "local", config = config)
```

## Scala Extensions

### R Packages

#### RStudio Projects

You can create with ease an Scala extension for Spark from RStudio. This feature requires RStudio 1.1 or newer and the `sparklyr` package to be installed. Then, from the `File` menu, `New Project...`, select `R Package using sparklyr`:

```{r contributing-r-rstudio-project, eval=TRUE, fig.align='center', echo=FALSE, fig.cap='Creating an Scala extension package from RStudio'}
render_image("images/contributing-r-rstudio-project.png")
```

### Prerequisites {#scala-extension-prereq}

Changes in the scala sources require the Scala compilers to be installed. You can install the required compilers by running:

```{r contributing-download-scalac}
library(sparklyr)
download_scalac()
```

Which will download the correct compilers from [https://www.scala-lang.org/](https://www.scala-lang.org).

## Spark Extensions

## Recap

