```{r include=FALSE}
knitr::opts_chunk$set(eval = FALSE)
source("r/render.R")
source("r/plots.R")
library(ggplot2)
```

# Extensions {#extensions}



H2O Models, Graphs (PageRank), XGBoost, Spatial, Deep Learning, Genomics, 

## H2O

[H2O](https://www.h2o.ai/) is open-source software for large-scale modeling created by H2O.ai, it allows users to fit thousands of potential models as part of discovering patterns in data. Many users that require advanced modeling capabilities in Spark consider H2O to complement or replace Spark's default modeling algorithms. An approach we recommend is to start with Spark's default modeling algorithms and transition to H2O running in Spark when Spark's algorithms fall short or when advanced functionality (like additional modeling metrics) are required.

However, we can't do justice to H2O's great modeling capabilities in a simgle paragraph since explaining H2O properly will require a book in itself. Instead, we would like to recommend reading the "Practical machine learning with H2O" [@extensions-practical-h2o] book to explore in-depth H2O's modeling algorithms and features and present this chapter as a brief guide to get started using H2O in Spark with R through the [rsparkling](https://github.com/h2oai/sparkling-water/tree/master/r) extension.

In order to use H2O with Spark, it is important to know that there are four compoinents involved: H2O, Sparkling Water, `rsparkling` and Spark. Sparkling Water allows users to combine the fast, scalable machine learning algorithms of H2O with the capabilities of Spark. You can think of Sparkling Water as a component bridging Spark with H2O and `rsparkling` as the R front-end for Sparkling Water.

```{r extensions-h2o-diagram, eval=TRUE}
render_nomnoml("
#spacing: 20
#padding: 16
[R | 
  [rsparkling]
  [sparklyr]
]->[Spark |
  [Sparkling Water]
  [H2O]
]
", "images/extensions-h2o-diagram.png")
```

First, install `rsparkling`:

```{r}
install.packages("rsparkling")
```

Is is then important to notice that you need to use compatible versions of Spark, Sparkling Water and H2O. So let's start by checking the version of H2O by running,

```{r eval=TRUE}
packageVersion("h2o")
```

Then we can explore the Spark and Sparkling Water versions that are compatible with `h2o_release_table()`.

```{r}
library(rsparkling)

h2o_release_table() %>%
  dplyr::filter(H2O_Version == !!as.character(packageVersion("h2o")))
```

We can then connect with the supported Spark versions as follows, you will have to adjust the `master` parameter for your particular cluster. We also recommend increasing the connection timeout since Spark might require to download various H2O components while the connection is established for the first time:

```{r}
library(rsparkling)
library(sparklyr)
library(h2o)

config <- spark_config()
config$sparklyr.connect.timeout <- 3 * 60
  
sc <- spark_connect(master = "local", version = "2.3", config = config)
cars <- copy_to(sc, mtcars)
```

H2O provides a web interface which can help you monitor training and access much of H2O's functionality. The web interface can be accessed through `h2o_flow(sc)`, it is reffered to as H2O Flow and is shown in Figure \@ref(fig:extensions-h2o-flow).

```{r extensions-h2o-flow, eval=TRUE, fig.width=4, fig.align='center', echo=FALSE, fig.cap='H2O Flow Interface using Spark with R'}
render_image("images/extensions-h2o-flow.png")
```

When using H2O, you will have to convert your Spark DataFrame into and H2O DataFrame through `as_h2o_frame`:

```{r}
cars_h2o <- as_h2o_frame(sc, cars)
cars_h2o
```
```
   mpg cyl disp  hp drat    wt  qsec vs am gear carb
1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
6 18.1   6  225 105 2.76 3.460 20.22  1  0    3    1

[32 rows x 11 columns] 
```

Then you can use many of the modeling functions available in the `h2o` package with ease. For instance, we can fir a generalized linear model with ease:

```{r eval=FALSEz}
model <- h2o.glm(x = c("wt", "cyl"), y = "mpg", training_frame = cars_h2o, lambda_search = TRUE)
```
```{r extensions-rsparkling, echo=FALSE}
saveRDS(model, "data/09-extensions-h2o-glm.rds")
```

H2O provides additional metrics not necessarily available in Spark's modeling algorithms, the model that we just fit  `Residual Deviance` is provided in the model while this would not be a standard metric when using Spark MLlib:

```{r}
model
```
```
Model Details:
==============

H2ORegressionModel: glm
Model ID:  GLM_model_R_1533086487173_1 
GLM Model: summary
    family     link                              regularization
1 gaussian identity Elastic Net (alpha = 0.5, lambda = 0.1013 )
                                                               lambda_search
1 nlambda = 100, lambda.max = 10.132, lambda.min = 0.1013, lambda.1se = -1.0
  number_of_predictors_total number_of_active_predictors number_of_iterations
1                          2                           2                  100
                                 training_frame
1 frame_rdd_33_a539369727fb5223dbccfbc5b7894962

Coefficients: glm coefficients
      names coefficients standardized_coefficients
1 Intercept    38.941654                 20.090625
2       cyl    -1.468783                 -2.623132
3        wt    -3.034558                 -2.969186

H2ORegressionMetrics: glm
** Reported on training data. **

MSE:  6.017684
RMSE:  2.453097
MAE:  1.940985
RMSLE:  0.1114801
Mean Residual Deviance :  6.017684
R^2 :  0.8289895
Null Deviance :1126.047
Null D.o.F. :31
Residual Deviance :192.5659
Residual D.o.F. :29
AIC :156.2425
```

Finally, we can run prediction over the generalized linear model model, a similar approach would work for many other models available in H2O:

```{r}
predictions <- as_h2o_frame(sc, copy_to(sc, data.frame(wt = 2, cyl = 6)))
h2o.predict(model, predictions)
```
```
   predict
1 24.05984

[1 row x 1 column]
```

We should then disconnect since using different extensions require us to reconnect to Spark.

```{r}
spark_disconnect(sc)
```

Many additional examples are available under [spark.rstudio.com/guides/h2o](http://spark.rstudio.com/guides/h2o/), you can also request help from [github.com/h2oai/sparkling-water/tree/master/r](https://github.com/h2oai/sparkling-water/tree/master/r), the official GitHub repository for the `rsparkling` package. 

## Graphs

[GraphFrames](https://graphframes.github.io/) provides graph algorithms: PageRank, ShortestPaths, etc.

```{r extensions-graphframes, echo=FALSE}
library(ggraph)
library(igraph)
library(graphframes)
library(sparklyr)
library(dplyr)

sc <- spark_connect(master = "local", version = "2.1.0")
highschool_tbl <- copy_to(sc, ggraph::highschool, "highschool", overwrite = TRUE)
highschool_tbl <- highschool_tbl %>% filter(year == 1957)

from_tbl <- highschool_tbl %>% distinct(from) %>% transmute(id = from)
to_tbl <- highschool_tbl %>% distinct(to) %>% transmute(id = to)

vertices_tbl <- from_tbl %>% sdf_bind_rows(to_tbl)
edges_tbl <- highschool_tbl %>% transmute(src = from, dst = to)

model <- gf_graphframe(vertices_tbl, edges_tbl) %>%
  gf_pagerank(reset_prob = 0.15, max_iter = 10L)

highschool_tbl %>% collect() %>%
  saveRDS("data/09-extensions-graphframes-highschool.rds")
```
```{r extensions-graphframes-code}
gf_graphframe(vertices_tbl, edges_tbl) %>% gf_pagerank(reset_prob = 0.15, max_iter = 10L)
```
```
GraphFrame
Vertices:
  $ id       <dbl> 12, 12, 59, 59, 1, 20, 20, 45, 45, 8, 8, 9, 9, 26, 26, 37, 37, 47, 47, 16, 16, 71, 71, ...
  $ pagerank <dbl> 0.0058199702, 0.0058199702, 0.0000000000, 0.0000000000, 0.1500000000, 0.0344953402, 0.0...
Edges:
  $ src    <dbl> 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 58, 58, 58, 58, 58, 58, 5...
  $ dst    <dbl> 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 65, 65, 65, 65, 65, 65, 6...
  $ weight <dbl> 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0...
```
```{r extensions-graphframes-chart, eval=TRUE, echo=FALSE, warning=FALSE, message=FALSE, fig.cap='Highschool ggraph dataset with pagerank highlighted', fig.align='center'}
library(ggraph)
library(igraph)
highschool_rdf <- readRDS("data/09-extensions-graphframes-highschool.rds")
highschool_rdf %>% graph_from_data_frame() %>%
  ggraph(layout = 'kk') + 
    geom_edge_link(alpha = 0.1) + 
    geom_node_point(size = 2, alpha = 0.4) + theme_light() +
    annotate("point", x = -1.18, y = -3.55, size = 3) +
    annotate("point", x = 6.25, y = 2.85, size = 3) + xlab("") + ylab("")
```

```{r}
spark_disconnect(sc)
```

See also [spark.rstudio.com/graphframes](http://spark.rstudio.com/graphframes/).

## XGBoost

sparkxgb is a new sparklyr extension that can be used to train XGBoost models in Spark and is installed as follows:

```{r}
install.packages("sparkxgb")
```

We can then use xgboost_classifier() to train and ml_predict() to predict over large datasets with ease:

```{r}
library(sparkxgb)
library(sparklyr)
library(dplyr)

sc <- spark_connect(master = "local")
iris <- copy_to(sc, iris)

xgb_model <- xgboost_classifier(iris,
                                Species ~ .,
                                num_class = 3,
                                num_round = 50,
                                max_depth = 4)

xgb_model %>%
  ml_predict(iris) %>%
  select(Species, predicted_label, starts_with("probability_")) %>%
  glimpse()

spark_disconnect(sc)
```
```
#> Observations: ??
#> Variables: 5
#> Database: spark_connection
#> $ Species                <chr> "setosa", "setosa", "setosa", "setosa", "…
#> $ predicted_label        <chr> "setosa", "setosa", "setosa", "setosa", "…
#> $ probability_versicolor <dbl> 0.003566429, 0.003564076, 0.003566429, 0.…
#> $ probability_virginica  <dbl> 0.001423170, 0.002082058, 0.001423170, 0.…
#> $ probability_setosa     <dbl> 0.9950104, 0.9943539, 0.9950104, 0.995010…
```

## Deep Learning

sparktf is a new sparklyr extension allowing you to write TensorFlow records in Spark. This can be used to preprocess large amounts of data before processing them in GPU instances with Keras or TensorFlow. sparktf is now available on CRAN and can be installed as follows:

```{r}
install.packages("sparktf")
```

You can simply preprocess data in Spark and write it as TensorFlow records using spark_write_tf():

```{r}
library(sparktf)
library(sparklyr)

sc <- spark_connect(master = "local")

copy_to(sc, iris) %>%
  ft_string_indexer_model(
    "Species", "label",
    labels = c("setosa", "versicolor", "virginica")
  ) %>%
  spark_write_tfrecord(path = "tfrecord")

spark_disconnect(sc)
```

## Genomics

VariantSpark is a framework based on scala and spark to analyze genome datasets. It is being developed by CSIRO Bioinformatics team in Australia. VariantSpark was tested on datasets with 3000 samples each one containing 80 million features in either unsupervised clustering approaches and supervised applications, like classification and regression.

The genome datasets are usually writing in Variant Call Format (VCF), a specific text file format used in bioinformatics for storing gene sequence variations. So, VariantSaprk is a great tool because it is able to read VCF files, run analyses and give us the output in a spark data frame.

```{r}
install.packages("sparktf")
```

```{r}
library(sparklyr)
library(variantspark)

sc <- spark_connect(master = "local")
vsc <- vs_connect(sc)
```

We can start by loading a VCF file,

```
hipster_vcf <- vs_read_vcf(vsc, "inst/extdata/hipster.vcf.bz2")
```

VariantSpark uses Random Forest to assign an "Importance" score to each tested variant reflecting its association to the interest phenotype. A variant with higher "Importance" score implies it is more strongly associated with the phenotype of interest.

```{r}
# calculate the "Importance"
importance <- vs_importance_analysis(vsc, hipster_vcf, labels, n_trees = 100)

# transform the output in a tibble spark
importance_tbl <- importance_tbl(importance) 
```

You can use dplyr and ggplot2 to transform the output and plot!

```{r}
importance_df <- importance_tbl %>% 
  arrange(-importance) %>% 
  head(20) %>% 
  collect()

# importance barplot
ggplot(importance_df) +
  aes(x = variable, y = importance) + 
  geom_bar(stat = 'identity') +          
  scale_x_discrete(limits = importance_df[order(importance_df$importance), 1]$variable) + 
  coord_flip()
```

## Spatial 

```{r}
library(sparklyr)
library(geospark)

conf <- spark_config()
sc <- spark_connect(master = "local", config = conf)
register_gis(sc)
```

Next we will load some spatial dataset containing as polygons and points.

```{r}
polygons <- read.table(system.file(package="geospark","examples/polygons.txt"), sep="|", col.names=c("area","geom"))
points <- read.table(system.file(package="geospark","examples/points.txt"), sep="|", col.names=c("city","state","geom"))

polygons_wkt <- copy_to(sc, polygons)
points_wkt <- copy_to(sc, points)
```

```{r}
library(dplyr)
polygons_wkt <- mutate(polygons_wkt, y = st_geomfromwkt(geom))
points_wkt <- mutate(points_wkt, x = st_geomfromwkt(geom))

sc_res <- st_join(polygons_wkt,
                  points_wkt,
                  join = sql("st_contains(y,x)")) %>% 
  group_by(area, state) %>%
  summarise(cnt = n()) 
  
sc_res %>%
  head()
```
```
    # Source: spark<?> [?? x 3]
    # Groups: area
      area            state   cnt
      <chr>           <chr> <dbl>
    1 texas area      TX       10
    2 dakota area     SD        1
    3 dakota area     ND       10
    4 california area CA       10
    5 new york area   NY        9
```

The final result can be present by leaflet, you would need to install the `leaflet` and `colormap` package for the following example to work:

```{r}
Idx_df = collect(sc_res) %>% 
right_join(polygons,by = (c("area"="area"))) %>% 
sf::st_as_sf(wkt="geom")

Idx_df %>% 
leaflet::leaflet() %>% 
leaflet::addTiles() %>% 
leaflet::addPolygons(popup = ~as.character(cnt),color=~colormap::colormap_pal()(cnt))
```

## Troubleshooting

[Apache IVY](http://ant.apache.org/ivy/) is a popular dependency manager focusing on flexibility and simplicity, which happens to be used by Apache Spark while installing extensions. When connection fails while using `rsparkling`, consider clearing your [IVY Cache](http://ant.apache.org/ivy/history/2.0.0/settings/caches.html) by running:

```{r extensions-rsparkling-cache}
unlink("~/.ivy2/cache", recursive = TRUE)
```

## Recap

## Geospatial Data {#extensions-geospatial-data}

[geospark](https://github.com/harryprince/geospark) enables distributed geospatial computing with spatial index on spark in production and keeps a [`dplyr`](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf) and [`sf`](https://github.com/rstudio/cheatsheets/raw/master/sf.pdf) style grammar.

In this example we will join spatial data using `quadrad tree indexing`. First, we will initialize the geospark extension and connect to Spark using sparklyr:

```{r extensions-geospatial}
pak::pkg_install("harryprince/geospark")

library(sparklyr)
library(geospark)

conf <- spark_config()
sc <- spark_connect(master = "local", config = conf)
register_gis(sc)
```

After enable Spark GIS mode, now we can use `st_join` to join spatial data using `quadrad tree indexing`.

```{r}
points_wkt <- st_example(sc, "points") %>%
  select(city, state, geom_x = geom)
polygons_wkt <- st_example(sc, "polygons") %>%
  mutate(area,geom_y = geom)

sc_res <- st_join(points_wkt,
                  polygons_wkt,
                  join = sql("st_contains(geom_y,geom_x)")) %>% 
           group_by(area, state) %>%
            summarise(cnt = n()) 

```

```
   # Source: spark<?> [?? x 3]
    # Groups: area
      area            state   cnt
      <chr>           <chr> <dbl>
    1 texas area      TX       10
    2 dakota area     SD        1
    3 dakota area     ND       10
    4 california area CA       10
    5 new york area   NY        9 
```


see more [spatial join](https://github.com/harryprince/geospark#spatial-join):

![](https://camo.githubusercontent.com/f18513c8002df02bdb6e3aac451519beb3c87ebb/68747470733a2f2f7365676d656e746661756c742e636f6d2f696d672f625662714665333f773d3132383026683d353038)

