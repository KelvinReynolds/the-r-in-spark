```{r include=FALSE}
knitr::opts_chunk$set(eval = FALSE)
source("r/render.R")
source("r/plots.R")
library(ggplot2)
```

# Extensions {#extensions}

In the previous chapter, [Tuning](tuning), you learned how Spark processes data at large-scale by allowing users to configure the cluster resources, partition data implicitly or explicitly, execute commands across distributed compute nodes, shuffle data across them when needed, cache data to improve performance and serialize data efficiently over the network. You also learned how to configure the different Spark settings used while connecting, submitting a job, running and application and particular settings applicable only to R and R extensions that we will present in this chapter.

The [Analysis](#analysis), [Modeling](#modeling) and [Data](#data) chapters provided a foundation to read and understand most datasets. However, the functionality that was presented was scoped to Spark's built-in features and tabular datasets. This chapter will go beyond tabular data and explore how to analyze and model networks of interconnected objects through graph processing, read genomics datasets, prepare data for deep learning, analyze geographic datasets and use advanced modeling libraries like H2O and XGBoost over large-scale datasets.

The combination of all the content presented in all the previous chapters should take care of most of your large-scale computing needs. However, for those few use cases where functionality is still lacking, the following chapters will teach you provide the tools to extend Spark yourself; either, through custom R transformation, custom Scala code or through recent new execution mode in Spark that enable analyzing realtime datasets. Although, before reinventing the wheel, we will present all the extensions available when using Spark with R.

## Overview

In the [Introduction](#intro) chapter we presented the R community as a vibrant group of individuals collaborating with each other in many ways, one of them, by moving open science forward by creating R packages that can be installed from CRAN. In a similar way, but in a much smaller scale, the R community has contributed extensions that increase the functionality initially supported in Spark and R. Spark itself also provides support for creating Spark extensions and, in-fact, many R extensions make use of Spark extensions.

Extensions are constantly being created so this section will be outdated at any given point in time, in addition, we might not be even aware of many Spark and R extensions; however, at the very least we can track the extensions that are available in CRAN by looking at the "reverse imports" for `sparklyr` in CRAN [@extensions-sparklyr-cran]. Extensions and R packages published in CRAN tend to be the most stable since when a package is published in CRAN, it will go through a review process which increases the overall quality of a contribution.

While we wish we could present all the extensions, we've picked a few that we believe should be interesting to most readers and which we will present next. You can find a few more under the [github.com/r-spark](https://github.com/r-spark) organization or by searching repos in GitHub with the `sparklyr` tag.

rsparkling
: The `rsparkling` extensions allows you to use H2O and Spark from R. This extension is what we would consider advanced modeling in Spark. While Spark's built-in modeling library, Spark MLlib, is quite useful in many cases; H2O's modeling capabilities can compute additional statistical metrics and can proivide performance and scalability improvements over Spark MLlib. We, ourselves, have not performed detailed comparisons nor benchamarks between MLlib and H2O; so this is something you will have to research on your own to create a complete picture of when to use H2O's capabilities.

graphframes
: The `graphframes` extensions adds support to process graphs in Spark. A graph is a structure that describes a set of objects in which some pairs of the objects are in some sense related. As you learned in the Introduction chapter, ranking web pages was an early motivation to develop precursos to Spark powered by MapReduce; web pages happen to form a graph if you consider a link between pages as the relationship between each pair of pages. Computing operations likes PageRank over graphs can be quite useful in web search and social networks to mention a few applications.

sparktf
: The `sparktf` extension provides support to write TensorFlow records in Spark. TensorFlow is one of the leading deep learning frameworks and it is often used with large amounts of numerical data represented as TensorFlow records, a file format optimized for TensorFlow. Spark it is often used to process unstructured and large-scale datasets into smaller numerical datasets that can easily fit into a GPU. You can use this extension to save datasets in the TensorFLow record file format.

xgboost
: The `xgboost` extension brings the well-known XGBoost modeling library to the world of large-scale computing. XGBoost is a scalable, portable and distributed library for gradient boosting. It became well known in the machine learning competition circles after its use in the winning solution of the Higgs Machine Learning Challenge [@extensions-higgs-challenge] and has remain popular in other Kaggle competitions since then.

variantspark
: The `variantspark` extension provides an interface to use Variant Spark, a scalable toolkit for genome-wide association studies (GWAS). It currently provides functionality to build random forest models, estimating variable importance and reading Variant Call Format (VCF) files. While there are other random forest implementations in Spark, most of them are not optimized to deal with GWAS datasets, which usually come with thousands of samples and millions of variables.

geospark
: The `geospark` extensions enables us to load and query large-scale geographic datasets. Usually datasets containing latitude and longitude points or complex areas defined in the Well-known Text (WKT) format, a text markup language for representing vector geometry objects on a map.

Before you learn how and when to use each extension, we should first briefly explain how extensions can be used with R and Spark.

First, an Spark extension is just and R package that happens to be aware of Spark. As any other R package, you will first have to install the R package with `install.packages("package-name")`. Once installed, it is important to know that you willn need to reconnect to Spark before the extension can be used. Son in general, the pattern you should follow goes as follows:

```{r}
library(sparkextension)
library(sparklyr)

sc <- spark_connect(master = "<master>")
```

Notice that `sparklyr` is loaded after the extensions to allow the extension to register properly. If you had to install and load a new extension you would simply have to disconnect first using `spark_disconnect(sc)` and repeat the steps above with the new extension.

As you can notice, it's not hard to install and use Spark extensions from R; however, each extension can be a world on it's own so most of the time you will have to spend time understand what the extension is, when to use it and how to use it properly. The first extension you will learn about is the `rsparkling` extension which enables you to use H2O in Spark with R.

## H2O

[H2O](https://www.h2o.ai/) is open-source software for large-scale modeling created by H2O.ai, which allows you to fit thousands of potential models as part of discovering patterns in data. You can consider using H2O to complement or replace Spark's default modeling algorithms. It is common to Spark's default modeling algorithms and transition to H2O when Spark's algorithms fall short or when advanced functionality (like additional modeling metrics) are required.

We can't do justice to H2O's great modeling capabilities in a single paragraph, explaining H2O properly will require a book in itself. Instead, we would like to recommend reading the "Practical machine learning with H2O" [@extensions-practical-h2o] book to explore in-depth H2O's modeling algorithms and features. In the meantime, you can use this section as a brief guide to get started using H2O in Spark with R.

In order to use H2O with Spark, it is important to know that there are four compoinents involved: H2O, Sparkling Water, [rsparkling][rsparkling](https://github.com/h2oai/sparkling-water/tree/master/r) and Spark. Sparkling Water allows users to combine the fast, scalable machine learning algorithms of H2O with the capabilities of Spark. You can think of Sparkling Water as a component bridging Spark with H2O and `rsparkling` as the R front-end for Sparkling Water, this is illustrated in Figure \@ref(fig:extensions-h2o-diagram).

```{r extensions-h2o-diagram, eval=TRUE, echo=FALSE, fig.height=3, fig.width=8, fig.align='center', fig.cap='H2O components with Spark and R'}
render_nomnoml("
#spacing: 20
#padding: 16
[R | 
  [rsparkling]
  [sparklyr]
]->[Spark |
  [Sparkling Water]
  [H2O]
]
", "images/extensions-h2o-diagram.png")
```

First, install `rsparkling`:

```{r}
install.packages("rsparkling")
```

Is is then important to notice that you need to use compatible versions of Spark, Sparkling Water and H2O. So let's start by checking the version of H2O by running,

```{r eval=TRUE}
packageVersion("h2o")
```

Then we can explore the Spark and Sparkling Water versions that are compatible with `h2o_release_table()`.

```{r extensions-h2o-versions}
rsparkling::h2o_release_table() %>%
  dplyr::filter(H2O_Version == !!as.character(packageVersion("h2o")))
```
```
# A tibble: 4 x 5
  Spark_Version Sparkling_Water_V… H2O_Version H2O_Release_Name H2O_Release_Patch…
          <dbl> <fct>              <fct>       <fct>                         <int>
1           2.4 2.4.5              3.22.1.3    rel-xu                            3
2           2.3 2.3.23             3.22.1.3    rel-xu                            3
3           2.2 2.2.34             3.22.1.3    rel-xu                            3
4           2.1 2.1.48             3.22.1.3    rel-xu                            3
```

We can then connect with the supported Spark versions as follows, you will have to adjust the `master` parameter for your particular cluster. We also recommend increasing the connection timeout since Spark might require to download various H2O components while the connection is established for the first time:

```{r}
library(rsparkling)
library(sparklyr)
library(h2o)

config <- spark_config()
config$sparklyr.connect.timeout <- 3 * 60
  
sc <- spark_connect(master = "local", version = "2.3", config = config)
cars <- copy_to(sc, mtcars)
```

H2O provides a web interface which can help you monitor training and access much of H2O's functionality. The web interface can be accessed through `h2o_flow(sc)`, it is reffered to as H2O Flow and is shown in Figure \@ref(fig:extensions-h2o-flow).

```{r extensions-h2o-flow, eval=TRUE, fig.width=4, fig.align='center', echo=FALSE, fig.cap='H2O Flow Interface using Spark with R'}
render_image("images/extensions-h2o-flow.png")
```

When using H2O, you will have to convert your Spark DataFrame into and H2O DataFrame through `as_h2o_frame`:

```{r}
cars_h2o <- as_h2o_frame(sc, cars)
cars_h2o
```
```
   mpg cyl disp  hp drat    wt  qsec vs am gear carb
1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2
6 18.1   6  225 105 2.76 3.460 20.22  1  0    3    1

[32 rows x 11 columns] 
```

Then you can use many of the modeling functions available in the `h2o` package with ease. For instance, we can fit a generalized linear model with ease:

```{r eval=FALSE}
model <- h2o.glm(x = c("wt", "cyl"),
                 y = "mpg",
                 training_frame = cars_h2o,
                 lambda_search = TRUE)
```
```{r extensions-rsparkling, echo=FALSE}
saveRDS(model, "data/09-extensions-h2o-glm.rds")
```

H2O provides additional metrics not necessarily available in Spark's modeling algorithms, the model that we just fit  `Residual Deviance` is provided in the model while this would not be a standard metric when using Spark MLlib.

```{r}
model
```
```
...
MSE:  6.017684
RMSE:  2.453097
MAE:  1.940985
RMSLE:  0.1114801
Mean Residual Deviance :  6.017684
R^2 :  0.8289895
Null Deviance :1126.047
Null D.o.F. :31
Residual Deviance :192.5659
Residual D.o.F. :29
AIC :156.2425
```

Then you can run prediction over the generalized linear model model, a similar approach would work for many other models available in H2O:

```{r}
predictions <- as_h2o_frame(sc, copy_to(sc, data.frame(wt = 2, cyl = 6)))
h2o.predict(model, predictions)
```
```
   predict
1 24.05984

[1 row x 1 column]
```

H2O can also be used to perform automatic training and tuning of many models; meaning that, H2O can choose which model to use for you using `h2o.automl`:

```{r}
automl <- h2o.automl(x = c("wt", "cyl"), y = "mpg",
                     training_frame = cars_h2o,
                     max_models = 20,
                     seed = 1)
```

For this particular dataset, H2O finds out that XGBoost is a better fit than GLM. Specifically, the H2O explored using XGBoost, Deep Learning, GLM and a Stacked Ensemble.

```{r}
automl
```
```
model_id              mean_residual_deviance     rmse      mse      mae     rmsle
1 XGBoost_1_...       6.627278               2.574350 6.627278 2.066412 0.1329469
2 DeepLearning_...    6.945850               2.635498 6.945850 2.209201 0.1258008
3 XGBoost_grid_1_...  7.025614               2.650587 7.025614 2.192791 0.1339280
4 XGBoost_grid_1_...  7.266691               2.695680 7.266691 2.167930 0.1331849
5 GLM_grid_...        7.416367               2.723301 7.416367 2.184664 0.1269808
6 StackedEnsemble...  7.596133               2.756108 7.596133 2.029900 0.1340302
```

Many additional examples are available under [spark.rstudio.com/guides/h2o](http://spark.rstudio.com/guides/h2o/), you can also request help from [github.com/h2oai/sparkling-water/tree/master/r](https://github.com/h2oai/sparkling-water/tree/master/r), the official GitHub repository for the `rsparkling` package.

The next extension we will present will allow you to process large-scale graph datasets; therefore, make sure to disconnect with `spark_disconnect(sc)` since using different extensions requires you to reconnect to Spark.

## Graphs

The first paper in the history of graph theory was written by Leonhard Euler on the Seven Bridges of Königsberg in 1736. The problem was to devise a walk through the city that would cross each of those bridges once and only, the original diagram is shown in Figure \@ref(fig:extensions-eulers-paths).

```{r extensions-eulers-paths, eval=TRUE, fig.width=4, fig.align='center', echo=FALSE, fig.cap='Seven Bridges of Königsberg from the Euler Archive'}
render_image("images/extensions-eulers-paths.png")
```

Today, a graph is defined as an ordered pair $G=(V,E)$, with $V$ a set of vertices (nodes or points) and $E \subseteq \{\{x, y\} | (x, y) ∈ \mathrm{V}^2 \land x \ne y\}$ a set of edges (links or lines) which are either an unordered pair for **undirected graphs** or an ordered pair for **directed graphs**. The former describing links where the direction does not matter and the latter linked where it does.

As a simple example, we can use the `highschool` dataset from the `ggraph` package which tracks friendship among high school boys,

```{r}
library(ggraph)
library(igraph)

highschool
```
```
# A tibble: 506 x 3
    from    to  year
   <dbl> <dbl> <dbl>
 1     1    14  1957
 2     1    15  1957
 3     1    21  1957
 4     1    54  1957
 5     1    55  1957
 6     2    21  1957
 7     2    22  1957
 8     3     9  1957
 9     3    15  1957
10     4     5  1957
# … with 496 more rows
```

A graph in [GraphFrames](https://graphframes.github.io/) is also represented as a table of edges and vertices; however, the format needs to follow an specific schema. Lets first install the `graphframes` extension,

```{r}
install.packages(graphframes)
```

Followed by connecting, copying the `highschool` dataset and transforming the graph to the format that this extension expects, we will scope this dataset to the friendships of year 1957.

```{r extensions-graphframes, echo=FALSE}
library(graphframes)
library(sparklyr)
library(dplyr)

sc <- spark_connect(master = "local", version = "2.1.0")
highschool_tbl <- copy_to(sc, highschool, "highschool") %>%
  filter(year == 1957) %>%
  transmute(from = as.character(as.integer(from)),
            to = as.character(as.integer(to)))

from_tbl <- highschool_tbl %>% distinct(from) %>% transmute(id = from)
to_tbl <- highschool_tbl %>% distinct(to) %>% transmute(id = to)

vertices_tbl <- distinct(sdf_bind_rows(from_tbl, to_tbl))
edges_tbl <- highschool_tbl %>% transmute(src = from, dst = to)
```

The `vertices_tbl` table is expected to have a single `id` column:

```
# Source: spark<?> [?? x 1]
   id   
   <chr>
 1 1    
 2 34   
 3 37   
 4 43   
 5 44   
 6 45   
 7 56   
 8 57   
 9 65   
10 71   
# … with more rows
```

While the `edges_tbl` is expected to have a `src` and `dst` columns:

```
# Source: spark<?> [?? x 2]
   src   dst  
   <chr> <chr>
 1 1     14   
 2 1     15   
 3 1     21   
 4 1     54   
 5 1     55   
 6 2     21   
 7 2     22   
 8 3     9    
 9 3     15   
10 4     5    
# … with more rows
```

You can now create a GraphFrame,

```{r}
graph <- gf_graphframe(vertices_tbl, edges_tbl)
```

We can now use this graph to start analyzing this dataset. For instance, by finding out how many friends on average every one has, this is reffered as the degree or valency of a vertex:

```{r}
gf_degrees(graph) %>% summarise(friends = mean(degree))
```
```
# Source: spark<?> [?? x 1]
  friends
    <dbl>
1    6.94
```

We can then find what the shortest path to some specific vertex (person for this dataset). Since the data is annonimized, we can just pick the person identified as $33$ and find how many degrees of separation exist between them:

```{r}
gf_shortest_paths(graph, 33) %>%
  filter(size(distances) > 0) %>%
  mutate(distance = explode(map_values(distances))) %>%
  select(id, distance)
```
```
# Source: spark<?> [?? x 2]
   id    distance
   <chr>    <int>
 1 19           5
 2 5            4
 3 27           6
 4 4            4
 5 11           6
 6 23           4
 7 36           1
 8 26           2
 9 33           0
10 18           5
# … with more rows
```

Finally, we can compute PageRank over this graph, which is named after Google's foudner Larry Page: 

```{r echo=FALSE}
model <- gf_graphframe(vertices_tbl, edges_tbl) %>%
  gf_pagerank(reset_prob = 0.15, max_iter = 10L)

highschool_tbl %>% collect() %>%
  saveRDS("data/09-extensions-graphframes-highschool.rds")
```
```{r extensions-graphframes-code}
gf_graphframe(vertices_tbl, edges_tbl) %>%
  gf_pagerank(reset_prob = 0.15, max_iter = 10L)
```
```
GraphFrame
Vertices:
  Database: spark_connection
  $ id       <dbl> 12, 12, 14, 14, 27, 27, 55, 55, 64, 64, 41, 41, 47, 47, 6…
  $ pagerank <dbl> 0.3573460, 0.3573460, 0.3893665, 0.3893665, 0.2362396, 0.…
Edges:
  Database: spark_connection
  $ src    <dbl> 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 12, 12, 12,…
  $ dst    <dbl> 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,…
  $ weight <dbl> 0.25000000, 0.25000000, 0.25000000, 0.25000000, 0.25000000,…
```

To give you some insights into this dataset, Figure \@ref(fig:extensions-graph-pagerank) plots this chart using the `ggraph` and highlights the highest PageRank scores fot this dataset, 

```{r extensions-graph-pagerank-create, eval=FALSE, echo=FALSE}
library(ggraph)
library(igraph)
highschool_rdf <- readRDS("data/09-extensions-graphframes-highschool.rds")
highschool_rdf %>% graph_from_data_frame(directed = FALSE) %>%
  ggraph(layout = 'kk') + 
    geom_edge_link(alpha = 0.2,
                   arrow = arrow(length = unit(2, 'mm')),
                   end_cap = circle(2, 'mm'),
                   start_cap = circle(2, 'mm')) + 
    geom_node_point(size = 2, alpha = 0.4) + theme_light() +
    annotate("point", x = -1.18, y = -3.55, size = 3) +
    annotate("point", x = 6.25, y = 2.85, size = 3) + xlab("") + ylab("") +
    ggsave("images/extensions-graph-pagerank.png", width = 10, height = 5)
```
```{r extensions-graph-pagerank, eval=TRUE, fig.width=4, fig.align='center', echo=FALSE, fig.cap='Highschool ggraph dataset with highest pagerank highlighted'}
render_image("images/extensions-graph-pagerank.png")
```

There are many more graph algorithms provided in `graphframes`, to mention some: bread depth search, connected components, label propagation for detecting communities, etc. For questions on this extension reffer to the official GitHub repo, [github.com/rstudio/graphframes](https://github.com/rstudio/graphframes). We will now present a popular gradient boosting framework.

## XGBoost

sparkxgb is a new sparklyr extension that can be used to train XGBoost models in Spark and is installed as follows:

```{r}
install.packages("sparkxgb")
```

We can then use xgboost_classifier() to train and ml_predict() to predict over large datasets with ease:

```{r}
library(sparkxgb)
library(sparklyr)
library(dplyr)

sc <- spark_connect(master = "local")
iris <- copy_to(sc, iris)

xgb_model <- xgboost_classifier(iris,
                                Species ~ .,
                                num_class = 3,
                                num_round = 50,
                                max_depth = 4)

xgb_model %>%
  ml_predict(iris) %>%
  select(Species, predicted_label, starts_with("probability_")) %>%
  glimpse()

spark_disconnect(sc)
```
```
#> Observations: ??
#> Variables: 5
#> Database: spark_connection
#> $ Species                <chr> "setosa", "setosa", "setosa", "setosa", "…
#> $ predicted_label        <chr> "setosa", "setosa", "setosa", "setosa", "…
#> $ probability_versicolor <dbl> 0.003566429, 0.003564076, 0.003566429, 0.…
#> $ probability_virginica  <dbl> 0.001423170, 0.002082058, 0.001423170, 0.…
#> $ probability_setosa     <dbl> 0.9950104, 0.9943539, 0.9950104, 0.995010…
```

## Deep Learning

sparktf is a new sparklyr extension allowing you to write TensorFlow records in Spark. This can be used to preprocess large amounts of data before processing them in GPU instances with Keras or TensorFlow. sparktf is now available on CRAN and can be installed as follows:

```{r}
install.packages("sparktf")
```

You can simply preprocess data in Spark and write it as TensorFlow records using spark_write_tf():

```{r}
library(sparktf)
library(sparklyr)

sc <- spark_connect(master = "local")

copy_to(sc, iris) %>%
  ft_string_indexer_model(
    "Species", "label",
    labels = c("setosa", "versicolor", "virginica")
  ) %>%
  spark_write_tfrecord(path = "tfrecord")

spark_disconnect(sc)
```

## Genomics

VariantSpark is a framework based on scala and spark to analyze genome datasets. It is being developed by CSIRO Bioinformatics team in Australia. VariantSpark was tested on datasets with 3000 samples each one containing 80 million features in either unsupervised clustering approaches and supervised applications, like classification and regression.

The genome datasets are usually writing in Variant Call Format (VCF), a specific text file format used in bioinformatics for storing gene sequence variations. So, VariantSaprk is a great tool because it is able to read VCF files, run analyses and give us the output in a spark data frame.

```{r}
install.packages("sparktf")
```

```{r}
library(sparklyr)
library(variantspark)

sc <- spark_connect(master = "local")
vsc <- vs_connect(sc)
```

We can start by loading a VCF file,

```
hipster_vcf <- vs_read_vcf(vsc, "inst/extdata/hipster.vcf.bz2")
```

VariantSpark uses Random Forest to assign an "Importance" score to each tested variant reflecting its association to the interest phenotype. A variant with higher "Importance" score implies it is more strongly associated with the phenotype of interest.

```{r}
# calculate the "Importance"
importance <- vs_importance_analysis(vsc, hipster_vcf, labels, n_trees = 100)

# transform the output in a tibble spark
importance_tbl <- importance_tbl(importance) 
```

You can use dplyr and ggplot2 to transform the output and plot!

```{r}
importance_df <- importance_tbl %>% 
  arrange(-importance) %>% 
  head(20) %>% 
  collect()

# importance barplot
ggplot(importance_df) +
  aes(x = variable, y = importance) + 
  geom_bar(stat = 'identity') +          
  scale_x_discrete(limits = importance_df[order(importance_df$importance), 1]$variable) + 
  coord_flip()
```

## Spatial 

[geospark](https://github.com/harryprince/geospark) enables distributed geospatial computing with spatial index on spark in production and keeps a [`dplyr`](https://www.rstudio.com/wp-content/uploads/2015/02/data-wrangling-cheatsheet.pdf) and [`sf`](https://github.com/rstudio/cheatsheets/raw/master/sf.pdf) style grammar.

You can install `geospark` from GitHub as follows:

```{r}
install.packages("remotes")
remotes::install_github("harryprince/geospark")
```

In this example we will join spatial data using `quadrad tree indexing`. First, we will initialize the geospark extension and connect to Spark using sparklyr:

```{r}
library(sparklyr)
library(geospark)

conf <- spark_config()
sc <- spark_connect(master = "local", config = conf)
register_gis(sc)
```

Next we will load some spatial dataset containing as polygons and points.

```{r}
polygons <- read.table(system.file(package="geospark","examples/polygons.txt"), sep="|", col.names=c("area","geom"))
points <- read.table(system.file(package="geospark","examples/points.txt"), sep="|", col.names=c("city","state","geom"))

polygons_wkt <- copy_to(sc, polygons)
points_wkt <- copy_to(sc, points)
```

There are various spatial operations defined in `geospark`, which Figure \@ref(fig:extensions-geospark-operations) describes.

```{r extensions-geospark-operations, eval=TRUE, fig.width=4, fig.align='center', echo=FALSE, fig.cap='Spatial operations available in geospark.'}
render_image("images/extensions-geospark-operations.png")
```

The following examples makes use of `st_contains()` to find the polygons that contain the given points.

```{r}
library(dplyr)
polygons_wkt <- mutate(polygons_wkt, y = st_geomfromwkt(geom))
points_wkt <- mutate(points_wkt, x = st_geomfromwkt(geom))

sc_res <- st_join(polygons_wkt,
                  points_wkt,
                  join = sql("st_contains(y,x)")) %>% 
  group_by(area, state) %>%
  summarise(cnt = n()) 
  
sc_res %>%
  head()
```
```
    # Source: spark<?> [?? x 3]
    # Groups: area
      area            state   cnt
      <chr>           <chr> <dbl>
    1 texas area      TX       10
    2 dakota area     SD        1
    3 dakota area     ND       10
    4 california area CA       10
    5 new york area   NY        9
```

The final result can be present by leaflet, you would need to install the `sf`, `leaflet` and `colormap` packages for the following example to work:

```{r}
Idx_df = collect(sc_res) %>% 
right_join(polygons,by = (c("area"="area"))) %>% 
  sf::st_as_sf(wkt="geom")

Idx_df %>% 
leaflet::leaflet() %>% 
leaflet::addTiles() %>% 
leaflet::addPolygons(popup = ~as.character(cnt),color=~colormap::colormap_pal()(cnt))
```

see more [spatial join](https://github.com/harryprince/geospark#spatial-join):

## Troubleshooting

[Apache IVY](http://ant.apache.org/ivy/) is a popular dependency manager focusing on flexibility and simplicity, which happens to be used by Apache Spark while installing extensions. When connection fails while using an extension, consider clearing your [IVY Cache](http://ant.apache.org/ivy/history/2.0.0/settings/caches.html) by running:

```{r extensions-rsparkling-cache}
unlink("~/.ivy2/cache", recursive = TRUE)
```

## Recap


