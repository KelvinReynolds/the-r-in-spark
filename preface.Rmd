# Preface {-}

Various books have been written for learning Apache Spark; for instance, [Spark: The Definitive Guide: Big Data Processing Made Simple](@information-technology) is a comprehensive resource while [Learning Spark: Lightning-Fast Big Data Analysis](@spark-learning-karau) is an introductory book meant to help users get up an running. However, as of this writting, there is no book to learn Apache Spark using the R programming language and neither, a book specifically designed for the R user nor the aspiring R user.

There are some resources online to learn Apache Spark with R, most notably, the [spark.rstudio.com](https://spark.rstudio.com) site and the Spark documentation site under [spark.apache.org](https://spark.apache.org/docs/latest/index.html). Both sites are great online resources; however, the content is not intended to be read from start to finish and assumes the reader has some context on Apache Spark, R and cluster computing.

The goal of this book is to help anyone get started with Apache Spark using R, with the first chapters being mostly introductory, but quickly ramping up to relevant data science topics, cluster computing and closing on advanced topics which should interst even the most advanced users.

This book is inteded to be a useful resource for a wide range of users; from those of you curious to learn the tools used in **big data** and **big compute**, to those of you experienced in those topics seeking to understand deeper topics while working with Apache Spark from R.

## Structure {-}

This book has the following general outline:

1. **Introduction**: The first chapters will provide a gentle introduction to Apache Spark, R and get you started with the tools required to execute the code provided in this book.
2. **Analysis**: The analysis chapters describe the tools required to analyse data in Apache Spark with R, you will learn about data exploration, transformation and visualization.
3. **Modeling**: The modeling chapters will describe how to create statistical models in Spark and R with the purpose of extracting information and performing predictions.
4. **Scaling**: Up to this point, chapters will have focused on performing operations on your personal computer; these chapters introduce clusters, distributed data and tuning techniques required to perform analysis and modeling across many machines to tackle the large-scale data and computation problems that Apache Spark was designed for.
5. **Extensions**: The extension chapters describe optional components and extended functionality that will interest particular readers, you will learn about alternative modeling frameworks, graph processing at scale and model deployment topics that will be relevant to many readers at some point in time.
6. **Advanced**: This book closes with a set of advanced chapters which the advanced user will most-likely need to consider adopting. By the time the new user reaches this section, these chapters won't seem as intimidating; instead, my hope is that they will be seen as equally relevant, useful and interesting as the previous chapters.

## Authors {-}

**Javier Luraschi**

Javier is a Software Engineer with experience in technologies ranging from desktop, web, mobile and backend; to augmented reality and deep learning applications. He previously worked for Microsoft Research and SAP and holds a double degree in Mathematics and Software Engineering.

**Kevin Kuo**

Kevin is a software engineer working on open source packages for big data analytics and machine learning. He has held data science positions in a variety of industries and was a credentialed actuary. He likes mixing cocktails and studying about wine.

**Edgar Ruiz**

Edgar has a background in deploying enterprise reporting and Business Intelligence solutions. He has posted multiple articles and blog posts sharing analytics insights and server infrastructure for Data Science. He lives with his family near Biloxi, MS.

## Acknowledgments {-}

This project would not have been possible without the work put into building **sparklyr** by Javier Luraschi, Kevin Kuo, Kevin Ushey and JJ Allaire, nor without **bookdown** by Yihui Xie, **dplyr** by Hadley Wickham, **DBI** by Kirill MÃ¼lller nor the **Apache Spark** project iteself.
