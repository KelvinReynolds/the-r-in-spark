# Preface {-}

In a world where information is growing exponentially, leading tools like Apache Spark, provide support to solve many of the relevant problems we face today. From companies looking for ways to improve based on data driven decisions, to research organizations solving problems in healthcare, finance, education, energy and so on; Spark enables analyzing much more information, faster, and more reliably, than ever before.

Various books have been written for learning Apache Spark; for instance, "Spark: The Definitive Guide: Big Data Processing Made Simple" [@preface-spark-definitive-guide] is a comprehensive resource while "Learning Spark: Lightning-Fast Big Data Analysis" [@preface-spark-learning-karau] is an introductory book meant to help users get up and running. However, as of this writing, there is no book to learn Apache Spark using the R computing language and neither, a book specifically designed for the R user nor the aspiring R user.

There are some resources online to learn Apache Spark with R, most notably, the [spark.rstudio.com](https://spark.rstudio.com) site and the Spark documentation site under [spark.apache.org](https://spark.apache.org/docs/latest/index.html). Both sites are great online resources; however, the content is not intended to be read from start to finish and assumes the reader has some knowledge of Apache Spark, R and cluster computing.

The goal of this book is to help anyone get started with Apache Spark using R. Additionally, since the R programming language was created to simplify data analysis, it is also our belief that this book provides the easiest path for anyone to learn the tools used to solve data analysis problems with Spark. The first chapters provide an introduction to help anyone get up to speed with these concepts and present the tools required to work on these problems in your own computer. We will then quickly ramp up to relevant data science topics, cluster computing, and advanced topics that should interest even the most advanced users.

Therefore, this book is intended to be a useful resource for a wide range of users; from those of you curious to learn Apache Spark, to the experienced reader seeking to understand why and how to use Apache Spark from R.

This book has the following general outline:

Introduction
: In the first two chapters **Introduction** and **Getting Started**, you will learn about Apache Spark, R and the tools to perform data analysis with Spark and R.

Analysis
: In the **Analysis** chapter, you will learn how to analyze, explore, transform and visualize data in Apache Spark with R.

Modeling
: In the **Modeling** and **Pipelines** chapters, you will learn how to create statistical models with the purpose of extracting information, predicticting outcomes and automating this process in production-ready workflows.

Scaling
: Up to this point, chapters will have focused on performing operations on your personal computer and with limitted data formats; the **Clusters**, **Connections**, **Data** and **Tuning** chapters, introduce distributed computing techniques required to perform analysis and modeling across many machines and data formats to tackle the large-scale data and computation problems that Apache Spark was designed for.

Extensions
: The extensions chapter describes optional components and extended functionality applicable to specific, yet relevant, use cases. You will learn about alternative modeling frameworks, graph processing, preprocessing data for deep learning, geospatial analysis and genomics at scale.

Advanced
: This book closes with a set of advanced chapters, **Distributed R**, **Streaming** and **Contributing**, which the advanced users will be most interested in. However, by the time you reach this section, these chapters won't seem as intimidating; instead, they will be equally relevant, useful and interesting as the previous chapters.

## Authors {-}

**Javier Luraschi**

Javier is a software engineer with experience in technologies ranging from desktop, web, mobile and backend; to augmented reality and deep learning applications. He previously worked in Microsoft Research and SAP and holds a double degree in Mathematics and Software Engineering. Javier is the creator of sparklyr, r2d3, cloudml and other R packages.

**Kevin Kuo**

Kevin is a software engineer working on open source packages for big data analytics and machine learning. He has held data science positions in a variety of industries and was a credentialed actuary. He likes mixing cocktails and studying about wine.

**Edgar Ruiz**

Edgar Ruiz is a solutions engineer at RStudio with a background in deploying enterprise reporting and business intelligence solutions. He is the author of multiple articles and blog posts sharing analytics insights and server infrastructure for data science. Edgar is the author and administrator of the [db.rstudio.com](https://db.rstudio.com) web site, and current administrator of the [sparklyr web site](https://spark.rstudio.com). Co-author of the dbplyr package, and creator of the dbplot, tidypredict and modeldb package.

## Formatting {-}

Tables generated from code are formatted as follows:

```{r eval=TRUE, echo=FALSE}
tibble::tibble(numbers = c(1,2,3), text = c("one","two","three"))
```

The dimensions of the table (number of rows and columns) are described in the first row, followed by column names in the second row and column types in the third row. There are also various subtle visual improvements provided by the `tibble` package that we make use of throughout this book.

Plots will be rendered using the `ggplot2` package and a custom `ggplot` theme available in the Appendix; however, since this book is not focused on data visualization, some examples make use of R's `plot()` function while the figures were rendered using `ggplot`. If you are interested to learn more about visualization in R, consider specialized books like "R graphics cookbook: practical recipes for visualizing data". [@r-graphics-cookbook]
 
## Acknowledgments {-}

To the package authors that enabled Spark from R we thank **sparklyr** by Javier Luraschi, Kevin Kuo, Kevin Ushey and JJ Allaire, **dplyr** by Romain François and Hadley Wickham, **dbplyr** by Hadley Wickham and Edgar Ruiz, **DBI** by Kirill Mülller nor the authors and maintainers of the **Apache Spark** project itself.

To the package authors that released extensions to enrich the Spark and R ecosystem we thank **crassy** by Akhil Nair, **geospark** by Harry Zhu, **graphframes** by Kevin Kuo, **mleap** by Kevin Kuo, **rsparkling** by Jakub Hava, Navdeep Gill, Erin LeDell, and Michal Malohlava, **spark.sas7bdat** by Jan Wijffels, **sparkavro** by Aki Ariga, **sparkbq** by Martin Studer, **sparklyr.nested** by Matt Pollock, **sparktf** by Kevin Kuo, **sparkts** by Nathan Eastwood, **sparkwarc** by Javier Luraschi, **sparkxgb** by Kevin Kuo and **variantspark** by Samuel Macêdo.

We thank our wonderful editor, Melissa Potter, for provided us with guidance, encouragement and countless hours of detailed feedback to make this book the best we could have ever written.

We want to thank RStudio, JJ Allaire and Tareef Kawaf from supporting this work and the R community itself for their continuous support and encouragement.

We also thank everyone inderectly involved not explicitly listed in this section, we are truly standing in the shoulders of giants.

This book itself was written in R using **bookdown** by Yihui Xie, **rmarkdown** by JJ Allaire and Yihui Xie, 
**knitr** by Yihui Xie, **ggplot2** by Hadley Wickham and Winston Chang, and **pandoc** by John MacFarlane.
