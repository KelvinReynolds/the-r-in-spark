```{r include=FALSE}
knitr::opts_chunk$set(eval = FALSE)
source("r/render.R")
source("r/plots.R")
library(ggplot2)
```

# Data {#data}

The goal of this chapter is to help you learn how to access, read and write data using Spark.  It will provide the necessesary background to help you work with a variaety of data.  

This chapter will cover specific source types and file protocols. It will also aim to teach a pattern. This pattern should help you access types or protocols not covered here. 

Additionally, this chapter will introduce some recommendations.  The focus of the recommendations will be to improve performance and efficiency.

## Types and protocols

Accessing new data for the first time may present challenges. The likely reasons are the source type, or the file system protocol. 

Spark is able to interact with several source types and file system protocols.  Source types include Comma separated values (CSV), Apache Parquet, or JDBC.  File system protocols includes the local file system (Linux, Windows, Mac), and the Hadoop file System (HDFS).  

Spark's capabilities can be extended. This is important when encountering new source type, or protocol. The next section will cover how to do this.

### Default packages

Spark is a very flexible computing platform.  It is able to use the libraries from external packages to add functionality.  This makes it possible to be able to read data from Amazon S3 buckets, only if it is needed. The proper package is loaded at Spark connection time. The package is typically found and loaded directly from the Interenet.  

The configuration variable passed to the `spark_connect()` function should contain the reference to the desired package.  Any package that is to be loaded at connection time is added to a vector in the `defaultPackages` value of the `spark_connect()` configuration. Here is an example that loads the package needed to access Amazon S3 buckets.

```{r}
conf <- spark_config()
conf$sparklyr.defaultPackages <- "org.apache.hadoop:hadoop-aws:2.7.7"
sc <- spark_connect(master = "local", config = conf)
```


### Source types

The main goal is to convert the source data into an object in Spark.  Logically, this requires a spec that "tells" Spark how to read the source data.  Each spec enables Spark to read source data formats.  

Currently, out-of-the box Spark can read several formats.  In `sparklyr`, the source data formats are aligned to R functions:


| Format                                        | Read                   | Write                   |
|-----------------------------------------------|------------------------|-------------------------|
| Comma separated values (CSV)                  | `spark_read_csv()`     | `spark_write_csv()`     |
| Javascript Object Notation (JSON)             | `spark_read_json()`    | `spark_write_json()`    |
| Library for Support Vector Machines (LIBSVM)  | `spark_read_libsvm()`  | `spark_write_libsvm()`  |
| Java Database Connectivity (JDBC)             | `spark_read_jdbc()`    | `spark_write_jdbc()`    |
| Optimized Row Columnar (ORC)                  | `spark_read_orc()`     | `spark_write_orc()`     |
| Apache Parquet                                | `spark_read_parquet()` | `spark_write_parquet()` |
| Text                                          | `spark_read_text()`    | `spark_write_text()`    |

It is possible to access data source types not listed above.  It will require to load a dafault package at connection time.  In `sparklyr`, the `spark_read_source()` and `spark_write_source()` are generic functions.  They are able to use the libraries imported by the default package.

The follow example code shows how to use the `datastax:spark-cassandra-connector` package to read from Cassandra. The key is to use the  `org.apache.spark.sql.cassandra` library as the `source` argument.  That is what provides the mapping Spark can use to make sense of the data source.

```{r}
con <- spark_config()
conf$sparklyr.defaultPackages <- "datastax:spark-cassandra-connector:2.0.0-RC1-s_2.11"
sc <- spark_connect(master = "local", config = conf)
spark_read_source(
  sc, 
  name = "emp",
  source = "org.apache.spark.sql.cassandra",
  options = list(keyspace = "dev", table = "emp")
  )
```


### File system protocols

Spark will default to the file protocol that it is running on.  This means that if it is running in a YARN managed cluster, the default protocol will be HDFS. For example, a given path of "/home/user/file.csv" will be read from cluster's HDFS folders, and not the Linux files.  The Operating System's file system will be accessed for other deployments, such as Stand Alone, and `sparklyr`'s local. 

The file system protocol can be overridden.  This is done in the `path` argument of the `sparklyr` function that will read or write.  Passing a full path of "file://home/user/file.csv" will force the search to be done inside the local Operating System's file system.

There are other file system protocols.  An example is Amazon's S3 service.  Spark is does not know how to read the S3 protocol.  Accessing the "s3a" protocol involves adding a package to the `defaultPackages` configuration variable passed at connection time.  

```{r}
conf <- spark_config()
conf$sparklyr.defaultPackages <- "org.apache.hadoop:hadoop-aws:2.7.7"
sc <- spark_connect(master = "local", config = conf)
my_file <- spark_read_csv(sc, "my-file", path =  "s3a://my-bucket/my-file.csv")
```

Currently, only "file://" and "hdfs://" file protocols are supported when used in their respective environments.  Accessing a different file protocol may require loading a default package.  In some cases the vendor providing the Spark environment could already be loading the package for you.  

## Reading data

### Folders as a source

### File layout

### Data caching 

### Column Names

By default, `sparklyr` sanitizes column names by translating characters like `.` to `_`, this was required in Spark 1.6.X to avoid couple nuances in Spark. However, to disable this functionality, you can run the following code:

```{r eval=FALSE}
options(sparklyr.sanitize.column.names = FALSE)
dplyr::copy_to(sc, iris, overwrite = TRUE)
```
```
# Source:   table<iris> [?? x 5]
# Database: spark_connection
   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
          <dbl>       <dbl>        <dbl>       <dbl> <chr>  
 1          5.1         3.5          1.4         0.2 setosa 
 2          4.9         3            1.4         0.2 setosa 
 3          4.7         3.2          1.3         0.2 setosa 
 4          4.6         3.1          1.5         0.2 setosa 
 5          5           3.6          1.4         0.2 setosa 
 6          5.4         3.9          1.7         0.4 setosa 
 7          4.6         3.4          1.4         0.3 setosa 
 8          5           3.4          1.5         0.2 setosa 
 9          4.4         2.9          1.4         0.2 setosa 
10          4.9         3.1          1.5         0.1 setosa 
# ... with more rows
```

## Date & time

Some Spark date/time functions make timezone assumptions. For instance, the following code makes use of `to_date()`. It assumes that the timestamp will be given in the local time zone. This is not to discourage use of date/time functions. Please be aware of time zones to be handled with care.

```{r eval=FALSE}
sdf_len(sc, 1) %>%
  transmute(
    date = timestamp(1419126103) %>% 
      from_utc_timestamp('UTC') %>% 
      to_date() %>% 
      as.character()
  )
```

## Specific types and protocols

### Amazon S3

Amazon Simple Storage Service, or S3, has become a common location to store file.  Spark is able to directly access S3.  This functionality can be used inside `sparklyr`.  There are three key items to have, or use, when working with data from an S3 bucket: 

- AWS Credentials - They are required by the S3 service, even for publicly accessible buckets.
- Hadoop-to-AWS package - It is loaded at connection time.
- A bucket location - The recommended file system to use is "s3a".

There are multiple ways to set the credentials to use to access the bucket.  Please refer to the official documentation for more information. It is found in the Apache Spark official site [@data-spark-cloud-integration]. 

The easiest way is to set the credentials using Environment variables.  Choose a secure way to load the values into variables in R, and then load them into the appropriate Environment variable name. In case show below, `AWS_ACCESS_KEY_ID` and `AWS_SECRET_ACCESS_KEY`.

```{r}
Sys.setenv(AWS_ACCESS_KEY_ID = my_key_id)
Sys.setenv(AWS_SECRET_ACCESS_KEY = my_secret_key)
```

Spark requires an integration package in order to access Amazon S3 buckets. Interestingly, the package is not a Spark package, it is a Hadoop package.  This means that the selected version will be a Hadoop version.  After some experiments, it seems that with Spark versions 2, only up to Hadoop 2.7.7 will work.  That may change when Spark enters version 3.  If using a YARN managed cluster, the package may be different.  That would depend on the Hadoop vendor.  The official site for Apache based project is called Maven [@data-maven-home].  Please visit that site to find alternative package versions if the recommended one does not work. The recommended search term to use would be: "hadoop-aws".

```{r}
conf <- spark_config()
conf$sparklyr.defaultPackages <- "org.apache.hadoop:hadoop-aws:2.7.7"

sc <- spark_connect(master = "local", config = conf)
```

For the file system prefix use "s3a".  There are other options, such as "s3" and "s3n".  As per the Hadoop documents, the "s3a" file system should be the default selection.

```{r}
my_file <- spark_read_csv(sc, "my-file", path =  "s3a://my-bucket/my-file.csv")
```

### Azure Storage

`wasb` files
```{r}

```

### Cassandra

See [https://blog.rstudio.com/2017/07/31/sparklyr-0-6/#external-data-sources](https://blog.rstudio.com/2017/07/31/sparklyr-0-6/#external-data-sources).

### Databases

See [https://blog.rstudio.com/2017/07/31/sparklyr-0-6/#external-data-sources](https://blog.rstudio.com/2017/07/31/sparklyr-0-6/#external-data-sources).

#### Switching

You can query multiple databases registered in Spark using the `.` syntax, as in:

```{r eval=FALSE}
DBI::dbSendQuery("SELECT * FROM databasename.table")
```

However, if you preffer to switch to a particular database and make it the default, you can run:

```{r eval=FALSE}
tbl_change_db(sc, "db_name")
```

which an alias over `DBI::dbGetQuery(sc, "use db_name”)`.

##### Schemas

```
in_schema("database", "table")
```

### HBase

### Nested Data

See [nested data extension](#extensions-nested-data).

## Troubleshooting

### Troubleshoot CSVs

```{r eval=FALSE}
writeLines(c("bad", 1, 2, 3, "broken"), "tmp/bad.csv")
```

There are a couple modes that can help troubleshoot parsing issues:
- **PERMISSIVE**: `NULL`s are inserted for missing tokens.
- **DROPMALFORMED**: Drops lines which are malformed.
- **FAILFAST**: Aborts if encounters any malformed line.

Which can be used as follows:

```{r eval=FALSE}
spark_read_csv(
  sc,
  "bad",
  "tmp/bad.csv",
  columns = list(foo = "integer"),
  infer_schema = FALSE,
  options = list(mode = "DROPMALFORMED"))
```
```
# Source:   table<bad> [?? x 1]
# Database: spark_connection
    foo
  <int>
1     1
2     2
3     3
```

In Spark 2.X, there is also a secret column `_corrupt_record` that can be used to output those incorrect records:

```{r eval=FALSE}
spark_read_csv(
  sc,
  "decimals",
  "tmp/bad.csv",
  columns = list(foo = "integer", "_corrupt_record" = "character"),
  infer_schema = FALSE,
  options = list(mode = "PERMISIVE")
)
```
```
# Source:   table<decimals> [?? x 2]
# Database: spark_connection
    foo `_corrupt_record`   
  <int> <chr>               
1     1 NA                  
2     2 NA                  
3     3 NA                  
4    NA sdfsdfds            
5    NA 2.16027303300001e+31
```



## Recap

In the next chapter, [Tuning], you will learn in-detail how Spark works and use this knowledge to optimize it's resource usage and performance.


