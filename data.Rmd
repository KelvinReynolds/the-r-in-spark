```{r include=FALSE}
knitr::opts_chunk$set(eval = FALSE)
source("r/render.R")
source("r/plots.R")
library(ggplot2)
```

# Data {#data}

The previous chapter, Connections, presented how to connect to Spark clusters, either on-premise or in the cloud; with previous knowledge acquired from previous chapters, you are up to this point equiped to do data analisys and modeling at scale! However, so far we haven't really explained much about how to read data into Spark, we've used `copy_to()` to upload small datasets or functions like `spark_read_csv()` without explaining in detail how to load and write data in Spark.

Therefore, in this chapter, you will learn how to read and write data using Spark; while this is important on it's own, you can't do much without being able to load the data you care about into Spark, this chapter will also present the exciting new world of Data Lakess -- a repository of data stored in its natural or raw format which provides various bennefits over existing storage architectures.

In addition, we will also present how to extend Spark's capabilities to work with data no accessible "out-of-the-box" and several recommendations focused on improving performance for writing or reading data. 

## Overview

In the Introduction chapter, you learned that beyond big data and big compute, you can also use Spark to improve velocity, variety and veracity in data tasks. While you can use the learnings of this chapter for any task requiring loading and storing data, it is particularily interesting to present this chapter in the context of dealing with a variety of data sources. To understand why, we should first take a quick detour understanding how data is currently processed in many organizations.

For several years, it's been a common practice to store large datasets in a relational **Database**, a system first proposed in 1970 by Edgar F. Codd. You can think of a database as a collection of tables that are related to one another where each table is carefully designed to hold specific data types and relationships to other tables.  most relational database systems use SQL (Structured Query Language) for querying and maintaining the database. Databases are still wildly used today, with good reason! -- Databases store data reliably and consistently, your bank probably stores account balances in a database and that's a good practice!

However, databases have also been used to store information from other applications and systems. For instance, your bank may also store data produced by other banks, say incoming checks. In order to accomplish this, the external data needs to be extracted from the external system, transformed into something that fits the current database and finally load it. This is known as **ETL** (extract-transform-load), a general procedure of copying data from one or more sources into a destination system which represents the data differently from the source, this process became a popular in the 1970s. Aside from databases, data is often also loaded into a **Data Warehouse**, a system used for reporting and data analysis; usually stored and indexed in a format that increases data analysis speed, but which is often not suitable for modeling or running custom distributed code. However, changing a databse or data warehouse is usually a long and delicate process since data needs to be re-indexed and carefully designed transform data from multiple data sources into a common column or table that they can all share.

Instead of trying to transform all data sources into a common format, you can embrace this variety of data sources in a **Data Lake**, a system or repository of data stored in its natural format, see Figure \@ref(fig:data-data-lake). Since the data in a Data Lakes is made available in it's raw format, it is easy for anyone to use it for analysis which adds significant flexibility to organizations. Spark can then be used to unify data processing from a Data Lakes, a database or a data warehouse. Some organization also use Spark to replace their existing ETL process; however, this falls in the realm of data engineering which falls beyond the scope of this book. We've marked this with dotted lines in Figure \@ref(fig:data-data-lake) to denote systems that are not always present when using Spark with Data Lakes.

```{r data-data-lake, eval=TRUE, echo=FALSE, fig.cap='Sparl processing raw data from a Data Lake, Databases and Data Warehouses'}
render_nomnoml("
#spacing: 20
#padding: 10
[Data Lake|
  [hdfs://data.csv]
  [hdfs://data.json]
  [s3://data.parquet]
]
[Data Lake]->[Spark|
  [<note>spark_read_csv()
  spark_read_json()
  spark_read_parquet()
  ...
  spark_read_jdbc()
  spark_write_jdbc()
  ]
]
[Spark]->[Data Lake]
[Data Lake]--->[Database]
[Data Lake]--->[Data Warehouse]
[Database]--->[Spark]
[Spark]--->[Database]
[Data Warehouse]--->[Spark]
[Spark]--->[Data Warehouse]
")
```

In order to support a broad variety of data source, Spark needs to be able to read and write data in several different file formats and access them while stored in also several storage systems, we will get to this later in the chapter; but first, we will start by presenting how to copy, read and write data using Spark.

## Copying Data

## Reading data

If you are new to Spark and `sparklyr`, it is highly recommended to review this section before starting work with large data sets. We will introduce several techniques that improve the speed and efficiency of reading data. Each subsection will cover a specific way you can take advantage of how Spark reads files, such as the ability to treat entire folders as table sources, as well as being able to pass a file layout to accelerate the file read. 

### Folders as a table

When analyzing data, loading multiple files into a single data object is a common scenario.  In R, we typically use a loop or functional programming to accomplish this.  That is because R has to load each file individually into the session.

```{r}
lapply(c("data-folder/file1.csv", "data-folder/file2.csv"), read.csv)
```

In Spark, there is the notion of a folder as a table.  Instead of enumerating each file, simply pass the path the containing folder's path.  Spark assumes that every file in that folder is part of the same table.  This implies that the target folder should only be used for data purposes. 

```{r}
spark_read_csv(sc, "my_data", path = "data-folder")
```

The "folder as a table" idea is found in other open source technologies as well.  Under the hood, Hive tables work the same way.  When querying a Hive table, the mapping is done over multiple files inside the same folder. The folder's name usually match the name of the table visible to the user.  

The following technique allows Spark to read file faster, as well as to reduce read failures, by passing a file layout. 

### File layout

When reading data, Spark is able to determine the data source's column names and types.  This comes at a cost.  To determine the type Spark has to do an initial pass on the data, and then assign a type.  For large data, this may add a significant amount of time to the data ingestion process, which can become costly even for medium size data loads.  For files that are read over and over again,  the additional read time accumulates over time.

Spark allows the user to provide a column layout. If provided, Spark will bypass the step that it uses to determine the file's layout. In `sparklyr`, we can use the `column` argument to take advantage of this functionality. The `infer_schema` argument also needs to be set to `FALSE`.  This arguments is the switch that indicates if the `column` argument should be used.

```{r, echo = FALSE}
x <- data.frame(x = letters, y = 1:length(letters))
write.csv(x, "test.csv", row.names = FALSE)
rx <- readr::read_csv("test.csv", n_max = 10)
readr::spec(rx)

top_rows <- read.csv("test.csv", nrows = 5)
file_columns <- top_rows %>% 
  purrr::map(function(x)"character")

purrr::map(rx, class)

col_spec <- c("character", "numeric")
names(col_spec) <- c("x", "y")

```

For example, lets take a file called *test.csv*, and load it to Spark. This is its layout:

```
"x","y"
"a",1
"b",2
"c",3
"d",4
"e",5
```

The column spec is started with a vector containing the column types. The vector's values are named to match the field names. 

```{r}
col_spec_1 <- c("character", "numeric")
names(col_spec_1) <- c("x", "y")
col_spec_1
```
```
##           x           y 
## "character"   "numeric" 
```

The accepted variable types are: 

- `integer`

- `character` 

- `logical`

- `double`

- `numeric`

- `factor`

- `Date`

- `POSIXct`

In `spark_read_csv()`, `col_spec_1` is passed to the `columns` argument, and `infer_schema` is set to `FALSE`. We will try match the names and types of the original file.  Also, we will pass the column specification. Doing this will help with performance because Spark will not have to figure out the column types.

```{r}
sc <- spark_connect(master = "local")
test_1 <- spark_read_csv(sc, "test1","test.csv", 
                         columns = col_spec_1, 
                         infer_schema = FALSE)
test_1
```

```
## # Source: spark<test1> [?? x 2]
##    x         y
##    <chr> <dbl>
##  1 a         1
##  2 b         2
##  3 c         3
##  4 d         4
##  5 e         5
```

The following example shows how to set the field type to something different. However, the new field type needs a compatible type from the original. For example, a `character` field could not be set to `numeric`. If an incompatible type is used, the file read will fail with an error. Additionally, the example also changes the names of the fields.

```{r}
col_spec_2 <- c("character", "character")
names(col_spec_2) <- c("my_letter", "my_number")

test_2 <- spark_read_csv(sc, "test2","test.csv", 
                         columns = col_spec_2, 
                         infer_schema = FALSE)
test_2
```

```
# Source: spark<test2> [?? x 2]
   my_letter my_number
   <chr>     <chr>    
 1 a         1        
 2 b         2        
 3 c         3        
 4 d         4        
 5 e         5    
```

In Spark, malformed entries can cause error during reading, specially for non-character fields.  To prevent such errors, we can use a file spec that imports them as character, and then use `dplyr` to coerce the field into the desired type.

This subsection reviewed how we can read files faster and with less failures, which lets us start our analysis quicker.  Another way to accelerate our analysis, is by loading less data into Spark memory, the next subsection will cover how to do this.

### Spark memory

Spark copies the data into its distributed memory, which makes analyses and other processes very fast.  There are cases, such as when the data is too big, that loading all of the data may not be practical, or even necessary. For those cases, Spark can then just "map" the files without copying data into memory.  

The mapping creates a sort of "virtual" table in Spark memory.  The implication is that when a query runs against that table, Spark has to read the data from the files at that time.  Any consecutive read after that will do the same.  In effect, Spark becomes a pass-through for the data. The advantage of this method is that there is almost no up-front time cost to "reading" the file,  the mapping is very fast. The downside is that running queries that actually extract data will take longer. 

In `sparklyr`, that is controlled by the `memory` argument of its read functions. Setting it to `FALSE` prevents the data copy.  It defaults to `TRUE`.

```{r}
mapped_test <- spark_read_csv(sc, "test","test.csv", memory = FALSE)
```

There are good use cases for this method. One of them is when not all columns of a table are needed.  For example, take a very large file that contain many columns. This is not first time we interact with this data. We know what columns are needed for the analysis.  The files can be read using `memory = FALSE`, and then select the needed columns with `dplyr`. The resulting `dplyr` variable can then be cached into memory, using the `compute()` function.  This will make Spark query the file(s), pull the selected fields, and copy only that data into memory. The result is a in-memory table that took comparatively less time to ingest.

```{r}
mapped_test %>%
  select(y) %>%
  compute("test")
```

The next subsection covers a short technique to make it easier to carry the original field names of imported data. 

### Column Names

Spark version 1.6 required that column names be sanitized, so `sparklyr` does that by default.  There may be cases when you would like to keep the original names intact, and are also working with Spark version 2.0 or above.  To do that set the `sparklyr.sanitize.column.names` option to `FALSE`.

```{r eval=FALSE}
options(sparklyr.sanitize.column.names = FALSE)
dplyr::copy_to(sc, iris, overwrite = TRUE)
```
```
# Source:   table<iris> [?? x 5]
# Database: spark_connection
   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
          <dbl>       <dbl>        <dbl>       <dbl> <chr>  
 1          5.1         3.5          1.4         0.2 setosa 
 2          4.9         3            1.4         0.2 setosa 
 3          4.7         3.2          1.3         0.2 setosa 
 4          4.6         3.1          1.5         0.2 setosa 
 5          5           3.6          1.4         0.2 setosa 
 6          5.4         3.9          1.7         0.4 setosa 
 7          4.6         3.4          1.4         0.3 setosa 
 8          5           3.4          1.5         0.2 setosa 
 9          4.4         2.9          1.4         0.2 setosa 
10          4.9         3.1          1.5         0.1 setosa 
# ... with more rows
```

After reviewing how to read data into Spark in this section, the next section will cover how we can write data from our Spark session. 

## Writing Data

Some projects require that new data generated in Spark to be written back to a remote source. For example, the data could be new predicted values returned by a Spark model.  The job processes the mass generation of predictions, and the predictions need to be stored. This section will focus on when and where we should use Spark, and R, for moving the data from Spark into an external destination.

### Spark, not R, as pass-through

Many new users start by downloading Spark data into R, to then upload it to a target. Figure \@ref(fig:data-avoid-approach)  illustrates this approach. It works for smaller data sets, but it becomes inefficient for larger ones.  The data typically grows in size to the point that it is no longer feasible for R to be the middle point. 

```{r data-avoid-approach, echo=FALSE,  fig.cap='Avoid using R as a pass through', fig.align = 'center', eval = TRUE}
render_nomnoml("
#direction: right
#arrowSize: 0.4
#lineWidth: 1
#spacing:90
[Avoid this...| 
[Source] -> [Spark | Process] 
[Spark]collect() -> [R]
[R] -> [Target]
]
", "images/data-r-pass-through.png")
```

All efforts should be made to have Spark connect to the target location.  This way, reading, processing and writing happens within the same Spark session.  

As figure \@ref(fig:data-recommended-approach) shows, a better approach is to use Spark to read, process, and write to the target.  This approach is able to scale as big as the Spark cluster allows, and prevents using R as a choke point.

```{r data-recommended-approach, echo=FALSE,  fig.cap='Spark as a pass through', fig.align = 'center', eval = TRUE}
render_nomnoml("
#direction: right
#arrowSize: 0.4
#lineWidth: 1
#spacing:90
[Source] -> [Spark | Reads -  Process - Writes] 
[Spark] -> [Target]
", "images/data-spark-pass-through.png")
```

Consider the following scenario: A Spark job just processed predictions for a large data set, resulting in a considerably large set of predictions. Choosing a method to write results will depend on the technology infrastructure you are working on. More specifically, it will depend on Spark and the target running, or not, in the same cluster.

Back to our scenario, we have a large dataset in Spark that needs to be saved.  In this case, Spark and the target Hive table are in the same cluster.  Copying the results is not a problem, since the data transfer is between RAM and disk of the same cluster. 

But what to do if the target is not Hive? In other words, Spark and the target location are not in the same cluster.  There are two options, choosing one will depend on the size of the data, and network speed:  

Spark connects to the remote target location, and copy the new data
: If this is done within the same Data Center, or cloud provider, the data transfer could be fast enough to have Spark write the data directly.  

Spark writes the results locally, and transfers the results via a third-party application
: Spark could write the results into CSV files, and then have a separate job copy the files over via FTP.  In the target location, you would use a separate process to transfer the data into the target location.  

It is best to recognize that Spark, R, and any other technology are tools.  No tool can do everything, nor should be expected to.  

## File Formats and File Systems

It may be challenging accessing data for the first time, particularly if you're working with new sources or file systems. 

"Out-of-the-box", Spark is able to interact with several source types and file system.  Source types include: Comma separated values (CSV), Apache Parquet, and JDBC.  File system protocols include: local file system (Linux, Windows, Mac), and Hadoop file System (HDFS).  

However, there is a way for Spark to interact with other source types and file systems, which will cover next. This chapter will also cover some of the most common source types and protocols.

The section [Source types and file systems](#data-source-files) in the Appendix contains tips on how to read and write data from specific source types and file systems.

### Default packages

Spark is a very flexible computing platform.  It can add functionality by using extension programs, called packages. Accessing a new source type or file system can be done by using the appropriate package. 

Packages need to be loaded into Spark at connection time.  To load the package, Spark needs its location, which could be inside the cluster, in a file share or the Internet.  

In `sparklyr`, the package location is passed to `spark_connect()`.  All packages should be listed in the `defaultPackages` entry of the connection configuration. Here is an example that loads the package needed to access Amazon S3 buckets:

```{r}
conf <- spark_config()
conf$sparklyr.defaultPackages <- "org.apache.hadoop:hadoop-aws:2.7.7"
sc <- spark_connect(master = "local", config = conf)
```

### Source types

Spark can read and write several source types.  In `sparklyr`, the source types are aligned to R functions.

| Format                                        | Read                   | Write                   |
|-----------------------------------------------|------------------------|-------------------------|
| Comma separated values (CSV)                  | `spark_read_csv()`     | `spark_write_csv()`     |
| JavaScript Object Notation (JSON)             | `spark_read_json()`    | `spark_write_json()`    |
| Library for Support Vector Machines (LIBSVM)  | `spark_read_libsvm()`  | `spark_write_libsvm()`  |
| Java Database Connectivity (JDBC)             | `spark_read_jdbc()`    | `spark_write_jdbc()`    |
| Optimized Row Columnar (ORC)                  | `spark_read_orc()`     | `spark_write_orc()`     |
| Apache Parquet                                | `spark_read_parquet()` | `spark_write_parquet()` |
| Text                                          | `spark_read_text()`    | `spark_write_text()`    |


It is possible to access data source types not listed above.  Loading the appropriate default package for Spark is the first of two steps  The second step is to actually read or write the data. The `spark_read_source()` and `spark_write_source()` functions do that.  They are generic functions that can use the libraries imported by a default package.

The following example code shows how to use the `datastax:spark-cassandra-connector` package to read from Cassandra. The key is to use the  `org.apache.spark.sql.cassandra` library as the `source` argument.  It provides the mapping Spark can use to make sense of the data source.

```{r}
con <- spark_config()
conf$sparklyr.defaultPackages <- "datastax:spark-cassandra-connector:2.0.0-RC1-s_2.11"
sc <- spark_connect(master = "local", config = conf)
spark_read_source(
  sc, 
  name = "emp",
  source = "org.apache.spark.sql.cassandra",
  options = list(keyspace = "dev", table = "emp")
  )
```

### File systems

Spark will default to the file system that it is currently running on.  In a YARN managed cluster, the default file system will be HDFS. An example path of "/home/user/file.csv" will be read from cluster's HDFS folders, and not the Linux folders.  The Operating System's file system will be accessed for other deployments, such as Stand Alone, and `sparklyr`'s local. 

The file system protocol can be changed when reading or writing.  It is done via the `path` argument of the `sparklyr` function.  For example, a full path of "file://home/user/file.csv" will force the use of the local Operating System's file system.

There are other file system protocols,  is Amazon's S3 service for example.  Spark is does not know how to read the S3 protocol, so accessing the "s3a" protocol involves adding a package to the `defaultPackages` configuration variable passed at connection time.  

```{r}
conf <- spark_config()
conf$sparklyr.defaultPackages <- "org.apache.hadoop:hadoop-aws:2.7.7"
sc <- spark_connect(master = "local", config = conf)
my_file <- spark_read_csv(sc, "my-file", path =  "s3a://my-bucket/my-file.csv")
```

Currently, only "file://" and "hdfs://" file protocols are supported when used in their respective environments.  Accessing a different file protocol requires loading a default package.  In some cases, the vendor providing the Spark environment could already be loading the package for you.  Please refer to your vendor's documentation to find out if that is the case.

## Recap

This chapter covered how we can extend Spark's capabilities to access different kinds of source data and file formats.  We also covered several techniques to performance when reading files into Spark.  We also introduced practical principles to keep in mind when writing data from Spark.  In the next chapter, [Tuning], you will learn how Spark manages tasks and data across multiple machines, which will in turn allow you to further improve the performance of your analyses.


