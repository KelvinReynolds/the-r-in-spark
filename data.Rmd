# Data {#data}

While **this chatper has not been written.**, a few resources are available to help explore these topics until this chapter gets written.

## Overview

## DataFrames

TODO: Introduce Data Frames

`sparklyr` provides access to  Data Frame functionality, mostly for completeness and advanced use cases. As `sparklyr` keeps evolving, it is our goal to reduce the use of this functions and provide instead propert wrappers with practices known in the R community. For instnace, we will introduce a function to **pivot** data, which is quite useful but would rather preffer to support `tidyr::spread()` and `tidyr::gather()` implementations which are already well known in the R community.

### Functions {#data-sdf-functions}

```
sdf_nrow()
sdf_ncol()
sdf_dim()
sdf_len()
sdf_pivot()
sdf_schema()
...
```

### Pivoting

```{r eval=F}
sensors <- data.frame(
  sensor = c("sensor1", "sensor1", "sensor1", "sensor2", "sensor2", "sensor2"),
  time = c(1, 2, 3, 1, 2, 3),
  metric = c(1, 2, 3, 10, 20, 30)
)
```

```
   sensor time metric
1 sensor1    1      1
2 sensor1    2      2
3 sensor1    3      3
4 sensor2    1     10
5 sensor2    2     20
6 sensor2    3     30
```

```{r eval=F}
sensors_tbl <- sdf_copy_to(sc, sensors, overwrite = T)
sensors_tbl %>% sdf_pivot(time ~ sensor, fun.aggregate = c("avg", "metric"))
```

```
# Source: spark<?> [?? x 3]
   time sensor1 sensor2
* <dbl>   <dbl>   <dbl>
1     3       3      30
2     1       1      10
3     2       2      20
```

## Formats

```
spark_read_csv()
spark_read_jdbc()
spark_read_json()
spark_read_libsvm()
spark_read_orc()
spark_read_parquet()
spark_read_source()
spark_read_table()
spark_read_text()
```

## Data Types

### Dates

Note: Some Spark date/time functions make timezone assumtions, for instance, the following code makes use of `to_date()` which assumes the timestamp will be given in the local time zone. This is not to discourage use of date/time functions, but to be aware of time zones to be handled with care.

```{r eval=FALSE}
sdf_len(sc, 1) %>%
  transmute(
    date = timestamp(1419126103) %>% from_utc_timestamp('UTC') %>% to_date() %>% as.character()
  )
```

## Sources

### Amazon S3

```{r}

```

### Azure Storage

`wasb` files
```{r}

```

### Cassandra

See [https://blog.rstudio.com/2017/07/31/sparklyr-0-6/#external-data-sources](https://blog.rstudio.com/2017/07/31/sparklyr-0-6/#external-data-sources).

### Databases

See [https://blog.rstudio.com/2017/07/31/sparklyr-0-6/#external-data-sources](https://blog.rstudio.com/2017/07/31/sparklyr-0-6/#external-data-sources).

#### Switching

You can query multiple databases registered in Spark using the `.` syntax, as in:

```{r eval=FALSE}
DBI::dbSendQuery("SELECT * FROM databasename.table")
```

However, if you preffer to switch to a particular database and make it the default, you can run:

```{r eval=FALSE}
tbl_change_db(sc, “db_name”)
```

which an alias over `DBI::dbGetQuery(sc, "use db_name”)`.

#### Schemas

```
in_schema("database", "table")
```

### HBase

### Nested Data

See [nested data extension](#extensions-nested-data).

## Troubleshooting

### Troubleshoot CSVs

```{r eval=FALSE}
writeLines(c("bad", 1, 2, 3, "broken"), "tmp/bad.csv")
```

There are a couple modes that can help troubleshoot parsing issues:
- **PERMISSIVE**: `NULL`s are inserted for missing tokens.
- **DROPMALFORMED**: Drops lines which are malformed.
- **FAILFAST**: Aborts if encounters any malformed line.

Which can be used as follows:

```{r eval=FALSE}
spark_read_csv(
  sc,
  "bad",
  "tmp/bad.csv",
  columns = list(foo = "integer"),
  infer_schema = FALSE,
  options = list(mode = "DROPMALFORMED"))
```
```
# Source:   table<bad> [?? x 1]
# Database: spark_connection
    foo
  <int>
1     1
2     2
3     3
```

In Spark 2.X, there is also a secret column `_corrupt_record` that can be used to output those incorrect records:

```{r eval=FALSE}
spark_read_csv(
  sc,
  "decimals",
  "tmp/bad.csv",
  columns = list(foo = "integer", "_corrupt_record" = "character"),
  infer_schema = FALSE,
  options = list(mode = "PERMISIVE")
)
```
```
# Source:   table<decimals> [?? x 2]
# Database: spark_connection
    foo `_corrupt_record`   
  <int> <chr>               
1     1 NA                  
2     2 NA                  
3     3 NA                  
4    NA sdfsdfds            
5    NA 2.16027303300001e+31
```

### Column Names

By default, `sparklyr` sanitizes column names by translating characters like `.` to `_`, this was required in Spark 1.6.X to avoid couple nuances in Spark. However, to disable this functionality, you can run the following code:

```{r eval=FALSE}
options(sparklyr.sanitize.column.names = FALSE)
dplyr::copy_to(sc, iris, overwrite = TRUE)
```
```
# Source:   table<iris> [?? x 5]
# Database: spark_connection
   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
          <dbl>       <dbl>        <dbl>       <dbl> <chr>  
 1          5.1         3.5          1.4         0.2 setosa 
 2          4.9         3            1.4         0.2 setosa 
 3          4.7         3.2          1.3         0.2 setosa 
 4          4.6         3.1          1.5         0.2 setosa 
 5          5           3.6          1.4         0.2 setosa 
 6          5.4         3.9          1.7         0.4 setosa 
 7          4.6         3.4          1.4         0.3 setosa 
 8          5           3.4          1.5         0.2 setosa 
 9          4.4         2.9          1.4         0.2 setosa 
10          4.9         3.1          1.5         0.1 setosa 
# ... with more rows
```

## Recap

In the next chapter, [Tuning], you will learn in-detail how Spark works and use this knowledge to optimize it's resource usage and performance.
