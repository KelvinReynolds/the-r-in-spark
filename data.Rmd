```{r include=FALSE}
knitr::opts_chunk$set(eval = FALSE)
source("r/render.R")
source("r/plots.R")
library(ggplot2)
```

# Data {#data}

The previous chapter, Connections, presented how to connect to Spark clusters, either on-premise or in the cloud; with previous knowledge acquired from previous chapters, you are up to this point equiped to do data analisys and modeling at scale! However, so far we haven't really explained much about how to read data into Spark, we've used `copy_to()` to upload small datasets or functions like `spark_read_csv()` without explaining in detail how to load and write data in Spark.

Therefore, in this chapter, you will learn how to read and write data using Spark; while this is important on it's own, you can't do much without being able to load the data you care about into Spark, this chapter will also present the exciting new world of Data Lakess -- a repository of data stored in its natural or raw format which provides various bennefits over existing storage architectures.

In addition, we will also present how to extend Spark's capabilities to work with data no accessible "out-of-the-box" and several recommendations focused on improving performance for writing or reading data. 

## Overview

In the Introduction chapter, you learned that beyond big data and big compute, you can also use Spark to improve velocity, variety and veracity in data tasks. While you can use the learnings of this chapter for any task requiring loading and storing data, it is particularily interesting to present this chapter in the context of dealing with a variety of data sources. To understand why, we should first take a quick detour understanding how data is currently processed in many organizations.

For several years, it's been a common practice to store large datasets in a relational **Database**, a system first proposed in 1970 by Edgar F. Codd. You can think of a database as a collection of tables that are related to one another where each table is carefully designed to hold specific data types and relationships to other tables.  most relational database systems use SQL (Structured Query Language) for querying and maintaining the database. Databases are still wildly used today, with good reason! -- Databases store data reliably and consistently, your bank probably stores account balances in a database and that's a good practice!

However, databases have also been used to store information from other applications and systems. For instance, your bank may also store data produced by other banks, say incoming checks. In order to accomplish this, the external data needs to be extracted from the external system, transformed into something that fits the current database and finally load it. This is known as **ETL** (extract-transform-load), a general procedure of copying data from one or more sources into a destination system which represents the data differently from the source, this process became a popular in the 1970s. Aside from databases, data is often also loaded into a **Data Warehouse**, a system used for reporting and data analysis; usually stored and indexed in a format that increases data analysis speed, but which is often not suitable for modeling or running custom distributed code. However, changing a databse or data warehouse is usually a long and delicate process since data needs to be re-indexed and carefully designed transform data from multiple data sources into a common column or table that they can all share.

Instead of trying to transform all data sources into a common format, you can embrace this variety of data sources in a **Data Lake**, a system or repository of data stored in its natural format, see Figure \@ref(fig:data-data-lake). Since the data in a Data Lakes is made available in it's raw format, it is easy for anyone to use it for analysis which adds significant flexibility to organizations. Spark can then be used to unify data processing from a Data Lakes, a database or a data warehouse. Some organization also use Spark to replace their existing ETL process; however, this falls in the realm of data engineering which falls beyond the scope of this book. We've marked this with dotted lines in Figure \@ref(fig:data-data-lake) to denote systems that are not always present when using Spark with Data Lakes.

```{r data-data-lake, eval=TRUE, echo=FALSE, fig.cap='Sparl processing raw data from a Data Lake, Databases and Data Warehouses'}
render_nomnoml("
#spacing: 20
#padding: 10
[Data Lake|
  [hdfs://data.csv]
  [hdfs://data.json]
  [s3://data.parquet]
]
[Data Lake]->[Spark|
  [<note>spark_read_csv()
  spark_read_json()
  spark_read_parquet()
  ...
  spark_read_jdbc()
  spark_write_jdbc()
  ]
]
[Spark]->[Data Lake]
[Data Lake]--->[Database]
[Data Lake]--->[Data Warehouse]
[Database]--->[Spark]
[Spark]--->[Database]
[Data Warehouse]--->[Spark]
[Spark]--->[Data Warehouse]
")
```

In order to support a broad variety of data source, Spark needs to be able to read and write data in several different file formats and access them while stored in also several storage systems, we will get to this later in the chapter; but first, we will start by presenting how to read, write and copy data using Spark.

## Reading

If you are new to Spark and `sparklyr`, it is highly recommended to review this section before starting work with large data sets. We will introduce several techniques that improve the speed and efficiency of reading data. Each subsection will cover a specific way you can take advantage of how Spark reads files, such as the ability to treat entire folders as datasets, as well as being able to pass a file layout to accelerate the file read. 

### Paths

When analyzing data, loading multiple files into a single data object is a common scenario.  In R, we typically use a loop or functional programming to accomplish this. That is because R has to load each file individually into the session.

```{r}
lapply(c("data-folder/file1.csv", "data-folder/file2.csv"), read.csv)
```

In Spark, there is the notion of a folder as a dataset.  Instead of enumerating each file, simply pass the path the containing folder's path.  Spark assumes that every file in that folder is part of the same dataset. This implies that the target folder should only be used for data purposes. 

```{r}
spark_read_csv(sc, "my_data", path = "data-folder")
```

The "folder as a table" idea is found in other open source technologies as well.  Under the hood, Hive tables work the same way.  When querying a Hive table, the mapping is done over multiple files inside the same folder. The folder's name usually match the name of the table visible to the user.  

The following technique allows Spark to read file faster, as well as to reduce read failures, by passing describing the structure of a dataset. 

### Schema

When reading data, Spark is able to determine the data source's column names and column types, also known as the **schema**. However, guessing the schema comes at a cost; Spark has to do an initial pass on the data to guess the schema, and then assign it. For large dataset, this may add a significant amount of time to the data ingestion process, which can become costly even for medium size data loads. For files that are read over and over again,  the additional read time accumulates over time.

Spark allows the user to provide a column layout. If provided, Spark will bypass the step that it uses to determine the file's layout. In `sparklyr`, we can use the `column` argument to take advantage of this functionality. The `infer_schema` argument also needs to be set to `FALSE`.  This arguments is the switch that indicates if the `column` argument should be used.

```{r, echo = FALSE}
x <- data.frame(x = letters, y = 1:length(letters))
write.csv(x, "test.csv", row.names = FALSE)
rx <- readr::read_csv("test.csv", n_max = 10)
readr::spec(rx)

top_rows <- read.csv("test.csv", nrows = 5)
file_columns <- top_rows %>% 
  purrr::map(function(x)"character")

purrr::map(rx, class)

col_spec <- c("character", "numeric")
names(col_spec) <- c("x", "y")

```

For example, lets take a file called *test.csv*, and load it to Spark. This is its layout:

```
"x","y"
"a",1
"b",2
"c",3
"d",4
"e",5
```

The column spec is started with a vector containing the column types. The vector's values are named to match the field names. 

```{r}
col_spec_1 <- c("character", "numeric")
names(col_spec_1) <- c("x", "y")
col_spec_1
```
```
##           x           y 
## "character"   "numeric" 
```

The accepted variable types are: `integer`, `character`, `logical`, `double`, `numeric`, `factor`, `Date` and `POSIXct`.

In `spark_read_csv()`, `col_spec_1` is passed to the `columns` argument, and `infer_schema` is set to `FALSE`. We will try match the names and types of the original file.  Also, we will pass the column specification. Doing this will help with performance because Spark will not have to figure out the column types.

```{r}
sc <- spark_connect(master = "local")
test_1 <- spark_read_csv(sc, "test1","test.csv", 
                         columns = col_spec_1, 
                         infer_schema = FALSE)
test_1
```

```
## # Source: spark<test1> [?? x 2]
##    x         y
##    <chr> <dbl>
##  1 a         1
##  2 b         2
##  3 c         3
##  4 d         4
##  5 e         5
```

The following example shows how to set the field type to something different. However, the new field type needs a compatible type from the original. For example, a `character` field could not be set to `numeric`. If an incompatible type is used, the file read will fail with an error. Additionally, the example also changes the names of the fields.

```{r}
col_spec_2 <- c("character", "character")
names(col_spec_2) <- c("my_letter", "my_number")

test_2 <- spark_read_csv(sc, "test2","test.csv", 
                         columns = col_spec_2, 
                         infer_schema = FALSE)
test_2
```

```
# Source: spark<test2> [?? x 2]
   my_letter my_number
   <chr>     <chr>    
 1 a         1        
 2 b         2        
 3 c         3        
 4 d         4        
 5 e         5    
```

In Spark, malformed entries can cause error during reading, specially for non-character fields.  To prevent such errors, we can use a file spec that imports them as character, and then use `dplyr` to coerce the field into the desired type.

This subsection reviewed how we can read files faster and with less failures, which lets us start our analysis quicker.  Another way to accelerate our analysis, is by loading less data into Spark memory, the next subsection will cover how to do this.

### Memory

Spark copies the data into its distributed memory, which makes analyses and other processes very fast.  There are cases, such as when the data is too big, that loading all of the data may not be practical, or even necessary. For those cases, Spark can then just "map" the files without copying data into memory.  

The mapping creates a sort of "virtual" table in Spark memory.  The implication is that when a query runs against that table, Spark has to read the data from the files at that time.  Any consecutive read after that will do the same.  In effect, Spark becomes a pass-through for the data. The advantage of this method is that there is almost no up-front time cost to "reading" the file,  the mapping is very fast. The downside is that running queries that actually extract data will take longer. 

In `sparklyr`, that is controlled by the `memory` argument of its read functions. Setting it to `FALSE` prevents the data copy.  It defaults to `TRUE`.

```{r}
mapped_test <- spark_read_csv(sc, "test","test.csv", memory = FALSE)
```

There are good use cases for this method. One of them is when not all columns of a table are needed.  For example, take a very large file that contain many columns. This is not first time we interact with this data. We know what columns are needed for the analysis.  The files can be read using `memory = FALSE`, and then select the needed columns with `dplyr`. The resulting `dplyr` variable can then be cached into memory, using the `compute()` function.  This will make Spark query the file(s), pull the selected fields, and copy only that data into memory. The result is a in-memory table that took comparatively less time to ingest.

```{r}
mapped_test %>%
  select(y) %>%
  compute("test")
```

The next subsection covers a short technique to make it easier to carry the original field names of imported data. 

### Columns

Spark version 1.6 required that column names be sanitized, so `sparklyr` does that by default.  There may be cases when you would like to keep the original names intact, and are also working with Spark version 2.0 or above.  To do that set the `sparklyr.sanitize.column.names` option to `FALSE`.

```{r eval=FALSE}
options(sparklyr.sanitize.column.names = FALSE)
dplyr::copy_to(sc, iris, overwrite = TRUE)
```
```
# Source:   table<iris> [?? x 5]
# Database: spark_connection
   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
          <dbl>       <dbl>        <dbl>       <dbl> <chr>  
 1          5.1         3.5          1.4         0.2 setosa 
 2          4.9         3            1.4         0.2 setosa 
 3          4.7         3.2          1.3         0.2 setosa 
 4          4.6         3.1          1.5         0.2 setosa 
 5          5           3.6          1.4         0.2 setosa 
 6          5.4         3.9          1.7         0.4 setosa 
 7          4.6         3.4          1.4         0.3 setosa 
 8          5           3.4          1.5         0.2 setosa 
 9          4.4         2.9          1.4         0.2 setosa 
10          4.9         3.1          1.5         0.1 setosa 
# ... with more rows
```

After reviewing how to read data into Spark in this section, the next section will cover how we can write data from our Spark session. 

## Writing

Some projects require that new data generated in Spark to be written back to a remote source. For example, the data could be new predicted values returned by a Spark model. The job processes the mass generation of predictions, and the predictions need to be stored. This section will focus on when and where we should use Spark, and R, for moving the data from Spark into an external destination.

Many new users start by downloading Spark data into R, to then upload it to a target. Figure \@ref(fig:data-avoid-approach)  illustrates this approach. It works for smaller data sets, but it becomes inefficient for larger ones.  The data typically grows in size to the point that it is no longer feasible for R to be the middle point. 

```{r data-avoid-approach, echo=FALSE,  fig.cap='Avoid using R to write large datasets', fig.align = 'center', eval = TRUE}
render_nomnoml("
#direction: right
#arrowSize: 0.4
#lineWidth: 1
#spacing:90
[Avoid Writing from R| 
[Source] -> [Spark | Process] 
[Spark]collect() -> [R]
[R] -> [Target]
]
", "images/data-r-pass-through.png")
```

All efforts should be made to have Spark connect to the target location.  This way, reading, processing and writing happens within the same Spark session.  

As figure \@ref(fig:data-recommended-approach) shows, a better approach is to use Spark to read, process, and write to the target.  This approach is able to scale as big as the Spark cluster allows, and prevents using R as a choke point.

```{r data-recommended-approach, echo=FALSE,  fig.cap='Use Spark to write large datasets', fig.align = 'center', eval = TRUE}
render_nomnoml("
#direction: right
#arrowSize: 0.4
#lineWidth: 1
#spacing:90
[Source] -> [Spark | Reads -  Process - Writes] 
[Spark] -> [Target]
", "images/data-spark-pass-through.png")
```

Consider the following scenario: A Spark job just processed predictions for a large data set, resulting in a considerably large set of predictions. Choosing a method to write results will depend on the technology infrastructure you are working on. More specifically, it will depend on Spark and the target running, or not, in the same cluster.

Back to our scenario, we have a large dataset in Spark that needs to be saved.  In this case, Spark and the target Hive table are in the same cluster.  Copying the results is not a problem, since the data transfer is between RAM and disk of the same cluster. 

But what to do if the target is not Hive? In other words, Spark and the target location are not in the same cluster. There are two options, choosing one will depend on the size of the data, and network speed:  

Spark connects to the remote target location, and copies the new data
: If this is done within the same Data Center, or cloud provider, the data transfer could be fast enough to have Spark write the data directly.  

Spark writes the results locally, and transfers the results via a third-party application
: Spark could write the results as files, and then have a separate job copy the files over. In the target location, you would use a separate process to transfer the data into the target location.

It is best to recognize that Spark, R, and any other technology are tools. No tool can do everything, nor should be expected to. Next we will describe how to copy data into Spark or collect large datasets that don't fit in-memory.

## Copying

Previous chapters used `copy_to()` as a handy helper to copy data into Spark; however, `copy_to()` can only be used to transfer in-memory datasets that are already loaded in memory; this dataset tend to be much smaller than the kind of datasets you would want to copy into Spark.

FOr instance, suppose that we have a 10GB generated as follows, please consider skipping this exercise if you don't have 10GB free to spare.

```{r}
for (i in 1:10^2)
  write.table(matrix(rnorm(6 * 10^6), ncol = 6) , "large-file.txt", append = T, col.names = F, row.names = F)
```

If we only had 2GB of memory in the driver node, we would not be able to load this 10GB file into memory to use `copy_to()`. Instead, when using the Hadoop File System as storage in your cluster, you can use `hadoop` to copy files from disk into Spark from the terminal as follows:

```{bash}
hadoop fs -copyFromLocal large-file.txt large-file.txt
```

You can then read the uploaded file as described in the [File Formats](#data-file-formats) section; for text files, you would run:

```{r}
spark_read_text(sc, "large-file.txt", memory = FALSE)
```
```
# Source: spark<largefile> [?? x 1]
   line                                                                                           
   <chr>                                                                                          
 1 2.22915990048427 -0.751300322410523 1.50335086310599 0.942952304730545 -0.45635164630794 0.288…
 2 0.228991867633462 1.16010454995414 0.640434186995658 0.117977945874947 1.19056919789055 0.9088…
 3 -0.445538772866474 0.394738051630893 -0.117715444264061 0.198629317507736 1.01608719040123 -0.…
 4 -1.85412076569947 -1.16790451863855 -0.125337529817688 2.27575857254252 -0.959749634495476 0.5…
 5 -0.926000568810982 0.685026681135839 0.338987735756166 0.0410026384268559 1.24919967496763 -1.…
 6 -1.47083284717391 0.46469533950513 0.91440834672717 0.607404405425841 0.38069114223894 0.54313…
 7 -1.21654002373859 0.868008761206347 2.21517023301723 -1.52756952555848 1.11386028028205 -0.932…
 8 -0.220151773718668 1.94232643071767 -1.2031718025536 -0.918773888980239 -0.420790286116946 -2.…
 9 -0.814688544522668 -0.128353504869735 -1.22237317012823 -1.69567734449631 0.11608449215945 0.7…
10 -0.629974783817445 1.01151978562673 -0.998990775688591 -0.323199582145582 1.21506113412837 -0.…
# … with more rows
```

`collect()` has a similar limitation, it can only collect datasets that fit your driver memory; however, if you had to extract a large dataset from Spark into the driver's storage, you could use specialized tools provided by the distributed storage, for HDFS:

```{bash}
hadoop fs -copyToLocal large-file.txt large-file.txt
```

Alternatevely, you can also collect datasets that don't fit in-memory by providing a callback to `collect()`. A callback is just an R function that will be called over each Spark partition, you can then write this dataset to disk. The following code will crete 10K files adding up to 10GB even if the driver node collecting this dataseet had less than 10GB of memory. That said, as e3xplained in the [Analysis](#analysis) chapter, you should avoid collecting large datasets into a single machine.

```{r}
spark_read_text(sc, "large-file.txt", memory = FALSE) %>%
  collect(callback = function(df, idx) writeLines(df$line, paste0("large-", idx, ".txt")))
```

Since in most cases data will already be stored in the cluster, you should not have to worry about copying large-datasets; instead, you can usually focus on reading and wrtting different file formats, which we will describe next.

## File Formats {#data-file-formats}

"Out-of-the-box", Spark is able to interact with several filte formats and file system.  File formats include: Comma separated values (CSV), Apache Parquet, and JDBC.  File system protocols include: local file system (Linux, Windows, Mac), and Hadoop file System (HDFS).  

Spark can read and write several source types.  In `sparklyr`, the source types are aligned to R functions.

| Format                                        | Read                   | Write                   |
|-----------------------------------------------|------------------------|-------------------------|
| Comma separated values (CSV)                  | `spark_read_csv()`     | `spark_write_csv()`     |
| JavaScript Object Notation (JSON)             | `spark_read_json()`    | `spark_write_json()`    |
| Library for Support Vector Machines (LIBSVM)  | `spark_read_libsvm()`  | `spark_write_libsvm()`  |
| Java Database Connectivity (JDBC)             | `spark_read_jdbc()`    | `spark_write_jdbc()`    |
| Optimized Row Columnar (ORC)                  | `spark_read_orc()`     | `spark_write_orc()`     |
| Apache Parquet                                | `spark_read_parquet()` | `spark_write_parquet()` |
| Text                                          | `spark_read_text()`    | `spark_write_text()`    |

The section [File Formats](#appendix-file-formats) in the Appendix contains tips on how to read and write data from specific file formats.

### Custom

Spark is a very flexible computing platform.  It can add functionality by using extension programs, called packages. Accessing a new source type or file system can be done by using the appropriate package. 

Packages need to be loaded into Spark at connection time.  To load the package, Spark needs its location, which could be inside the cluster, in a file share or the Internet.  

In `sparklyr`, the package location is passed to `spark_connect()`.  All packages should be listed in the `defaultPackages` entry of the connection configuration. 

It is possible to access data source types not listed above.  Loading the appropriate default package for Spark is the first of two steps  The second step is to actually read or write the data. The `spark_read_source()` and `spark_write_source()` functions do that.  They are generic functions that can use the libraries imported by a default package.

The following example code shows how to use the `datastax:spark-cassandra-connector` package to read from Cassandra. The key is to use the  `org.apache.spark.sql.cassandra` library as the `source` argument.  It provides the mapping Spark can use to make sense of the data source.

```{r}
con <- spark_config()
conf$sparklyr.defaultPackages <- "datastax:spark-cassandra-connector:2.0.0-RC1-s_2.11"
sc <- spark_connect(master = "local", config = conf)
spark_read_source(
  sc, 
  name = "emp",
  source = "org.apache.spark.sql.cassandra",
  options = list(keyspace = "dev", table = "emp")
  )
```

## File Systems {#data0file-systems}

Spark will default to the file system that it is currently running on.  In a YARN managed cluster, the default file system will be HDFS. An example path of "/home/user/file.csv" will be read from cluster's HDFS folders, and not the Linux folders.  The Operating System's file system will be accessed for other deployments, such as Stand Alone, and `sparklyr`'s local. 

The file system protocol can be changed when reading or writing.  It is done via the `path` argument of the `sparklyr` function.  For example, a full path of "file://home/user/file.csv" will force the use of the local Operating System's file system.

There are other file system protocols,  is Amazon's S3 service for example.  Spark is does not know how to read the S3 protocol, so accessing the "s3a" protocol involves adding a package to the `defaultPackages` configuration variable passed at connection time.  

```{r}
conf <- spark_config()
conf$sparklyr.defaultPackages <- "org.apache.hadoop:hadoop-aws:2.7.7"
sc <- spark_connect(master = "local", config = conf)
my_file <- spark_read_csv(sc, "my-file", path =  "s3a://my-bucket/my-file.csv")
```

Currently, only "file://" and "hdfs://" file protocols are supported when used in their respective environments.  Accessing a different file protocol requires loading a default package.  In some cases, the vendor providing the Spark environment could already be loading the package for you.  Please refer to your vendor's documentation to find out if that is the case.

The section [File Systems](#appendix-file-systems) in the Appendix contains tips on how to read and write data from specific file systems

## Recap

This chapter covered how we can extend Spark's capabilities to access different kinds of source data and file formats.  We also covered several techniques to performance when reading files into Spark.  We also introduced practical principles to keep in mind when writing data from Spark.  In the next chapter, [Tuning], you will learn how Spark manages tasks and data across multiple machines, which will in turn allow you to further improve the performance of your analyses.


