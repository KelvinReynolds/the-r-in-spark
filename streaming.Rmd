```{r include=FALSE}
knitr::opts_chunk$set(eval = FALSE)
source("r/render.R")
source("r/plots.R")
library(ggplot2)
```

# Streaming {#streaming}

> "Our stories aren't over yet."
>
> --- Arya Star

We can look back at the previous eleven chapters and think, what else are we missing? We've done everything from analysing tabular datasets, performed unsupervised learning over raw text, analyzed graphs and geographic datasets and even transformd data with custom R code! So now what?

We were not explicit about this, but we've always assumed your data is static, we assumed it doesn't change over time. But suppose for a moment your job is to analize traffic patterns to give recomendations to the department of transportation. A very reasonable approach would be to analyze historical data and then design predictive models that compute forecasts overnight. Overnight? That's very useful but traffic patterns change to the hour and even by the minute. You could try to preprocess and predict faster and faster, but eventually this model breaks -- you can't load large-scale datasets, transform them, score them, unload them and repeat this process by the second.

Instead, we need to introduce a different kind of dataset, one that is not static but rather dynamic; one that is like a table but is growing constantly, we will refer to such datasets as stream.

## Overview

We know how to work with large-scale static datasets, all the previous chapters dealt with them; but how should we reason about large-scale real-time datasets? We will define as *streams* -- datasets with an infinite amount entries, as illustrated in Figure 1.1.

```{r}

```

For instance, if we wanted to do real-time scoring using a pre-trained topic model, the enries would be lines of text for static datasets; for real-time datasets, we would perform the same scoring over an infinite amount of lines of text. Now, in practice, you will never process an infinite amount of records, you will eventually stop the stream -- or this universe might end, whatever comes first -- regardless, thinking of them as infinite datasets makes it so much easier to reason about them.

Streams are most relevant when processing real time data; for example, when analyzing a Twitter feed or stock prices. Both examples have well defined columns, like 'tweet' or 'price', but there are always new rows of data to be analyzed.

Spark *Structured Streams* provide scalable and fault-torerant data processing over streams of data. That means, one can use many machines to process multiple streaming sources, perform joins with other streams or static sources, and recover from failures with at-least-once guarantees (each message is certain to be delivered, but may do so multiple times).

In Spark, you create streams by defining a *source*, a *transformation* and a *sink*; you can think of these steps as reading, transforming and writting a stream, which Figure \@ref(fig:streaming-working) describes.

Reading
: Streams read data using any of the `stream_read_*()` functions, the read operation defines the *source* of the stream; you can define one or multiple sources from which to read from.

Transforming
: A stream can perform one or multiple **transformation** using `dplyr`, `SQL`, feature transformers, scoring pipelines or distributed R code. Transformations can join against streaming sources or static datsets.

Writing
: The write operations are performed with the family of `stream_write_*()` functions, the read operation defined the *sink* of the stream. You can specify a single sink to write data to or multiple ones.

```{r streaming-working, echo=FALSE, fig.cap='Working with Spark Streams', fig.align = 'center', eval = TRUE}
render_nomnoml("
#direction: right
[Real-Time|[<note>File Source
               Kafka Source
]]->[Transform|[<note>dplyr
                SQL
                Feature Transformers
                Pipelines
                Distributed R
]]
[Static|[<note>File Systems
               Storage Systems]
]->[Transform]
[Transform]->[Sink|[<note>File Sink
                          Kafka Sink]]
", "images/streaming-working.png")
```

You can read and write to streams in several different file formats: CSV, JSON, Parquet, ORC and text; and form Kafka which we will introduce later on.

| Format  | Read                | Write                |
|---------|---------------------|----------------------|
| CSV     | stream_read_csv     | stream_write_csv     |
| JSON    | stream_read_json    | stream_write_json    |
| Kafka   | stream_read_kafka   | stream_write_kafka   |
| ORC     | stream_read_orc     | stream_write_orc     |
| Parquet | stream_read_parquet | stream_write_parquet |
| Text    | stream_read_text    | stream_write_text    |
| Memory  |                     | stream_write_memory  |

Since the transformation step is optional, the simplest stream we can define is one that continuously copies text files between source and destination.

First, install the `future` package with `install.packages("future")` and connect to Spark:

```{r streaming-overview-prepare, echo=FALSE}
library(sparklyr)
sc <- spark_connect(master = "local", version = "2.3")
```

Since a stream requires the source to exist, create a `source` folder:

```{r}
dir.create("source")
```

We are now ready to define our first stream!

```{r}
stream <- stream_read_text(sc, "source/") %>%
  stream_write_text("destination/")
```

The streams starts running with `stream_write_*()`; once executed, the stream will monitor the `source` path and process data into the `destination/` path as it arrives.

We can use `stream_generate_test()` to produce a file every second containing lines of text that follow overlapping binomial distributions, you can read more about this in the Appendix. In practice, you would connect to existing sources without having to generate data artificially. We can use `view_stream()` to track the **rows per second (rps)** being processed in the source, destination and their latest values over time:

```{r streaming-overview-test, eval=FALSE}
invisible(future::future(stream_generate_test()))

stream_view(stream)
```

Notice that the rows-per-second in the destination stream are higher than the rows-per-second in the source stream; this is expected and desireable since Spark measures incoming rates from the source, but actual row processing times in the destination stream. For example, if 10 rows-per-second are written to the `source/` path, the incoming rate is 10 RPS. However, if it takes Spark only 0.01 seconds to write all those 10 rows, the output rate is 100 RPS.

Use `stream_stop()` to properly stop processing data from this stream, good time to clear the folders the stream created to allow you to rerun this code.

```{r streaming-overview-stop, eval=FALSE}
stream_stop(stream)

unlink(c("source", "destination"))
```

This exercise introduced how we can easily start a Spark stream that reads and writes data based on a simulated stream, let's do something more interesting than just copying data with proper transformations.

## Transformations 

In a real life scenario, the incoming data from a stream would not be written as-is to the output.  The Spark Stream job would make transformations to the data, and then write the transformed data.

Streams can be transformed using `dplyr`, SQL queries, ML Pipelines or R code. We can use as many transformations as needed in the same way that Spark data frames can be transformed with `sparklyr`. 

The source of the transformation can be a stream or data frames, but the output is always a stream. If needed, one can always take a snapshot from the destination stream, and then save the output as a data frame. That is what `sparklyr` will do for you if a destination stream is not specified. 

Each sub-section will cover an option provided by `sparklyr` to perform transformations on a stream. 

### Analysis

`dplyr` verbs and SQL using `DBI` can be used on top of a Spark Stream.  The following example shows how easy it is to filter rows and add columns to data from an input folder.

```{r}
sc <- spark_connect(master = "local")
if(file.exists("source")) unlink("source", TRUE)

stream_generate_test(iterations = 5)

stream_read_csv(sc, "source") %>%
  filter(x > 700) %>%
  mutate(y = round(x / 100))
```
```
## # Source: spark<?> [inf x 2]
##        x     y
##    <int> <dbl>
##  1   701     7
##  2   702     7
##  3   703     7
##  4   704     7
##  5   705     7
##  6   706     7
##  7   707     7
##  8   708     7
##  9   709     7
## 10   710     7
## # ... with more rows
```

It also is possible to perform aggregations over the entire history of the stream.  The history could be filtered or not.
```{r}
stream_read_csv(sc, "source") %>%
  filter(x > 700) %>%
  mutate(y = round(x / 100)) %>%
  count(y) 
```
```
## # Source: spark<?> [inf x 2]
##       y     n
##   <dbl> <dbl>
## 1     8   200
## 2     9   200
## 3    10   102
## 4     7    98
```

Grouped aggregations of the latest data in the stream require a time stamp.  The time stamp will be of when reading function, in this case `stream_read_csv()` , first "saw" that specific record. In Spark stream terms, that time stamp is called a "watermark".  The `spark_watermark()` function is used to add the time stamp.  For this exercise, the watermark will be the same for all records, since the five files were read by the stream after they were created.  Please note that only Kafka and memory *outputs* support watermarks.  

```{r}
stream_read_csv(sc, "source") %>%
  stream_watermark()
```

```
## # Source: spark<?> [inf x 2]
##        x timestamp          
##    <int> <dttm>             
##  1   630 2019-04-07 15:44:50
##  2   631 2019-04-07 15:44:50
##  3   632 2019-04-07 15:44:50
##  4   633 2019-04-07 15:44:50
##  5   634 2019-04-07 15:44:50
##  6   635 2019-04-07 15:44:50
##  7   636 2019-04-07 15:44:50
##  8   637 2019-04-07 15:44:50
##  9   638 2019-04-07 15:44:50
## 10   639 2019-04-07 15:44:50
## # ... with more rows
```

After the watermark is created, it can be used in the `group_by()` verb.  It can then be piped into a `summarise()` function to get some stats of the stream.

```{r}
stream_read_csv(sc, "source") %>%
  stream_watermark() %>%
  group_by(timestamp) %>%
  summarise(
    max_x = max(x, na.rm = TRUE),
    min_x = min(x, na.rm = TRUE),
    count = n()
  ) 
```
```
## # Source: spark<?> [inf x 4]
##   timestamp           max_x min_x count
##   <dttm>              <int> <int> <dbl>
## 1 2019-04-07 15:45:59  1000     1  2122
```

### Modeling

Spark feature transformer functions can also be used to modify a stream. They can also be combined with the regular `dplyr` functions.  

```{r}
stream_read_csv(sc, "source") %>%
  mutate(x = as.numeric(x)) %>%
  ft_bucketizer("x", "buckets", splits = 0:10 * 100) %>%
  count(buckets)  %>%
  arrange(buckets)
```

```
## # Source:     spark<?> [inf x 2]
## # Ordered by: buckets
##    buckets     n
##      <dbl> <dbl>
##  1       0   299
##  2       1   220
##  3       2   200
##  4       3   200
##  5       4   200
##  6       5   200
##  7       6   201
##  8       7   200
##  9       8   200
## 10       9   202
```

### Pipelines

Spark pipelines can be used for scoring streams, but not to train over streaming data. The former is fully supported while the latter is a feature under active development by the Spark community.

The following example shows how to 

1. In order to try  scoring data in a stream, it is necessary to first create a Pipeline Model.  The following build, fits and saves a simple pipeline. It also opens and closes the Spark connection.
    ```{r}
    sc <- spark_connect(master = "local")
    cars <- copy_to(sc, mtcars, "mtcars_remote")
    sc %>%
      ml_pipeline() %>%
      ft_binarizer("mpg", "over_30",30) %>%
      ft_r_formula(over_30 ~ wt) %>%
      ml_logistic_regression() %>%
      ml_fit(cars) %>%
      ml_save("cars_model")
    spark_disconnect(sc)
    ```

1. A new connection of Spark is opened.  The saved model is loaded into the new connection.  
    ```{r}
    sc <- spark_connect(master = "local")
    model <- ml_load(sc, "cars_model")
    ```

1. Data that can be used for predictions is needed.  The `stream_generate_test()` can be used for this as well.  Instead of relying on the default output, the *mtcars* variable is passed to it.  
    ```{r}
    if(file.exists("source")) unlink("source", TRUE)
    stream_generate_test(mtcars, iterations = 5)
    ```

1. The `ml_transform()` function can now be used on top of the stream. Because the function expects the model as the first function, the piping works a little different. Instead of starting with reading the stream, we start with the model, and use the stream input as the argument on `ml_transform()`
    ```{r}
    model %>%
      ml_transform(stream_read_csv(sc, "source"))
    ```

    ```
    ## # Source: spark<?> [inf x 17]
    ##      mpg   cyl  disp    hp  drat    wt  qsec    vs    am
    ##    <dbl> <int> <dbl> <int> <dbl> <dbl> <dbl> <int> <int>
    ##  1  15.5     8 318     150  2.76  3.52  16.9     0     0
    ##  2  15.2     8 304     150  3.15  3.44  17.3     0     0
    ##  3  13.3     8 350     245  3.73  3.84  15.4     0     0
    ##  4  19.2     8 400     175  3.08  3.84  17.0     0     0
    ##  5  27.3     4  79      66  4.08  1.94  18.9     1     1
    ##  6  26       4 120.     91  4.43  2.14  16.7     0     1
    ##  7  30.4     4  95.1   113  3.77  1.51  16.9     1     1
    ##  8  15.8     8 351     264  4.22  3.17  14.5     0     1
    ##  9  19.7     6 145     175  3.62  2.77  15.5     0     1
    ## 10  15       8 301     335  3.54  3.57  14.6     0     1
    ## # ... with more rows, and 8 more variables: gear <int>,
    ## #   carb <int>, over_30 <dbl>, features <list>,
    ## #   label <dbl>, rawPrediction <list>,
    ## #   probability <list>, prediction <dbl>
    ```

This section covered the different ways to perform transformations on a stream.  Unless we are working with Kafka, there will always be a need to generate an output stream with the results of the transformations.   The next section will cover an alternative unique to `sparklyr` and R.

### Distributed R {streaming-r-code}

Arbitrary R code can also be used to transform a stream with the use of `spark_apply()`. Following the same principles from executing R code over Spark data frames, for structured streams, `spark_apply()` runs R code over each executor in the cluster where data is available, this enables processing high-throughput streams and fulfill low-latency requirements.

```{r}
stream_read_csv(sc, "source") %>%
  spark_apply(~ nrow(.x), list(n="integer"))
```
```
## # Source: spark<?> [inf x 1]
##       n
##   <int>
## 1  1962
## 2   148
## 3    12
```

## Shiny

Shiny's reactive framework is well suited to support streaming information. The idea is that your Shiny application can automatically display the latest results as fast as Spark can process them.  The `reactiveSpark()` function provides that integration. It lets us circumvent the need for writing an output.

This section's example will result in a Shiny app. It will start to accumulate and display the current and historical results. The app's output is shown on Figure \@ref(fig:streaming-shiny-app).

```{r}
library(shiny)

# Open a Spark connection and generate a test stream
sc <- spark_connect(master = "local")
if(file.exists("source")) unlink("source", TRUE)
invisible(future(stream_generate_test(interval = 0.2, iterations = 10)))
```

```{r}
# Create a simple *UI* function with one table output.
ui <- function() tableOutput("table")

# The *server* function contains a `reactiveSpark()` function.  This function reads the stream, adds the watermark and then performs the aggregation.  The results are then rendered via the *table* output.
server <- function(input, output, session){
  ps <- stream_read_csv(sc, "source")  %>%
    stream_watermark() %>%
    group_by(timestamp) %>%
    summarise(
      max_x = max(x, na.rm = TRUE),
      min_x = min(x, na.rm = TRUE),
      count = n()) %>%
    reactiveSpark()  # Spark stream reactive
  output$table <- renderTable(
    ps() %>%
      mutate(timestamp = as.character(timestamp))
  )}
```

The Shiny app can be activated with `runApp()`.

```{r}
shiny::runApp(ui, server)
```

```{r streaming-shiny-app, eval = TRUE,  fig.align = 'center', fig.cap = 'Shiny reactive', echo = FALSE}
render_image("images/streaming-shiny-app.png")
```

This section showed how easy it is to create a Shiny app that can be used for several purposes, such as monitoring, and dashboarding.  

In a more complex implementation, the source would more likely be a Kafka stream.  The next section will cover how to integrate Kafka, Spark Stream and `sparklyr`.

## Kafka

Apache Kafka is to streaming as what Hadoop is to data storage/retrieval. Hadoop provides a a distributed, resilient and reliable architecture for large-scale data storage. Kafka does the same, but for large-scale streaming applications.

The most basic Kafka workflow is illustrated on Figure \@ref(fig:streaming-kafka-apis). An application that streams data into Kafka is called a *Producer*.  Kafka stores the stream as records. Each record has a key, a value and a timestamp. Kafka can handle multiple streams that contain different information, to properly categorize each stream, Kafka uses a mechanism called topic. A topic is a alpha-numeric identifier of the stream.  A *Consumer* is an app that is external to Kafka that reads what is stored in Kafka for a given topic. The Consumer app is constantly monitoring the topic, that is called a subscription. 

```{r streaming-kafka-apis, echo=FALSE, fig.cap='Basic workflow', fig.align = 'center', eval = TRUE}
render_nomnoml("
#direction: right
#padding: 10
#fontSize: 14
#leading: 2
[Producer | Stream topic A] -> [Kafka | Store topic A]
[Kafka] -> [Consumer | Subscribe to topic A]
", "images/streaming-kafka-apis.png")
```

Kafka also allows for an application to read from one topic, process its data, and then write the results to a different topic. That is called a *Stream Processor*. In Figure \@ref(fig:streaming-kafka-two-outputs), the Stream Processor reads topic A, and then writes results to topic B. This allows for a given *Consumer* application to read results instead of “raw” feed data.

```{r streaming-kafka-two-outputs, echo=FALSE, fig.cap='Kafka workflow', fig.align = 'center', eval = TRUE}
render_nomnoml("
#direction: right
#padding: 10
#fontSize: 14
#leading: 2

[Producer | Stream topic A] -> [Kafka | Store topics A & B]
[Kafka] <-> [Stream Processor | Subscribes to topic A
Results to topic B]
[Kafka] -> [Consumer | Subscribe to topic B]

", "images/streaming-kafka-two-outputs.png")
```

Spark Streaming enables the integration of Spark with Kafka.  Spark is able to both, read from and write into Kafka topics.  This means that Spark could be a Consumer, Stream Processor or Producer application of a Kafka implementation.  

Unless there is a very specific need, using Spark as a Producer does not make much sense.  That is because Spark Streaming reacts to a stream, it doesn't generate it.  
Use Spark to read (Consumer) from one, or several, topics and then reactively write (Producer) to a different topic with the results of the analysis, all within the same Kafka cluster. This effectively makes Spark a Stream Processor.

There are nuances on how the Spark-to-Kafka write-back modes works, so it is important for us to offer some clarification.  There are three modes available: *complete*, *update* and *append*.  The *complete* mode will provide the totals for every group every time there is a new batch.  The *update* mode will provide totals for only the groups that have updates in the latest batch.  The *append* mode is able to add raw records to the target topic. This mode is not meant for aggregates, but works well for passing a filtered subset to the target topic.

The additional parameters are passed via the `options` argument.  The contents of the `options` argument are passed down to Kafka as-is. This means that the same Kafka options used in your other applications can be reused here. There are three basic Kafka options to keep in mind: `kafka.bootstrap.server`, `topic` and `subscribe`.  The former expects a list of the of one or more hosts from the Kafka cluster.  The other two set the topic that the function is either reading from or writing to.  One is used at the exclusion of the other.  For reading the `subscribe` option is used, and for writing, `topic` is used. 
    
```{r}
stream_read_kafka(
  sc, 
  options = list(
    kafka.bootstrap.server = "host1:9092, host2:9092", 
    subscribe = "topic"
    )
  ) 
```

In this example, we will setup a Producer, and a Kafka engine outside of R. Inside R, we will start a Spark Stream Processor and a Consumer. 

The *Producer* will stream random, single letters of the alphabet into Kafka. The topic will be called "letters". 

A *Stream Processor* will be built in Spark by having it read the "letters" topic, and then produce the count by unique letter passed through the stream.  The count will be passed back to Kafka in separate topic called "totals". 

To see the results, the same Spark connection will be used to setup a *Consumer* that reads the "totals" topic. Figure \@ref(fig:streaming-kafka-example) is a diagram of how this example will work. 

```{r streaming-kafka-example, echo=FALSE, fig.cap='Kafka example', fig.align = 'center', eval = TRUE}
render_nomnoml("
#direction: right
#padding: 10
#fontSize: 14
#leading: 2

[Producer | Streams 'letters'] -> [Kafka | Stores 'letters' & 'totals']
[Kafka] <-> [Stream Processor 
(Spark)| Subscribes to 'letters'
Writes results to 'totals']
[Kafka] -> [Consumer
(Spark) | Subscribes to 'totals']

", "images/streaming-kafka-example.png")
```

The example will use the `update` mode for writing back into Kafka.  This means that only the totals of the letters that changed will be sent to the "totals" topic.  The change in totals is determined after each batch from the "letters" topic is received. Figure \@ref(fig:streaming-kafka-processor) offers an deeper look of what the **Stream Processor (Spark)** process is supposed to do. 

```{r streaming-kafka-processor, echo=FALSE, fig.cap='Stream Processor - Update mode', fig.align = 'center', eval = TRUE}
render_nomnoml("
#direction: right
#padding: 10
#fontSize: 14
#leading: 2

['totals' topic |
[timestamp 1 | A B A A B C ]
[timestamp 2 | B B C C B C ]
] -> [Stream Processor | Spark Streaming]
[Stream Processor] -> ['totals' topic | 
[timestamp 1 | A=3 B=2 C=1 ] 
[timestamp 2 | B=5 C=4 ]
]
", "images/streaming-kafka-processor.png")
```

The *infrastructure* used for this example was a local, single node Kafka cluster.  The external Producer uses the Kafka command line interface (CLI) to send the stream of letters. The installation instructions that were used can be found in the Appendix under the Kafka section. 

```{r}
library(sparklyr)
library(dplyr)

config <- spark_config()
config$sparklyr.shell.packages <- "org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0"
sc <- spark_connect(master = "local", config = config)

# The local Kafka cluster is served on port 9092, by default.
hosts  <- "localhost:9092"

read_options <- list(kafka.bootstrap.servers = hosts, subscribe = "letters")
write_options <- list(kafka.bootstrap.servers = hosts, topic = "totals")

stream_read_kafka(sc, options = read_options) %>%
  mutate(value = as.character(value)) %>%     # coerce into a character
  count(value) %>%                            # group and count letters
  mutate(value = paste0(value, "=", n)) %>%   # value field expected in Kafka
  stream_write_kafka(mode = "update", options = write_options)
```

The last step starts a Spark job which will remain active until stopped or until you disconnect form Spark. At this point, there is no visible output, even if there was an active Producer sending letters over.  

A simple Shiny application can be used as a *Consumer* app. It will read the "totals" topic, select the latest count for a given letter, and then display the results on a table. 

```{r}
library(shiny)
ui <- function() tableOutput("table")
server <- function(input, output, session){
  totals_options  <- list(kafka.bootstrap.servers = hosts, subscribe = "totals")
  ps <- stream_read_kafka(sc, options = totals_options) %>%
    mutate(value = as.character(value),
           letter = substr(value, 1,1),
           total = as.numeric(substr(value, 3, 100))
           ) %>%
    group_by(letter) %>%
    summarise(total = max(total, na.rm = TRUE)) %>%
    arrange(letter) %>%
    reactiveSpark()  
  output$table <- renderTable(ps())
}
runGadget(ui, server)
```

A new terminal session is started. Kafka's CLI provides a simple Producer program that runs in the console.  Using that program, we can manually write a single letter, and then press enter.  

```
user@laptop:~/kafka_2.12-2.2.0$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic letters
>A
>B
>C
>A
>A
>C
>D
```

The Shiny reactive function will poll and refresh the results as the letters are being entered.  This is shown in figure 11.8.

```{r streaming-shiny-kafka, eval = TRUE,  fig.align = 'center', fig.cap = 'Shiny with Kafka', echo = FALSE}
render_image("images/streaming-shiny-kafka.png")
```

## Recap


