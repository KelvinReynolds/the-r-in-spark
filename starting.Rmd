\mainmatter

```{r include=FALSE, eval=TRUE}
knitr::opts_chunk$set(eval = FALSE)
source("r/render.R")
library(sparklyr)
```

# Getting Started {#starting}

From R, getting started with Spark using `sparklyr` and a local cluster is as easy as running:

```{r starting-connect}
spark_install()
sc <- spark_connect(master = "local")
```

To make sure we can all run the code above and understand it, this section will walk you through the prerequisites, installing `sparklyr` and Spark, connecting to a local Spark cluster and briefly explaining how to use Spark.

However, if a Spark cluster and R environment have been made available to you, you do not need to install the [prerequisites](#starting-prerequisites) nor [install Spark](#starting-installing-spark) yourself. Instead, you should ask for the Spark `master` parameter and connect as follows; this parameter will be formally introduced under the [clusters](#clusters) and [connections](#connections) chapters.

```{r starting-connect-master}
sc <- spark_connect(master = "<cluster-master>")
```

## Prerequisites {#starting-prerequisites}

R can run in many platforms and environments; therfore, whether you use Windows, Mac or Linux, the first step is to install R from the [r-project.org](https://r-project.org/), detailed instructions are provided in the [Installing R](#appendix-install-r) appendix.

Most people use programming languages with tools to make them more productive; for R, RStudio would be such tool. Strictly speaking, RStudio is an Integrated Development Environment (or IDE), which also happens to support many platforms and environments. We strongly recommend you get RStudio installed if you haven't done so already, see details under the [Installing RStudio](#appendix-install-rstudio) appendix.

Additionally, since Spark is built in the Scala programming language which is run by the Java Virtual Machine, you also need to install Java 8 in your system. It is likely that your system already has Java installed, but you should still check the version and update if needed as described in the [Installing Java](#appendix-install-java) appendix.

## Installing sparklyr {#starting-install-sparklyr}

As many other R packages, `sparkylr` is available to be installed from [CRAN](https://CRAN.R-project.org/package=sparklyr) and can be easily installed as follows:

```{r starting-install-sparklyr, eval=FALSE}
install.packages("sparklyr")
```

The CRAN release of `sparklyr` contains the most stable version and it's the recommended version to use; however, to try out features being developed in `sparklyr`, you can install directly from GitHub using the `devtools` package. First, install the `devtools` package and then install `sparklyr` as follows:

```{r starting-install-sparklyr-devel, eval=FALSE}
install.packages("devtools")
devtools::install_github("rstudio/sparklyr")
```

The `sparklyr` GitHub repository contains all the latest features,issues and project updates; it's the place where `sparklyr` is actevely developed and a resource that is helpful while troubleshooting issues.

```{r starting-install-github, eval=TRUE, fig.width=4, fig.align='center', echo=FALSE, fig.cap='GitHub Repository for sparklyr.'}
render_image("images/starting-sparklyr-github.png", "GitHub Repository for sparklyr.")
```

## Installing Spark {#starting-installing-spark}

Start by loading `sparklyr`, 

```{r starting-install-spark-header, warning=FALSE, message=FALSE}
library(sparklyr)
```

This will makes all `sparklyr` functions available in R, which is really helpful; otherwise, we would have to run each `sparklyr` command prefixed with `sparklyr::`.

As mentioned, Spark can be easily installed by running `spark_install()`; this will install the latest version of Spark locally in your computer, go ahead and run `spark_install()`. Notice that this command requires internet connectivity to download Spark.

```{r starting-install-spark}
spark_install()
```

All the versions of Spark that are available for installation can be displayed by running:

```{r starting-install-available, eval=TRUE}
spark_available_versions()
```

A specific version can be installed using the Spark version and, optionally, by also specifying the Hadoop version. For instance, to install Spark 1.6.3, we would run:

```{r starting-install-install-version}
spark_install(version = "1.6.3")
```

You can also check which versions are installed by running:

```{r starting-install-installed}
spark_installed_versions()
```
```
  spark hadoop                              dir
7 2.3.1    2.7 /spark/spark-2.3.1-bin-hadoop2.7
```

The path where Spark is installed is referenced as Spark's home, which is defined in R code and system configuration settings with the `SPARK_HOME` identifier. When using a local Spark cluster installed with `sparklyr`, this path is already known and no additional configuration needs to take place.

Finally, in order to uninstall an specific version of Spark you can run `spark_uninstall()` by specifying the Spark and Hadoop versions, for instance:

```{r starting-install-uninstall}
spark_uninstall(version = "2.3.1", hadoop = "2.7")
```

## Connecting to Spark {#starting-connect-to-spark}

It's important to mention that, so far, we've only installed a local Spark cluster. A local cluster is really helpful to get started, test code and troubleshoot with ease. Further chapters will explain where to find, install and connect to real Spark clusters with many machines, but for the first few chapters, we will focus on using local clusters.

To connect to this local cluster we simply run:

```{r starting-connect-local}
sc <- spark_connect(master = "local")
```

The `master` parameter identifies which is the "main" machine from the Spark cluster; this machine is often called the driver node. While working with real clusters using many machines, most machines will be worker machines and one will be the master. Since we only have a local cluster with only one machine, we will default to use `"local"` for now.

## Using Spark {#starting-sparklyr-hello-world}

Now that you are connected, we can run a few simple commands. For instance, let's start by loading some data into Apache Spark.

To accomplish this, lets first create a text file by running:

```{r starting-hello-write}
write("Hello World!", "hello.txt")
```

We can now read this text file back from Spark by running:

```{r starting-hello-read}
spark_read_text(sc, "hello", "hello.txt")
```
```
## # Source: spark<hello> [?? x 1]
##   line        
## * <chr>       
## 1 Hello World!
```

**Congrats!** You have successfully connected and loaded your first dataset into Spark.

We'll explain what's going on in `spark_read_text()`. The first parameter, `sc`, gives the function a reference to the active Spark Connection that was earlier created with `spark_connect()`. The second parameter names this dataset in Spark. The third parameter specifies a path to the file to load into Spark. Now, `spark_read_text()` returns a reference to the dataset in Spark which R automatically prints. Whenever a Spark dataset is printed, Spark will **collect** some of the records and display them for you. In this particular case, that dataset contains just one row for the line: `Hello World!`.

We will now use this simple example to present various useful tools in Spark we should get familiar with.

### Web Interface {#starting-spark-web-interface}

Most of the Spark commands are executed from the R console; however, monitoring and analyzing execution is done through Spark's web interface. This interface is a web page provided by Spark which can be accessed by running:

```{r starting-spark-web}
spark_web(sc)
```

```{r starting-spark-web-shot, echo=FALSE}
invisible(webshot::webshot(
  "http://localhost:4040/",
  "images/starting-spark-web.png",
  cliprect = c(0, 0, 992, 600),
  zoom = 2
))
```
```{r starting-spark-render, eval=TRUE, fig.width=4, fig.align='center', echo=FALSE, fig.cap='Apache Spark Web Interface.'}
render_image("images/starting-spark-web.png", "Apache Spark Web Interface.")
```

As we mentioned, printing the "hello" dataset collected a few records to be displayed in the R console. You can see in the Spark web interface that a job was started to collect this information back from Spark. You can also select the **storage** tab to see the "hello" dataset cached in-memory in Spark:

```{r starting-spark-web-storage, echo=FALSE}
invisible(webshot::webshot(
  "http://localhost:4040/storage/",
  "images/starting-spark-web-storage.png",
  cliprect = c(0, 0, 992, 300),
  zoom = 2
))
```
```{r starting-spark-web-storage-render, eval=TRUE, fig.width=4, fig.align='center', echo=FALSE, fig.cap='Apache Spark Web Interface - Storage Tab.'}
render_image("images/starting-spark-web-storage.png", "Apache Spark Web Interface - Storage Tab.")
```

The [caching](#tunning-caching) section in the [tunning](#tunning) chapter will cover this in detail, but as a start, it's worth noticing that this dataset is fully loaded into memory since the **fraction cached** is 100%, is useful also to note the **size in memory** column, which tracks the total memory being used by this dataset.

### Logs {#starting-logs}

Another common tool in Spark that you should familiarize with are the Spark logs. A log is just a text file where Spark will append information relevant to the execution of tasks in the cluster. For local clusters, we can retrieve all the logs by running:

```{r starting-logs}
spark_log(sc)
```
```
18/10/09 19:41:46 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5)...
18/10/09 19:41:46 INFO TaskSetManager: Finished task 0.0 in stage 5.0...
18/10/09 19:41:46 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose...
18/10/09 19:41:46 INFO DAGScheduler: ResultStage 5 (collect at utils...
18/10/09 19:41:46 INFO DAGScheduler: Job 3 finished: collect at utils...
```

Or we can retrieve specific log entries containing, say `sparklyr`, by using the `filter` parameter as follows:

```{r starting-logs-filter}
spark_log(sc, filter = "sparklyr")
```
```
## 18/10/09 18:53:23 INFO SparkContext: Submitted application: sparklyr
## 18/10/09 18:53:23 INFO SparkContext: Added JAR...
## 18/10/09 18:53:27 INFO Executor: Fetching spark://localhost:52930/...
## 18/10/09 18:53:27 INFO Utils: Fetching spark://localhost:52930/...
## 18/10/09 18:53:27 INFO Executor: Adding file:/private/var/folders/...
```

You won't need to read logs or filter them while using Spark, except in cases where you need to troubleshoot a failed computation; in those cases, logs are an invaluable resource to have at hand and therefore; worth introducing early on.

## Disconnecting {#starting-disconnecting}

For local clusters (really, any cluster) once you are done processing data you should disconnect by running:

```{r starting-disconnect}
spark_disconnect(sc)
```

This will terminate the connection to the cluster as well as the cluster tasks . If multiple Spark connections are active, or if the conneciton instance `sc` is no longer available, you can also disconnect all your Spark connections by running:

```{r starting-disconnect-all}
spark_disconnect_all()
```

Notice that exiting R, RStudio or restarting your R session will also cause the Spark connection to terminate, which in turn terminates the Spark cluster and cached data that is not explicitly persisted.

## Using RStudio {#starting-using-spark-from-rstudio}

Since it's very common to use RStudio with R, `sparklyr` provides RStudio extensions to help simplify your workflows and increase your productivity while using Spark in RStudio. If you are not familiar with RStudio, take a quick look at the [Using RStudio](#appendix-using-rstudio) appendix section. Otherwise, there are a couple extensions worth highlighting.

First, instead of starging a new connections using `spark_connect()` from RStudio's R console, you can use the **new connection** action from the **connections pane** and then, select the Spark connection. You can then customize the versions and connect to Spark which will simply generate the right `spark_connect()` command and execute this in the R console for you.

```{r starting-rstudio-new-connection, eval=TRUE, fig.width=4, fig.align='center', echo=FALSE, fig.cap='RStudio New Spark Connection.'}
render_image("images/starting-rstudio-new-spark-connection.png", "RStudio New Spark Connection.")
```

Second, once connected to Spark, either by using the R console or through RStudio's connections pane, RStudio will display your datasets available in the connections pane. This is a useful way to track your existing datasets and provides an easy way to explore each of them.

```{r starting-rstudio-connections-pane, eval=TRUE, fig.width=4, fig.align='center', echo=FALSE, fig.cap='RStudio Connections Pane.'}
render_image("images/starting-rstudio-connections-pane.png", "RStudio Connections Pane.")
```

Additionally, an active connection provides the following custom actions:

- **Spark UI**: Opens the Spark web interface, a shortcut to `spark_ui(sc)`.
- **Log**: Opens the Spark web logs, a shortcut to `spark_log(sc)`.
- **SQL**: Opens a new SQL query, see `DBI` and SQL support in the data [analysis](#analysis) chapter.
- **Help**: Opens the reference documentation in a new web browser window.
- **Disconnect**: Disconnects from Spark, a shortcut to `spark_disconnect(sc)`.

The rest of this book will use plain R code, it is up to you to execute this code in the R console, RStudio, Jupyter Notebooks or any other tool that support executing R code since, the code provided in this book executes in any R environment.

## Resources {#starting-resources}

While we've put significant effort into simplifying the onboarding process, there are many additional resources that can help you troubleshoot particular issues while getting started and, in general, introduce you to the broader Spark and R communities to help you get specific answers, discuss topics and get connected with many users actevely using Spark with R. 

- **Documentation**: This should be your first stop to learn more about Spark when using R. The documentation is kept up to date with examples, reference functions and many more relevant resources, [spark.rstudio.com](https://spark.rstudio.com).
- **Blog**: To keep up to date with major `sparklyr` announcements, you can follow the RStudio blog, [blog.rstudio.com/tags/sparklyr](https://blog.rstudio.com/tags/sparklyr).
- **Community**: For general `sparklyr` questions, you can post then in the RStudio Community tagged as `sparklyr`, [community.rstudio.com/tags/sparklyr](https://community.rstudio.com/tags/sparklyr).
- **Stack Overflow**: For general Spark questions, Stack Overflow is a great resource, [stackoverflow.com/questions/tagged/apache-spark](https://stackoverflow.com/questions/tagged/apache-spark); there are also many topics specifically about `sparklyr`, [stackoverflow.com/questions/tagged/sparklyr](https://stackoverflow.com/questions/tagged/sparklyr).
- **Github**: If you believe something needs to be fixed, open a GitHub issue or send us a pull request, [github.com/rstudio/sparklyr](https://github.com/rstudio/sparklyr).
- **Gitter**: For urgent issues, or to keep in touch, you can chat with us in Gitter, [gitter.im/rstudio/sparklyr](https://gitter.im/rstudio/sparklyr).

## Recap {#starting-recap}

In this chapter you learned about the prerequisites required to work with Spark, how to connect to Spark using `spark_connect()`, install a local cluster using `spark_install()`, load a simple dataset, launch the web interface and display logs using `spark_web(sc)` and `spark_log(sc)` respectively, disconnect from RStudio using `spark_disconnect()` and we closed this chapter presenting the RStudio extensions `sparklyr` provides.

It is our hope that this chapter will help anyone interested in learning cluster computing using Spark and R getting started, ready to experiment on your own and ready to tackle actual data analysis and modeling problems which, the next two chapters will introduce you. The next chapter, analysis, will present data analysis as the process to inspect, clean, and transform data with the goal of discovering useful information. Modeling can be considered part of data analysis; however, it deserves it's own chapter to truly understand and take advantage of the modeling functionality available in Spark.
