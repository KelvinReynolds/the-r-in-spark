# Clusters {#clusters}

Previous chapters focused on using Spark over a single computing instance, your personal computer. In this chapter we will introduce techniques to run Spark over multiple computing instances, also known as a computing cluster, to analyze and model data at scale.

If you already have a Spark cluster in your organization, you could consider skipping to the next chapter, [connections](#connections), which will teach you how to connect to an existing cluster.

If you don't have a cluster or are considering improvements to your existing infrastructure, this chapter will introduce the cluster trends, managers and providers available today.

## Overview {#clusters-overview}

There are three major trends in cluster computing worth discussing: **on-premise**, **cloud computing** and **kubernetes**. Framing these trends over time will help us understand how they came to be, what they are and what their future might be. The following chart takes data from Google trends and [plots these trends over time](#appendix-cluster-trends):

```{r eval=FALSE, echo=FALSE, message=FALSE, fig.align = 'center', out.width='100%', fig.cap='Google trends for on-premise (mainframe), cloud computing and kubernetes.'}
library(dplyr)
library(r2d3)

read.csv("data/clusters-trends.csv") %>%
  mutate(month = as.Date(paste(month, "-01", sep = ""))) %>%
  r2d3(script="images/clusters-trends.js")
```
![Google trends for on-premise (mainframe), cloud computing and kubernetes.](images/clusters-trends.png)

For **on-premise** clusters, someone, either yourself or someone in your organization purchased physical computers that were intended to be used for cluster computing. The computers in this cluster are made of _off-the-shelf_ hardware, meaning that someone placed an order to purchase computers usually found in stores shelves or, _high-performance_ hardware, meaning that a computing vendor provided highly customized computing hardware which also comes optimized for high-performance network connectivity, power consumption, etc. When purchasing hundreds or thousands of computing instances, it doesn't make sense to keep them in the usual computing case that we are all familiar with, but rather, it makes sense to stack them as efficient as possible on top of each other to minimize room space. This group of efficiently stacked computing instances is known as a [rack](https://en.wikipedia.org/wiki/Rack_unit). Once a cluster grows to thousands of computers, you will also need to host hundreds of racks of computing devices, at this scale, you would also need significant physical space to hosts those racks. A building that provides racks of computing instances is usually known as a _data-center_. At the scale of a data center, optimizing the building that holds them, their heating system, power supply, network connectivity, etc. becomes also relevant to optimize. In 2011, Facebook [announced](https://code.facebook.com/posts/187637061409082/building-efficient-data-centers-with-the-open-compute-project/) the [Open Compute Project](http://www.opencompute.org/) initiative which provides a set of data center blueprints free for anyone to use.

There is nothing preventing us from building our own data centers and in fact, many organizations have followed this path. For instance, Amazon started as an online book store, over the years Amazon grew to sell much more than just books and, with it's online store growth, their data centers also grew in size. In 2002, Amazon considered [selling access to virtual servers](https://en.wikipedia.org/wiki/Amazon_Web_Services#History), in their data centers to the public and, in 2004, Amazon Web Services launched as a way to let anyone rent a subset of their data centers on-demand, meaning that one did not have to purchase, configure, maintain nor teardown it's own clusters but could rather rent them from Amazon directly.

The on-demand compute model is what we know today as **Cloud Computing**. It's a concept that evolved from Amazon Web Services providing their data centers as a service. In the cloud, the cluster you use is not owned by you and it's neither in your physical building, but rather, it's a data center owned and managed by someone else. Today, there are many cloud providers in this space ranging from Amazon, Databricks, IBM, Googla, Microsoft and many others. Most cloud computing platforms provide a user interface either through a web application and command line to request and manage resources.

While the benefits of processing data in the _cloud_ were obvious for many years, picking a cloud provider had the unintended side-effect of locking organizations with one particular provider, making it hard to switch between providers or back to on-premise clusters. **Kubernetes**, announced by Google in 2014, is an [open source system for managing containerized applications across multiple hosts](https://github.com/kubernetes/kubernetes/). In practice, it provides common infrastructure otherwise proprietary to cloud providers making it much easier to deploy across multiple cloud providers and on-premise as well. However, being a much newer paradigm than on-premise or cloud computing, it is still in it's adoption phase but, nevertheless, promising for cluster computing in general and, specifically, for Apache Spark.

## Managers

In order to run Spark within a computing cluster, one needs to run something capable of initializing Spark over each compute instance, this is known as a [cluster manager](https://en.wikipedia.org/wiki/Cluster_manager). The available cluster managers in Spark are: **Spark Standalone**, **YARN**, **Mesos** and **Kubernetes**.

### Standalone {#clusters-standalone}

In **Spark Standalone**, Spark uses itself as it's own cluster manager, this means that you can use Spark without installing additional software in your cluster. This can be useful if you are planning to use your cluster to only run Spark applications; if this cluster is not dedicated to Spark, a generic cluster manager like YARN, Mesos or Kubernetes would be more suitable. The landing page for Spark Standalone is available under [spark.apache.org](https://spark.apache.org/docs/latest/spark-standalone.html) and contains detailed information on configuring, launching, monitoring and enabling high-availability.

```{r spark-standalone, eval = FALSE, fig.width = 4, fig.align = 'center', echo=FALSE, fig.cap='Spark Standalone Site.'}
knitr::include_graphics("images/clusters-spark-standalone.png")
```
![Spark Standalone Site.](images/clusters-spark-standalone.png)

However, since Spark Standalone is contained within a Spark installation; then, by completing the [getting started](#starting) chapter, you have now a local Spark installation available that we can use to initialize a local Spark Standalone cluster in a single machine. In practice, you would want to start the worker nodes in different machines but, for simplicity, we will present the code to start a standalone cluster in a single machine.

First, retrieve the `SPARK_HOME` directory by running `spark_home_dir()` then, run `start-master.sh` and `start-slave.sh` as follows:

```{r message=FALSE, eval=FALSE}
# Retrieve the Spark installation directory
spark_home <- spark_home_dir()

# Build path to start-master.sh
start_master <- file.path(spark_home, "sbin", "start-master.sh")

# Execute start-master.sh to start the cluster manager master node
system2(start_master)

# Build path to start-slave
start_slave <- file.path(spark_home, "sbin", "start-slave.sh")

# Execute start-slave.sh to start a worker and register in master node
system2(start_slave, paste0("spark://", system2("hostname", stdout = TRUE), ":7077"))
```

The previous command initialized the master node and a worker node, the master node interface can be accessed under [localhost:8080](http://localhost:8080) and looks like the following:

```{r eval=FALSE, echo=FALSE}
invisible(webshot::webshot(
  "http://localhost:8080/",
  "images/clusters-spark-standalone-web-ui.png",
  cliprect = "viewport",
  vheight = 744 * 0.7,
  zoom = 2
))
```

```{r spark-standalone-web-ui, eval = FALSE, fig.width = 4, fig.align = 'center', echo=FALSE, fig.cap='Spark Standalone Web Interface.'}
knitr::include_graphics("images/clusters-spark-standalone-web-ui.png")
```
![Spark Standalone Web Interface.](images/clusters-spark-standalone-web-ui.png)

Notice that there is one worker register in Spark standalone, you can follow the link to this worker node to see additional information:

```{r echo=FALSE, eval=FALSE}
invisible(webshot::webshot(
  "http://localhost:8081/",
  "images/clusters-spark-standalone-web-ui-worker.png",
  cliprect = "viewport",
  vheight = 744 * 0.4,
  zoom = 2
))
```

```{r spark-standalone-web-ui-worker, eval = FALSE, fig.width = 4, fig.align = 'center', echo=FALSE, fig.cap='Spark Standalone Worker Web Interface.'}
knitr::include_graphics("images/clusters-spark-standalone-web-ui-worker.png")
```
![Spark Standalone Worker Web Interface.](images/clusters-spark-standalone-web-ui-worker.png)

Once you are done performing computations in this cluster, you can simply stop all the running nodes in this local cluster by running:

```{r message=FALSE, eval=FALSE}
# Build path to stop-all
stop_all <- file.path(spark_home, "sbin", "stop-all.sh")

# Execute stop-all.sh to stop the workers and master nodes
system2(stop_all)
```

A similar approach can be followed to configure a cluster by running each `start-slave.sh` command over each machine in the cluster.

**Note:** When running on a Mac, you might hit `ssh: connect to host localhost port 22: Connection refused` and would require to manually turn off the workers using `system2("jps")` to list the running Java process and then, `system2("kill", c("-9", "<process id>"))` to stop the specific workers.

### Yarn

YARN for short, or Hadoop YARN, is the resource manager introduced in 2012 to the Hadoop project. As mentioned in in the [introduction](#introduction) chapter, Spark was built initially to speed up computation over Hadoop; then, when Hadoop 2 was launched, it introduced YARN as a component to manage resources in the cluster, to this date, it's still very common to use  Hadoop YARN with Apache Spark.

One advantage of YARN, is that it is likely to be already installed in many existing clusters that support Hadoop; which means that you can easily use Spark with many existing clusters without requesting any major changes to the existing cluster infrastructure.

YARN applications can be submitted in two modes: **yarn-client** and **yarn-cluster**. In yarn-cluster mode the driver is running remotely, while in yarn-client mode, the driver is on the machine that started the job, both modes are supported and are explained further in the [connections](#connections) chapter.

Since YARN is the cluster manager from the Hadoop project, the main documentation can be found under the [hadoop.apache.org](https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html) site, you can also reference the "Running Spark on YARN" guide from [spark.apache.org](https://spark.apache.org/docs/latest/running-on-yarn.html).

```{r hadoop-yarn, eval = FALSE, fig.width = 4, fig.align = 'center', echo=FALSE, fig.cap='Hadoop YARN Site'}
knitr::include_graphics("images/clusters-yarn.png")
```
![Hadoop YARN Site.](images/clusters-yarn.png)

### Mesos

Apache Mesos is an open-source project to manage computer clusters. Mesos began as a research project in the UC Berkeley RAD Lab by then PhD students Benjamin Hindman, Andy Konwinski, Matei Zaharia and Ion Stoica. Mesos uses Linux [Cgroups](https://en.wikipedia.org/wiki/Cgroups) to provide isolation for CPU, memory, I/O and file system.

Mesos, like YARN, supports executing many cluster frameworks, including Spark. However, one advantage particular to Mesos is that, it allows custom schedulers implemented over each cluster framework rather than having a central scheduler that can't be optimized for each particular framework. This means that Spark can optimize how resources are allocated which makes it ideal for executing many types of Spark jobs, from very short ones to very lengthy ones; therefore, connecting to a Spark cluster running Mesos will be faster and resource utilization is likely to be more efficient.

Mesos is also an Apache project with its documentaiton available under [mesos.apache.org](https://mesos.apache.org/), the "Running Spark on Mesos" guide from [spark.apache.org](https://spark.apache.org/docs/latest/running-on-mesos.html) is a great resource if you choose to use Mesos as your cluster manager.

```{r mesos-spark, eval = FALSE, fig.width = 4, fig.align = 'center', echo=FALSE, fig.cap='Mesos Landing Site.'}
knitr::include_graphics("images/clusters-mesos.png")
```
![Mesos Landing Site.](images/clusters-mesos.png)

### Kubernetes

Kubernetes is an open-source container-orchestration system for automating deployment, scaling and management of containerized applications that was originally designed by Google and now maintained by the [Cloud Native Computing Foundation](https://www.cncf.io/). Kubernetes was originally based on [Docker](https://www.docker.com/) while, like Mesos, it's also based on Linux Cgroups.

Kubernetes can also execute many cluster frameworks, it's based on container images which provide a comprehensive isolation from it's operating system, this allows a single Kubernetes cluster to be used for many different purpuses beyond data analysis, which in turn helps orgnaizations manage their compute resources with ease. However, one tradeoffs is that it adds additional overheaad and compared to Mesos, does not provide a custom scheduler, trading efficiency for convenience. Nevertheless, this convenience has proven to be instrumental to reuse cluster resources in many organizations and, as shown in the [overview](#clusters-overview) section, it's becoming the most popular cluster framework.

More resources are available under [kubernetes.io](https://kubernetes.io/) and the "Running Spark on Kubernetes" guide from [spark.apache.org](https://spark.apache.org/docs/latest/running-on-kubernetes.html) it's another place to visit to dive deeper in this topic.

```{r kubernetes-spark, eval = FALSE, fig.width = 4, fig.align = 'center', echo=FALSE, fig.cap='Kubernetes Landing Site.'}
knitr::include_graphics("images/clusters-kubernetes.png")
```
![Kubernetes Landing Site.](images/clusters-kubernetes.png)

## On-Premise

As mentioned in the overview section, on-premise clusters represent a set of computing instances procured, colocated and managed by staff members from your organization. These clusters can be highly customized and controlled; however, they can also incur significant initial expenses and maintenance costs.

One can use a cluster manager in on-premise clusters as described in the previous section; however, many organizations choose to partner with companies providing additional management software, services and resources to manage software in their cluster including, but not limited to, Apache Spark. Some of the on-premise cluster providers include: Cloudera, Hortonworks and MapR to mention a few which will be briefly introduced.

### Cloudera

Cloudera, Inc. is a United States-based software company that provides Apache Hadoop and Apache Spark-based software, support and services, and training to business customers.

Cloudera's hybrid open-source Apache Hadoop distribution, CDH (Cloudera Distribution Including Apache Hadoop), targets enterprise-class deployments of that technology. Cloudera says that more than 50% of its engineering output is donated upstream to the various Apache-licensed open source projects (Apache Hive, Apache Avro, Apache HBase, and so on) that combine to form the Apache Hadoop platform. Cloudera is also a sponsor of the Apache Software Foundation.

```{r cloudera-spark, eval = FALSE, fig.width = 4, fig.align = 'center', echo=FALSE, fig.cap='Cloudera Landing Site.'}
knitr::include_graphics("images/clusters-cloudera.png")
```
![Cloudera Landing Site.](images/clusters-cloudera.png)

### Hortonworks

Hortonworks is a big data software company based in Santa Clara, California. The company develops, supports, and provides expertise on an expansive set of entirely open source software designed to manage data and processing for everything from IOT, to advanced analytics and machine learning. Hortonworks believes it is a data management company bridging the cloud and the datacenter.

```{r hortonworks-spark, eval = FALSE, fig.width = 4, fig.align = 'center', echo=FALSE, fig.cap='Hortonworks Landing Site.'}
knitr::include_graphics("images/clusters-hortonworks.png")
```
![Hortonworks Landing Site.](images/clusters-hortonworks.png)

### MapR

MapR is a business software company headquartered in Santa Clara, California. MapR provides access to a variety of data sources from a single computer cluster, including big data workloads such as Apache Hadoop and Apache Spark, a distributed file system, a multi-model database management system, and event stream processing, combining analytics in real-time with operational applications. Its technology runs on both commodity hardware and public cloud computing services.

```{r mapr-spark, eval = FALSE, fig.width = 4, fig.align = 'center', echo=FALSE, fig.cap='MapR Landing Site.'}
knitr::include_graphics("images/clusters-mapr.png")
```
![MapR Landing Site.](images/clusters-mapr.png)

## Cloud

For those readers that don't have a cluster yet, it is likely that you will want to choose a cloud cluster, this section will briefly mention some of the major cloud infrastructure providers as a starting point to choose the right one for you.

It is worth mentioning that in a cloud service model, the compute instances are charged by the hour and times the number of instances reserved for your cluster. Since the cluster size is flexible, it is a good practice to start with small clusters and scale compute resources as needed. Even if you know in advance that a cluster of significant size will be required, starting small provides an opportunity to troubleshoot issues at a lower cost since it's unlikely that your data analysis will run at scale flawlessly on the first try.

The major providers of cloud computing infrastructure are: Amazon, Google and Microsoft that this section will briefly introduce.

### Amazon

Amazon provides cloud services through [Amazon Web Services](https://aws.amazon.com/); more specifically, they provide an on-demand Spark cluster through [Amazon Elastic MapReduce](https://aws.amazon.com/emr/) or EMR for short.

```{r amazon-emr, eval = FALSE, fig.width = 4, fig.align = 'center', echo=FALSE, fig.cap='Amazon EMR Landing Site.'}
knitr::include_graphics("images/clusters-amazon-emr.png")
```
![Amazon EMR Landing Site.](images/clusters-amazon-emr.png)

### Databricks

[Databricks](http://databricks.com) is a company founded by the creators of Apache Spark, that aims to help clients with cloud-based big data processing using Spark. Databricks grew out of the [AMPLab](https://amplab.cs.berkeley.edu/) project at University of California, Berkeley.

```{r databricks, eval = FALSE, fig.width = 4, fig.align = 'center', echo=FALSE, fig.cap='Databricks Landing Site.'}
knitr::include_graphics("images/clusters-databricks.png")
```
![Databricks Landing Site.](images/clusters-databricks.png)

### Google

Google provides their on-demand computing services through their [Google Cloud](https://cloud.google.com/), on-demand Spark cluster are provided by [Google Dataproc](https://cloud.google.com/dataproc/).

```{r google-dataproc, eval = FALSE, fig.width = 4, fig.align = 'center', echo=FALSE, fig.cap='Google Dataprox Landing Site.'}
knitr::include_graphics("images/clusters-dataproc.png")
```
![Google Dataprox Landing Site.](images/clusters-dataproc.png)

### IBM

IBM provides cloud services through [IBM Cloud](https://www.ibm.com/cloud), it supports several programming languages and services, including [Apache Spark](https://www.ibm.com/cloud/spark).

```{r ibm, eval = FALSE, fig.width = 4, fig.align = 'center', echo=FALSE, fig.cap='IBM Landing Site.'}
knitr::include_graphics("images/clusters-ibm.png")
```
![IBM Landing Site.](images/clusters-ibm.png)

### Microsoft

Microsoft provides cloud services through [Microsoft Azure](https://azure.microsoft.com/) and Spark clusters through [Azure HDInsight](https://azure.microsoft.com/en-us/services/hdinsight/).

```{r azure-hdinsight, eval = FALSE, fig.width = 4, fig.align = 'center', echo=FALSE, fig.cap='Azure HDInsight Landing Site.'}
knitr::include_graphics("images/clusters-azure.png")
```
![Azure HDInsight Landing Site.](images/clusters-azure.png)

## Tools

While using only R and Spark can be sufficient for some clusters, it is common to install complementary tools in your cluster to improve: monitoring, sql analysis, workflow coordination, etc. with applications like [Ganglia](http://ganglia.info/), [Hue](http://gethue.com/) and [Oozie](https://oozie.apache.org) respectively. This section is not meant to cover all, but rather mention two that are relevant to R and `sparklyr`.

### RStudio

RStudio's open source and professional products, like: RStudio Server, [RStudio Server Pro](https://www.rstudio.com/products/rstudio-server-pro/), [Shiny Server](https://www.rstudio.com/products/shiny/), [Shiny Server Pro](https://www.rstudio.com/products/shiny-server-pro/), or [RStudio Connect](https://www.rstudio.com/products/connect/); can be installed within the cluster to support many R workflows, while `sparklyr` does not require any additional tools, they provide significant productivity gains worth considering.

```{r rstudio-server, eval = FALSE, fig.width = 4, fig.align = 'center', echo=FALSE, fig.cap='RStudio Server.'}
knitr::include_graphics("images/clusters-rstudio-server.png")
```
![RStudio Server.](images/clusters-rstudio-server.png)

### Jupyter

Project [Jupyter](http://jupyter.org/) exists to develop open-source software, open-standards, and services for interactive computing across dozens of programming languages.

```{r jupyter-project, eval = FALSE, fig.width = 4, fig.align = 'center', echo=FALSE, fig.cap='Project Jupyter.'}
knitr::include_graphics("images/clusters-jupyter.png")
```
![Project Jupyter.](images/clusters-jupyter.png)

Their notebooks, provide support for various programming languages, including R. `sparklyr` can be used with Jupyter notebooks using the R Kernel.

### Livy {#clusters-livy}

[Apache Livy](https://livy.incubator.apache.org/) is an incubation project in Apache providing support to use Spark clusters remotely through a web interface. It is ideal to connect directly into the Spark cluster; however, there are times where connecting directly to the cluster is not feasible. When facing those constraints, one can consider installing Livy in their cluster and secure it properly to enable remote use over web protocols.

However, there is a significant performance overhead from using Livy in `sparklyr` for experimentation, meaning that, executing many client comamnds over Livy has a significant overhead; however, running a few commands to generate complex analysis is usually performant since the performance overhead of starting computation can be insignificant compared to the actual cluster computation.

```{r apache-livy, eval = FALSE, fig.width = 4, fig.align = 'center', echo=FALSE, fig.cap='Apache Livy Landing Site.'}
knitr::include_graphics("images/clusters-apache-livy.png")
```
![Apache Livy Landing Site.](images/clusters-apache-livy.png)

To help test Livy locally, `sparklyr` provides support to list, install, start and stop a local Livy instance by executing:

```{r echo=FALSE}
library(sparklyr)
```

```{r}
# List versions of Livy available to install
livy_available_versions()
```

```{r eval=FALSE}
# Install default Livy version
livy_install()

# List installed Livy services
livy_installed_versions()

# Start the Livy service
livy_service_start()

# Stops the Livy service
livy_service_stop()
```

The default address for this local Livy service is http://localhost:8998

## Recap

This chapter explained the history and tradeoffs of on-premise, cloud computing and presented Kubernetes as a promising framework to provide flexibility across on-premise and cloud providers. It also introduced cluster managers (Spark Standalone, YARN, Mesos and Kubernetes) as the software needed to run Spark as a cluster application. This chapter briefly mentioned on-premise cluster providers like Cloudera, Hortonworks and MapR as well as the major cloud providers: Amazon, Google and Microsoft.

While this chapter provided a solid foundation to understand current cluster computing trends,  tools and providers useful to perform data science at scale; it did not provide a comprehensive framework to decide which cluster technologies to choose. Instead, use this chapter as an overview and a starting point to reach out to additional resources to complement your understanding the cluster stack that best fits your organization needs.

The next chapter, [connections](#connections), will focus on understanding how to connect to existing clusters; therefore, it assumes a Spark cluster like the ones presented in this chapter, is already available to you.
