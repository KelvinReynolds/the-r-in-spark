<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 8 Data | Mastering Apache Spark with R</title>
  <meta name="description" content="The complete guide to large-scale analysis and modeling.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 8 Data | Mastering Apache Spark with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The complete guide to large-scale analysis and modeling." />
  <meta name="github-repo" content="javierluraschi/the-r-in-spark" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Data | Mastering Apache Spark with R" />
  
  <meta name="twitter:description" content="The complete guide to large-scale analysis and modeling." />
  

<meta name="author" content="Javier Luraschi, Kevin Kuo, Edgar Ruiz">


<meta name="date" content="2019-05-14">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="connections.html">
<link rel="next" href="tuning.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.3/htmlwidgets.js"></script>
<script src="libs/dagre-0.0.1/dagre.min.js"></script>
<script src="libs/lodash-3.7.0/lodash.js"></script>
<script src="libs/nomnoml-0.2.0/nomnoml.js"></script>
<script src="libs/nomnoml-binding-0.1.0/nomnoml.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-119986300-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-119986300-1');
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./">Learning Apache Spark with R</a></strong></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#intro-background"><i class="fa fa-check"></i><b>1.1</b> Information</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#intro-hadoop"><i class="fa fa-check"></i><b>1.2</b> Hadoop</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#intro-spark"><i class="fa fa-check"></i><b>1.3</b> Spark</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#intro-r"><i class="fa fa-check"></i><b>1.4</b> R</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#intro-sparklyr"><i class="fa fa-check"></i><b>1.5</b> sparklyr</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#intro-recap"><i class="fa fa-check"></i><b>1.6</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="starting.html"><a href="starting.html"><i class="fa fa-check"></i><b>2</b> Getting Started</a><ul>
<li class="chapter" data-level="2.1" data-path="starting.html"><a href="starting.html#starting-prerequisites"><i class="fa fa-check"></i><b>2.1</b> Prerequisites</a></li>
<li class="chapter" data-level="2.2" data-path="starting.html"><a href="starting.html#starting-install-sparklyr"><i class="fa fa-check"></i><b>2.2</b> Installing sparklyr</a></li>
<li class="chapter" data-level="2.3" data-path="starting.html"><a href="starting.html#starting-installing-spark"><i class="fa fa-check"></i><b>2.3</b> Installing Spark</a></li>
<li class="chapter" data-level="2.4" data-path="starting.html"><a href="starting.html#starting-connect-to-spark"><i class="fa fa-check"></i><b>2.4</b> Connecting to Spark</a></li>
<li class="chapter" data-level="2.5" data-path="starting.html"><a href="starting.html#starting-sparklyr-hello-world"><i class="fa fa-check"></i><b>2.5</b> Using Spark</a><ul>
<li class="chapter" data-level="2.5.1" data-path="starting.html"><a href="starting.html#starting-spark-web-interface"><i class="fa fa-check"></i><b>2.5.1</b> Web Interface</a></li>
<li class="chapter" data-level="2.5.2" data-path="starting.html"><a href="starting.html#starting-analysis"><i class="fa fa-check"></i><b>2.5.2</b> Analysis</a></li>
<li class="chapter" data-level="2.5.3" data-path="starting.html"><a href="starting.html#starting-modeling"><i class="fa fa-check"></i><b>2.5.3</b> Modeling</a></li>
<li class="chapter" data-level="2.5.4" data-path="starting.html"><a href="starting.html#starting-data"><i class="fa fa-check"></i><b>2.5.4</b> Data</a></li>
<li class="chapter" data-level="2.5.5" data-path="starting.html"><a href="starting.html#starting-extensions"><i class="fa fa-check"></i><b>2.5.5</b> Extensions</a></li>
<li class="chapter" data-level="2.5.6" data-path="starting.html"><a href="starting.html#starting-distributed-r"><i class="fa fa-check"></i><b>2.5.6</b> Distributed R</a></li>
<li class="chapter" data-level="2.5.7" data-path="starting.html"><a href="starting.html#starting-streaming"><i class="fa fa-check"></i><b>2.5.7</b> Streaming</a></li>
<li class="chapter" data-level="2.5.8" data-path="starting.html"><a href="starting.html#starting-logs"><i class="fa fa-check"></i><b>2.5.8</b> Logs</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="starting.html"><a href="starting.html#starting-disconnecting"><i class="fa fa-check"></i><b>2.6</b> Disconnecting</a></li>
<li class="chapter" data-level="2.7" data-path="starting.html"><a href="starting.html#starting-using-spark-from-rstudio"><i class="fa fa-check"></i><b>2.7</b> Using RStudio</a></li>
<li class="chapter" data-level="2.8" data-path="starting.html"><a href="starting.html#starting-resources"><i class="fa fa-check"></i><b>2.8</b> Resources</a></li>
<li class="chapter" data-level="2.9" data-path="starting.html"><a href="starting.html#starting-recap"><i class="fa fa-check"></i><b>2.9</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="analysis.html"><a href="analysis.html"><i class="fa fa-check"></i><b>3</b> Analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="analysis.html"><a href="analysis.html#search-for-an-answer"><i class="fa fa-check"></i><b>3.1</b> Search for an answer</a></li>
<li class="chapter" data-level="3.2" data-path="analysis.html"><a href="analysis.html#r-as-an-interface-to-spark"><i class="fa fa-check"></i><b>3.2</b> R as an interface to Spark</a></li>
<li class="chapter" data-level="3.3" data-path="analysis.html"><a href="analysis.html#exercise"><i class="fa fa-check"></i><b>3.3</b> Exercise</a></li>
<li class="chapter" data-level="3.4" data-path="analysis.html"><a href="analysis.html#import-access"><i class="fa fa-check"></i><b>3.4</b> Import / Access</a></li>
<li class="chapter" data-level="3.5" data-path="analysis.html"><a href="analysis.html#wrangle"><i class="fa fa-check"></i><b>3.5</b> Wrangle</a><ul>
<li class="chapter" data-level="3.5.1" data-path="analysis.html"><a href="analysis.html#correlations"><i class="fa fa-check"></i><b>3.5.1</b> Correlations</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="analysis.html"><a href="analysis.html#visualize"><i class="fa fa-check"></i><b>3.6</b> Visualize</a><ul>
<li class="chapter" data-level="3.6.1" data-path="analysis.html"><a href="analysis.html#recommended-approach"><i class="fa fa-check"></i><b>3.6.1</b> Recommended approach</a></li>
<li class="chapter" data-level="3.6.2" data-path="analysis.html"><a href="analysis.html#simple-plots"><i class="fa fa-check"></i><b>3.6.2</b> Simple plots</a></li>
<li class="chapter" data-level="3.6.3" data-path="analysis.html"><a href="analysis.html#histograms"><i class="fa fa-check"></i><b>3.6.3</b> Histograms</a></li>
<li class="chapter" data-level="3.6.4" data-path="analysis.html"><a href="analysis.html#scatter-plots"><i class="fa fa-check"></i><b>3.6.4</b> Scatter plots</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="analysis.html"><a href="analysis.html#model"><i class="fa fa-check"></i><b>3.7</b> Model</a><ul>
<li class="chapter" data-level="3.7.1" data-path="analysis.html"><a href="analysis.html#models-during-analysis"><i class="fa fa-check"></i><b>3.7.1</b> Models during analysis</a></li>
<li class="chapter" data-level="3.7.2" data-path="analysis.html"><a href="analysis.html#cache-model-data"><i class="fa fa-check"></i><b>3.7.2</b> Cache model data</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="analysis.html"><a href="analysis.html#communicate"><i class="fa fa-check"></i><b>3.8</b> Communicate</a><ul>
<li class="chapter" data-level="3.8.1" data-path="analysis.html"><a href="analysis.html#analysis-versus-production-work"><i class="fa fa-check"></i><b>3.8.1</b> Analysis versus Production work</a></li>
<li class="chapter" data-level="3.8.2" data-path="analysis.html"><a href="analysis.html#using-r-markdown-documents"><i class="fa fa-check"></i><b>3.8.2</b> Using R Markdown documents</a></li>
<li class="chapter" data-level="3.8.3" data-path="analysis.html"><a href="analysis.html#reporting-results"><i class="fa fa-check"></i><b>3.8.3</b> Reporting results</a></li>
<li class="chapter" data-level="3.8.4" data-path="analysis.html"><a href="analysis.html#presentation-decks"><i class="fa fa-check"></i><b>3.8.4</b> Presentation decks</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="analysis.html"><a href="analysis.html#recap"><i class="fa fa-check"></i><b>3.9</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="modeling.html"><a href="modeling.html"><i class="fa fa-check"></i><b>4</b> Modeling</a><ul>
<li class="chapter" data-level="4.1" data-path="modeling.html"><a href="modeling.html#overview"><i class="fa fa-check"></i><b>4.1</b> Overview</a></li>
<li class="chapter" data-level="4.2" data-path="modeling.html"><a href="modeling.html#the-data"><i class="fa fa-check"></i><b>4.2</b> The Data</a></li>
<li class="chapter" data-level="4.3" data-path="modeling.html"><a href="modeling.html#exploratory-data-analysis"><i class="fa fa-check"></i><b>4.3</b> Exploratory Data Analysis</a></li>
<li class="chapter" data-level="4.4" data-path="modeling.html"><a href="modeling.html#feature-engineering"><i class="fa fa-check"></i><b>4.4</b> Feature Engineering</a></li>
<li class="chapter" data-level="4.5" data-path="modeling.html"><a href="modeling.html#model-building"><i class="fa fa-check"></i><b>4.5</b> Model Building</a><ul>
<li class="chapter" data-level="4.5.1" data-path="modeling.html"><a href="modeling.html#logistic-regression-as-a-generalized-linear-regression"><i class="fa fa-check"></i><b>4.5.1</b> Logistic Regression as a Generalized Linear Regression</a></li>
<li class="chapter" data-level="4.5.2" data-path="modeling.html"><a href="modeling.html#more-machine-learning-algorithms"><i class="fa fa-check"></i><b>4.5.2</b> More Machine Learning Algorithms</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="modeling.html"><a href="modeling.html#working-with-textual-data"><i class="fa fa-check"></i><b>4.6</b> Working with Textual Data</a><ul>
<li class="chapter" data-level="4.6.1" data-path="modeling.html"><a href="modeling.html#data-prep"><i class="fa fa-check"></i><b>4.6.1</b> Data Prep</a></li>
<li class="chapter" data-level="4.6.2" data-path="modeling.html"><a href="modeling.html#topic-modeling"><i class="fa fa-check"></i><b>4.6.2</b> Topic Modeling</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="modeling.html"><a href="modeling.html#conclusion"><i class="fa fa-check"></i><b>4.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="pipelines.html"><a href="pipelines.html"><i class="fa fa-check"></i><b>5</b> Pipelines</a><ul>
<li class="chapter" data-level="5.1" data-path="pipelines.html"><a href="pipelines.html#estimators-and-transformers"><i class="fa fa-check"></i><b>5.1</b> Estimators and Transformers</a></li>
<li class="chapter" data-level="5.2" data-path="pipelines.html"><a href="pipelines.html#pipelines-and-pipeline-models"><i class="fa fa-check"></i><b>5.2</b> Pipelines and Pipeline Models</a></li>
<li class="chapter" data-level="5.3" data-path="pipelines.html"><a href="pipelines.html#applying-pipelines-to-okcupid-data"><i class="fa fa-check"></i><b>5.3</b> Applying Pipelines to OKCupid Data</a><ul>
<li class="chapter" data-level="5.3.1" data-path="pipelines.html"><a href="pipelines.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>5.3.1</b> Hyperparameter Tuning</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="pipelines.html"><a href="pipelines.html#operating-modes-of-pipelines-functions"><i class="fa fa-check"></i><b>5.4</b> Operating Modes of Pipelines Functions</a></li>
<li class="chapter" data-level="5.5" data-path="pipelines.html"><a href="pipelines.html#model-persistence-and-interoperability"><i class="fa fa-check"></i><b>5.5</b> Model Persistence and Interoperability</a><ul>
<li class="chapter" data-level="5.5.1" data-path="pipelines.html"><a href="pipelines.html#sparklyr-ml-models"><i class="fa fa-check"></i><b>5.5.1</b> Sparklyr ML Models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="clusters.html"><a href="clusters.html"><i class="fa fa-check"></i><b>6</b> Clusters</a><ul>
<li class="chapter" data-level="6.1" data-path="clusters.html"><a href="clusters.html#clusters-overview"><i class="fa fa-check"></i><b>6.1</b> Overview</a></li>
<li class="chapter" data-level="6.2" data-path="clusters.html"><a href="clusters.html#on-premise"><i class="fa fa-check"></i><b>6.2</b> On-Premise</a><ul>
<li class="chapter" data-level="6.2.1" data-path="clusters.html"><a href="clusters.html#clusters-manager"><i class="fa fa-check"></i><b>6.2.1</b> Managers</a></li>
<li class="chapter" data-level="6.2.2" data-path="clusters.html"><a href="clusters.html#distributions"><i class="fa fa-check"></i><b>6.2.2</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="clusters.html"><a href="clusters.html#cloud"><i class="fa fa-check"></i><b>6.3</b> Cloud</a><ul>
<li class="chapter" data-level="6.3.1" data-path="clusters.html"><a href="clusters.html#clusters-amazon-emr"><i class="fa fa-check"></i><b>6.3.1</b> Amazon</a></li>
<li class="chapter" data-level="6.3.2" data-path="clusters.html"><a href="clusters.html#databricks"><i class="fa fa-check"></i><b>6.3.2</b> Databricks</a></li>
<li class="chapter" data-level="6.3.3" data-path="clusters.html"><a href="clusters.html#google"><i class="fa fa-check"></i><b>6.3.3</b> Google</a></li>
<li class="chapter" data-level="6.3.4" data-path="clusters.html"><a href="clusters.html#ibm"><i class="fa fa-check"></i><b>6.3.4</b> IBM</a></li>
<li class="chapter" data-level="6.3.5" data-path="clusters.html"><a href="clusters.html#microsoft"><i class="fa fa-check"></i><b>6.3.5</b> Microsoft</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="clusters.html"><a href="clusters.html#kubernetes"><i class="fa fa-check"></i><b>6.4</b> Kubernetes</a></li>
<li class="chapter" data-level="6.5" data-path="clusters.html"><a href="clusters.html#tools"><i class="fa fa-check"></i><b>6.5</b> Tools</a><ul>
<li class="chapter" data-level="6.5.1" data-path="clusters.html"><a href="clusters.html#rstudio"><i class="fa fa-check"></i><b>6.5.1</b> RStudio</a></li>
<li class="chapter" data-level="6.5.2" data-path="clusters.html"><a href="clusters.html#jupyter"><i class="fa fa-check"></i><b>6.5.2</b> Jupyter</a></li>
<li class="chapter" data-level="6.5.3" data-path="clusters.html"><a href="clusters.html#clusters-livy"><i class="fa fa-check"></i><b>6.5.3</b> Livy</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="clusters.html"><a href="clusters.html#recap-1"><i class="fa fa-check"></i><b>6.6</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="connections.html"><a href="connections.html"><i class="fa fa-check"></i><b>7</b> Connections</a><ul>
<li class="chapter" data-level="7.1" data-path="connections.html"><a href="connections.html#connections-overview"><i class="fa fa-check"></i><b>7.1</b> Overview</a><ul>
<li class="chapter" data-level="7.1.1" data-path="connections.html"><a href="connections.html#connections-spark-edge-nodes"><i class="fa fa-check"></i><b>7.1.1</b> Edge Nodes</a></li>
<li class="chapter" data-level="7.1.2" data-path="connections.html"><a href="connections.html#connections-spark-home"><i class="fa fa-check"></i><b>7.1.2</b> Spark Home</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="connections.html"><a href="connections.html#connections-local"><i class="fa fa-check"></i><b>7.2</b> Local</a></li>
<li class="chapter" data-level="7.3" data-path="connections.html"><a href="connections.html#connections-standalone"><i class="fa fa-check"></i><b>7.3</b> Standalone</a></li>
<li class="chapter" data-level="7.4" data-path="connections.html"><a href="connections.html#connections-yarn"><i class="fa fa-check"></i><b>7.4</b> Yarn</a><ul>
<li class="chapter" data-level="7.4.1" data-path="connections.html"><a href="connections.html#connections-yarn-client"><i class="fa fa-check"></i><b>7.4.1</b> Yarn Client</a></li>
<li class="chapter" data-level="7.4.2" data-path="connections.html"><a href="connections.html#connections-yarn-cluster"><i class="fa fa-check"></i><b>7.4.2</b> Yarn Cluster</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="connections.html"><a href="connections.html#connections-livy"><i class="fa fa-check"></i><b>7.5</b> Livy</a></li>
<li class="chapter" data-level="7.6" data-path="connections.html"><a href="connections.html#connections-mesos"><i class="fa fa-check"></i><b>7.6</b> Mesos</a></li>
<li class="chapter" data-level="7.7" data-path="connections.html"><a href="connections.html#connections-kubernetes"><i class="fa fa-check"></i><b>7.7</b> Kubernetes</a></li>
<li class="chapter" data-level="7.8" data-path="connections.html"><a href="connections.html#cloud-1"><i class="fa fa-check"></i><b>7.8</b> Cloud</a></li>
<li class="chapter" data-level="7.9" data-path="connections.html"><a href="connections.html#batches"><i class="fa fa-check"></i><b>7.9</b> Batches</a></li>
<li class="chapter" data-level="7.10" data-path="connections.html"><a href="connections.html#tools-1"><i class="fa fa-check"></i><b>7.10</b> Tools</a></li>
<li class="chapter" data-level="7.11" data-path="connections.html"><a href="connections.html#multiple"><i class="fa fa-check"></i><b>7.11</b> Multiple</a></li>
<li class="chapter" data-level="7.12" data-path="connections.html"><a href="connections.html#connections-troubleshooting"><i class="fa fa-check"></i><b>7.12</b> Troubleshooting</a><ul>
<li class="chapter" data-level="7.12.1" data-path="connections.html"><a href="connections.html#logging"><i class="fa fa-check"></i><b>7.12.1</b> Logging</a></li>
<li class="chapter" data-level="7.12.2" data-path="connections.html"><a href="connections.html#troubleshoot-spark-submit"><i class="fa fa-check"></i><b>7.12.2</b> Spark Submit</a></li>
<li class="chapter" data-level="7.12.3" data-path="connections.html"><a href="connections.html#windows"><i class="fa fa-check"></i><b>7.12.3</b> Windows</a></li>
</ul></li>
<li class="chapter" data-level="7.13" data-path="connections.html"><a href="connections.html#recap-2"><i class="fa fa-check"></i><b>7.13</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>8</b> Data</a><ul>
<li class="chapter" data-level="8.1" data-path="data.html"><a href="data.html#source-types-and-file-systems"><i class="fa fa-check"></i><b>8.1</b> Source types and file systems</a><ul>
<li class="chapter" data-level="8.1.1" data-path="data.html"><a href="data.html#default-packages"><i class="fa fa-check"></i><b>8.1.1</b> Default packages</a></li>
<li class="chapter" data-level="8.1.2" data-path="data.html"><a href="data.html#source-types"><i class="fa fa-check"></i><b>8.1.2</b> Source types</a></li>
<li class="chapter" data-level="8.1.3" data-path="data.html"><a href="data.html#file-systems"><i class="fa fa-check"></i><b>8.1.3</b> File systems</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="data.html"><a href="data.html#reading-data"><i class="fa fa-check"></i><b>8.2</b> Reading data</a><ul>
<li class="chapter" data-level="8.2.1" data-path="data.html"><a href="data.html#folders-as-a-table"><i class="fa fa-check"></i><b>8.2.1</b> Folders as a table</a></li>
<li class="chapter" data-level="8.2.2" data-path="data.html"><a href="data.html#file-layout"><i class="fa fa-check"></i><b>8.2.2</b> File layout</a></li>
<li class="chapter" data-level="8.2.3" data-path="data.html"><a href="data.html#spark-memory"><i class="fa fa-check"></i><b>8.2.3</b> Spark memory</a></li>
<li class="chapter" data-level="8.2.4" data-path="data.html"><a href="data.html#column-names"><i class="fa fa-check"></i><b>8.2.4</b> Column Names</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="data.html"><a href="data.html#writing-data"><i class="fa fa-check"></i><b>8.3</b> Writing Data</a><ul>
<li class="chapter" data-level="8.3.1" data-path="data.html"><a href="data.html#spark-not-r-as-pass-through"><i class="fa fa-check"></i><b>8.3.1</b> Spark, not R, as pass-through</a></li>
<li class="chapter" data-level="8.3.2" data-path="data.html"><a href="data.html#practical-approach"><i class="fa fa-check"></i><b>8.3.2</b> Practical approach</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="data.html"><a href="data.html#date-time"><i class="fa fa-check"></i><b>8.4</b> Date &amp; time</a></li>
<li class="chapter" data-level="8.5" data-path="data.html"><a href="data.html#specific-types-and-protocols"><i class="fa fa-check"></i><b>8.5</b> Specific types and protocols</a><ul>
<li class="chapter" data-level="8.5.1" data-path="data.html"><a href="data.html#amazon-s3"><i class="fa fa-check"></i><b>8.5.1</b> Amazon S3</a></li>
<li class="chapter" data-level="8.5.2" data-path="data.html"><a href="data.html#sql"><i class="fa fa-check"></i><b>8.5.2</b> SQL</a></li>
<li class="chapter" data-level="8.5.3" data-path="data.html"><a href="data.html#hive"><i class="fa fa-check"></i><b>8.5.3</b> Hive</a></li>
<li class="chapter" data-level="8.5.4" data-path="data.html"><a href="data.html#comma-delimited-values-csv"><i class="fa fa-check"></i><b>8.5.4</b> Comma Delimited Values (CSV)</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="data.html"><a href="data.html#recap-3"><i class="fa fa-check"></i><b>8.6</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="tuning.html"><a href="tuning.html"><i class="fa fa-check"></i><b>9</b> Tuning</a><ul>
<li class="chapter" data-level="9.1" data-path="tuning.html"><a href="tuning.html#overview-1"><i class="fa fa-check"></i><b>9.1</b> Overview</a><ul>
<li class="chapter" data-level="9.1.1" data-path="tuning.html"><a href="tuning.html#tuning-graph-visualization"><i class="fa fa-check"></i><b>9.1.1</b> Graph</a></li>
<li class="chapter" data-level="9.1.2" data-path="tuning.html"><a href="tuning.html#tuning-event-timeline"><i class="fa fa-check"></i><b>9.1.2</b> Timeline</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="tuning.html"><a href="tuning.html#tuning-configuring"><i class="fa fa-check"></i><b>9.2</b> Configuring</a><ul>
<li class="chapter" data-level="9.2.1" data-path="tuning.html"><a href="tuning.html#connect-settings"><i class="fa fa-check"></i><b>9.2.1</b> Connect Settings</a></li>
<li class="chapter" data-level="9.2.2" data-path="tuning.html"><a href="tuning.html#submit-settings"><i class="fa fa-check"></i><b>9.2.2</b> Submit Settings</a></li>
<li class="chapter" data-level="9.2.3" data-path="tuning.html"><a href="tuning.html#runtime-settings"><i class="fa fa-check"></i><b>9.2.3</b> Runtime Settings</a></li>
<li class="chapter" data-level="9.2.4" data-path="tuning.html"><a href="tuning.html#sparklyr-settings"><i class="fa fa-check"></i><b>9.2.4</b> sparklyr Settings</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="tuning.html"><a href="tuning.html#tuning-partitioning"><i class="fa fa-check"></i><b>9.3</b> Partitioning</a><ul>
<li class="chapter" data-level="9.3.1" data-path="tuning.html"><a href="tuning.html#implicit"><i class="fa fa-check"></i><b>9.3.1</b> Implicit</a></li>
<li class="chapter" data-level="9.3.2" data-path="tuning.html"><a href="tuning.html#explicit"><i class="fa fa-check"></i><b>9.3.2</b> Explicit</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="tuning.html"><a href="tuning.html#tuning-caching"><i class="fa fa-check"></i><b>9.4</b> Caching</a><ul>
<li class="chapter" data-level="9.4.1" data-path="tuning.html"><a href="tuning.html#checkpointing"><i class="fa fa-check"></i><b>9.4.1</b> Checkpointing</a></li>
<li class="chapter" data-level="9.4.2" data-path="tuning.html"><a href="tuning.html#tuning-memory"><i class="fa fa-check"></i><b>9.4.2</b> Memory</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="tuning.html"><a href="tuning.html#tuning-shuffling"><i class="fa fa-check"></i><b>9.5</b> Shuffling</a></li>
<li class="chapter" data-level="9.6" data-path="tuning.html"><a href="tuning.html#tuning-serialization"><i class="fa fa-check"></i><b>9.6</b> Serialization</a></li>
<li class="chapter" data-level="9.7" data-path="tuning.html"><a href="tuning.html#configuration-files"><i class="fa fa-check"></i><b>9.7</b> Configuration Files</a></li>
<li class="chapter" data-level="9.8" data-path="tuning.html"><a href="tuning.html#recap-4"><i class="fa fa-check"></i><b>9.8</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="extensions.html"><a href="extensions.html"><i class="fa fa-check"></i><b>10</b> Extensions</a><ul>
<li class="chapter" data-level="10.1" data-path="extensions.html"><a href="extensions.html#rsparkling"><i class="fa fa-check"></i><b>10.1</b> RSparkling</a><ul>
<li class="chapter" data-level="10.1.1" data-path="extensions.html"><a href="extensions.html#troubleshooting"><i class="fa fa-check"></i><b>10.1.1</b> Troubleshooting</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="extensions.html"><a href="extensions.html#graphframes"><i class="fa fa-check"></i><b>10.2</b> GraphFrames</a></li>
<li class="chapter" data-level="10.3" data-path="extensions.html"><a href="extensions.html#mleap"><i class="fa fa-check"></i><b>10.3</b> Mleap</a></li>
<li class="chapter" data-level="10.4" data-path="extensions.html"><a href="extensions.html#extensions-nested-data"><i class="fa fa-check"></i><b>10.4</b> Nested Data</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="distributed.html"><a href="distributed.html"><i class="fa fa-check"></i><b>11</b> Distributed R</a><ul>
<li class="chapter" data-level="11.1" data-path="distributed.html"><a href="distributed.html#overview-2"><i class="fa fa-check"></i><b>11.1</b> Overview</a></li>
<li class="chapter" data-level="11.2" data-path="distributed.html"><a href="distributed.html#use-cases"><i class="fa fa-check"></i><b>11.2</b> Use Cases</a><ul>
<li class="chapter" data-level="11.2.1" data-path="distributed.html"><a href="distributed.html#custom-parsers"><i class="fa fa-check"></i><b>11.2.1</b> Custom Parsers</a></li>
<li class="chapter" data-level="11.2.2" data-path="distributed.html"><a href="distributed.html#partitioned-modeling"><i class="fa fa-check"></i><b>11.2.2</b> Partitioned Modeling</a></li>
<li class="chapter" data-level="11.2.3" data-path="distributed.html"><a href="distributed.html#distributed-grid-search"><i class="fa fa-check"></i><b>11.2.3</b> Grid Search</a></li>
<li class="chapter" data-level="11.2.4" data-path="distributed.html"><a href="distributed.html#web-apis"><i class="fa fa-check"></i><b>11.2.4</b> Web APIs</a></li>
<li class="chapter" data-level="11.2.5" data-path="distributed.html"><a href="distributed.html#distributed-rendering"><i class="fa fa-check"></i><b>11.2.5</b> Distributed Rendering</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="distributed.html"><a href="distributed.html#partitions"><i class="fa fa-check"></i><b>11.3</b> Partitions</a></li>
<li class="chapter" data-level="11.4" data-path="distributed.html"><a href="distributed.html#grouping"><i class="fa fa-check"></i><b>11.4</b> Grouping</a></li>
<li class="chapter" data-level="11.5" data-path="distributed.html"><a href="distributed.html#columns"><i class="fa fa-check"></i><b>11.5</b> Columns</a></li>
<li class="chapter" data-level="11.6" data-path="distributed.html"><a href="distributed.html#context"><i class="fa fa-check"></i><b>11.6</b> Context</a></li>
<li class="chapter" data-level="11.7" data-path="distributed.html"><a href="distributed.html#functions"><i class="fa fa-check"></i><b>11.7</b> Functions</a></li>
<li class="chapter" data-level="11.8" data-path="distributed.html"><a href="distributed.html#packages"><i class="fa fa-check"></i><b>11.8</b> Packages</a></li>
<li class="chapter" data-level="11.9" data-path="distributed.html"><a href="distributed.html#cluster-requirements"><i class="fa fa-check"></i><b>11.9</b> Cluster Requirements</a><ul>
<li class="chapter" data-level="11.9.1" data-path="distributed.html"><a href="distributed.html#installing-r"><i class="fa fa-check"></i><b>11.9.1</b> Installing R</a></li>
<li class="chapter" data-level="11.9.2" data-path="distributed.html"><a href="distributed.html#apache-arrow"><i class="fa fa-check"></i><b>11.9.2</b> Apache Arrow</a></li>
</ul></li>
<li class="chapter" data-level="11.10" data-path="distributed.html"><a href="distributed.html#troubleshooting-1"><i class="fa fa-check"></i><b>11.10</b> Troubleshooting</a><ul>
<li class="chapter" data-level="11.10.1" data-path="distributed.html"><a href="distributed.html#worker-logs"><i class="fa fa-check"></i><b>11.10.1</b> Worker Logs</a></li>
<li class="chapter" data-level="11.10.2" data-path="distributed.html"><a href="distributed.html#resolving-timeouts"><i class="fa fa-check"></i><b>11.10.2</b> Resolving Timeouts</a></li>
<li class="chapter" data-level="11.10.3" data-path="distributed.html"><a href="distributed.html#inspecting-partition"><i class="fa fa-check"></i><b>11.10.3</b> Inspecting Partition</a></li>
<li class="chapter" data-level="11.10.4" data-path="distributed.html"><a href="distributed.html#debugging-workers"><i class="fa fa-check"></i><b>11.10.4</b> Debugging Workers</a></li>
</ul></li>
<li class="chapter" data-level="11.11" data-path="distributed.html"><a href="distributed.html#recap-5"><i class="fa fa-check"></i><b>11.11</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="streaming.html"><a href="streaming.html"><i class="fa fa-check"></i><b>12</b> Streaming</a><ul>
<li class="chapter" data-level="12.1" data-path="streaming.html"><a href="streaming.html#spark-streaming"><i class="fa fa-check"></i><b>12.1</b> Spark Streaming</a></li>
<li class="chapter" data-level="12.2" data-path="streaming.html"><a href="streaming.html#working-with-spark-streams"><i class="fa fa-check"></i><b>12.2</b> Working with Spark Streams</a></li>
<li class="chapter" data-level="12.3" data-path="streaming.html"><a href="streaming.html#sparklyr-extras"><i class="fa fa-check"></i><b>12.3</b> <code>sparklyr</code> extras</a><ul>
<li class="chapter" data-level="12.3.1" data-path="streaming.html"><a href="streaming.html#stream-monitor"><i class="fa fa-check"></i><b>12.3.1</b> Stream monitor</a></li>
<li class="chapter" data-level="12.3.2" data-path="streaming.html"><a href="streaming.html#stream-generator"><i class="fa fa-check"></i><b>12.3.2</b> Stream generator</a></li>
<li class="chapter" data-level="12.3.3" data-path="streaming.html"><a href="streaming.html#shiny-reactive"><i class="fa fa-check"></i><b>12.3.3</b> Shiny reactive</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="streaming.html"><a href="streaming.html#intro-example"><i class="fa fa-check"></i><b>12.4</b> Intro example</a></li>
<li class="chapter" data-level="12.5" data-path="streaming.html"><a href="streaming.html#transformations"><i class="fa fa-check"></i><b>12.5</b> Transformations</a><ul>
<li class="chapter" data-level="12.5.1" data-path="streaming.html"><a href="streaming.html#dplyr"><i class="fa fa-check"></i><b>12.5.1</b> dplyr</a></li>
<li class="chapter" data-level="12.5.2" data-path="streaming.html"><a href="streaming.html#transformer-functions"><i class="fa fa-check"></i><b>12.5.2</b> Transformer functions</a></li>
<li class="chapter" data-level="12.5.3" data-path="streaming.html"><a href="streaming.html#r-code"><i class="fa fa-check"></i><b>12.5.3</b> R code</a></li>
<li class="chapter" data-level="12.5.4" data-path="streaming.html"><a href="streaming.html#ml-pipelines"><i class="fa fa-check"></i><b>12.5.4</b> ML Pipelines</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="streaming.html"><a href="streaming.html#shiny-integration"><i class="fa fa-check"></i><b>12.6</b> Shiny integration</a></li>
<li class="chapter" data-level="12.7" data-path="streaming.html"><a href="streaming.html#kafka"><i class="fa fa-check"></i><b>12.7</b> Kafka</a><ul>
<li class="chapter" data-level="12.7.1" data-path="streaming.html"><a href="streaming.html#workflow"><i class="fa fa-check"></i><b>12.7.1</b> Workflow</a></li>
<li class="chapter" data-level="12.7.2" data-path="streaming.html"><a href="streaming.html#spark-integration"><i class="fa fa-check"></i><b>12.7.2</b> Spark integration</a></li>
<li class="chapter" data-level="12.7.3" data-path="streaming.html"><a href="streaming.html#r-integration"><i class="fa fa-check"></i><b>12.7.3</b> R integration</a></li>
<li class="chapter" data-level="12.7.4" data-path="streaming.html"><a href="streaming.html#example"><i class="fa fa-check"></i><b>12.7.4</b> Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="contributing.html"><a href="contributing.html"><i class="fa fa-check"></i><b>13</b> Contributing</a><ul>
<li class="chapter" data-level="13.1" data-path="contributing.html"><a href="contributing.html#contributing-overview"><i class="fa fa-check"></i><b>13.1</b> Overview</a></li>
<li class="chapter" data-level="13.2" data-path="contributing.html"><a href="contributing.html#contributing-spark-api"><i class="fa fa-check"></i><b>13.2</b> Spark API</a></li>
<li class="chapter" data-level="13.3" data-path="contributing.html"><a href="contributing.html#spark-extensions"><i class="fa fa-check"></i><b>13.3</b> Spark Extensions</a></li>
<li class="chapter" data-level="13.4" data-path="contributing.html"><a href="contributing.html#scala-code"><i class="fa fa-check"></i><b>13.4</b> Scala Code</a></li>
<li class="chapter" data-level="13.5" data-path="contributing.html"><a href="contributing.html#recap-6"><i class="fa fa-check"></i><b>13.5</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>14</b> Appendix</a><ul>
<li class="chapter" data-level="14.1" data-path="appendix.html"><a href="appendix.html#appendix-prerequisites"><i class="fa fa-check"></i><b>14.1</b> Prerequisites</a><ul>
<li class="chapter" data-level="14.1.1" data-path="appendix.html"><a href="appendix.html#appendix-install-r"><i class="fa fa-check"></i><b>14.1.1</b> Installing R</a></li>
<li class="chapter" data-level="14.1.2" data-path="appendix.html"><a href="appendix.html#appendix-install-java"><i class="fa fa-check"></i><b>14.1.2</b> Installing Java</a></li>
<li class="chapter" data-level="14.1.3" data-path="appendix.html"><a href="appendix.html#appendix-install-rstudio"><i class="fa fa-check"></i><b>14.1.3</b> Installing RStudio</a></li>
<li class="chapter" data-level="14.1.4" data-path="appendix.html"><a href="appendix.html#appendix-using-rstudio"><i class="fa fa-check"></i><b>14.1.4</b> Using RStudio</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="appendix.html"><a href="appendix.html#diagrams"><i class="fa fa-check"></i><b>14.2</b> Diagrams</a><ul>
<li class="chapter" data-level="14.2.1" data-path="appendix.html"><a href="appendix.html#appendix-storage-capacity"><i class="fa fa-check"></i><b>14.2.1</b> Worlds Store Capacity</a></li>
<li class="chapter" data-level="14.2.2" data-path="appendix.html"><a href="appendix.html#appendix-cran-downloads"><i class="fa fa-check"></i><b>14.2.2</b> Daily downloads of CRAN packages</a></li>
<li class="chapter" data-level="14.2.3" data-path="appendix.html"><a href="appendix.html#appendix-cluster-trends"><i class="fa fa-check"></i><b>14.2.3</b> Google trends for mainframes, cloud computing and kubernetes</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="appendix.html"><a href="appendix.html#appendix-ggplot2-theme"><i class="fa fa-check"></i><b>14.3</b> Formatting</a></li>
<li class="chapter" data-level="14.4" data-path="appendix.html"><a href="appendix.html#ml-functionlist"><i class="fa fa-check"></i><b>14.4</b> List of ML Functions</a><ul>
<li class="chapter" data-level="14.4.1" data-path="appendix.html"><a href="appendix.html#classification"><i class="fa fa-check"></i><b>14.4.1</b> Classification</a></li>
<li class="chapter" data-level="14.4.2" data-path="appendix.html"><a href="appendix.html#regression"><i class="fa fa-check"></i><b>14.4.2</b> Regression</a></li>
<li class="chapter" data-level="14.4.3" data-path="appendix.html"><a href="appendix.html#clustering"><i class="fa fa-check"></i><b>14.4.3</b> Clustering</a></li>
<li class="chapter" data-level="14.4.4" data-path="appendix.html"><a href="appendix.html#recommendation"><i class="fa fa-check"></i><b>14.4.4</b> Recommendation</a></li>
<li class="chapter" data-level="14.4.5" data-path="appendix.html"><a href="appendix.html#frequent-pattern-mining"><i class="fa fa-check"></i><b>14.4.5</b> Frequent Pattern Mining</a></li>
<li class="chapter" data-level="14.4.6" data-path="appendix.html"><a href="appendix.html#feature-transformers"><i class="fa fa-check"></i><b>14.4.6</b> Feature Transformers</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="appendix.html"><a href="appendix.html#kafka-1"><i class="fa fa-check"></i><b>14.5</b> Kafka</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>15</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Mastering Apache Spark with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="data" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Data</h1>
<p>The goal of this chapter is to help you learn how to access, read and write data using Spark. It will provide the necessary background to help you work with a variety of data.</p>
<p>This chapter will cover how to access data in different source types and file systems. It will show you the pattern of how to extend Spark’s capabilities to work with data no accessible “out-of-the-box”.</p>
<p>Additionally, this chapter will introduce several recommendations. The recommendations will focused on improving performance and efficiency for writing or reading data.</p>
<div id="source-types-and-file-systems" class="section level2">
<h2><span class="header-section-number">8.1</span> Source types and file systems</h2>
<p>It may be challenging accessing data for the first time. The likely reasons are problems with a new source type, or file system.</p>
<p>“Out-of-the-box”, Spark is able to interact with several source types and file system. Source types include: Comma separated values (CSV), Apache Parquet, and JDBC. File system protocols include: local file system (Linux, Windows, Mac), and Hadoop file System (HDFS).</p>
<p>There is a way for Spark to interact with other source types and file systems. The next sub section will cover how to do that.</p>
<div id="default-packages" class="section level3">
<h3><span class="header-section-number">8.1.1</span> Default packages</h3>
<p>Spark is a very flexible computing platform. It can add functionality by using extension programs, called packages. Accessing a new source type or file system can be done by using the appropriate package.</p>
<p>Packages need to be loaded into Spark at connection time. To load the package, Spark needs its location. The location could be inside the cluster, in a file share or the Internet.</p>
<p>In <code>sparklyr</code>, the package location is passed to <code>spark_connect()</code>. All packages should be listed in the <code>defaultPackages</code> entry of the connection configuration. Here is an example that loads the package needed to access Amazon S3 buckets:</p>
<pre class="sourceCode r"><code class="sourceCode r">conf &lt;-<span class="st"> </span><span class="kw">spark_config</span>()
conf<span class="op">$</span>sparklyr.defaultPackages &lt;-<span class="st"> &quot;org.apache.hadoop:hadoop-aws:2.7.7&quot;</span>
sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>, <span class="dt">config =</span> conf)</code></pre>
</div>
<div id="source-types" class="section level3">
<h3><span class="header-section-number">8.1.2</span> Source types</h3>
<p>Spark can read and write several source types. In <code>sparklyr</code>, the source types are aligned to R functions:</p>
<table>
<colgroup>
<col width="48%" />
<col width="25%" />
<col width="26%" />
</colgroup>
<thead>
<tr class="header">
<th>Format</th>
<th>Read</th>
<th>Write</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Comma separated values (CSV)</td>
<td><code>spark_read_csv()</code></td>
<td><code>spark_write_csv()</code></td>
</tr>
<tr class="even">
<td>JavaScript Object Notation (JSON)</td>
<td><code>spark_read_json()</code></td>
<td><code>spark_write_json()</code></td>
</tr>
<tr class="odd">
<td>Library for Support Vector Machines (LIBSVM)</td>
<td><code>spark_read_libsvm()</code></td>
<td><code>spark_write_libsvm()</code></td>
</tr>
<tr class="even">
<td>Java Database Connectivity (JDBC)</td>
<td><code>spark_read_jdbc()</code></td>
<td><code>spark_write_jdbc()</code></td>
</tr>
<tr class="odd">
<td>Optimized Row Columnar (ORC)</td>
<td><code>spark_read_orc()</code></td>
<td><code>spark_write_orc()</code></td>
</tr>
<tr class="even">
<td>Apache Parquet</td>
<td><code>spark_read_parquet()</code></td>
<td><code>spark_write_parquet()</code></td>
</tr>
<tr class="odd">
<td>Text</td>
<td><code>spark_read_text()</code></td>
<td><code>spark_write_text()</code></td>
</tr>
</tbody>
</table>
<div id="new-source-type" class="section level4">
<h4><span class="header-section-number">8.1.2.1</span> New Source Type</h4>
<p>It is possible to access data source types not listed above. Loading the appropriate default package for Spark is the first of two steps The second step is to actually read or write the data. The <code>spark_read_source()</code> and <code>spark_write_source()</code> functions do that. They are generic functions that can use the libraries imported by a default package.</p>
<p>The following example code shows how to use the <code>datastax:spark-cassandra-connector</code> package to read from Cassandra. The key is to use the <code>org.apache.spark.sql.cassandra</code> library as the <code>source</code> argument. It provides the mapping Spark can use to make sense of the data source.</p>
<pre class="sourceCode r"><code class="sourceCode r">con &lt;-<span class="st"> </span><span class="kw">spark_config</span>()
conf<span class="op">$</span>sparklyr.defaultPackages &lt;-<span class="st"> &quot;datastax:spark-cassandra-connector:2.0.0-RC1-s_2.11&quot;</span>
sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>, <span class="dt">config =</span> conf)
<span class="kw">spark_read_source</span>(
  sc, 
  <span class="dt">name =</span> <span class="st">&quot;emp&quot;</span>,
  <span class="dt">source =</span> <span class="st">&quot;org.apache.spark.sql.cassandra&quot;</span>,
  <span class="dt">options =</span> <span class="kw">list</span>(<span class="dt">keyspace =</span> <span class="st">&quot;dev&quot;</span>, <span class="dt">table =</span> <span class="st">&quot;emp&quot;</span>)
  )</code></pre>
</div>
</div>
<div id="file-systems" class="section level3">
<h3><span class="header-section-number">8.1.3</span> File systems</h3>
<p>Spark will default to the file system that it is currently running on. In a YARN managed cluster, the default file system will be HDFS. An example path of “/home/user/file.csv” will be read from cluster’s HDFS folders, and not the Linux folders. The Operating System’s file system will be accessed for other deployments, such as Stand Alone, and <code>sparklyr</code>’s local.</p>
<p>The file system protocol can be changed when reading or writing. It is done via the <code>path</code> argument of the <code>sparklyr</code> function. For example, a full path of “<a href="file://home/user/file.csv" class="uri">file://home/user/file.csv</a>” will force the use of the local Operating System’s file system.</p>
<p>There are other file system protocols. An example is Amazon’s S3 service. Spark is does not know how to read the S3 protocol. Accessing the “s3a” protocol involves adding a package to the <code>defaultPackages</code> configuration variable passed at connection time.</p>
<pre class="sourceCode r"><code class="sourceCode r">conf &lt;-<span class="st"> </span><span class="kw">spark_config</span>()
conf<span class="op">$</span>sparklyr.defaultPackages &lt;-<span class="st"> &quot;org.apache.hadoop:hadoop-aws:2.7.7&quot;</span>
sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>, <span class="dt">config =</span> conf)
my_file &lt;-<span class="st"> </span><span class="kw">spark_read_csv</span>(sc, <span class="st">&quot;my-file&quot;</span>, <span class="dt">path =</span>  <span class="st">&quot;s3a://my-bucket/my-file.csv&quot;</span>)</code></pre>
<p>Currently, only “<a href="file://" class="uri">file://</a>” and “hdfs://” file protocols are supported when used in their respective environments. Accessing a different file protocol requires loading a default package. In some cases, the vendor providing the Spark environment could already be loading the package for you.</p>
</div>
</div>
<div id="reading-data" class="section level2">
<h2><span class="header-section-number">8.2</span> Reading data</h2>
<p>This section will introduce several techniques that improve the speed and efficiency of reading data. If new to Spark and <code>sparklyr</code>, it is highly recommended to review this section before starting work with large data sets.</p>
<div id="folders-as-a-table" class="section level3">
<h3><span class="header-section-number">8.2.1</span> Folders as a table</h3>
<p>Loading multiple files into a single data object is a common scenario. In R, we typically use a loop or functional programming to accomplish this.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">lapply</span>(<span class="kw">c</span>(<span class="st">&quot;data-folder/file1.csv&quot;</span>, <span class="st">&quot;data-folder/file2.csv&quot;</span>), read.csv)</code></pre>
<p>In Spark, there is the notion of a folder as a table. Instead of enumerating each file, simply pass the path the containing folder’s path. Spark assumes that every file in that folder is part of the same table. This implies that the target folder should only be used for data purposes.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">spark_read_csv</span>(sc, <span class="st">&quot;my_data&quot;</span>, <span class="dt">path =</span> <span class="st">&quot;data-folder&quot;</span>)</code></pre>
<p>The folder as a table notion is also found in other open source technologies. Under the hood, Hive tables work the same way. When querying a Hive table, the mapping is done over multiple files inside the same folder. The folder’s name usually match the name of the table visible to the user.</p>
</div>
<div id="file-layout" class="section level3">
<h3><span class="header-section-number">8.2.2</span> File layout</h3>
<p>When reading data, Spark is able to determine the data source’s column names and types. This comes at a cost. To determine the type Spark has to do an initial pass on the data, and then assign a type. For large data, this may add a significant amount of time to the data ingestion process. This can become costly even for medium size data loads. For files that are read over and over again, the additional read time accumulates over time.</p>
<p>Spark allows the user to provide a column layout. If provided, Spark will bypass the step that it uses to determine the file’s layout. In <code>sparklyr</code>, the <code>column</code> argument is used to take advantage of this functionality. The <code>infer_schema</code> argument also needs to be set to <code>FALSE</code>. This arguments is the switch that indicates if the <code>column</code> argument should be used.</p>
<p>For example, a file called <em>test.csv</em> is going to be loaded to Spark. This is its layout:</p>
<pre><code>&quot;x&quot;,&quot;y&quot;
&quot;a&quot;,1
&quot;b&quot;,2
&quot;c&quot;,3
&quot;d&quot;,4
&quot;e&quot;,5</code></pre>
<p>The column spec is started with a vector containing the column types. The vector’s values are named to match the field names.</p>
<pre class="sourceCode r"><code class="sourceCode r">col_spec_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;character&quot;</span>, <span class="st">&quot;numeric&quot;</span>)
<span class="kw">names</span>(col_spec_<span class="dv">1</span>) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;x&quot;</span>, <span class="st">&quot;y&quot;</span>)
col_spec_<span class="dv">1</span></code></pre>
<pre><code>##           x           y 
## &quot;character&quot;   &quot;numeric&quot; </code></pre>
<p>The accepted variable types are:</p>
<ul>
<li><p><code>integer</code></p></li>
<li><p><code>character</code></p></li>
<li><p><code>logical</code></p></li>
<li><p><code>double</code></p></li>
<li><p><code>numeric</code></p></li>
<li><p><code>factor</code></p></li>
<li><p><code>Date</code></p></li>
<li><p><code>POSIXct</code></p></li>
</ul>
<p>In <code>spark_read_csv()</code>, <code>col_spec_1</code> is passed to the <code>columns</code> argument, and <code>infer_schema</code> is set to <code>FALSE</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>)
test_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">spark_read_csv</span>(sc, <span class="st">&quot;test1&quot;</span>,<span class="st">&quot;test.csv&quot;</span>, 
                         <span class="dt">columns =</span> col_spec_<span class="dv">1</span>, 
                         <span class="dt">infer_schema =</span> <span class="ot">FALSE</span>)
test_<span class="dv">1</span></code></pre>
<pre><code>## # Source: spark&lt;test1&gt; [?? x 2]
##    x         y
##    &lt;chr&gt; &lt;dbl&gt;
##  1 a         1
##  2 b         2
##  3 c         3
##  4 d         4
##  5 e         5</code></pre>
<p>In the example we tried to match the names and types of the original file. The ability to pass a column spec provides additional flexibility. The following example shows how to set the field type to something different. The new field type needs a compatible type. For example, a <code>character</code> field could not be set tp <code>numeric</code>. The example also changes the names of the fields.</p>
<pre class="sourceCode r"><code class="sourceCode r">col_spec_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;character&quot;</span>, <span class="st">&quot;character&quot;</span>)
<span class="kw">names</span>(col_spec_<span class="dv">2</span>) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;my_letter&quot;</span>, <span class="st">&quot;my_number&quot;</span>)

test_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">spark_read_csv</span>(sc, <span class="st">&quot;test2&quot;</span>,<span class="st">&quot;test.csv&quot;</span>, 
                         <span class="dt">columns =</span> col_spec_<span class="dv">2</span>, 
                         <span class="dt">infer_schema =</span> <span class="ot">FALSE</span>)
test_<span class="dv">2</span></code></pre>
<pre><code># Source: spark&lt;test2&gt; [?? x 2]
   my_letter my_number
   &lt;chr&gt;     &lt;chr&gt;    
 1 a         1        
 2 b         2        
 3 c         3        
 4 d         4        
 5 e         5    </code></pre>
<p>The ability to change the field type can be very useful. Malformed entries can cause error during reading. This is common in non-character fields. The practical approach is to import the field as a character field, and then use <code>dplyr</code> to coerce the field’s conversion.</p>
</div>
<div id="spark-memory" class="section level3">
<h3><span class="header-section-number">8.2.3</span> Spark memory</h3>
<p>Spark copies the data into its distributed memory. This makes analyses and other processes very fast. There are cases where loading all of the data may not be practical, or necessary. For those cases, Spark can then just “map” the files without copying data into memory.</p>
<p>The mapping creates a sort of “virtual” table in Spark memory. The implication is that when a query runs against that table, Spark has to read the data from the files at that time. Any consecutive read after that will do the same. In effect, Spark becomes a pass-through for the data. The advantage of this method is that there is almost no up-front time cost to “reading” the file. The mapping process is comparatively fast.</p>
<p>In <code>sparklyr</code>, that is controlled by the <code>memory</code> argument of its read functions. Setting it to <code>FALSE</code> prevents the data copy. It defaults to <code>TRUE</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">mapped_test &lt;-<span class="st"> </span><span class="kw">spark_read_csv</span>(sc, <span class="st">&quot;test&quot;</span>,<span class="st">&quot;test.csv&quot;</span>, <span class="dt">memory =</span> <span class="ot">FALSE</span>)</code></pre>
<p>There are good use cases for this method. One of them is when not all columns of a table are needed. For example, take a very large file that contain many columns. This is not first time we interact with this data. We know what columns are needed for the analysis. The files can be read using <code>memory = FALSE</code>, and then select the needed columns with <code>dplyr</code>. The resulting <code>dplyr</code> variable can then be cached into memory, using the <code>compute()</code> function. This will make Spark query the file(s), pull the selected fields, and copy only that data into memory. The result is a in-memory table that took comparatively less time to ingest.</p>
<pre class="sourceCode r"><code class="sourceCode r">mapped_test <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(y) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">compute</span>(<span class="st">&quot;test&quot;</span>)</code></pre>
</div>
<div id="column-names" class="section level3">
<h3><span class="header-section-number">8.2.4</span> Column Names</h3>
<p>By default, <code>sparklyr</code> sanitizes column names. It translates characters such as <code>.</code> to <code>_</code>. This was required in Spark 1.6.X. To disable this functionality, you can run the following code:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">options</span>(<span class="dt">sparklyr.sanitize.column.names =</span> <span class="ot">FALSE</span>)
dplyr<span class="op">::</span><span class="kw">copy_to</span>(sc, iris, <span class="dt">overwrite =</span> <span class="ot">TRUE</span>)</code></pre>
<pre><code># Source:   table&lt;iris&gt; [?? x 5]
# Database: spark_connection
   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  
 1          5.1         3.5          1.4         0.2 setosa 
 2          4.9         3            1.4         0.2 setosa 
 3          4.7         3.2          1.3         0.2 setosa 
 4          4.6         3.1          1.5         0.2 setosa 
 5          5           3.6          1.4         0.2 setosa 
 6          5.4         3.9          1.7         0.4 setosa 
 7          4.6         3.4          1.4         0.3 setosa 
 8          5           3.4          1.5         0.2 setosa 
 9          4.4         2.9          1.4         0.2 setosa 
10          4.9         3.1          1.5         0.1 setosa 
# ... with more rows</code></pre>
</div>
</div>
<div id="writing-data" class="section level2">
<h2><span class="header-section-number">8.3</span> Writing Data</h2>
<p>Some projects require that new data generated in Spark to be written back to a remote source. For example, the data could be new predicted values returned by a Spark model. The job processes the mass generation of predictions, and the predictions need to be stored. This section will cover recommendations when working on such projects.</p>
<div id="spark-not-r-as-pass-through" class="section level3">
<h3><span class="header-section-number">8.3.1</span> Spark, not R, as pass-through</h3>
<p>Avoid collecting data in R to then upload it to the target (see Figure 8.1) That seems to be the first approach attempted by new users. It may look like a faster alternative. Performance wise, it is not faster. Additionally, this approach will not scale properly. The data will eventually grow to the point where R cannot handle being the middle point.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-169"></span>
<div id="htmlwidget-bd262b9929a5a3c48e68" style="width:672px;height:480px;" class="nomnoml html-widget"></div>
<script type="application/json" data-for="htmlwidget-bd262b9929a5a3c48e68">{"x":{"code":"\n  #fill: #FEFEFF\n  #lineWidth: 2\n  #zoom: 4\n  #direction: right\n   \n\n#direction: right\n#arrowSize: 0.4\n#lineWidth: 1\n#spacing:90\n[Avoid this...| \n[Source] -> [Spark | Process] \n[Spark]collect() -> [R]\n[R] -> [Target]\n]\n","className":null,"svg":false},"evals":[],"jsHooks":[]}</script>
<p class="caption">
FIGURE 8.1: Avoid using R as a pass through
</p>
</div>
<p>All efforts should be made to have Spark connect to the target location. This way, reading, processing and writing all happens within the same Spark session.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-170"></span>
<div id="htmlwidget-0c9382884b0dae8a7fca" style="width:672px;height:480px;" class="nomnoml html-widget"></div>
<script type="application/json" data-for="htmlwidget-0c9382884b0dae8a7fca">{"x":{"code":"\n  #fill: #FEFEFF\n  #lineWidth: 2\n  #zoom: 4\n  #direction: right\n   \n\n#direction: right\n#arrowSize: 0.4\n#lineWidth: 1\n#spacing:90\n[Source] -> [Spark | Reads -  Process - Writes] \n[Spark] -> [Target]\n","className":null,"svg":false},"evals":[],"jsHooks":[]}</script>
<p class="caption">
FIGURE 8.2: Spark as a pass through
</p>
</div>
</div>
<div id="practical-approach" class="section level3">
<h3><span class="header-section-number">8.3.2</span> Practical approach</h3>
<p>Consider the following use scenario: A Spark job just processed predictions for a large data set. The data size of only the predictions are also considerable. Choosing a method to write results will depend on infrastructure.</p>
<p>For example, Spark and the target location share the same infrastructure. For example, Spark and the target Hive table are in the same cluster. Copying the results is not a problem. The data transfer is between RAM and disk of the same cluster.</p>
<p>A contrasting example, Spark and the target location are not in the same infrastructure. There are two options, choosing one will depend on the size of the data, and network speed:</p>
<ul>
<li><p><em>Spark connects to the remote target location, and copy the new data</em> If this is done within the same Data Center, or cloud provider, the data transfer could be fast enough to have Spark write the data directly.</p></li>
<li><p><em>Spark writes the results locally, and transfer the results via a third-party application</em> For example, Spark could write the results into CSV files, and then have a separate job copy the files over via FTP. In the target location, use a separate process to transfer the data into the target location. Spark, R, and any other technology are tools. It is best to recognize that one tool cannot, and should not be expected to, do everything.</p></li>
</ul>
</div>
</div>
<div id="date-time" class="section level2">
<h2><span class="header-section-number">8.4</span> Date &amp; time</h2>
<p>Some Spark date/time functions make timezone assumptions. For instance, the following code makes use of <code>to_date()</code>. It assumes that the timestamp will be given in the local time zone. This is not to discourage use of date/time functions. Please be aware of time zones to be handled with care.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sdf_len</span>(sc, <span class="dv">1</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">transmute</span>(
    <span class="dt">date =</span> <span class="kw">timestamp</span>(<span class="dv">1419126103</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">      </span><span class="kw">from_utc_timestamp</span>(<span class="st">&#39;UTC&#39;</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">      </span><span class="kw">to_date</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">      </span><span class="kw">as.character</span>()
  )</code></pre>
</div>
<div id="specific-types-and-protocols" class="section level2">
<h2><span class="header-section-number">8.5</span> Specific types and protocols</h2>
<p>This section will cover techniques to help you interface with some of the most popular data types and protocols.</p>
<div id="amazon-s3" class="section level3">
<h3><span class="header-section-number">8.5.1</span> Amazon S3</h3>
<p>Amazon Simple Storage Service, or S3, has become a common location to store file. Spark is able to directly access S3. This functionality can be used inside <code>sparklyr</code>. There are three key items to have, or use, when working with data from an S3 bucket:</p>
<ul>
<li><em>AWS Credentials</em> - They are required by the S3 service, even for publicly accessible buckets.</li>
<li><em>Hadoop-to-AWS package</em> - It is loaded at connection time.</li>
<li><em>A bucket location</em> - The recommended file system to use is “s3a”.</li>
</ul>
<p>There are multiple ways to set the credentials to use to access the bucket. Please refer to the official documentation for more information. It is found in the Apache Spark official site <span class="citation">(“Spark Integration with Cloud Infrastructures” <a href="#ref-data-spark-cloud-integration">2019</a>)</span>.</p>
<p>The easiest way is to set the credentials using Environment variables. Choose a secure way to load the values into variables in R, and then load them into the appropriate Environment variable name. In case show below, <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">Sys.setenv</span>(<span class="dt">AWS_ACCESS_KEY_ID =</span> my_key_id)
<span class="kw">Sys.setenv</span>(<span class="dt">AWS_SECRET_ACCESS_KEY =</span> my_secret_key)</code></pre>
<p>Spark requires an integration package in order to access Amazon S3 buckets. Interestingly, the package is not a Spark package, it is a Hadoop package. This means that the selected version will be a Hadoop version. After some experiments, it seems that with Spark versions 2, only up to Hadoop 2.7.7 will work. That may change when Spark enters version 3. If using a YARN managed cluster, the package may be different. That would depend on the Hadoop vendor. The official site for Apache based project is called Maven <span class="citation">(“Maven Repository: Home Page” <a href="#ref-data-maven-home">2019</a>)</span>. Please visit that site to find alternative package versions if the recommended one does not work. The recommended search term to use would be: “hadoop-aws”.</p>
<pre class="sourceCode r"><code class="sourceCode r">conf &lt;-<span class="st"> </span><span class="kw">spark_config</span>()
conf<span class="op">$</span>sparklyr.defaultPackages &lt;-<span class="st"> &quot;org.apache.hadoop:hadoop-aws:2.7.7&quot;</span>

sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>, <span class="dt">config =</span> conf)</code></pre>
<p>For the file system prefix use “s3a”. There are other options, such as “s3” and “s3n”. As per the Hadoop documents, the “s3a” file system should be the default selection.</p>
<pre class="sourceCode r"><code class="sourceCode r">my_file &lt;-<span class="st"> </span><span class="kw">spark_read_csv</span>(sc, <span class="st">&quot;my-file&quot;</span>, <span class="dt">path =</span>  <span class="st">&quot;s3a://my-bucket/my-file.csv&quot;</span>)</code></pre>
</div>
<div id="sql" class="section level3">
<h3><span class="header-section-number">8.5.2</span> SQL</h3>
<p>The <code>sparklyr</code> package provides a DBI compliant interface <span class="citation">(<span class="citeproc-not-found" data-reference-id="data-r-dbi"><strong>???</strong></span>)</span>. This means that DBI functions can be used to interact with data via Spark. This includes non-SQL sources that are accessible via or cached in Spark.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(sparklyr)
<span class="kw">library</span>(dplyr)
sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>)
cars &lt;-<span class="st"> </span><span class="kw">copy_to</span>(sc, mtcars, <span class="st">&quot;remote_mtcars&quot;</span>)

DBI<span class="op">::</span><span class="kw">dbGetQuery</span>(sc, <span class="st">&quot;SELECT * FROM remote_mtcars LIMIT 5&quot;</span>)</code></pre>
<pre><code>##    mpg cyl disp  hp drat    wt  qsec vs am gear carb
## 1 21.0   6  160 110 3.90 2.620 16.46  0  1    4    4
## 2 21.0   6  160 110 3.90 2.875 17.02  0  1    4    4
## 3 22.8   4  108  93 3.85 2.320 18.61  1  1    4    1
## 4 21.4   6  258 110 3.08 3.215 19.44  1  0    3    1
## 5 18.7   8  360 175 3.15 3.440 17.02  0  0    3    2</code></pre>
</div>
<div id="hive" class="section level3">
<h3><span class="header-section-number">8.5.3</span> Hive</h3>
<p>In YARN managed clusters, Spark provides a deeper integration with Apache Hive. Hive tables are easily accessible after opening a Spark connection.</p>
<pre class="sourceCode r"><code class="sourceCode r">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;yarn-client&quot;</span>, 
                    <span class="dt">spark_home =</span> <span class="st">&quot;/usr/lib/spark/&quot;</span>,
                    <span class="dt">version =</span> <span class="st">&quot;2.1.0&quot;</span>,
                    <span class="dt">config =</span> conf)</code></pre>
<p>Accessing a Hive table’s data can be done with a simple reference. Using <code>DBI</code>, a table can be referenced within a SQL statement.</p>
<pre class="sourceCode r"><code class="sourceCode r">DBI<span class="op">::</span><span class="kw">dbSendQuery</span>(sc, <span class="st">&quot;SELECT * FROM table limit 10&quot;</span>)</code></pre>
<p>Another way to reference a table is with <code>dplyr</code>. The <code>tbl()</code> function, creates a pointer to the table.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(dplyr)

t &lt;-<span class="st"> </span><span class="kw">tbl</span>(sc, <span class="st">&quot;table&quot;</span>)</code></pre>
<p>It is important to reiterate that no data is imported into R, the <code>tbl()</code> function creates only a reference. The expectation is that there will be more <code>dplyr</code> verbs following the <code>tbl()</code> command.</p>
<pre class="sourceCode r"><code class="sourceCode r">t <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(field1) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">totals =</span> <span class="kw">sum</span>(field2))</code></pre>
<div id="database-selection" class="section level5">
<h5><span class="header-section-number">8.5.3.0.1</span> Database selection</h5>
<p>Hive table references assume a default database source. Often, the table needed table is in a different database within the Metastore. To access it using SQL, prefix the database name to the table. Separate them using a period.</p>
<pre class="sourceCode r"><code class="sourceCode r">DBI<span class="op">::</span><span class="kw">dbSendQuery</span>(sc, <span class="st">&quot;SELECT * FROM databasename.table&quot;</span>)</code></pre>
<p>In <code>dplyr</code>, the <code>in_schema()</code> function can be used. The function is used inside the <code>tbl()</code> call.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tbl</span>(sc, <span class="kw">in_schema</span>(<span class="st">&quot;databasename&quot;</span>, <span class="st">&quot;table&quot;</span>))</code></pre>
<p>In <code>sparklyr</code>, the <code>tbl_change_db()</code> function sets the current session’s default database. Any subsequent call via <code>DBI</code> or <code>dplyr</code> will use the selected name as the default database.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tbl_change_db</span>(sc, <span class="st">&quot;databasename&quot;</span>)</code></pre>
</div>
</div>
<div id="comma-delimited-values-csv" class="section level3">
<h3><span class="header-section-number">8.5.4</span> Comma Delimited Values (CSV)</h3>
<p>The CSV format may be the most common file type in use today. Spark offers a couple of techniques to help you troubleshoot issues when reading these kinds of files.</p>
<p>Spark offers the following modes for addressing parsing issues:</p>
<ul>
<li><p><strong>PERMISSIVE</strong>: <code>NULL</code>s are inserted for missing tokens.</p></li>
<li><p><strong>DROPMALFORMED</strong>: Drops lines which are malformed.</p></li>
<li><p><strong>FAILFAST</strong>: Aborts if encounters any malformed line.</p></li>
</ul>
<p>These can be used in <code>sparklyr</code> by passing them inside the <code>options</code> argument. The following example creates a file with a broken entry. It then shows how it can be read into Spark.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(sparklyr)
sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>)

<span class="co">## Creates bad test file</span>
<span class="kw">writeLines</span>(<span class="kw">c</span>(<span class="st">&quot;bad&quot;</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="st">&quot;broken&quot;</span>), <span class="st">&quot;bad.csv&quot;</span>)

<span class="kw">spark_read_csv</span>(
  sc,
  <span class="st">&quot;bad3&quot;</span>,
  <span class="st">&quot;bad.csv&quot;</span>,
  <span class="dt">columns =</span> <span class="kw">list</span>(<span class="dt">foo =</span> <span class="st">&quot;integer&quot;</span>),
  <span class="dt">infer_schema =</span> <span class="ot">FALSE</span>,
  <span class="dt">options =</span> <span class="kw">list</span>(<span class="dt">mode =</span> <span class="st">&quot;DROPMALFORMED&quot;</span>))</code></pre>
<pre><code>## # Source: spark&lt;bad3&gt; [?? x 1]
##     foo
##   &lt;int&gt;
## 1     1
## 2     2
## 3     3</code></pre>
<p>Spark 2 provides an issue tracking column. The column is hidden by default. To enable it, add <code>_corrupt_record</code> to the <code>columns</code> list. This can be combines with the use of the <em>PERMISIVE</em> mode. All rows will be imported, invalid entries will receive an <code>NA</code>, and the issue tracked in the <code>_corrupt_record</code> column.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">spark_read_csv</span>(
  sc,
  <span class="st">&quot;bad2&quot;</span>,
  <span class="st">&quot;bad.csv&quot;</span>,
  <span class="dt">columns =</span> <span class="kw">list</span>(<span class="dt">foo =</span> <span class="st">&quot;integer&quot;</span>, <span class="st">&quot;_corrupt_record&quot;</span> =<span class="st"> &quot;character&quot;</span>),
  <span class="dt">infer_schema =</span> <span class="ot">FALSE</span>,
  <span class="dt">options =</span> <span class="kw">list</span>(<span class="dt">mode =</span> <span class="st">&quot;PERMISIVE&quot;</span>)
)</code></pre>
<pre><code>## # Source: spark&lt;bad2&gt; [?? x 2]
##     foo `_corrupt_record`
##   &lt;int&gt; &lt;chr&gt;            
## 1     1 NA               
## 2     2 NA               
## 3     3 NA               
## 4    NA broken  </code></pre>
</div>
</div>
<div id="recap-3" class="section level2">
<h2><span class="header-section-number">8.6</span> Recap</h2>
<p>In the next chapter, <a href="tuning.html#tuning">Tuning</a>, you will learn in-detail how Spark works and use this knowledge to optimize it’s resource usage and performance.</p>

</div>
</div>
<h3> References</h3>
<div id="refs" class="references">
<div id="ref-data-spark-cloud-integration">
<p>“Spark Integration with Cloud Infrastructures.” 2019. <a href="https://spark.apache.org/docs/latest/cloud-integration.html">https://spark.apache.org/docs/latest/cloud-integration.html</a>.</p>
</div>
<div id="ref-data-maven-home">
<p>“Maven Repository: Home Page.” 2019. <a href="https://mvnrepository.com">https://mvnrepository.com</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="connections.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="tuning.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
