<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 8 Data | Mastering Spark with R</title>
  <meta name="description" content="The Complete Guide to Large-Scale Analysis and Modeling." />
  <meta name="generator" content="bookdown 0.13 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 8 Data | Mastering Spark with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The Complete Guide to Large-Scale Analysis and Modeling." />
  <meta name="github-repo" content="r-spark/the-r-in-spark" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 8 Data | Mastering Spark with R" />
  
  <meta name="twitter:description" content="The Complete Guide to Large-Scale Analysis and Modeling." />
  

<meta name="author" content="Javier Luraschi, Kevin Kuo, Edgar Ruiz" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="connections.html"/>
<link rel="next" href="tuning.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-119986300-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-119986300-1');
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./">Mastering Spark with R</a></strong></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#intro-background"><i class="fa fa-check"></i><b>1.1</b> Overview</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#intro-hadoop"><i class="fa fa-check"></i><b>1.2</b> Hadoop</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#intro-spark"><i class="fa fa-check"></i><b>1.3</b> Spark</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#intro-r"><i class="fa fa-check"></i><b>1.4</b> R</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#intro-sparklyr"><i class="fa fa-check"></i><b>1.5</b> sparklyr</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#intro-recap"><i class="fa fa-check"></i><b>1.6</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="starting.html"><a href="starting.html"><i class="fa fa-check"></i><b>2</b> Getting Started</a><ul>
<li class="chapter" data-level="2.1" data-path="starting.html"><a href="starting.html#overview"><i class="fa fa-check"></i><b>2.1</b> Overview</a></li>
<li class="chapter" data-level="2.2" data-path="starting.html"><a href="starting.html#starting-prerequisites"><i class="fa fa-check"></i><b>2.2</b> Prerequisites</a><ul>
<li class="chapter" data-level="2.2.1" data-path="starting.html"><a href="starting.html#starting-install-sparklyr"><i class="fa fa-check"></i><b>2.2.1</b> Installing sparklyr</a></li>
<li class="chapter" data-level="2.2.2" data-path="starting.html"><a href="starting.html#starting-installing-spark"><i class="fa fa-check"></i><b>2.2.2</b> Installing Spark</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="starting.html"><a href="starting.html#starting-connect-to-spark"><i class="fa fa-check"></i><b>2.3</b> Connecting</a></li>
<li class="chapter" data-level="2.4" data-path="starting.html"><a href="starting.html#starting-sparklyr-hello-world"><i class="fa fa-check"></i><b>2.4</b> Using Spark</a><ul>
<li class="chapter" data-level="2.4.1" data-path="starting.html"><a href="starting.html#starting-spark-web-interface"><i class="fa fa-check"></i><b>2.4.1</b> Web Interface</a></li>
<li class="chapter" data-level="2.4.2" data-path="starting.html"><a href="starting.html#starting-analysis"><i class="fa fa-check"></i><b>2.4.2</b> Analysis</a></li>
<li class="chapter" data-level="2.4.3" data-path="starting.html"><a href="starting.html#starting-modeling"><i class="fa fa-check"></i><b>2.4.3</b> Modeling</a></li>
<li class="chapter" data-level="2.4.4" data-path="starting.html"><a href="starting.html#starting-data"><i class="fa fa-check"></i><b>2.4.4</b> Data</a></li>
<li class="chapter" data-level="2.4.5" data-path="starting.html"><a href="starting.html#starting-extensions"><i class="fa fa-check"></i><b>2.4.5</b> Extensions</a></li>
<li class="chapter" data-level="2.4.6" data-path="starting.html"><a href="starting.html#starting-distributed-r"><i class="fa fa-check"></i><b>2.4.6</b> Distributed R</a></li>
<li class="chapter" data-level="2.4.7" data-path="starting.html"><a href="starting.html#starting-streaming"><i class="fa fa-check"></i><b>2.4.7</b> Streaming</a></li>
<li class="chapter" data-level="2.4.8" data-path="starting.html"><a href="starting.html#starting-logs"><i class="fa fa-check"></i><b>2.4.8</b> Logs</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="starting.html"><a href="starting.html#starting-disconnecting"><i class="fa fa-check"></i><b>2.5</b> Disconnecting</a></li>
<li class="chapter" data-level="2.6" data-path="starting.html"><a href="starting.html#starting-using-spark-from-rstudio"><i class="fa fa-check"></i><b>2.6</b> Using RStudio</a></li>
<li class="chapter" data-level="2.7" data-path="starting.html"><a href="starting.html#starting-resources"><i class="fa fa-check"></i><b>2.7</b> Resources</a></li>
<li class="chapter" data-level="2.8" data-path="starting.html"><a href="starting.html#starting-recap"><i class="fa fa-check"></i><b>2.8</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="analysis.html"><a href="analysis.html"><i class="fa fa-check"></i><b>3</b> Analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="analysis.html"><a href="analysis.html#analysis-overview"><i class="fa fa-check"></i><b>3.1</b> Overview</a></li>
<li class="chapter" data-level="3.2" data-path="analysis.html"><a href="analysis.html#import"><i class="fa fa-check"></i><b>3.2</b> Import</a></li>
<li class="chapter" data-level="3.3" data-path="analysis.html"><a href="analysis.html#wrangle"><i class="fa fa-check"></i><b>3.3</b> Wrangle</a><ul>
<li class="chapter" data-level="3.3.1" data-path="analysis.html"><a href="analysis.html#built-in-functions"><i class="fa fa-check"></i><b>3.3.1</b> Built-in Functions</a></li>
<li class="chapter" data-level="3.3.2" data-path="analysis.html"><a href="analysis.html#correlations"><i class="fa fa-check"></i><b>3.3.2</b> Correlations</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="analysis.html"><a href="analysis.html#visualize"><i class="fa fa-check"></i><b>3.4</b> Visualize</a><ul>
<li class="chapter" data-level="3.4.1" data-path="analysis.html"><a href="analysis.html#using-ggplot2"><i class="fa fa-check"></i><b>3.4.1</b> Using ggplot2</a></li>
<li class="chapter" data-level="3.4.2" data-path="analysis.html"><a href="analysis.html#using-dbplot"><i class="fa fa-check"></i><b>3.4.2</b> Using dbplot</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="analysis.html"><a href="analysis.html#model"><i class="fa fa-check"></i><b>3.5</b> Model</a><ul>
<li class="chapter" data-level="3.5.1" data-path="analysis.html"><a href="analysis.html#caching"><i class="fa fa-check"></i><b>3.5.1</b> Caching</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="analysis.html"><a href="analysis.html#communicate"><i class="fa fa-check"></i><b>3.6</b> Communicate</a></li>
<li class="chapter" data-level="3.7" data-path="analysis.html"><a href="analysis.html#recap"><i class="fa fa-check"></i><b>3.7</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="modeling.html"><a href="modeling.html"><i class="fa fa-check"></i><b>4</b> Modeling</a><ul>
<li class="chapter" data-level="4.1" data-path="modeling.html"><a href="modeling.html#overview-1"><i class="fa fa-check"></i><b>4.1</b> Overview</a></li>
<li class="chapter" data-level="4.2" data-path="modeling.html"><a href="modeling.html#exploratory-data-analysis"><i class="fa fa-check"></i><b>4.2</b> Exploratory Data Analysis</a></li>
<li class="chapter" data-level="4.3" data-path="modeling.html"><a href="modeling.html#feature-engineering"><i class="fa fa-check"></i><b>4.3</b> Feature Engineering</a></li>
<li class="chapter" data-level="4.4" data-path="modeling.html"><a href="modeling.html#supervised-learning"><i class="fa fa-check"></i><b>4.4</b> Supervised Learning</a><ul>
<li class="chapter" data-level="4.4.1" data-path="modeling.html"><a href="modeling.html#generalized-linear-regression"><i class="fa fa-check"></i><b>4.4.1</b> Generalized Linear Regression</a></li>
<li class="chapter" data-level="4.4.2" data-path="modeling.html"><a href="modeling.html#other-models"><i class="fa fa-check"></i><b>4.4.2</b> Other Models</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="modeling.html"><a href="modeling.html#unsupervised-learning"><i class="fa fa-check"></i><b>4.5</b> Unsupervised Learning</a><ul>
<li class="chapter" data-level="4.5.1" data-path="modeling.html"><a href="modeling.html#data-preparation"><i class="fa fa-check"></i><b>4.5.1</b> Data Preparation</a></li>
<li class="chapter" data-level="4.5.2" data-path="modeling.html"><a href="modeling.html#topic-modeling"><i class="fa fa-check"></i><b>4.5.2</b> Topic Modeling</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="modeling.html"><a href="modeling.html#recap-1"><i class="fa fa-check"></i><b>4.6</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="pipelines.html"><a href="pipelines.html"><i class="fa fa-check"></i><b>5</b> Pipelines</a><ul>
<li class="chapter" data-level="5.1" data-path="pipelines.html"><a href="pipelines.html#overview-2"><i class="fa fa-check"></i><b>5.1</b> Overview</a></li>
<li class="chapter" data-level="5.2" data-path="pipelines.html"><a href="pipelines.html#creation"><i class="fa fa-check"></i><b>5.2</b> Creation</a></li>
<li class="chapter" data-level="5.3" data-path="pipelines.html"><a href="pipelines.html#use-cases"><i class="fa fa-check"></i><b>5.3</b> Use Cases</a><ul>
<li class="chapter" data-level="5.3.1" data-path="pipelines.html"><a href="pipelines.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>5.3.1</b> Hyperparameter Tuning</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="pipelines.html"><a href="pipelines.html#operating-modes"><i class="fa fa-check"></i><b>5.4</b> Operating Modes</a></li>
<li class="chapter" data-level="5.5" data-path="pipelines.html"><a href="pipelines.html#interoperability"><i class="fa fa-check"></i><b>5.5</b> Interoperability</a></li>
<li class="chapter" data-level="5.6" data-path="pipelines.html"><a href="pipelines.html#deployment"><i class="fa fa-check"></i><b>5.6</b> Deployment</a><ul>
<li class="chapter" data-level="5.6.1" data-path="pipelines.html"><a href="pipelines.html#batch-scoring"><i class="fa fa-check"></i><b>5.6.1</b> Batch Scoring</a></li>
<li class="chapter" data-level="5.6.2" data-path="pipelines.html"><a href="pipelines.html#real-time-scoring"><i class="fa fa-check"></i><b>5.6.2</b> Real-Time Scoring</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="pipelines.html"><a href="pipelines.html#recap-2"><i class="fa fa-check"></i><b>5.7</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="clusters.html"><a href="clusters.html"><i class="fa fa-check"></i><b>6</b> Clusters</a><ul>
<li class="chapter" data-level="6.1" data-path="clusters.html"><a href="clusters.html#clusters-overview"><i class="fa fa-check"></i><b>6.1</b> Overview</a></li>
<li class="chapter" data-level="6.2" data-path="clusters.html"><a href="clusters.html#on-premise"><i class="fa fa-check"></i><b>6.2</b> On-Premise</a><ul>
<li class="chapter" data-level="6.2.1" data-path="clusters.html"><a href="clusters.html#clusters-manager"><i class="fa fa-check"></i><b>6.2.1</b> Managers</a></li>
<li class="chapter" data-level="6.2.2" data-path="clusters.html"><a href="clusters.html#distributions"><i class="fa fa-check"></i><b>6.2.2</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="clusters.html"><a href="clusters.html#cloud"><i class="fa fa-check"></i><b>6.3</b> Cloud</a><ul>
<li class="chapter" data-level="6.3.1" data-path="clusters.html"><a href="clusters.html#clusters-amazon-emr"><i class="fa fa-check"></i><b>6.3.1</b> Amazon</a></li>
<li class="chapter" data-level="6.3.2" data-path="clusters.html"><a href="clusters.html#databricks"><i class="fa fa-check"></i><b>6.3.2</b> Databricks</a></li>
<li class="chapter" data-level="6.3.3" data-path="clusters.html"><a href="clusters.html#google"><i class="fa fa-check"></i><b>6.3.3</b> Google</a></li>
<li class="chapter" data-level="6.3.4" data-path="clusters.html"><a href="clusters.html#ibm"><i class="fa fa-check"></i><b>6.3.4</b> IBM</a></li>
<li class="chapter" data-level="6.3.5" data-path="clusters.html"><a href="clusters.html#microsoft"><i class="fa fa-check"></i><b>6.3.5</b> Microsoft</a></li>
<li class="chapter" data-level="6.3.6" data-path="clusters.html"><a href="clusters.html#qubole"><i class="fa fa-check"></i><b>6.3.6</b> Qubole</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="clusters.html"><a href="clusters.html#kubernetes"><i class="fa fa-check"></i><b>6.4</b> Kubernetes</a></li>
<li class="chapter" data-level="6.5" data-path="clusters.html"><a href="clusters.html#tools"><i class="fa fa-check"></i><b>6.5</b> Tools</a><ul>
<li class="chapter" data-level="6.5.1" data-path="clusters.html"><a href="clusters.html#rstudio"><i class="fa fa-check"></i><b>6.5.1</b> RStudio</a></li>
<li class="chapter" data-level="6.5.2" data-path="clusters.html"><a href="clusters.html#jupyter"><i class="fa fa-check"></i><b>6.5.2</b> Jupyter</a></li>
<li class="chapter" data-level="6.5.3" data-path="clusters.html"><a href="clusters.html#clusters-livy"><i class="fa fa-check"></i><b>6.5.3</b> Livy</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="clusters.html"><a href="clusters.html#recap-3"><i class="fa fa-check"></i><b>6.6</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="connections.html"><a href="connections.html"><i class="fa fa-check"></i><b>7</b> Connections</a><ul>
<li class="chapter" data-level="7.1" data-path="connections.html"><a href="connections.html#connections-overview"><i class="fa fa-check"></i><b>7.1</b> Overview</a><ul>
<li class="chapter" data-level="7.1.1" data-path="connections.html"><a href="connections.html#connections-spark-edge-nodes"><i class="fa fa-check"></i><b>7.1.1</b> Edge Nodes</a></li>
<li class="chapter" data-level="7.1.2" data-path="connections.html"><a href="connections.html#connections-spark-home"><i class="fa fa-check"></i><b>7.1.2</b> Spark Home</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="connections.html"><a href="connections.html#connections-local"><i class="fa fa-check"></i><b>7.2</b> Local</a></li>
<li class="chapter" data-level="7.3" data-path="connections.html"><a href="connections.html#connections-standalone"><i class="fa fa-check"></i><b>7.3</b> Standalone</a></li>
<li class="chapter" data-level="7.4" data-path="connections.html"><a href="connections.html#connections-yarn"><i class="fa fa-check"></i><b>7.4</b> Yarn</a><ul>
<li class="chapter" data-level="7.4.1" data-path="connections.html"><a href="connections.html#connections-yarn-client"><i class="fa fa-check"></i><b>7.4.1</b> Yarn Client</a></li>
<li class="chapter" data-level="7.4.2" data-path="connections.html"><a href="connections.html#connections-yarn-cluster"><i class="fa fa-check"></i><b>7.4.2</b> Yarn Cluster</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="connections.html"><a href="connections.html#connections-livy"><i class="fa fa-check"></i><b>7.5</b> Livy</a></li>
<li class="chapter" data-level="7.6" data-path="connections.html"><a href="connections.html#connections-mesos"><i class="fa fa-check"></i><b>7.6</b> Mesos</a></li>
<li class="chapter" data-level="7.7" data-path="connections.html"><a href="connections.html#connections-kubernetes"><i class="fa fa-check"></i><b>7.7</b> Kubernetes</a></li>
<li class="chapter" data-level="7.8" data-path="connections.html"><a href="connections.html#cloud-1"><i class="fa fa-check"></i><b>7.8</b> Cloud</a></li>
<li class="chapter" data-level="7.9" data-path="connections.html"><a href="connections.html#batches"><i class="fa fa-check"></i><b>7.9</b> Batches</a></li>
<li class="chapter" data-level="7.10" data-path="connections.html"><a href="connections.html#tools-1"><i class="fa fa-check"></i><b>7.10</b> Tools</a></li>
<li class="chapter" data-level="7.11" data-path="connections.html"><a href="connections.html#multiple"><i class="fa fa-check"></i><b>7.11</b> Multiple</a></li>
<li class="chapter" data-level="7.12" data-path="connections.html"><a href="connections.html#connections-troubleshooting"><i class="fa fa-check"></i><b>7.12</b> Troubleshooting</a><ul>
<li class="chapter" data-level="7.12.1" data-path="connections.html"><a href="connections.html#logging"><i class="fa fa-check"></i><b>7.12.1</b> Logging</a></li>
<li class="chapter" data-level="7.12.2" data-path="connections.html"><a href="connections.html#connections-troubleshoot-spark-submit"><i class="fa fa-check"></i><b>7.12.2</b> Spark Submit</a></li>
<li class="chapter" data-level="7.12.3" data-path="connections.html"><a href="connections.html#windows"><i class="fa fa-check"></i><b>7.12.3</b> Windows</a></li>
</ul></li>
<li class="chapter" data-level="7.13" data-path="connections.html"><a href="connections.html#recap-4"><i class="fa fa-check"></i><b>7.13</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>8</b> Data</a><ul>
<li class="chapter" data-level="8.1" data-path="data.html"><a href="data.html#overview-3"><i class="fa fa-check"></i><b>8.1</b> Overview</a></li>
<li class="chapter" data-level="8.2" data-path="data.html"><a href="data.html#reading-data"><i class="fa fa-check"></i><b>8.2</b> Reading Data</a><ul>
<li class="chapter" data-level="8.2.1" data-path="data.html"><a href="data.html#paths"><i class="fa fa-check"></i><b>8.2.1</b> Paths</a></li>
<li class="chapter" data-level="8.2.2" data-path="data.html"><a href="data.html#schema"><i class="fa fa-check"></i><b>8.2.2</b> Schema</a></li>
<li class="chapter" data-level="8.2.3" data-path="data.html"><a href="data.html#memory"><i class="fa fa-check"></i><b>8.2.3</b> Memory</a></li>
<li class="chapter" data-level="8.2.4" data-path="data.html"><a href="data.html#columns"><i class="fa fa-check"></i><b>8.2.4</b> Columns</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="data.html"><a href="data.html#writing-data"><i class="fa fa-check"></i><b>8.3</b> Writing Data</a></li>
<li class="chapter" data-level="8.4" data-path="data.html"><a href="data.html#copy"><i class="fa fa-check"></i><b>8.4</b> Copy</a></li>
<li class="chapter" data-level="8.5" data-path="data.html"><a href="data.html#data-file-formats"><i class="fa fa-check"></i><b>8.5</b> File Formats</a><ul>
<li class="chapter" data-level="8.5.1" data-path="data.html"><a href="data.html#csv"><i class="fa fa-check"></i><b>8.5.1</b> CSV</a></li>
<li class="chapter" data-level="8.5.2" data-path="data.html"><a href="data.html#json"><i class="fa fa-check"></i><b>8.5.2</b> JSON</a></li>
<li class="chapter" data-level="8.5.3" data-path="data.html"><a href="data.html#parquet"><i class="fa fa-check"></i><b>8.5.3</b> Parquet</a></li>
<li class="chapter" data-level="8.5.4" data-path="data.html"><a href="data.html#others"><i class="fa fa-check"></i><b>8.5.4</b> Others</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="data.html"><a href="data.html#data-file-systems"><i class="fa fa-check"></i><b>8.6</b> File Systems</a></li>
<li class="chapter" data-level="8.7" data-path="data.html"><a href="data.html#data-storage-systems"><i class="fa fa-check"></i><b>8.7</b> Storage Systems</a><ul>
<li class="chapter" data-level="8.7.1" data-path="data.html"><a href="data.html#hive"><i class="fa fa-check"></i><b>8.7.1</b> Hive</a></li>
<li class="chapter" data-level="8.7.2" data-path="data.html"><a href="data.html#cassandra"><i class="fa fa-check"></i><b>8.7.2</b> Cassandra</a></li>
<li class="chapter" data-level="8.7.3" data-path="data.html"><a href="data.html#jdbc"><i class="fa fa-check"></i><b>8.7.3</b> JDBC</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="data.html"><a href="data.html#recap-5"><i class="fa fa-check"></i><b>8.8</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="tuning.html"><a href="tuning.html"><i class="fa fa-check"></i><b>9</b> Tuning</a><ul>
<li class="chapter" data-level="9.1" data-path="tuning.html"><a href="tuning.html#overview-4"><i class="fa fa-check"></i><b>9.1</b> Overview</a><ul>
<li class="chapter" data-level="9.1.1" data-path="tuning.html"><a href="tuning.html#tuning-graph-visualization"><i class="fa fa-check"></i><b>9.1.1</b> Graph</a></li>
<li class="chapter" data-level="9.1.2" data-path="tuning.html"><a href="tuning.html#tuning-event-timeline"><i class="fa fa-check"></i><b>9.1.2</b> Timeline</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="tuning.html"><a href="tuning.html#tuning-configuring"><i class="fa fa-check"></i><b>9.2</b> Configuring</a><ul>
<li class="chapter" data-level="9.2.1" data-path="tuning.html"><a href="tuning.html#connect-settings"><i class="fa fa-check"></i><b>9.2.1</b> Connect Settings</a></li>
<li class="chapter" data-level="9.2.2" data-path="tuning.html"><a href="tuning.html#submit-settings"><i class="fa fa-check"></i><b>9.2.2</b> Submit Settings</a></li>
<li class="chapter" data-level="9.2.3" data-path="tuning.html"><a href="tuning.html#runtime-settings"><i class="fa fa-check"></i><b>9.2.3</b> Runtime Settings</a></li>
<li class="chapter" data-level="9.2.4" data-path="tuning.html"><a href="tuning.html#sparklyr-settings"><i class="fa fa-check"></i><b>9.2.4</b> sparklyr Settings</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="tuning.html"><a href="tuning.html#tuning-partitioning"><i class="fa fa-check"></i><b>9.3</b> Partitioning</a><ul>
<li class="chapter" data-level="9.3.1" data-path="tuning.html"><a href="tuning.html#implicit-partitions"><i class="fa fa-check"></i><b>9.3.1</b> Implicit Partitions</a></li>
<li class="chapter" data-level="9.3.2" data-path="tuning.html"><a href="tuning.html#explicit-partitions"><i class="fa fa-check"></i><b>9.3.2</b> Explicit Partitions</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="tuning.html"><a href="tuning.html#tuning-caching"><i class="fa fa-check"></i><b>9.4</b> Caching</a><ul>
<li class="chapter" data-level="9.4.1" data-path="tuning.html"><a href="tuning.html#checkpointing"><i class="fa fa-check"></i><b>9.4.1</b> Checkpointing</a></li>
<li class="chapter" data-level="9.4.2" data-path="tuning.html"><a href="tuning.html#tuning-memory"><i class="fa fa-check"></i><b>9.4.2</b> Memory</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="tuning.html"><a href="tuning.html#tuning-shuffling"><i class="fa fa-check"></i><b>9.5</b> Shuffling</a></li>
<li class="chapter" data-level="9.6" data-path="tuning.html"><a href="tuning.html#tuning-serialization"><i class="fa fa-check"></i><b>9.6</b> Serialization</a></li>
<li class="chapter" data-level="9.7" data-path="tuning.html"><a href="tuning.html#configuration-files"><i class="fa fa-check"></i><b>9.7</b> Configuration Files</a></li>
<li class="chapter" data-level="9.8" data-path="tuning.html"><a href="tuning.html#recap-6"><i class="fa fa-check"></i><b>9.8</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="extensions.html"><a href="extensions.html"><i class="fa fa-check"></i><b>10</b> Extensions</a><ul>
<li class="chapter" data-level="10.1" data-path="extensions.html"><a href="extensions.html#overview-5"><i class="fa fa-check"></i><b>10.1</b> Overview</a></li>
<li class="chapter" data-level="10.2" data-path="extensions.html"><a href="extensions.html#h2o"><i class="fa fa-check"></i><b>10.2</b> H2O</a></li>
<li class="chapter" data-level="10.3" data-path="extensions.html"><a href="extensions.html#graphs"><i class="fa fa-check"></i><b>10.3</b> Graphs</a></li>
<li class="chapter" data-level="10.4" data-path="extensions.html"><a href="extensions.html#xgboost"><i class="fa fa-check"></i><b>10.4</b> XGBoost</a></li>
<li class="chapter" data-level="10.5" data-path="extensions.html"><a href="extensions.html#deep-learning"><i class="fa fa-check"></i><b>10.5</b> Deep Learning</a></li>
<li class="chapter" data-level="10.6" data-path="extensions.html"><a href="extensions.html#genomics"><i class="fa fa-check"></i><b>10.6</b> Genomics</a></li>
<li class="chapter" data-level="10.7" data-path="extensions.html"><a href="extensions.html#spatial"><i class="fa fa-check"></i><b>10.7</b> Spatial</a></li>
<li class="chapter" data-level="10.8" data-path="extensions.html"><a href="extensions.html#troubleshooting"><i class="fa fa-check"></i><b>10.8</b> Troubleshooting</a></li>
<li class="chapter" data-level="10.9" data-path="extensions.html"><a href="extensions.html#recap-7"><i class="fa fa-check"></i><b>10.9</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="distributed.html"><a href="distributed.html"><i class="fa fa-check"></i><b>11</b> Distributed R</a><ul>
<li class="chapter" data-level="11.1" data-path="distributed.html"><a href="distributed.html#overview-6"><i class="fa fa-check"></i><b>11.1</b> Overview</a></li>
<li class="chapter" data-level="11.2" data-path="distributed.html"><a href="distributed.html#use-cases-1"><i class="fa fa-check"></i><b>11.2</b> Use Cases</a><ul>
<li class="chapter" data-level="11.2.1" data-path="distributed.html"><a href="distributed.html#custom-parsers"><i class="fa fa-check"></i><b>11.2.1</b> Custom Parsers</a></li>
<li class="chapter" data-level="11.2.2" data-path="distributed.html"><a href="distributed.html#partitioned-modeling"><i class="fa fa-check"></i><b>11.2.2</b> Partitioned Modeling</a></li>
<li class="chapter" data-level="11.2.3" data-path="distributed.html"><a href="distributed.html#distributed-grid-search"><i class="fa fa-check"></i><b>11.2.3</b> Grid Search</a></li>
<li class="chapter" data-level="11.2.4" data-path="distributed.html"><a href="distributed.html#web-apis"><i class="fa fa-check"></i><b>11.2.4</b> Web APIs</a></li>
<li class="chapter" data-level="11.2.5" data-path="distributed.html"><a href="distributed.html#simulations"><i class="fa fa-check"></i><b>11.2.5</b> Simulations</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="distributed.html"><a href="distributed.html#partitions"><i class="fa fa-check"></i><b>11.3</b> Partitions</a></li>
<li class="chapter" data-level="11.4" data-path="distributed.html"><a href="distributed.html#grouping"><i class="fa fa-check"></i><b>11.4</b> Grouping</a></li>
<li class="chapter" data-level="11.5" data-path="distributed.html"><a href="distributed.html#columns-1"><i class="fa fa-check"></i><b>11.5</b> Columns</a></li>
<li class="chapter" data-level="11.6" data-path="distributed.html"><a href="distributed.html#context"><i class="fa fa-check"></i><b>11.6</b> Context</a></li>
<li class="chapter" data-level="11.7" data-path="distributed.html"><a href="distributed.html#functions"><i class="fa fa-check"></i><b>11.7</b> Functions</a></li>
<li class="chapter" data-level="11.8" data-path="distributed.html"><a href="distributed.html#packages"><i class="fa fa-check"></i><b>11.8</b> Packages</a></li>
<li class="chapter" data-level="11.9" data-path="distributed.html"><a href="distributed.html#cluster-requirements"><i class="fa fa-check"></i><b>11.9</b> Cluster Requirements</a><ul>
<li class="chapter" data-level="11.9.1" data-path="distributed.html"><a href="distributed.html#installing-r"><i class="fa fa-check"></i><b>11.9.1</b> Installing R</a></li>
<li class="chapter" data-level="11.9.2" data-path="distributed.html"><a href="distributed.html#apache-arrow"><i class="fa fa-check"></i><b>11.9.2</b> Apache Arrow</a></li>
</ul></li>
<li class="chapter" data-level="11.10" data-path="distributed.html"><a href="distributed.html#troubleshooting-1"><i class="fa fa-check"></i><b>11.10</b> Troubleshooting</a><ul>
<li class="chapter" data-level="11.10.1" data-path="distributed.html"><a href="distributed.html#worker-logs"><i class="fa fa-check"></i><b>11.10.1</b> Worker Logs</a></li>
<li class="chapter" data-level="11.10.2" data-path="distributed.html"><a href="distributed.html#resolving-timeouts"><i class="fa fa-check"></i><b>11.10.2</b> Resolving Timeouts</a></li>
<li class="chapter" data-level="11.10.3" data-path="distributed.html"><a href="distributed.html#inspecting-partitions"><i class="fa fa-check"></i><b>11.10.3</b> Inspecting Partitions</a></li>
<li class="chapter" data-level="11.10.4" data-path="distributed.html"><a href="distributed.html#debugging-workers"><i class="fa fa-check"></i><b>11.10.4</b> Debugging Workers</a></li>
</ul></li>
<li class="chapter" data-level="11.11" data-path="distributed.html"><a href="distributed.html#recap-8"><i class="fa fa-check"></i><b>11.11</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="streaming.html"><a href="streaming.html"><i class="fa fa-check"></i><b>12</b> Streaming</a><ul>
<li class="chapter" data-level="12.1" data-path="streaming.html"><a href="streaming.html#overview-7"><i class="fa fa-check"></i><b>12.1</b> Overview</a></li>
<li class="chapter" data-level="12.2" data-path="streaming.html"><a href="streaming.html#transformations"><i class="fa fa-check"></i><b>12.2</b> Transformations</a><ul>
<li class="chapter" data-level="12.2.1" data-path="streaming.html"><a href="streaming.html#analysis-1"><i class="fa fa-check"></i><b>12.2.1</b> Analysis</a></li>
<li class="chapter" data-level="12.2.2" data-path="streaming.html"><a href="streaming.html#modeling-1"><i class="fa fa-check"></i><b>12.2.2</b> Modeling</a></li>
<li class="chapter" data-level="12.2.3" data-path="streaming.html"><a href="streaming.html#pipelines-1"><i class="fa fa-check"></i><b>12.2.3</b> Pipelines</a></li>
<li class="chapter" data-level="12.2.4" data-path="streaming.html"><a href="streaming.html#distributed-r-streaming-r-code"><i class="fa fa-check"></i><b>12.2.4</b> Distributed R {streaming-r-code}</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="streaming.html"><a href="streaming.html#kafka"><i class="fa fa-check"></i><b>12.3</b> Kafka</a></li>
<li class="chapter" data-level="12.4" data-path="streaming.html"><a href="streaming.html#shiny"><i class="fa fa-check"></i><b>12.4</b> Shiny</a></li>
<li class="chapter" data-level="12.5" data-path="streaming.html"><a href="streaming.html#recap-9"><i class="fa fa-check"></i><b>12.5</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="contributing.html"><a href="contributing.html"><i class="fa fa-check"></i><b>13</b> Contributing</a><ul>
<li class="chapter" data-level="13.1" data-path="contributing.html"><a href="contributing.html#contributing-overview"><i class="fa fa-check"></i><b>13.1</b> Overview</a></li>
<li class="chapter" data-level="13.2" data-path="contributing.html"><a href="contributing.html#contributing-spark-api"><i class="fa fa-check"></i><b>13.2</b> Spark API</a></li>
<li class="chapter" data-level="13.3" data-path="contributing.html"><a href="contributing.html#spark-extensions"><i class="fa fa-check"></i><b>13.3</b> Spark Extensions</a></li>
<li class="chapter" data-level="13.4" data-path="contributing.html"><a href="contributing.html#scala-code"><i class="fa fa-check"></i><b>13.4</b> Scala Code</a></li>
<li class="chapter" data-level="13.5" data-path="contributing.html"><a href="contributing.html#recap-10"><i class="fa fa-check"></i><b>13.5</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>14</b> Appendix</a><ul>
<li class="chapter" data-level="14.1" data-path="appendix.html"><a href="appendix.html#appendix-preface"><i class="fa fa-check"></i><b>14.1</b> Preface</a><ul>
<li class="chapter" data-level="14.1.1" data-path="appendix.html"><a href="appendix.html#appendix-ggplot2-theme"><i class="fa fa-check"></i><b>14.1.1</b> Formatting</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="appendix.html"><a href="appendix.html#appendix-intro"><i class="fa fa-check"></i><b>14.2</b> Introduction</a><ul>
<li class="chapter" data-level="14.2.1" data-path="appendix.html"><a href="appendix.html#appendix-storage-capacity"><i class="fa fa-check"></i><b>14.2.1</b> Worlds Store Capacity</a></li>
<li class="chapter" data-level="14.2.2" data-path="appendix.html"><a href="appendix.html#appendix-cran-downloads"><i class="fa fa-check"></i><b>14.2.2</b> Daily downloads of CRAN packages</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="appendix.html"><a href="appendix.html#appendix-starting"><i class="fa fa-check"></i><b>14.3</b> Getting Started</a><ul>
<li class="chapter" data-level="14.3.1" data-path="appendix.html"><a href="appendix.html#appendix-prerequisites"><i class="fa fa-check"></i><b>14.3.1</b> Prerequisites</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="appendix.html"><a href="appendix.html#appendix-analysis"><i class="fa fa-check"></i><b>14.4</b> Analysis</a><ul>
<li class="chapter" data-level="14.4.1" data-path="appendix.html"><a href="appendix.html#hive-functions"><i class="fa fa-check"></i><b>14.4.1</b> Hive Functions</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="appendix.html"><a href="appendix.html#appendix-modeling"><i class="fa fa-check"></i><b>14.5</b> Modeling</a><ul>
<li class="chapter" data-level="14.5.1" data-path="appendix.html"><a href="appendix.html#appendix-ml-functionlist"><i class="fa fa-check"></i><b>14.5.1</b> MLlib Functions</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="appendix.html"><a href="appendix.html#appendix-clusters"><i class="fa fa-check"></i><b>14.6</b> Clusters</a><ul>
<li class="chapter" data-level="14.6.1" data-path="appendix.html"><a href="appendix.html#appendix-cluster-trends"><i class="fa fa-check"></i><b>14.6.1</b> Google trends for mainframes, cloud computing and kubernetes</a></li>
</ul></li>
<li class="chapter" data-level="14.7" data-path="appendix.html"><a href="appendix.html#appendix-streaming"><i class="fa fa-check"></i><b>14.7</b> Streaming</a><ul>
<li class="chapter" data-level="14.7.1" data-path="appendix.html"><a href="appendix.html#appendix-streaming-generator"><i class="fa fa-check"></i><b>14.7.1</b> Stream Generator</a></li>
<li class="chapter" data-level="14.7.2" data-path="appendix.html"><a href="appendix.html#appendix-streaming-kafka"><i class="fa fa-check"></i><b>14.7.2</b> Installing Kafka</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>15</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Mastering Spark with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="data" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Data</h1>
<blockquote>
<p>Has it occurred to you that she might not have been a reliable source of information?</p>
<p>— Jon Snow</p>
</blockquote>
<p>With the knowledge acquired in previous chapters, you are now equipped to start doing analysis and modeling at scale! So far, however, we haven’t really explained much about how to read data into Spark. We’ve explored how to use <code>copy_to()</code> to upload small datasets or functions like <code>spark_read_csv()</code> or <code>spark_write_csv()</code> without explaining in detail how and why.</p>
<p>So, you are about to learn how to read and write data using Spark. And, while this is important on its own, this chapter will also introduce you to the <em>data lake</em>—a repository of data stored in its natural or raw format that provides various benefits over existing storage architectures. For instance, you can easily integrate data from external systems without transforming it into a common format and without assuming those sources are as reliable as your internal data sources.</p>
<p>In addition, we will also discuss how to extend Spark’s capabilities to work with data not accessible out of the box and make several recommendations focused on improving performance for reading and writing data. Reading large datasets often requires you to fine-tune your Spark cluster configuration, but that’s the topic of <a href="tuning.html#tuning">Chapter 9</a>.</p>
<div id="overview-3" class="section level2">
<h2><span class="header-section-number">8.1</span> Overview</h2>
<p>In <a href="intro.html#intro">Chapter 1</a>, you<!--((("data handling", "overview of")))--> learned that beyond big data and big compute, you can also use Spark to improve velocity, variety, and veracity in data tasks. While you can use the learnings of this chapter for any task requiring loading and storing data, it is particularly interesting to present this chapter in the context of dealing with a variety of data sources. To understand why, we should first take a quick detour to examine how data is currently processed in many organizations.</p>
<p>For several years, it’s been a common practice to store large datasets in a relational <em>database</em>, a system first proposed in 1970 by Edgar F. Codd.<a href="#fn21" class="footnote-ref" id="fnref21"><sup>21</sup></a> You can think of a database as a collection of tables that are related to one another, where each table is carefully designed to hold specific data types and relationships to other tables. Most relational database systems use <em>Structured Query Language</em> (SQL) for querying and maintaining the database. Databases are still widely used today, with good reason: they store data reliably and consistently; in fact, your bank probably stores account balances in a database and that’s a good practice.</p>
<p>However, databases have also been used to store information from other applications and systems. For instance, your bank might also store data produced by other banks, such as incoming checks. To accomplish this, the external data needs to be extracted from the external system, transformed into something that fits the current database, and finally be loaded into it. This is known as <em>Extract, Transform, and Load</em> (ETL), a general procedure for copying data from one or more sources into a destination system that represents the data differently from the source. The ETL process became popular in the 1970s.</p>
<p>Aside from databases, data is often also loaded into a <em>data warehouse</em>, a system used for reporting and data analysis. The data is usually stored and indexed in a format that increases data analysis speed but that is often not suitable for modeling or running custom distributed code. The challenge is that changing databases and data warehouses is usually a long and delicate process, since data needs to be reindexed and the data from multiple data sources needs to be carefully transformed into single tables that are shared across data sources.</p>
<p>Instead of trying to transform all data sources into a common format, you can embrace this variety of data sources in a <em>data lake</em>, a system or repository of data stored in its natural format (see Figure <a href="data.html#fig:data-data-lake">8.1</a>). Since data lakes make data available in its original format, there is no need to carefully transform it in advance; anyone can use it for analysis, which adds significant flexibility over ETL. You then can use Spark to unify data processing from data lakes, databases, and data warehouses through a single interface that is scalable across all of them. Some organizations also use Spark to replace their existing ETL process; however, this falls in the realm of data engineering, which is well beyond the scope of this book. We illustrate this with dotted lines in Figure <a href="data.html#fig:data-data-lake">8.1</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:data-data-lake"></span>
<img src="the-r-in-spark_files/figure-html/data-data-lake-1.png" alt="Spark processing raw data from a data lakes, databases, and data warehouses" width="auto" height="280pt" />
<p class="caption">
FIGURE 8.1: Spark processing raw data from a data lakes, databases, and data warehouses
</p>
</div>
<p>In order to support a broad variety of data source, Spark needs to be able to read and write data in several different <em>file formats</em> (CSV, JSON, Parquet, etc), access them while stored in several <em>file systems</em> (HDFS, S3, DBFS, etc) and, potentially, interoperate with other <em>storage systems</em> (databases, data warehouses, etc). We will get to all of that; but first, we will start by presenting how to read, write and copy data using Spark.</p>
</div>
<div id="reading-data" class="section level2">
<h2><span class="header-section-number">8.2</span> Reading Data</h2>
<p>To support a broad variety of data sources, Spark needs to be able to read and write data in several different file formats (CSV, JSON, Parquet, and others), and access them while stored in several file systems (HDFS, S3, DBFS, and more) and, potentially, interoperate with other storage systems (databases, data warehouses, etc.). We will get to all of that, but first, we will start by presenting how to read, write, and copy data using Spark.</p>
<div id="paths" class="section level3">
<h3><span class="header-section-number">8.2.1</span> Paths</h3>
<p>If<!--((("data handling", "reading data", id="DHread08")))--> you are new to Spark, it is highly recommended to review this section before you start working with large datasets. We will introduce several techniques that improve the speed and efficiency of reading data. Each subsection presents specific ways to take advantage of how Spark reads files, such as the ability to treat entire folders as datasets as well as being able to describe them to read datasets faster in Spark.</p>
<div class="sourceCode" id="cb250"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb250-1" title="1">letters &lt;-<span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">x =</span> letters, <span class="dt">y =</span> <span class="dv">1</span><span class="op">:</span><span class="kw">length</span>(letters))</a>
<a class="sourceLine" id="cb250-2" title="2"></a>
<a class="sourceLine" id="cb250-3" title="3"><span class="kw">dir.create</span>(<span class="st">&quot;data-csv&quot;</span>)</a>
<a class="sourceLine" id="cb250-4" title="4"><span class="kw">write.csv</span>(letters[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>, ], <span class="st">&quot;data-csv/letters1.csv&quot;</span>, <span class="dt">row.names =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb250-5" title="5"><span class="kw">write.csv</span>(letters[<span class="dv">1</span><span class="op">:</span><span class="dv">3</span>, ], <span class="st">&quot;data-csv/letters2.csv&quot;</span>, <span class="dt">row.names =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb250-6" title="6"></a>
<a class="sourceLine" id="cb250-7" title="7"><span class="kw">do.call</span>(<span class="st">&quot;rbind&quot;</span>, <span class="kw">lapply</span>(<span class="kw">dir</span>(<span class="st">&quot;data-csv&quot;</span>, <span class="dt">full.names =</span> <span class="ot">TRUE</span>), read.csv))</a></code></pre></div>
<pre><code>  x y
1 a 1
2 b 2
3 c 3
4 a 1
5 b 2
6 c 3</code></pre>
<p>In Spark, there<!--((("folder as a table idea")))--> is the notion of a folder as a dataset. Instead of enumerating each file, simply pass the path containing all the files. Spark assumes that every file in that folder is part of the same dataset. This implies that the target folder should be used only for data purposes. This is especially important since storage systems like HDFS store files across multiple machines, but, conceptually, they are stored in the same folder; when Spark reads the files from this folder, it’s actually executing distributed code to read each file within each machine—no data is transferred between machines when distributed files are read:</p>
<div class="sourceCode" id="cb252"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb252-1" title="1"><span class="kw">library</span>(sparklyr)</a>
<a class="sourceLine" id="cb252-2" title="2">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>, <span class="dt">version =</span> <span class="st">&quot;2.3&quot;</span>)</a>
<a class="sourceLine" id="cb252-3" title="3"></a>
<a class="sourceLine" id="cb252-4" title="4"><span class="kw">spark_read_csv</span>(sc, <span class="st">&quot;data-csv/&quot;</span>)</a></code></pre></div>
<pre><code># Source: spark&lt;datacsv&gt; [?? x 2]
   x         y
   &lt;chr&gt; &lt;int&gt;
 1 a         1
 2 b         2
 3 c         3
 4 d         4
 5 e         5
 6 a         1
 7 b         2
 8 c         3
 9 d         4
10 e         5</code></pre>
<p>The “folder as a table” idea is found in other open source technologies as well. Under the hood, Hive tables work the same way. When you query a Hive table, the mapping is done over multiple files within the same folder. The folder’s name usually matches the name of the table visible to the user.</p>
<p>Next, we will present a technique that allows Spark to read files faster as well as to reduce read failures by describing the structure of a dataset in advance.</p>
</div>
<div id="schema" class="section level3">
<h3><span class="header-section-number">8.2.2</span> Schema</h3>
<p>When<!--((("reading data", "schema")))((("schemas")))--> reading data, Spark is able to determine the data source’s column names and column types, also known as the <em>schema</em>. However, guessing the schema comes at a cost; Spark needs to do an initial pass on the data to guess what it is. For a large dataset, this can add a significant amount of time to the data ingestion process, which can become costly even for medium-size datasets. For files that are read over and over again, the additional read time accumulates over time.</p>
<p>To avoid this, Spark allows you to provide a column definition by providing a <code>columns</code> argument to describe your dataset. You can create this schema by sampling a small portion of the original file yourself:</p>
<div class="sourceCode" id="cb254"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb254-1" title="1">spec_with_r &lt;-<span class="st"> </span><span class="kw">sapply</span>(<span class="kw">read.csv</span>(<span class="st">&quot;data-csv/letters1.csv&quot;</span>, <span class="dt">nrows =</span> <span class="dv">10</span>), class)</a>
<a class="sourceLine" id="cb254-2" title="2">spec_with_r</a></code></pre></div>
<pre><code>        x         y 
 &quot;factor&quot; &quot;integer&quot; </code></pre>
<p>Or, you can set the column specification to a vector containing the column types explicitly. The vector’s values are named to match the field names:</p>
<div class="sourceCode" id="cb256"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb256-1" title="1">spec_explicit &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dt">x =</span> <span class="st">&quot;character&quot;</span>, <span class="dt">y =</span> <span class="st">&quot;numeric&quot;</span>)</a>
<a class="sourceLine" id="cb256-2" title="2">spec_explicit</a></code></pre></div>
<pre><code>          x           y 
&quot;character&quot;   &quot;numeric&quot; </code></pre>
<p>The accepted variable types are: <code>integer</code>, <code>character</code>, <code>logical</code>, <code>double</code>, <code>numeric</code>, <code>factor</code>, <code>Date</code>, and <code>POSIXct</code>.</p>
<p>Then, when<!--((("commands", "spark_read_csv()")))--> reading using <code>spark_read_csv()</code>, you can pass <code>spec_with_r</code> to the <code>columns</code> argument to match the names and types of the original file. This helps to improve performance since Spark will not need to determine the column types.</p>
<div class="sourceCode" id="cb258"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb258-1" title="1"><span class="kw">spark_read_csv</span>(sc, <span class="st">&quot;data-csv/&quot;</span>, <span class="dt">columns =</span> spec_with_r)</a></code></pre></div>
<pre><code># Source: spark&lt;datacsv&gt; [?? x 2]
  x         y
  &lt;chr&gt; &lt;int&gt;
1 a         1
2 b         2
3 c         3
4 a         1
5 b         2
6 c         3</code></pre>
<p>The following example shows how to set the field type to something different. However, the new field type needs to be a compatible type in the original dataset. For example, you cannot set a <code>character</code> field to <code>numeric</code>. If you use an incompatible type, the file read will fail with an error. Additionally, the following example also changes the names of the original fields:</p>
<div class="sourceCode" id="cb260"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb260-1" title="1">spec_compatible &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="dt">my_letter =</span> <span class="st">&quot;character&quot;</span>, <span class="dt">my_number =</span> <span class="st">&quot;character&quot;</span>)</a>
<a class="sourceLine" id="cb260-2" title="2"></a>
<a class="sourceLine" id="cb260-3" title="3"><span class="kw">spark_read_csv</span>(sc, <span class="st">&quot;data-csv/&quot;</span>, <span class="dt">columns =</span> spec_compatible)</a></code></pre></div>
<pre><code># Source: spark&lt;datacsv&gt; [?? x 2]
  my_letter my_number
  &lt;chr&gt;     &lt;chr&gt;    
1 a         1        
2 b         2        
3 c         3        
4 a         1        
5 b         2        
6 c         3    </code></pre>
<p>In Spark, <!--((("malformed entries")))-->malformed entries can cause errors during reading, particularly for non-character fields. To prevent such errors, we can use a file specification that imports them as characters and then use <code>dplyr</code> to coerce the field into the desired type.</p>
<p>This subsection reviewed how we can read files faster and with fewer failures, which lets us start our analysis more quickly. Another way to accelerate our analysis is by loading less data into Spark memory, which we examine in the next section.</p>
</div>
<div id="memory" class="section level3">
<h3><span class="header-section-number">8.2.3</span> Memory</h3>
<p>By<!--((("reading data", "memory")))((("memory")))--> default, when using Spark with R, when you read data, it is copied into Spark’s distributed memory, making data analysis and other operations very fast. There are cases, such as when the data is too big, for which loading all the data might not be practical or even necessary. For those cases, Spark can just “map” the files without copying data into memory.</p>
<p>The mapping creates a sort of virtual table in Spark. The implication is that when a query runs against that table, Spark needs to read the data from the files at that time. Any consecutive reads after that will do the same. In effect, Spark becomes a pass-through for the data. The advantage of this method is that there is almost no up-front time cost to “reading” the file; the mapping is very fast. The downside is that running queries that actually extract data will take longer.</p>
<p>This is controlled by the <code>memory</code> argument of the read functions. Setting it to <code>FALSE</code> prevents the data copy (the default is <code>TRUE</code>):</p>
<div class="sourceCode" id="cb262"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb262-1" title="1">mapped_csv &lt;-<span class="st"> </span><span class="kw">spark_read_csv</span>(sc, <span class="st">&quot;data-csv/&quot;</span>, <span class="dt">memory =</span> <span class="ot">FALSE</span>)</a></code></pre></div>
<p>There are good use cases for this method, one of which is when not all columns of a table are needed. For example, take a very large file that contains many columns. Assuming this is not the first time you interact with this data, you would know what columns are needed for the analysis. When you know which columns you need, the files can be read using <code>memory = FALSE</code>, and then the needed columns can be selected with <code>dplyr</code>. The resulting <code>dplyr</code> variable can then be cached into memory, using the <code>compute()</code> function. This will make Spark query the file(s), pull the selected fields, and copy only that data into memory. The result is an in-memory table that took comparatively less time to ingest:</p>
<div class="sourceCode" id="cb263"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb263-1" title="1">mapped_csv <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb263-2" title="2"><span class="st">  </span>dplyr<span class="op">::</span><span class="kw">select</span>(y) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb263-3" title="3"><span class="st">  </span>dplyr<span class="op">::</span><span class="kw">compute</span>(<span class="st">&quot;test&quot;</span>)</a></code></pre></div>
<p>The next section covers a short technique to make it easier to carry the original field names of imported data.</p>
</div>
<div id="columns" class="section level3">
<h3><span class="header-section-number">8.2.4</span> Columns</h3>
<p>Spark 1.6 required<!--((("reading data", "columns")))((("columns")))--> that column names be sanitized, so R does that by default. There might be cases when you would like to keep the original names intact, or when working with Spark version 2.0 or above. To do that, set the <code>sparklyr.sanitize.column.names</code> option to <code>FALSE</code>:</p>
<div class="sourceCode" id="cb264"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb264-1" title="1"><span class="kw">options</span>(<span class="dt">sparklyr.sanitize.column.names =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb264-2" title="2"><span class="kw">copy_to</span>(sc, iris, <span class="dt">overwrite =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code># Source:   table&lt;iris&gt; [?? x 5]
# Database: spark_connection
   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  
 1          5.1         3.5          1.4         0.2 setosa 
 2          4.9         3            1.4         0.2 setosa 
 3          4.7         3.2          1.3         0.2 setosa 
 4          4.6         3.1          1.5         0.2 setosa 
 5          5           3.6          1.4         0.2 setosa 
 6          5.4         3.9          1.7         0.4 setosa 
 7          4.6         3.4          1.4         0.3 setosa 
 8          5           3.4          1.5         0.2 setosa 
 9          4.4         2.9          1.4         0.2 setosa 
10          4.9         3.1          1.5         0.1 setosa 
# ... with more rows</code></pre>
<p>With this review of how to read data into Spark, we move on to look at how we can write data from our Spark<!--((("", startref="DHread08")))--> session.</p>
</div>
</div>
<div id="writing-data" class="section level2">
<h2><span class="header-section-number">8.3</span> Writing Data</h2>
<p>Some<!--((("writing data")))((("data handling", "writing data")))--> projects require that new data generated in Spark be written back to a remote source. For example, the data could be new predicted values returned by a Spark model. The job processes the mass generation of predictions, but then the predictions need to be stored. This section focuses on how you should use Spark for moving the data from Spark into an external destination.</p>
<p>Many new users start by downloading Spark data into R, and then upload it to a target, as illustrated in Figure <a href="data.html#fig:data-avoid-approach">8.2</a>. It works for smaller datasets, but it becomes inefficient for larger ones. The data typically grows in size to the point that it is no longer feasible for R to be the middle point.</p>
<div class="figure" style="text-align: center"><span id="fig:data-avoid-approach"></span>
<img src="the-r-in-spark_files/figure-html/data-avoid-approach-1.png" alt="Incorrect use of Spark when writing large datasets" width="auto" height="100pt" />
<p class="caption">
FIGURE 8.2: Incorrect use of Spark when writing large datasets
</p>
</div>
<p>All efforts should be made to have Spark connect to the target location. This way, reading, processing, and writing happens within the same Spark session.</p>
<p>As Figure <a href="data.html#fig:data-recommended-approach">8.3</a> shows, a better approach is to use Spark to read, process, and write to the target. This approach is able to scale as big as the Spark cluster allows, and prevents R from becoming a choke point.</p>
<div class="figure" style="text-align: center"><span id="fig:data-recommended-approach"></span>
<img src="the-r-in-spark_files/figure-html/data-recommended-approach-1.png" alt="Correct use of Spark when writing large datasets" width="auto" height="100pt" />
<p class="caption">
FIGURE 8.3: Correct use of Spark when writing large datasets
</p>
</div>
<p>Consider the following scenario: a Spark job just processed predictions for a large dataset, resulting in a considerable amount of predictions. Choosing a method to write results will depend on the technology infrastructure you are working on. More specifically, it will depend on Spark and the target running, or not, in the same cluster.</p>
<p>Back to our scenario, we have a large dataset in Spark that needs to be saved. When Spark and the target are in the same cluster, copying the results is not a problem; the data transfer is between RAM and disk of the same cluster or efficiently shuffled through a high-bandwidth connection.</p>
<p>But what to do if the target is not within the Spark cluster? There are two options, and choosing one will depend on the size of the data and network speed:</p>
<dl>
<dt>Spark transfer</dt>
<dd>In this case, Spark connects to the remote target location and copies the new data. If this is done within the same datacenter, or cloud provider, the data transfer could be fast enough to have Spark write the data directly.
</dd>
<dt>External transfer and otherwise</dt>
<dd>Spark can write the results to disk and transfers them via a third-party application. Spark writes the results as files and then a separate job copies the files over. In the target location, you would use a separate process to transfer the data into the target location.
</dd>
</dl>
<p>It is best to recognize that Spark, R, and any other technology are tools. No tool can do everything, nor should be expected to. Next we will describe how to copy data into Spark or collect large datasets that don’t fit in-memory, this can be used to transfer data across clusters, or help initialize your distributed datasets.</p>
</div>
<div id="copy" class="section level2">
<h2><span class="header-section-number">8.4</span> Copy</h2>
<p>Previous<!--((("copying data")))((("data handling", "copying data")))--> chapters used <code>copy_to()</code> as a handy helper to copy data into Spark; however, you can use <code>copy_to()</code> only to transfer in-memory datasets that are already loaded in memory. These datasets tend to be much smaller than the kind of datasets you would want to copy into Spark.</p>
<p>For instance, suppose that we have a 3 GB dataset generated as follows:</p>
<div class="sourceCode" id="cb266"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb266-1" title="1"><span class="kw">dir.create</span>(<span class="st">&quot;largefile.txt&quot;</span>)</a>
<a class="sourceLine" id="cb266-2" title="2"><span class="kw">write.table</span>(<span class="kw">matrix</span>(<span class="kw">rnorm</span>(<span class="dv">10</span> <span class="op">*</span><span class="st"> </span><span class="dv">10</span><span class="op">^</span><span class="dv">6</span>), <span class="dt">ncol =</span> <span class="dv">10</span>), <span class="st">&quot;largefile.txt/1&quot;</span>,</a>
<a class="sourceLine" id="cb266-3" title="3">            <span class="dt">append =</span> T, <span class="dt">col.names =</span> F, <span class="dt">row.names =</span> F)</a>
<a class="sourceLine" id="cb266-4" title="4"><span class="cf">for</span> (i <span class="cf">in</span> <span class="dv">2</span><span class="op">:</span><span class="dv">30</span>) </a>
<a class="sourceLine" id="cb266-5" title="5">  <span class="kw">file.copy</span>(<span class="st">&quot;largefile.txt/1&quot;</span>, <span class="kw">paste</span>(<span class="st">&quot;largefile.txt/&quot;</span>, i))</a></code></pre></div>
<p>If we had only 2 GB of memory in the driver node, we would not be able to load this 3 GB file into memory using <code>copy_to()</code>. Instead, when using the HDFS as storage in your cluster, you can use the <code>hadoop</code> command-line tool to copy files from disk into Spark from the terminal as follows. Notice that the following code works only in clusters using HDFS, not in local environments.</p>
<div class="sourceCode" id="cb267"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb267-1" title="1"><span class="ex">hadoop</span> fs -copyFromLocal largefile.txt largefile.txt</a></code></pre></div>
<p>You then can read the uploaded file, as described in the <a href="data.html#data-file-formats">File Formats</a> section; for text files, you would run:</p>
<div class="sourceCode" id="cb268"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb268-1" title="1"><span class="kw">spark_read_text</span>(sc, <span class="st">&quot;largefile.txt&quot;</span>, <span class="dt">memory =</span> <span class="ot">FALSE</span>)</a></code></pre></div>
<pre><code># Source: spark&lt;largefile&gt; [?? x 1]
   line                                                                   
   &lt;chr&gt;                                                                  
 1 0.0982531064914565 -0.577567317599452 -1.66433938237253 -0.20095089489…
 2 -1.08322304504007 1.05962389624635 1.1852771207729 -0.230934710049462 …
 3 -0.398079835552421 0.293643382374479 0.727994248743204 -1.571547990532…
 4 0.418899768227183 0.534037617828835 0.921680317620166 -1.6623094393911…
 5 -0.204409401553028 -0.0376212693728992 -1.13012269711811 0.56149527218…
 6 1.41192628218417 -0.580413572014808 0.727722566256326 0.5746066486689 …
 7 -0.313975036262443 -0.0166426329807508 -0.188906975208319 -0.986203251…
 8 -0.571574679637623 0.513472254005066 0.139050812059352 -0.822738334753…
 9 1.39983023148955 -1.08723592838627 1.02517804413913 -0.412680186313667…
10 0.6318328148434 -1.08741784644221 -0.550575696474202 0.971967251067794…
# … with more rows</code></pre>
<p><code>collect()</code> has a similar limitation in that it can collect only datasets that fit your driver memory; however, if you had to extract a large dataset from Spark through the driver node, you could use specialized tools provided by the distributed storage. For HDFS, you would run the following:</p>
<div class="sourceCode" id="cb270"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb270-1" title="1"><span class="ex">hadoop</span> fs -copyToLocal largefile.txt largefile.txt</a></code></pre></div>
<p>Alternatively, you can also collect datasets that don’t fit in memory by providing a callback to <code>collect()</code>. A callback is just an R function that will be called over each Spark partition. You then can write this dataset to disk or push to other clusters over the network.</p>
<p>You could use the following code to collect 3 GB even if the driver node collecting this dataset had less than 3 GB of memory. That said, as <a href="analysis.html#analysis">Chapter 3</a> explains, you should avoid collecting large datasets into a single machine since this creates a significant performance bottleneck. For conciseness, we will collect only the first million rows; feel free to remove <code>head(10^6)</code> if you have a few minutes to spare:</p>
<div class="sourceCode" id="cb271"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb271-1" title="1"><span class="kw">dir.create</span>(<span class="st">&quot;large&quot;</span>)</a>
<a class="sourceLine" id="cb271-2" title="2"><span class="kw">spark_read_text</span>(sc, <span class="st">&quot;largefile.txt&quot;</span>, <span class="dt">memory =</span> <span class="ot">FALSE</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb271-3" title="3"><span class="st">  </span><span class="kw">head</span>(<span class="dv">10</span><span class="op">^</span><span class="dv">6</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb271-4" title="4"><span class="st">  </span><span class="kw">collect</span>(<span class="dt">callback =</span> <span class="cf">function</span>(df, idx) {</a>
<a class="sourceLine" id="cb271-5" title="5">    <span class="kw">writeLines</span>(df<span class="op">$</span>line, <span class="kw">paste0</span>(<span class="st">&quot;large/large-&quot;</span>, idx, <span class="st">&quot;.txt&quot;</span>))</a>
<a class="sourceLine" id="cb271-6" title="6">  })</a></code></pre></div>
<p>Make sure you clean up these large files and empty your recycle bin as well:</p>
<div class="sourceCode" id="cb272"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb272-1" title="1"><span class="kw">unlink</span>(<span class="st">&quot;largefile.txt&quot;</span>, <span class="dt">recursive =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb272-2" title="2"><span class="kw">unlink</span>(<span class="st">&quot;large&quot;</span>, <span class="dt">recursive =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<p>In most cases, data will already be stored in the cluster, so you should not need to worry about copying large datasets; instead, you can usually focus on reading and writing different file formats, which we describe next.</p>
</div>
<div id="data-file-formats" class="section level2">
<h2><span class="header-section-number">8.5</span> File Formats</h2>
<p>Out<!--((("data handling", "file formats", id="DHfile08")))--> of the box, Spark is able to interact with several file formats, like CSV, JSON, LIBSVM, ORC, and Parquet. Table <a href="data.html#tab:data-formats-table">8.1</a> maps the file format to the function you should use to read and write data in Spark.</p>
<table>
<caption><span id="tab:data-formats-table">TABLE 8.1: </span>Spark functions to read and write file formats</caption>
<thead>
<tr class="header">
<th align="left">Format</th>
<th align="left">Read</th>
<th align="left">Write</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">Comma separated values (CSV)</td>
<td align="left">spark_read_csv()</td>
<td align="left">spark_write_csv()</td>
</tr>
<tr class="even">
<td align="left">JavaScript Object Notation (JSON)</td>
<td align="left">spark_read_json()</td>
<td align="left">spark_write_json()</td>
</tr>
<tr class="odd">
<td align="left">Library for Support Vector Machines (LIBSVM)</td>
<td align="left">spark_read_libsvm()</td>
<td align="left">spark_write_libsvm()</td>
</tr>
<tr class="even">
<td align="left">Optimized Row Columnar (ORC)</td>
<td align="left">spark_read_orc()</td>
<td align="left">spark_write_orc()</td>
</tr>
<tr class="odd">
<td align="left">Apache Parquet</td>
<td align="left">spark_read_parquet()</td>
<td align="left">spark_write_parquet()</td>
</tr>
<tr class="even">
<td align="left">Text</td>
<td align="left">spark_read_text()</td>
<td align="left">spark_write_text()</td>
</tr>
</tbody>
</table>
<p>The following sections will describe special considerations particular to each file format as well as some of the strengths and weaknesses of some popular file formats, starting with the well-known CSV file format.</p>
<div id="csv" class="section level3">
<h3><span class="header-section-number">8.5.1</span> CSV</h3>
<p>The<!--((("file formats", "CSV")))((("CSV format")))--> CSV format might be the most common file type in use today. It is defined by a text file separated by a given character, usually a comma. It should be pretty straightforward to read CSV files; however, it’s worth mentioning a couple techniques that can help you process CSVs that are not fully compliant with a well-formed CSV file. Spark offers the following modes for addressing parsing issues:</p>
<dl>
<dt>Permissive</dt>
<dd>Inserts NULL values for missing tokens
</dd>
<dt>Drop Malformed</dt>
<dd>Drops lines that are malformed
</dd>
<dt>Fail Fast</dt>
<dd>Aborts if it encounters any malformed line
</dd>
</dl>
<p>You can use these in <code>sparklyr</code> by passing them inside the <code>options</code> argument. The following example creates a file with a broken entry. It then shows how it can be read into Spark:</p>
<div class="sourceCode" id="cb273"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb273-1" title="1"><span class="co">## Creates bad test file</span></a>
<a class="sourceLine" id="cb273-2" title="2"><span class="kw">writeLines</span>(<span class="kw">c</span>(<span class="st">&quot;bad&quot;</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="st">&quot;broken&quot;</span>), <span class="st">&quot;bad.csv&quot;</span>)</a>
<a class="sourceLine" id="cb273-3" title="3"></a>
<a class="sourceLine" id="cb273-4" title="4"><span class="kw">spark_read_csv</span>(</a>
<a class="sourceLine" id="cb273-5" title="5">  sc,</a>
<a class="sourceLine" id="cb273-6" title="6">  <span class="st">&quot;bad3&quot;</span>,</a>
<a class="sourceLine" id="cb273-7" title="7">  <span class="st">&quot;bad.csv&quot;</span>,</a>
<a class="sourceLine" id="cb273-8" title="8">  <span class="dt">columns =</span> <span class="kw">list</span>(<span class="dt">foo =</span> <span class="st">&quot;integer&quot;</span>),</a>
<a class="sourceLine" id="cb273-9" title="9">  <span class="dt">options =</span> <span class="kw">list</span>(<span class="dt">mode =</span> <span class="st">&quot;DROPMALFORMED&quot;</span>))</a></code></pre></div>
<pre><code># Source: spark&lt;bad3&gt; [?? x 1]
    foo
  &lt;int&gt;
1     1
2     2
3     3</code></pre>
<p>Spark provides an issue tracking column, which was hidden by default. To enable it, add <code>_corrupt_record</code> to the <code>columns</code> list. You can combine this with the use of the <code>PERMISSIVE</code> mode. All rows will be imported, invalid entries will receive an <code>NA</code>, and the issue will be tracked in the <code>_corrupt_record</code> column:</p>
<div class="sourceCode" id="cb275"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb275-1" title="1"><span class="kw">spark_read_csv</span>(</a>
<a class="sourceLine" id="cb275-2" title="2">  sc,</a>
<a class="sourceLine" id="cb275-3" title="3">  <span class="st">&quot;bad2&quot;</span>,</a>
<a class="sourceLine" id="cb275-4" title="4">  <span class="st">&quot;bad.csv&quot;</span>,</a>
<a class="sourceLine" id="cb275-5" title="5">  <span class="dt">columns =</span> <span class="kw">list</span>(<span class="dt">foo =</span> <span class="st">&quot;integer&quot;</span>, <span class="st">&quot;_corrupt_record&quot;</span> =<span class="st"> &quot;character&quot;</span>),</a>
<a class="sourceLine" id="cb275-6" title="6">  <span class="dt">options =</span> <span class="kw">list</span>(<span class="dt">mode =</span> <span class="st">&quot;PERMISSIVE&quot;</span>)</a>
<a class="sourceLine" id="cb275-7" title="7">)</a></code></pre></div>
<pre><code># Source: spark&lt;bad2&gt; [?? x 2]
    foo `_corrupt_record`
  &lt;int&gt; &lt;chr&gt;            
1     1 NA               
2     2 NA               
3     3 NA               
4    NA broken  </code></pre>
<p>Reading and storing data as CSVs is quite common and supported across most systems. For tabular datasets, it is still a popular option, but for datasets containing nested structures and nontabular data, JSON is usually preferred.</p>
</div>
<div id="json" class="section level3">
<h3><span class="header-section-number">8.5.2</span> JSON</h3>
<p>JSON<!--((("file formats", "JSON")))((("JSON file format")))--> is a file format originally derived from JavaScript that has grown to be language-independent and very popular due to its flexibility and ubiquitous support. Reading and writing JSON files is quite straightforward:</p>
<div class="sourceCode" id="cb277"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb277-1" title="1"><span class="kw">writeLines</span>(<span class="st">&quot;{&#39;a&#39;:1, &#39;b&#39;: {&#39;f1&#39;: 2, &#39;f3&#39;: 3}}&quot;</span>, <span class="st">&quot;data.json&quot;</span>)</a>
<a class="sourceLine" id="cb277-2" title="2">simple_json &lt;-<span class="st"> </span><span class="kw">spark_read_json</span>(sc, <span class="st">&quot;data.json&quot;</span>)</a>
<a class="sourceLine" id="cb277-3" title="3">simple_json</a></code></pre></div>
<pre><code># Source: spark&lt;data&gt; [?? x 2]
      a b         
  &lt;dbl&gt; &lt;list&gt;    
1     1 &lt;list [2]&gt;</code></pre>
<p>However, when you deal with a dataset containing nested fields like the one from this example, it is worth pointing out how to extract nested fields. One approach is to use a JSON path, which is a domain-specific syntax commonly used to extract and query JSON files. You can use a combination of <code>get_json_object()</code> and <code>to_json()</code> to specify the JSON path you are interested in. To extract <code>f1</code> you would run the following transformation:</p>
<div class="sourceCode" id="cb279"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb279-1" title="1">simple_json <span class="op">%&gt;%</span><span class="st"> </span>dplyr<span class="op">::</span><span class="kw">transmute</span>(<span class="dt">z =</span> <span class="kw">get_json_object</span>(<span class="kw">to_json</span>(b), <span class="st">&#39;$.f1&#39;</span>))</a></code></pre></div>
<pre><code># Source: spark&lt;?&gt; [?? x 3]
      a b          z    
  &lt;dbl&gt; &lt;list&gt;     &lt;chr&gt;
1     1 &lt;list [2]&gt; 2 </code></pre>
<p>Another approach is to install <code>sparkly.nested</code> from CRAN with <code>install.packages("sparklyr.nested")</code> and then unnest nested data with <code>sdf_unnest()</code>:</p>
<div class="sourceCode" id="cb281"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb281-1" title="1">sparklyr.nested<span class="op">::</span><span class="kw">sdf_unnest</span>(simple_json, <span class="st">&quot;b&quot;</span>)</a></code></pre></div>
<pre><code># Source: spark&lt;?&gt; [?? x 3]
      a    f1    f3
  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
1     1     2     3</code></pre>
<p>While JSON and CSVs are quite simple to use and versatile, they are not optimized for performance; however, other formats like ORC, AVRO, and Parquet are.</p>
</div>
<div id="parquet" class="section level3">
<h3><span class="header-section-number">8.5.3</span> Parquet</h3>
<p>Apache Parquet, Apache ORC,<!--((("file formats", "Parquet")))((("Parquet")))((("Apache Parquet")))--> and Apache AVRO are all file formats designed with performance in mind. Parquet and ORC store data in columnar format, while AVRO is row-based. All of them are binary file formats, which reduces storage space and improves performance. This comes at the cost of making them a bit more difficult to read by external systems and libraries; however, this is usually not an issue when used as intermediate data storage within Spark.</p>
<p>To illustrate this, Figure <a href="data.html#fig:data-file-format-benchmark">8.4</a> plots the result of running a 1-million-row write-speed benchmark using the <code>bench</code> package; feel free to use your own benchmarks over meaningful datasets when deciding which format best fits your needs:</p>
<div class="sourceCode" id="cb283"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb283-1" title="1">numeric &lt;-<span class="st"> </span><span class="kw">copy_to</span>(sc, <span class="kw">data.frame</span>(<span class="dt">nums =</span> <span class="kw">runif</span>(<span class="dv">10</span><span class="op">^</span><span class="dv">6</span>)))</a>
<a class="sourceLine" id="cb283-2" title="2">bench<span class="op">::</span><span class="kw">mark</span>(</a>
<a class="sourceLine" id="cb283-3" title="3">  <span class="dt">CSV =</span> <span class="kw">spark_write_csv</span>(numeric, <span class="st">&quot;data.csv&quot;</span>, <span class="dt">mode =</span> <span class="st">&quot;overwrite&quot;</span>),</a>
<a class="sourceLine" id="cb283-4" title="4">  <span class="dt">JSON =</span> <span class="kw">spark_write_json</span>(numeric, <span class="st">&quot;data.json&quot;</span>, <span class="dt">mode =</span> <span class="st">&quot;overwrite&quot;</span>),</a>
<a class="sourceLine" id="cb283-5" title="5">  <span class="dt">Parquet =</span> <span class="kw">spark_write_parquet</span>(numeric, <span class="st">&quot;data.parquet&quot;</span>, <span class="dt">mode =</span> <span class="st">&quot;overwrite&quot;</span>),</a>
<a class="sourceLine" id="cb283-6" title="6">  <span class="dt">ORC =</span> <span class="kw">spark_write_parquet</span>(numeric, <span class="st">&quot;data.orc&quot;</span>, <span class="dt">mode =</span> <span class="st">&quot;overwrite&quot;</span>),</a>
<a class="sourceLine" id="cb283-7" title="7">  <span class="dt">iterations =</span> <span class="dv">20</span></a>
<a class="sourceLine" id="cb283-8" title="8">) <span class="op">%&gt;%</span><span class="st"> </span>ggplot2<span class="op">::</span><span class="kw">autoplot</span>()</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:data-file-format-benchmark"></span>
<img src="images/data-file-format-benchmark-resized.png" alt="One-million-rows write benchmark between CSV, JSON, Parquet, and ORC" width="1500" />
<p class="caption">
FIGURE 8.4: One-million-rows write benchmark between CSV, JSON, Parquet, and ORC
</p>
</div>
<p>From now on, be sure to disconnect from Spark whenever we present a new <code>spark_connect()</code> command:</p>
<div class="sourceCode" id="cb284"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb284-1" title="1"><span class="kw">spark_disconnect</span>(sc)</a></code></pre></div>
<p>This concludes the introduction to some of the out-of-the-box supported file formats, we will present next how to deal with formats that require external packages and customization.</p>
</div>
<div id="others" class="section level3">
<h3><span class="header-section-number">8.5.4</span> Others</h3>
<p>Spark<!--((("file formats", "others")))--> is a very flexible computing platform. It can add functionality by using extension programs, called packages. You can access a new source type or file system by using the appropriate package.</p>
<p>Packages need to be loaded into Spark at connection time. To load the package, Spark needs its location, which could be inside the cluster, in a file share, or the internet.</p>
<p>In <code>sparklyr</code>, the package location is passed to <code>spark_connect()</code>. All packages should be listed in the <code>sparklyr.connect.packages</code> entry of the connection configuration.</p>
<p>It is possible to access data source types that we didn’t previously list. Loading the appropriate default package for Spark is the first of two steps The second step is to actually read or write the data. The <code>spark_read_source()</code> and <code>spark_write_source()</code> functions do that. They are generic functions that can use the libraries imported by a default package.</p>
<p>For instance, we can read XML files as follows:</p>
<div class="sourceCode" id="cb285"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb285-1" title="1">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>, <span class="dt">version =</span> <span class="st">&quot;2.3&quot;</span>, <span class="dt">config =</span> <span class="kw">list</span>(</a>
<a class="sourceLine" id="cb285-2" title="2">  <span class="dt">sparklyr.connect.packages =</span> <span class="st">&quot;com.databricks:spark-xml_2.11:0.5.0&quot;</span>))</a>
<a class="sourceLine" id="cb285-3" title="3"></a>
<a class="sourceLine" id="cb285-4" title="4"><span class="kw">writeLines</span>(<span class="st">&quot;&lt;ROWS&gt;&lt;ROW&gt;&lt;text&gt;Hello World&lt;/text&gt;&lt;/ROW&gt;&quot;</span>, <span class="st">&quot;simple.xml&quot;</span>)</a>
<a class="sourceLine" id="cb285-5" title="5"><span class="kw">spark_read_source</span>(sc, <span class="st">&quot;simple_xml&quot;</span>, <span class="st">&quot;simple.xml&quot;</span>, <span class="st">&quot;xml&quot;</span>)</a></code></pre></div>
<pre><code># Source: spark&lt;data&gt; [?? x 1]
  text       
  &lt;chr&gt;      
1 Hello World</code></pre>
<p>which you can also write back to XML with ease, as follows:</p>
<div class="sourceCode" id="cb287"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb287-1" title="1"><span class="kw">tbl</span>(sc, <span class="st">&quot;simple_xml&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb287-2" title="2"><span class="st">  </span><span class="kw">spark_write_source</span>(<span class="st">&quot;xml&quot;</span>, <span class="dt">options =</span> <span class="kw">list</span>(<span class="dt">path =</span> <span class="st">&quot;data.xml&quot;</span>))</a></code></pre></div>
<p>In addition, there are a few extensions developed by the R community to load additional file formats, such as <code>sparklyr.nested</code> to assist with nested data, <code>spark.sas7bdat</code> to read data from SAS, <code>sparkavro</code> to read data in AVRO format, and <code>sparkwarc</code> to read WARC files, which use extensibility mechanisms introduced in <a href="extensions.html#extensions">Chapter 10</a>. <a href="distributed.html#distributed">Chapter 11</a> presents techniques to use R packages to load additional file formats, and <a href="contributing">Chapter 13</a> presents techniques to use Java libraries to complement this further. But first, let’s explore how to retrieve and store files from several different file systems.<!--((("", startref="DHfile08")))--></p>
</div>
</div>
<div id="data-file-systems" class="section level2">
<h2><span class="header-section-number">8.6</span> File Systems</h2>
<p>Spark<!--((("file systems")))((("data handling", "file systems")))--> defaults to the file system on which it is currently running. In a YARN managed cluster, the default file system will be HDFS. An example path of <em>/home/user/file.csv</em> will be read from the cluster’s HDFS folders, not the Linux folders. The operating system’s file system will be accessed for other deployments, such as Standalone, and <code>sparklyr</code>’s local.</p>
<p>The file system protocol can be changed when reading or writing. You do this via the <code>path</code> argument of the <code>sparklyr</code> function. For example, a full path of _<a href="file://home/user/file.csv_" class="uri">file://home/user/file.csv_</a> forces the use of the local operating system’s file system.</p>
<p>There are many other file system protocols, such as <code>_dbfs://_</code> for Databricks’ file system, <code>_s3a://_</code> for Amazon’s S3 service, <code>_wasb://_</code> for Microsoft Azure storage, and <code>_gs://_</code> for Google storage.</p>
<p>Spark does not provide support for all them directly; instead, they are configured as needed. For instance, accessing the “s3a” protocol requires adding a package to the <code>sparklyr.connect.packages</code> configuration setting, while connecting and specifying appropriate credentials might require using the <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code> environment variables.</p>
<div class="sourceCode" id="cb288"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb288-1" title="1"><span class="kw">Sys.setenv</span>(<span class="dt">AWS_ACCESS_KEY_ID =</span> my_key_id)</a>
<a class="sourceLine" id="cb288-2" title="2"><span class="kw">Sys.setenv</span>(<span class="dt">AWS_SECRET_ACCESS_KEY =</span> my_secret_key)</a>
<a class="sourceLine" id="cb288-3" title="3"></a>
<a class="sourceLine" id="cb288-4" title="4">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>, <span class="dt">version =</span> <span class="st">&quot;2.3&quot;</span>, <span class="dt">config =</span> <span class="kw">list</span>(</a>
<a class="sourceLine" id="cb288-5" title="5">  <span class="dt">sparklyr.connect.packages =</span> <span class="st">&quot;org.apache.hadoop:hadoop-aws:2.7.7&quot;</span>))</a>
<a class="sourceLine" id="cb288-6" title="6"></a>
<a class="sourceLine" id="cb288-7" title="7">my_file &lt;-<span class="st"> </span><span class="kw">spark_read_csv</span>(sc, <span class="st">&quot;my-file&quot;</span>, <span class="dt">path =</span> <span class="st">&quot;s3a://my-bucket/my-file.csv&quot;</span>)</a></code></pre></div>
<p>Accessing other file protocols requires loading different packages, although, in some cases, the vendor providing the Spark environment might load the package for you. Please refer to your vendor’s documentation to find out whether that is the case.</p>
</div>
<div id="data-storage-systems" class="section level2">
<h2><span class="header-section-number">8.7</span> Storage Systems</h2>
<p>A<!--((("data handling", "storage systems", id="DHstor08")))((("storage systems", "overview of")))--> data lake and Spark usually go hand-in-hand, with optional access to storage systems like databases and data warehouses. Presenting all the different storage systems with appropriate examples would be quite time-consuming, so instead we present some of the commonly used storage systems.</p>
<p>As a start, Apache <em>Hive</em> is a data warehouse software that facilitates reading, writing, and managing large datasets residing in distributed storage using SQL. In fact, Spark has components from Hive built directly into its sources. It is very common to have installations of Spark or Hive side-by-side, so we will start by presenting Hive, followed by Cassandra, and then close by looking at JDBC connections.</p>
<div id="hive" class="section level3">
<h3><span class="header-section-number">8.7.1</span> Hive</h3>
<p>In<!--((("storage systems", "Hive")))((("Hive project", "Spark integration with")))--> YARN managed clusters, Spark provides a deeper integration with Apache Hive. Hive tables are easily accessible after opening a Spark connection.</p>
<p>You can access a Hive table’s data using <code>DBI</code> by referencing the table in a SQL statement:</p>
<div class="sourceCode" id="cb289"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb289-1" title="1">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>, <span class="dt">version =</span> <span class="st">&quot;2.3&quot;</span>)</a>
<a class="sourceLine" id="cb289-2" title="2"><span class="kw">spark_read_csv</span>(sc, <span class="st">&quot;test&quot;</span>, <span class="st">&quot;data-csv/&quot;</span>, <span class="dt">memory =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb289-3" title="3"></a>
<a class="sourceLine" id="cb289-4" title="4">DBI<span class="op">::</span><span class="kw">dbGetQuery</span>(sc, <span class="st">&quot;SELECT * FROM test limit 10&quot;</span>)</a></code></pre></div>
<p>Another way to reference a table is with <code>dplyr</code> using the <code>tbl()</code> function, which retrieves a reference to the table:</p>
<div class="sourceCode" id="cb290"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb290-1" title="1">dplyr<span class="op">::</span><span class="kw">tbl</span>(sc, <span class="st">&quot;test&quot;</span>)</a></code></pre></div>
<p>It is important to reiterate that no data is imported into R; the <code>tbl()</code> function only creates a reference. You then can pipe more <code>dplyr</code> verbs following the <code>tbl()</code> command:</p>
<div class="sourceCode" id="cb291"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb291-1" title="1">dplyr<span class="op">::</span><span class="kw">tbl</span>(sc, <span class="st">&quot;test&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb291-2" title="2"><span class="st">  </span>dplyr<span class="op">::</span><span class="kw">group_by</span>(y) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb291-3" title="3"><span class="st">  </span>dplyr<span class="op">::</span><span class="kw">summarise</span>(<span class="dt">totals =</span> <span class="kw">sum</span>(y))</a></code></pre></div>
<p>Hive table references assume a default database source. Often, the needed table is in a different database within the metastore. To access it using SQL, prefix the database name to the table. Separate them using a period, as demonstrated here:</p>
<div class="sourceCode" id="cb292"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb292-1" title="1">DBI<span class="op">::</span><span class="kw">dbSendQuery</span>(sc, <span class="st">&quot;SELECT * FROM databasename.table&quot;</span>)</a></code></pre></div>
<p>In <code>dplyr</code>, the <code>in_schema()</code> function can be used. The function is used inside the <code>tbl()</code> call:</p>
<div class="sourceCode" id="cb293"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb293-1" title="1"><span class="kw">tbl</span>(sc, dbplyr<span class="op">::</span><span class="kw">in_schema</span>(<span class="st">&quot;databasename&quot;</span>, <span class="st">&quot;table&quot;</span>))</a></code></pre></div>
<p>You can also use the <code>tbl_change_db()</code> function to set the current session’s default database. Any subsequent call via <code>DBI</code> or <code>dplyr</code> will use the selected name as the default database:</p>
<div class="sourceCode" id="cb294"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb294-1" title="1"><span class="kw">tbl_change_db</span>(sc, <span class="st">&quot;databasename&quot;</span>)</a></code></pre></div>
<p>The following examples require additional Spark packages and databases which might be difficult to follow unless you happen to have a JDBC driver or Cassandra database accessible to you.</p>
<div class="sourceCode" id="cb295"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb295-1" title="1"><span class="kw">spark_disconnect</span>(sc)</a></code></pre></div>
<p>Next, we explore a less structured storage system, often<!--((("NoSQL databases")))--> referred to as a <em>NoSQL database</em>.</p>
</div>
<div id="cassandra" class="section level3">
<h3><span class="header-section-number">8.7.2</span> Cassandra</h3>
<p>Apache <em>Cassandra</em> is<!--((("storage systems", "Cassandra")))((("Cassandra")))--> a free and open source, distributed, wide-column store, NoSQL database management system designed to handle large amounts of data across many commodity servers. While there are many other database systems beyond Cassandra, taking a quick look at how Cassandra can be used from Spark will give you insight into how to make use of other database and storage systems like Solr, Redshift, Delta Lake, and others.</p>
<p>The following example code shows how to use the <code>datastax:spark-cassandra-connector</code> package to read from Cassandra. The key is to use the <code>org.apache.spark.sql.cassandra</code> library as the <code>source</code> argument. It provides the mapping Spark can use to make sense of the data source. Unless you have a Cassandra database, skip executing the following statement:</p>
<div class="sourceCode" id="cb296"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb296-1" title="1">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>, <span class="dt">version =</span> <span class="st">&quot;2.3&quot;</span>, <span class="dt">config =</span> <span class="kw">list</span>(</a>
<a class="sourceLine" id="cb296-2" title="2">  <span class="dt">sparklyr.connect.packages =</span> <span class="st">&quot;datastax:spark-cassandra-connector:2.3.1-s_2.11&quot;</span>))</a>
<a class="sourceLine" id="cb296-3" title="3"></a>
<a class="sourceLine" id="cb296-4" title="4"><span class="kw">spark_read_source</span>(</a>
<a class="sourceLine" id="cb296-5" title="5">  sc, </a>
<a class="sourceLine" id="cb296-6" title="6">  <span class="dt">name =</span> <span class="st">&quot;emp&quot;</span>,</a>
<a class="sourceLine" id="cb296-7" title="7">  <span class="dt">source =</span> <span class="st">&quot;org.apache.spark.sql.cassandra&quot;</span>,</a>
<a class="sourceLine" id="cb296-8" title="8">  <span class="dt">options =</span> <span class="kw">list</span>(<span class="dt">keyspace =</span> <span class="st">&quot;dev&quot;</span>, <span class="dt">table =</span> <span class="st">&quot;emp&quot;</span>),</a>
<a class="sourceLine" id="cb296-9" title="9">  <span class="dt">memory =</span> <span class="ot">FALSE</span>)</a></code></pre></div>
<p>One<!--((("pushdown predicates")))--> of the most useful features of Spark when dealing with external databases and data warehouses is that Spark can push down computation to the database, a feature known as <em>pushdown predicates</em>. In a nutshell, pushdown predicates improve performance by asking remote databases smart questions. When you execute a query that contains the <code>filter(age &gt; 20)</code> expression against a remote table referenced through <code>spark_read_source()</code> and not loaded in memory, rather than bringing the entire table into Spark, it will be passed to the remote database and only a subset of the remote table is retrieved.</p>
<p>While it is ideal to find Spark packages that support the remote storage system, there will be times when a package is not available and you need to consider vendor JDBC drivers.</p>
</div>
<div id="jdbc" class="section level3">
<h3><span class="header-section-number">8.7.3</span> JDBC</h3>
<p>When<!--((("storage systems", "JDBC")))((("JDBC")))--> a Spark package is not available to provide connectivity, you can consider a JDBC connection. JDBC is an interface for the programming language Java, which defines how a client can access a database.</p>
<p>It is quite easy to connect to a remote database with <code>spark_read_jdbc()</code>, and <code>spark_write_jdbc()</code>; as long as you have access to the appropriate JDBC driver, which at times is trivial and other times is quite an adventure. To keep this simple, we can briefly consider how a connection to a remote MySQL database could be accomplished.</p>
<p>First, you would need to download the appropriate JDBC driver from MySQL’s developer portal and specify this additional driver as a <code>sparklyr.shell.driver-class-path</code> connection option. Since JDBC drivers are Java-based, the code is contained within a <em>JAR</em> (Java ARchive) file. As soon as you’re connected to Spark with the appropriate driver, you can use the <em>jdbc://</em> protocol to access particular drivers and databases. Unless you are willing to download and configure MySQL on your own, skip executing the following statement:</p>
<div class="sourceCode" id="cb297"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb297-1" title="1">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>, <span class="dt">version =</span> <span class="st">&quot;2.3&quot;</span>, <span class="dt">config =</span> <span class="kw">list</span>(</a>
<a class="sourceLine" id="cb297-2" title="2">  <span class="st">&quot;sparklyr.shell.driver-class-path&quot;</span> =</a>
<a class="sourceLine" id="cb297-3" title="3"><span class="st">    &quot;~/Downloads/mysql-connector-java-5.1.41/mysql-connector-java-5.1.41-bin.jar&quot;</span></a>
<a class="sourceLine" id="cb297-4" title="4">))</a>
<a class="sourceLine" id="cb297-5" title="5"></a>
<a class="sourceLine" id="cb297-6" title="6"><span class="kw">spark_read_jdbc</span>(sc, <span class="st">&quot;person_jdbc&quot;</span>,  <span class="dt">options =</span> <span class="kw">list</span>(</a>
<a class="sourceLine" id="cb297-7" title="7">  <span class="dt">url =</span> <span class="st">&quot;jdbc:mysql://localhost:3306/sparklyr&quot;</span>,</a>
<a class="sourceLine" id="cb297-8" title="8">  <span class="dt">user =</span> <span class="st">&quot;root&quot;</span>, <span class="dt">password =</span> <span class="st">&quot;&lt;password&gt;&quot;</span>,</a>
<a class="sourceLine" id="cb297-9" title="9">  <span class="dt">dbtable =</span> <span class="st">&quot;person&quot;</span>))</a></code></pre></div>
<p>If you are a customer of particular database vendors, making use of the vendor-provided resources is usually the best place to start looking for appropriate drivers.<!--((("", startref="DHstor08")))--></p>
</div>
</div>
<div id="recap-5" class="section level2">
<h2><span class="header-section-number">8.8</span> Recap</h2>
<p>This chapter expanded on how and why you should use Spark to connect and process a variety of data sources through a new data storage model known as data lakes—a storage pattern that provides more flexibility than standard ETL processes by enabling you to use raw datasets with, potentially, more information to enrich data analysis and modeling.</p>
<p>We also presented best practices for reading, writing, and copying data into and from Spark. We then returned to exploring the components of a data lake: file formats and file systems, with the former representing how data is stored, and the latter where the data is stored. You then learned how to tackle file formats and storage systems that require additional Spark packages, reviewed some of the performance trade-offs across file formats, and learned the concepts required to make use of storage systems (databases and warehouses) in Spark.</p>
<p>While reading and writing datasets should come naturally to you, you might still hit resource restrictions while reading and writing large datasets. To handle these situations, <a href="tuning.html#tuning">Chapter 9</a> shows you how Spark manages tasks and data across multiple machines, which in turn allows you to further improve the performance of your analysis and modeling tasks.</p>

</div>
</div>
<div class="footnotes">
<hr />
<ol start="21">
<li id="fn21"><p>Codd EF (1970). “A relational model of data for large shared data banks.”<a href="data.html#fnref21" class="footnote-back">↩</a></p></li>
</ol>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="connections.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="tuning.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
