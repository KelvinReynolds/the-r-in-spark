<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 7 Data | The R in Spark: Learning Apache Spark with R</title>
  <meta name="description" content="A book to learn Apache Spark with R using the sparklyr R package.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 7 Data | The R in Spark: Learning Apache Spark with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A book to learn Apache Spark with R using the sparklyr R package." />
  <meta name="github-repo" content="javierluraschi/the-r-in-spark" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Data | The R in Spark: Learning Apache Spark with R" />
  
  <meta name="twitter:description" content="A book to learn Apache Spark with R using the sparklyr R package." />
  



<meta name="date" content="2019-05-05">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="connections.html">
<link rel="next" href="tuning.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.3/htmlwidgets.js"></script>
<script src="libs/dagre-0.0.1/dagre.min.js"></script>
<script src="libs/lodash-3.7.0/lodash.js"></script>
<script src="libs/nomnoml-0.2.0/nomnoml.js"></script>
<script src="libs/nomnoml-binding-0.1.0/nomnoml.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-119986300-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-119986300-1');
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./">Learning Apache Spark with R</a></strong></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#intro-background"><i class="fa fa-check"></i><b>1.1</b> Information</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#intro-hadoop"><i class="fa fa-check"></i><b>1.2</b> Hadoop</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#intro-spark"><i class="fa fa-check"></i><b>1.3</b> Spark</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#intro-r"><i class="fa fa-check"></i><b>1.4</b> R</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#intro-sparklyr"><i class="fa fa-check"></i><b>1.5</b> sparklyr</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#intro-recap"><i class="fa fa-check"></i><b>1.6</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="starting.html"><a href="starting.html"><i class="fa fa-check"></i><b>2</b> Getting Started</a><ul>
<li class="chapter" data-level="2.1" data-path="starting.html"><a href="starting.html#starting-prerequisites"><i class="fa fa-check"></i><b>2.1</b> Prerequisites</a></li>
<li class="chapter" data-level="2.2" data-path="starting.html"><a href="starting.html#starting-install-sparklyr"><i class="fa fa-check"></i><b>2.2</b> Installing sparklyr</a></li>
<li class="chapter" data-level="2.3" data-path="starting.html"><a href="starting.html#starting-installing-spark"><i class="fa fa-check"></i><b>2.3</b> Installing Spark</a></li>
<li class="chapter" data-level="2.4" data-path="starting.html"><a href="starting.html#starting-connect-to-spark"><i class="fa fa-check"></i><b>2.4</b> Connecting to Spark</a></li>
<li class="chapter" data-level="2.5" data-path="starting.html"><a href="starting.html#starting-sparklyr-hello-world"><i class="fa fa-check"></i><b>2.5</b> Using Spark</a><ul>
<li class="chapter" data-level="2.5.1" data-path="starting.html"><a href="starting.html#starting-spark-web-interface"><i class="fa fa-check"></i><b>2.5.1</b> Web Interface</a></li>
<li class="chapter" data-level="2.5.2" data-path="starting.html"><a href="starting.html#starting-analysis"><i class="fa fa-check"></i><b>2.5.2</b> Analysis</a></li>
<li class="chapter" data-level="2.5.3" data-path="starting.html"><a href="starting.html#starting-modeling"><i class="fa fa-check"></i><b>2.5.3</b> Modeling</a></li>
<li class="chapter" data-level="2.5.4" data-path="starting.html"><a href="starting.html#starting-data"><i class="fa fa-check"></i><b>2.5.4</b> Data</a></li>
<li class="chapter" data-level="2.5.5" data-path="starting.html"><a href="starting.html#starting-extensions"><i class="fa fa-check"></i><b>2.5.5</b> Extensions</a></li>
<li class="chapter" data-level="2.5.6" data-path="starting.html"><a href="starting.html#starting-distributed-r"><i class="fa fa-check"></i><b>2.5.6</b> Distributed R</a></li>
<li class="chapter" data-level="2.5.7" data-path="starting.html"><a href="starting.html#starting-streaming"><i class="fa fa-check"></i><b>2.5.7</b> Streaming</a></li>
<li class="chapter" data-level="2.5.8" data-path="starting.html"><a href="starting.html#starting-logs"><i class="fa fa-check"></i><b>2.5.8</b> Logs</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="starting.html"><a href="starting.html#starting-disconnecting"><i class="fa fa-check"></i><b>2.6</b> Disconnecting</a></li>
<li class="chapter" data-level="2.7" data-path="starting.html"><a href="starting.html#starting-using-spark-from-rstudio"><i class="fa fa-check"></i><b>2.7</b> Using RStudio</a></li>
<li class="chapter" data-level="2.8" data-path="starting.html"><a href="starting.html#starting-resources"><i class="fa fa-check"></i><b>2.8</b> Resources</a></li>
<li class="chapter" data-level="2.9" data-path="starting.html"><a href="starting.html#starting-recap"><i class="fa fa-check"></i><b>2.9</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="analysis.html"><a href="analysis.html"><i class="fa fa-check"></i><b>3</b> Analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="analysis.html"><a href="analysis.html#search-for-an-answer"><i class="fa fa-check"></i><b>3.1</b> Search for an answer</a></li>
<li class="chapter" data-level="3.2" data-path="analysis.html"><a href="analysis.html#r-as-an-interface-to-spark"><i class="fa fa-check"></i><b>3.2</b> R as an interface to Spark</a></li>
<li class="chapter" data-level="3.3" data-path="analysis.html"><a href="analysis.html#exercise"><i class="fa fa-check"></i><b>3.3</b> Exercise</a></li>
<li class="chapter" data-level="3.4" data-path="analysis.html"><a href="analysis.html#import-access"><i class="fa fa-check"></i><b>3.4</b> Import / Access</a></li>
<li class="chapter" data-level="3.5" data-path="analysis.html"><a href="analysis.html#wrangle"><i class="fa fa-check"></i><b>3.5</b> Wrangle</a><ul>
<li class="chapter" data-level="3.5.1" data-path="analysis.html"><a href="analysis.html#correlations"><i class="fa fa-check"></i><b>3.5.1</b> Correlations</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="analysis.html"><a href="analysis.html#visualize"><i class="fa fa-check"></i><b>3.6</b> Visualize</a><ul>
<li class="chapter" data-level="3.6.1" data-path="analysis.html"><a href="analysis.html#recommended-approach"><i class="fa fa-check"></i><b>3.6.1</b> Recommended approach</a></li>
<li class="chapter" data-level="3.6.2" data-path="analysis.html"><a href="analysis.html#simple-plots"><i class="fa fa-check"></i><b>3.6.2</b> Simple plots</a></li>
<li class="chapter" data-level="3.6.3" data-path="analysis.html"><a href="analysis.html#histograms"><i class="fa fa-check"></i><b>3.6.3</b> Histograms</a></li>
<li class="chapter" data-level="3.6.4" data-path="analysis.html"><a href="analysis.html#scatter-plots"><i class="fa fa-check"></i><b>3.6.4</b> Scatter plots</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="analysis.html"><a href="analysis.html#model"><i class="fa fa-check"></i><b>3.7</b> Model</a><ul>
<li class="chapter" data-level="3.7.1" data-path="analysis.html"><a href="analysis.html#models-during-analysis"><i class="fa fa-check"></i><b>3.7.1</b> Models during analysis</a></li>
<li class="chapter" data-level="3.7.2" data-path="analysis.html"><a href="analysis.html#cache-model-data"><i class="fa fa-check"></i><b>3.7.2</b> Cache model data</a></li>
</ul></li>
<li class="chapter" data-level="3.8" data-path="analysis.html"><a href="analysis.html#communicate"><i class="fa fa-check"></i><b>3.8</b> Communicate</a><ul>
<li class="chapter" data-level="3.8.1" data-path="analysis.html"><a href="analysis.html#analysis-versus-production-work"><i class="fa fa-check"></i><b>3.8.1</b> Analysis versus Production work</a></li>
<li class="chapter" data-level="3.8.2" data-path="analysis.html"><a href="analysis.html#using-r-markdown-documents"><i class="fa fa-check"></i><b>3.8.2</b> Using R Markdown documents</a></li>
<li class="chapter" data-level="3.8.3" data-path="analysis.html"><a href="analysis.html#reporting-results"><i class="fa fa-check"></i><b>3.8.3</b> Reporting results</a></li>
<li class="chapter" data-level="3.8.4" data-path="analysis.html"><a href="analysis.html#presentation-decks"><i class="fa fa-check"></i><b>3.8.4</b> Presentation decks</a></li>
</ul></li>
<li class="chapter" data-level="3.9" data-path="analysis.html"><a href="analysis.html#recap"><i class="fa fa-check"></i><b>3.9</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="modeling.html"><a href="modeling.html"><i class="fa fa-check"></i><b>4</b> Modeling</a><ul>
<li class="chapter" data-level="4.1" data-path="modeling.html"><a href="modeling.html#overview"><i class="fa fa-check"></i><b>4.1</b> Overview</a></li>
<li class="chapter" data-level="4.2" data-path="modeling.html"><a href="modeling.html#the-data"><i class="fa fa-check"></i><b>4.2</b> The Data</a></li>
<li class="chapter" data-level="4.3" data-path="modeling.html"><a href="modeling.html#exploratory-data-analysis"><i class="fa fa-check"></i><b>4.3</b> Exploratory Data Analysis</a></li>
<li class="chapter" data-level="4.4" data-path="modeling.html"><a href="modeling.html#feature-engineering"><i class="fa fa-check"></i><b>4.4</b> Feature Engineering</a></li>
<li class="chapter" data-level="4.5" data-path="modeling.html"><a href="modeling.html#model-building"><i class="fa fa-check"></i><b>4.5</b> Model Building</a><ul>
<li class="chapter" data-level="4.5.1" data-path="modeling.html"><a href="modeling.html#logistic-regression-as-a-generalized-linear-regression"><i class="fa fa-check"></i><b>4.5.1</b> Logistic Regression as a Generalized Linear Regression</a></li>
<li class="chapter" data-level="4.5.2" data-path="modeling.html"><a href="modeling.html#more-machine-learning-algorithms"><i class="fa fa-check"></i><b>4.5.2</b> More Machine Learning Algorithms</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="modeling.html"><a href="modeling.html#working-with-textual-data"><i class="fa fa-check"></i><b>4.6</b> Working with Textual Data</a><ul>
<li class="chapter" data-level="4.6.1" data-path="modeling.html"><a href="modeling.html#data-prep"><i class="fa fa-check"></i><b>4.6.1</b> Data Prep</a></li>
<li class="chapter" data-level="4.6.2" data-path="modeling.html"><a href="modeling.html#topic-modeling"><i class="fa fa-check"></i><b>4.6.2</b> Topic Modeling</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="modeling.html"><a href="modeling.html#conclusion"><i class="fa fa-check"></i><b>4.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="clusters.html"><a href="clusters.html"><i class="fa fa-check"></i><b>5</b> Clusters</a><ul>
<li class="chapter" data-level="5.1" data-path="clusters.html"><a href="clusters.html#clusters-overview"><i class="fa fa-check"></i><b>5.1</b> Overview</a></li>
<li class="chapter" data-level="5.2" data-path="clusters.html"><a href="clusters.html#on-premise"><i class="fa fa-check"></i><b>5.2</b> On-Premise</a><ul>
<li class="chapter" data-level="5.2.1" data-path="clusters.html"><a href="clusters.html#clusters-manager"><i class="fa fa-check"></i><b>5.2.1</b> Managers</a></li>
<li class="chapter" data-level="5.2.2" data-path="clusters.html"><a href="clusters.html#distributions"><i class="fa fa-check"></i><b>5.2.2</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="clusters.html"><a href="clusters.html#cloud"><i class="fa fa-check"></i><b>5.3</b> Cloud</a><ul>
<li class="chapter" data-level="5.3.1" data-path="clusters.html"><a href="clusters.html#clusters-amazon-emr"><i class="fa fa-check"></i><b>5.3.1</b> Amazon</a></li>
<li class="chapter" data-level="5.3.2" data-path="clusters.html"><a href="clusters.html#databricks"><i class="fa fa-check"></i><b>5.3.2</b> Databricks</a></li>
<li class="chapter" data-level="5.3.3" data-path="clusters.html"><a href="clusters.html#google"><i class="fa fa-check"></i><b>5.3.3</b> Google</a></li>
<li class="chapter" data-level="5.3.4" data-path="clusters.html"><a href="clusters.html#ibm"><i class="fa fa-check"></i><b>5.3.4</b> IBM</a></li>
<li class="chapter" data-level="5.3.5" data-path="clusters.html"><a href="clusters.html#microsoft"><i class="fa fa-check"></i><b>5.3.5</b> Microsoft</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="clusters.html"><a href="clusters.html#kubernetes"><i class="fa fa-check"></i><b>5.4</b> Kubernetes</a></li>
<li class="chapter" data-level="5.5" data-path="clusters.html"><a href="clusters.html#tools"><i class="fa fa-check"></i><b>5.5</b> Tools</a><ul>
<li class="chapter" data-level="5.5.1" data-path="clusters.html"><a href="clusters.html#rstudio"><i class="fa fa-check"></i><b>5.5.1</b> RStudio</a></li>
<li class="chapter" data-level="5.5.2" data-path="clusters.html"><a href="clusters.html#jupyter"><i class="fa fa-check"></i><b>5.5.2</b> Jupyter</a></li>
<li class="chapter" data-level="5.5.3" data-path="clusters.html"><a href="clusters.html#clusters-livy"><i class="fa fa-check"></i><b>5.5.3</b> Livy</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="clusters.html"><a href="clusters.html#recap-1"><i class="fa fa-check"></i><b>5.6</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="connections.html"><a href="connections.html"><i class="fa fa-check"></i><b>6</b> Connections</a><ul>
<li class="chapter" data-level="6.1" data-path="connections.html"><a href="connections.html#connections-overview"><i class="fa fa-check"></i><b>6.1</b> Overview</a><ul>
<li class="chapter" data-level="6.1.1" data-path="connections.html"><a href="connections.html#connections-spark-edge-nodes"><i class="fa fa-check"></i><b>6.1.1</b> Edge Nodes</a></li>
<li class="chapter" data-level="6.1.2" data-path="connections.html"><a href="connections.html#connections-spark-home"><i class="fa fa-check"></i><b>6.1.2</b> Spark Home</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="connections.html"><a href="connections.html#connections-local"><i class="fa fa-check"></i><b>6.2</b> Local</a></li>
<li class="chapter" data-level="6.3" data-path="connections.html"><a href="connections.html#connections-standalone"><i class="fa fa-check"></i><b>6.3</b> Standalone</a></li>
<li class="chapter" data-level="6.4" data-path="connections.html"><a href="connections.html#connections-yarn"><i class="fa fa-check"></i><b>6.4</b> Yarn</a><ul>
<li class="chapter" data-level="6.4.1" data-path="connections.html"><a href="connections.html#connections-yarn-client"><i class="fa fa-check"></i><b>6.4.1</b> Yarn Client</a></li>
<li class="chapter" data-level="6.4.2" data-path="connections.html"><a href="connections.html#connections-yarn-cluster"><i class="fa fa-check"></i><b>6.4.2</b> Yarn Cluster</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="connections.html"><a href="connections.html#connections-livy"><i class="fa fa-check"></i><b>6.5</b> Livy</a></li>
<li class="chapter" data-level="6.6" data-path="connections.html"><a href="connections.html#connections-mesos"><i class="fa fa-check"></i><b>6.6</b> Mesos</a></li>
<li class="chapter" data-level="6.7" data-path="connections.html"><a href="connections.html#connections-kubernetes"><i class="fa fa-check"></i><b>6.7</b> Kubernetes</a></li>
<li class="chapter" data-level="6.8" data-path="connections.html"><a href="connections.html#cloud-1"><i class="fa fa-check"></i><b>6.8</b> Cloud</a></li>
<li class="chapter" data-level="6.9" data-path="connections.html"><a href="connections.html#batches"><i class="fa fa-check"></i><b>6.9</b> Batches</a></li>
<li class="chapter" data-level="6.10" data-path="connections.html"><a href="connections.html#tools-1"><i class="fa fa-check"></i><b>6.10</b> Tools</a></li>
<li class="chapter" data-level="6.11" data-path="connections.html"><a href="connections.html#multiple"><i class="fa fa-check"></i><b>6.11</b> Multiple</a></li>
<li class="chapter" data-level="6.12" data-path="connections.html"><a href="connections.html#connections-troubleshooting"><i class="fa fa-check"></i><b>6.12</b> Troubleshooting</a><ul>
<li class="chapter" data-level="6.12.1" data-path="connections.html"><a href="connections.html#logging"><i class="fa fa-check"></i><b>6.12.1</b> Logging</a></li>
<li class="chapter" data-level="6.12.2" data-path="connections.html"><a href="connections.html#troubleshoot-spark-submit"><i class="fa fa-check"></i><b>6.12.2</b> Spark Submit</a></li>
<li class="chapter" data-level="6.12.3" data-path="connections.html"><a href="connections.html#windows"><i class="fa fa-check"></i><b>6.12.3</b> Windows</a></li>
</ul></li>
<li class="chapter" data-level="6.13" data-path="connections.html"><a href="connections.html#recap-2"><i class="fa fa-check"></i><b>6.13</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>7</b> Data</a><ul>
<li class="chapter" data-level="7.1" data-path="data.html"><a href="data.html#types-and-protocols"><i class="fa fa-check"></i><b>7.1</b> Types and protocols</a><ul>
<li class="chapter" data-level="7.1.1" data-path="data.html"><a href="data.html#default-packages"><i class="fa fa-check"></i><b>7.1.1</b> Default packages</a></li>
<li class="chapter" data-level="7.1.2" data-path="data.html"><a href="data.html#source-types"><i class="fa fa-check"></i><b>7.1.2</b> Source types</a></li>
<li class="chapter" data-level="7.1.3" data-path="data.html"><a href="data.html#file-system-protocols"><i class="fa fa-check"></i><b>7.1.3</b> File system protocols</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="data.html"><a href="data.html#reading-data"><i class="fa fa-check"></i><b>7.2</b> Reading data</a><ul>
<li class="chapter" data-level="7.2.1" data-path="data.html"><a href="data.html#folders-as-a-table"><i class="fa fa-check"></i><b>7.2.1</b> Folders as a table</a></li>
<li class="chapter" data-level="7.2.2" data-path="data.html"><a href="data.html#file-layout"><i class="fa fa-check"></i><b>7.2.2</b> File layout</a></li>
<li class="chapter" data-level="7.2.3" data-path="data.html"><a href="data.html#spark-memory"><i class="fa fa-check"></i><b>7.2.3</b> Spark memory</a></li>
<li class="chapter" data-level="7.2.4" data-path="data.html"><a href="data.html#column-names"><i class="fa fa-check"></i><b>7.2.4</b> Column Names</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="data.html"><a href="data.html#date-time"><i class="fa fa-check"></i><b>7.3</b> Date &amp; time</a></li>
<li class="chapter" data-level="7.4" data-path="data.html"><a href="data.html#specific-types-and-protocols"><i class="fa fa-check"></i><b>7.4</b> Specific types and protocols</a><ul>
<li class="chapter" data-level="7.4.1" data-path="data.html"><a href="data.html#amazon-s3"><i class="fa fa-check"></i><b>7.4.1</b> Amazon S3</a></li>
<li class="chapter" data-level="7.4.2" data-path="data.html"><a href="data.html#google-storage"><i class="fa fa-check"></i><b>7.4.2</b> Google Storage</a></li>
<li class="chapter" data-level="7.4.3" data-path="data.html"><a href="data.html#azure-storage"><i class="fa fa-check"></i><b>7.4.3</b> Azure Storage</a></li>
<li class="chapter" data-level="7.4.4" data-path="data.html"><a href="data.html#cassandra"><i class="fa fa-check"></i><b>7.4.4</b> Cassandra</a></li>
<li class="chapter" data-level="7.4.5" data-path="data.html"><a href="data.html#databases"><i class="fa fa-check"></i><b>7.4.5</b> Databases</a></li>
<li class="chapter" data-level="7.4.6" data-path="data.html"><a href="data.html#hbase"><i class="fa fa-check"></i><b>7.4.6</b> HBase</a></li>
<li class="chapter" data-level="7.4.7" data-path="data.html"><a href="data.html#nested-data"><i class="fa fa-check"></i><b>7.4.7</b> Nested Data</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="data.html"><a href="data.html#troubleshooting"><i class="fa fa-check"></i><b>7.5</b> Troubleshooting</a><ul>
<li class="chapter" data-level="7.5.1" data-path="data.html"><a href="data.html#troubleshoot-csvs"><i class="fa fa-check"></i><b>7.5.1</b> Troubleshoot CSVs</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="data.html"><a href="data.html#recap-3"><i class="fa fa-check"></i><b>7.6</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="tuning.html"><a href="tuning.html"><i class="fa fa-check"></i><b>8</b> Tuning</a><ul>
<li class="chapter" data-level="8.1" data-path="tuning.html"><a href="tuning.html#overview-1"><i class="fa fa-check"></i><b>8.1</b> Overview</a><ul>
<li class="chapter" data-level="8.1.1" data-path="tuning.html"><a href="tuning.html#tuning-graph-visualization"><i class="fa fa-check"></i><b>8.1.1</b> Graph</a></li>
<li class="chapter" data-level="8.1.2" data-path="tuning.html"><a href="tuning.html#tuning-event-timeline"><i class="fa fa-check"></i><b>8.1.2</b> Timeline</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="tuning.html"><a href="tuning.html#tuning-configuring"><i class="fa fa-check"></i><b>8.2</b> Configuring</a><ul>
<li class="chapter" data-level="8.2.1" data-path="tuning.html"><a href="tuning.html#connect-settings"><i class="fa fa-check"></i><b>8.2.1</b> Connect Settings</a></li>
<li class="chapter" data-level="8.2.2" data-path="tuning.html"><a href="tuning.html#submit-settings"><i class="fa fa-check"></i><b>8.2.2</b> Submit Settings</a></li>
<li class="chapter" data-level="8.2.3" data-path="tuning.html"><a href="tuning.html#runtime-settings"><i class="fa fa-check"></i><b>8.2.3</b> Runtime Settings</a></li>
<li class="chapter" data-level="8.2.4" data-path="tuning.html"><a href="tuning.html#sparklyr-settings"><i class="fa fa-check"></i><b>8.2.4</b> sparklyr Settings</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="tuning.html"><a href="tuning.html#tuning-partitioning"><i class="fa fa-check"></i><b>8.3</b> Partitioning</a><ul>
<li class="chapter" data-level="8.3.1" data-path="tuning.html"><a href="tuning.html#implicit"><i class="fa fa-check"></i><b>8.3.1</b> Implicit</a></li>
<li class="chapter" data-level="8.3.2" data-path="tuning.html"><a href="tuning.html#explicit"><i class="fa fa-check"></i><b>8.3.2</b> Explicit</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="tuning.html"><a href="tuning.html#tuning-caching"><i class="fa fa-check"></i><b>8.4</b> Caching</a><ul>
<li class="chapter" data-level="8.4.1" data-path="tuning.html"><a href="tuning.html#checkpointing"><i class="fa fa-check"></i><b>8.4.1</b> Checkpointing</a></li>
<li class="chapter" data-level="8.4.2" data-path="tuning.html"><a href="tuning.html#tuning-memory"><i class="fa fa-check"></i><b>8.4.2</b> Memory</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="tuning.html"><a href="tuning.html#tuning-shuffling"><i class="fa fa-check"></i><b>8.5</b> Shuffling</a></li>
<li class="chapter" data-level="8.6" data-path="tuning.html"><a href="tuning.html#tuning-serialization"><i class="fa fa-check"></i><b>8.6</b> Serialization</a></li>
<li class="chapter" data-level="8.7" data-path="tuning.html"><a href="tuning.html#configuration-files"><i class="fa fa-check"></i><b>8.7</b> Configuration Files</a></li>
<li class="chapter" data-level="8.8" data-path="tuning.html"><a href="tuning.html#recap-4"><i class="fa fa-check"></i><b>8.8</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="extensions.html"><a href="extensions.html"><i class="fa fa-check"></i><b>9</b> Extensions</a><ul>
<li class="chapter" data-level="9.1" data-path="extensions.html"><a href="extensions.html#rsparkling"><i class="fa fa-check"></i><b>9.1</b> RSparkling</a><ul>
<li class="chapter" data-level="9.1.1" data-path="extensions.html"><a href="extensions.html#troubleshooting-1"><i class="fa fa-check"></i><b>9.1.1</b> Troubleshooting</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="extensions.html"><a href="extensions.html#graphframes"><i class="fa fa-check"></i><b>9.2</b> GraphFrames</a></li>
<li class="chapter" data-level="9.3" data-path="extensions.html"><a href="extensions.html#mleap"><i class="fa fa-check"></i><b>9.3</b> Mleap</a></li>
<li class="chapter" data-level="9.4" data-path="extensions.html"><a href="extensions.html#extensions-nested-data"><i class="fa fa-check"></i><b>9.4</b> Nested Data</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="distributed.html"><a href="distributed.html"><i class="fa fa-check"></i><b>10</b> Distributed R</a><ul>
<li class="chapter" data-level="10.1" data-path="distributed.html"><a href="distributed.html#overview-2"><i class="fa fa-check"></i><b>10.1</b> Overview</a></li>
<li class="chapter" data-level="10.2" data-path="distributed.html"><a href="distributed.html#use-cases"><i class="fa fa-check"></i><b>10.2</b> Use Cases</a><ul>
<li class="chapter" data-level="10.2.1" data-path="distributed.html"><a href="distributed.html#custom-parsers"><i class="fa fa-check"></i><b>10.2.1</b> Custom Parsers</a></li>
<li class="chapter" data-level="10.2.2" data-path="distributed.html"><a href="distributed.html#partitioned-modeling"><i class="fa fa-check"></i><b>10.2.2</b> Partitioned Modeling</a></li>
<li class="chapter" data-level="10.2.3" data-path="distributed.html"><a href="distributed.html#distributed-grid-search"><i class="fa fa-check"></i><b>10.2.3</b> Grid Search</a></li>
<li class="chapter" data-level="10.2.4" data-path="distributed.html"><a href="distributed.html#web-apis"><i class="fa fa-check"></i><b>10.2.4</b> Web APIs</a></li>
<li class="chapter" data-level="10.2.5" data-path="distributed.html"><a href="distributed.html#distributed-rendering"><i class="fa fa-check"></i><b>10.2.5</b> Distributed Rendering</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="distributed.html"><a href="distributed.html#partitions"><i class="fa fa-check"></i><b>10.3</b> Partitions</a></li>
<li class="chapter" data-level="10.4" data-path="distributed.html"><a href="distributed.html#grouping"><i class="fa fa-check"></i><b>10.4</b> Grouping</a></li>
<li class="chapter" data-level="10.5" data-path="distributed.html"><a href="distributed.html#columns"><i class="fa fa-check"></i><b>10.5</b> Columns</a></li>
<li class="chapter" data-level="10.6" data-path="distributed.html"><a href="distributed.html#context"><i class="fa fa-check"></i><b>10.6</b> Context</a></li>
<li class="chapter" data-level="10.7" data-path="distributed.html"><a href="distributed.html#functions"><i class="fa fa-check"></i><b>10.7</b> Functions</a></li>
<li class="chapter" data-level="10.8" data-path="distributed.html"><a href="distributed.html#packages"><i class="fa fa-check"></i><b>10.8</b> Packages</a></li>
<li class="chapter" data-level="10.9" data-path="distributed.html"><a href="distributed.html#cluster-requirements"><i class="fa fa-check"></i><b>10.9</b> Cluster Requirements</a><ul>
<li class="chapter" data-level="10.9.1" data-path="distributed.html"><a href="distributed.html#installing-r"><i class="fa fa-check"></i><b>10.9.1</b> Installing R</a></li>
<li class="chapter" data-level="10.9.2" data-path="distributed.html"><a href="distributed.html#apache-arrow"><i class="fa fa-check"></i><b>10.9.2</b> Apache Arrow</a></li>
</ul></li>
<li class="chapter" data-level="10.10" data-path="distributed.html"><a href="distributed.html#troubleshooting-2"><i class="fa fa-check"></i><b>10.10</b> Troubleshooting</a><ul>
<li class="chapter" data-level="10.10.1" data-path="distributed.html"><a href="distributed.html#worker-logs"><i class="fa fa-check"></i><b>10.10.1</b> Worker Logs</a></li>
<li class="chapter" data-level="10.10.2" data-path="distributed.html"><a href="distributed.html#resolving-timeouts"><i class="fa fa-check"></i><b>10.10.2</b> Resolving Timeouts</a></li>
<li class="chapter" data-level="10.10.3" data-path="distributed.html"><a href="distributed.html#inspecting-partition"><i class="fa fa-check"></i><b>10.10.3</b> Inspecting Partition</a></li>
<li class="chapter" data-level="10.10.4" data-path="distributed.html"><a href="distributed.html#debugging-workers"><i class="fa fa-check"></i><b>10.10.4</b> Debugging Workers</a></li>
</ul></li>
<li class="chapter" data-level="10.11" data-path="distributed.html"><a href="distributed.html#recap-5"><i class="fa fa-check"></i><b>10.11</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="streaming.html"><a href="streaming.html"><i class="fa fa-check"></i><b>11</b> Streaming</a><ul>
<li class="chapter" data-level="11.1" data-path="streaming.html"><a href="streaming.html#spark-streaming"><i class="fa fa-check"></i><b>11.1</b> Spark Streaming</a></li>
<li class="chapter" data-level="11.2" data-path="streaming.html"><a href="streaming.html#working-with-spark-streams"><i class="fa fa-check"></i><b>11.2</b> Working with Spark Streams</a></li>
<li class="chapter" data-level="11.3" data-path="streaming.html"><a href="streaming.html#sparklyr-extras"><i class="fa fa-check"></i><b>11.3</b> <code>sparklyr</code> extras</a><ul>
<li class="chapter" data-level="11.3.1" data-path="streaming.html"><a href="streaming.html#stream-monitor"><i class="fa fa-check"></i><b>11.3.1</b> Stream monitor</a></li>
<li class="chapter" data-level="11.3.2" data-path="streaming.html"><a href="streaming.html#stream-generator"><i class="fa fa-check"></i><b>11.3.2</b> Stream generator</a></li>
<li class="chapter" data-level="11.3.3" data-path="streaming.html"><a href="streaming.html#shiny-reactive"><i class="fa fa-check"></i><b>11.3.3</b> Shiny reactive</a></li>
</ul></li>
<li class="chapter" data-level="11.4" data-path="streaming.html"><a href="streaming.html#intro-example"><i class="fa fa-check"></i><b>11.4</b> Intro example</a></li>
<li class="chapter" data-level="11.5" data-path="streaming.html"><a href="streaming.html#transformations"><i class="fa fa-check"></i><b>11.5</b> Transformations</a><ul>
<li class="chapter" data-level="11.5.1" data-path="streaming.html"><a href="streaming.html#dplyr"><i class="fa fa-check"></i><b>11.5.1</b> dplyr</a></li>
<li class="chapter" data-level="11.5.2" data-path="streaming.html"><a href="streaming.html#transformer-functions"><i class="fa fa-check"></i><b>11.5.2</b> Transformer functions</a></li>
<li class="chapter" data-level="11.5.3" data-path="streaming.html"><a href="streaming.html#r-code"><i class="fa fa-check"></i><b>11.5.3</b> R code</a></li>
<li class="chapter" data-level="11.5.4" data-path="streaming.html"><a href="streaming.html#ml-pipelines"><i class="fa fa-check"></i><b>11.5.4</b> ML Pipelines</a></li>
</ul></li>
<li class="chapter" data-level="11.6" data-path="streaming.html"><a href="streaming.html#shiny-integration"><i class="fa fa-check"></i><b>11.6</b> Shiny integration</a></li>
<li class="chapter" data-level="11.7" data-path="streaming.html"><a href="streaming.html#kafka"><i class="fa fa-check"></i><b>11.7</b> Kafka</a><ul>
<li class="chapter" data-level="11.7.1" data-path="streaming.html"><a href="streaming.html#workflow"><i class="fa fa-check"></i><b>11.7.1</b> Workflow</a></li>
<li class="chapter" data-level="11.7.2" data-path="streaming.html"><a href="streaming.html#spark-integration"><i class="fa fa-check"></i><b>11.7.2</b> Spark integration</a></li>
<li class="chapter" data-level="11.7.3" data-path="streaming.html"><a href="streaming.html#r-integration"><i class="fa fa-check"></i><b>11.7.3</b> R integration</a></li>
<li class="chapter" data-level="11.7.4" data-path="streaming.html"><a href="streaming.html#example"><i class="fa fa-check"></i><b>11.7.4</b> Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="12" data-path="contributing.html"><a href="contributing.html"><i class="fa fa-check"></i><b>12</b> Contributing</a><ul>
<li class="chapter" data-level="12.1" data-path="contributing.html"><a href="contributing.html#contributing-overview"><i class="fa fa-check"></i><b>12.1</b> Overview</a></li>
<li class="chapter" data-level="12.2" data-path="contributing.html"><a href="contributing.html#contributing-spark-api"><i class="fa fa-check"></i><b>12.2</b> Spark API</a></li>
<li class="chapter" data-level="12.3" data-path="contributing.html"><a href="contributing.html#spark-extensions"><i class="fa fa-check"></i><b>12.3</b> Spark Extensions</a></li>
<li class="chapter" data-level="12.4" data-path="contributing.html"><a href="contributing.html#scala-code"><i class="fa fa-check"></i><b>12.4</b> Scala Code</a></li>
<li class="chapter" data-level="12.5" data-path="contributing.html"><a href="contributing.html#recap-6"><i class="fa fa-check"></i><b>12.5</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>13</b> Appendix</a><ul>
<li class="chapter" data-level="13.1" data-path="appendix.html"><a href="appendix.html#appendix-prerequisites"><i class="fa fa-check"></i><b>13.1</b> Prerequisites</a><ul>
<li class="chapter" data-level="13.1.1" data-path="appendix.html"><a href="appendix.html#appendix-install-r"><i class="fa fa-check"></i><b>13.1.1</b> Installing R</a></li>
<li class="chapter" data-level="13.1.2" data-path="appendix.html"><a href="appendix.html#appendix-install-java"><i class="fa fa-check"></i><b>13.1.2</b> Installing Java</a></li>
<li class="chapter" data-level="13.1.3" data-path="appendix.html"><a href="appendix.html#appendix-install-rstudio"><i class="fa fa-check"></i><b>13.1.3</b> Installing RStudio</a></li>
<li class="chapter" data-level="13.1.4" data-path="appendix.html"><a href="appendix.html#appendix-using-rstudio"><i class="fa fa-check"></i><b>13.1.4</b> Using RStudio</a></li>
</ul></li>
<li class="chapter" data-level="13.2" data-path="appendix.html"><a href="appendix.html#diagrams"><i class="fa fa-check"></i><b>13.2</b> Diagrams</a><ul>
<li class="chapter" data-level="13.2.1" data-path="appendix.html"><a href="appendix.html#appendix-storage-capacity"><i class="fa fa-check"></i><b>13.2.1</b> Worlds Store Capacity</a></li>
<li class="chapter" data-level="13.2.2" data-path="appendix.html"><a href="appendix.html#appendix-cran-downloads"><i class="fa fa-check"></i><b>13.2.2</b> Daily downloads of CRAN packages</a></li>
<li class="chapter" data-level="13.2.3" data-path="appendix.html"><a href="appendix.html#appendix-cluster-trends"><i class="fa fa-check"></i><b>13.2.3</b> Google trends for mainframes, cloud computing and kubernetes</a></li>
</ul></li>
<li class="chapter" data-level="13.3" data-path="appendix.html"><a href="appendix.html#appendix-ggplot2-theme"><i class="fa fa-check"></i><b>13.3</b> Formatting</a></li>
<li class="chapter" data-level="13.4" data-path="appendix.html"><a href="appendix.html#ml-functionlist"><i class="fa fa-check"></i><b>13.4</b> List of ML Functions</a><ul>
<li class="chapter" data-level="13.4.1" data-path="appendix.html"><a href="appendix.html#classification"><i class="fa fa-check"></i><b>13.4.1</b> Classification</a></li>
<li class="chapter" data-level="13.4.2" data-path="appendix.html"><a href="appendix.html#regression"><i class="fa fa-check"></i><b>13.4.2</b> Regression</a></li>
<li class="chapter" data-level="13.4.3" data-path="appendix.html"><a href="appendix.html#clustering"><i class="fa fa-check"></i><b>13.4.3</b> Clustering</a></li>
<li class="chapter" data-level="13.4.4" data-path="appendix.html"><a href="appendix.html#recommendation"><i class="fa fa-check"></i><b>13.4.4</b> Recommendation</a></li>
<li class="chapter" data-level="13.4.5" data-path="appendix.html"><a href="appendix.html#frequent-pattern-mining"><i class="fa fa-check"></i><b>13.4.5</b> Frequent Pattern Mining</a></li>
<li class="chapter" data-level="13.4.6" data-path="appendix.html"><a href="appendix.html#feature-transformers"><i class="fa fa-check"></i><b>13.4.6</b> Feature Transformers</a></li>
</ul></li>
<li class="chapter" data-level="13.5" data-path="appendix.html"><a href="appendix.html#kafka-1"><i class="fa fa-check"></i><b>13.5</b> Kafka</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>14</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The R in Spark: Learning Apache Spark with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="data" class="section level1">
<h1><span class="header-section-number">Chapter 7</span> Data</h1>
<p>The goal of this chapter is to help you learn how to access, read and write data using Spark. It will provide the necessesary background to help you work with a variaety of data.</p>
<p>This chapter will cover specific source types and file protocols. It will also aim to teach a pattern. This pattern should help you access types or protocols not covered here.</p>
<p>Additionally, this chapter will introduce some recommendations focused on improving performance and efficiency.</p>
<div id="types-and-protocols" class="section level2">
<h2><span class="header-section-number">7.1</span> Types and protocols</h2>
<p>Accessing new data for the first time may present challenges. The likely reasons are the source type, or the file system protocol.</p>
<p>Spark is able to interact with several source types and file system protocols. Source types include Comma separated values (CSV), Apache Parquet, or JDBC. File system protocols includes the local file system (Linux, Windows, Mac), and the Hadoop file System (HDFS).</p>
<p>Spark’s capabilities can be extended. This is important when encountering new source type, or protocol. The next section will cover how to do this.</p>
<div id="default-packages" class="section level3">
<h3><span class="header-section-number">7.1.1</span> Default packages</h3>
<p>Spark is a very flexible computing platform. It is able to use the libraries from external packages to add functionality. This makes it possible to be able to read data from Amazon S3 buckets, only if it is needed. The proper package is loaded at Spark connection time. The package is typically found and loaded directly from the Internet.</p>
<p>The configuration variable passed to the <code>spark_connect()</code> function should contain the reference to the desired package. Any package that is to be loaded at connection time is added to a vector in the <code>defaultPackages</code> value of the <code>spark_connect()</code> configuration. Here is an example that loads the package needed to access Amazon S3 buckets.</p>
<pre class="sourceCode r"><code class="sourceCode r">conf &lt;-<span class="st"> </span><span class="kw">spark_config</span>()
conf<span class="op">$</span>sparklyr.defaultPackages &lt;-<span class="st"> &quot;org.apache.hadoop:hadoop-aws:2.7.7&quot;</span>
sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>, <span class="dt">config =</span> conf)</code></pre>
</div>
<div id="source-types" class="section level3">
<h3><span class="header-section-number">7.1.2</span> Source types</h3>
<p>The main goal is to convert the source data into an object in Spark. Logically, this requires a spec that “tells” Spark how to read the source data. Each spec enables Spark to read source data formats.</p>
<p>Currently, out-of-the box Spark can read several formats. In <code>sparklyr</code>, the source data formats are aligned to R functions:</p>
<table>
<colgroup>
<col width="48%" />
<col width="25%" />
<col width="26%" />
</colgroup>
<thead>
<tr class="header">
<th>Format</th>
<th>Read</th>
<th>Write</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>Comma separated values (CSV)</td>
<td><code>spark_read_csv()</code></td>
<td><code>spark_write_csv()</code></td>
</tr>
<tr class="even">
<td>Javascript Object Notation (JSON)</td>
<td><code>spark_read_json()</code></td>
<td><code>spark_write_json()</code></td>
</tr>
<tr class="odd">
<td>Library for Support Vector Machines (LIBSVM)</td>
<td><code>spark_read_libsvm()</code></td>
<td><code>spark_write_libsvm()</code></td>
</tr>
<tr class="even">
<td>Java Database Connectivity (JDBC)</td>
<td><code>spark_read_jdbc()</code></td>
<td><code>spark_write_jdbc()</code></td>
</tr>
<tr class="odd">
<td>Optimized Row Columnar (ORC)</td>
<td><code>spark_read_orc()</code></td>
<td><code>spark_write_orc()</code></td>
</tr>
<tr class="even">
<td>Apache Parquet</td>
<td><code>spark_read_parquet()</code></td>
<td><code>spark_write_parquet()</code></td>
</tr>
<tr class="odd">
<td>Text</td>
<td><code>spark_read_text()</code></td>
<td><code>spark_write_text()</code></td>
</tr>
</tbody>
</table>
<p>It is possible to access data source types not listed above. It will require to load a dafault package at connection time. In <code>sparklyr</code>, the <code>spark_read_source()</code> and <code>spark_write_source()</code> are generic functions. They are able to use the libraries imported by the default package.</p>
<p>The following example code shows how to use the <code>datastax:spark-cassandra-connector</code> package to read from Cassandra. The key is to use the <code>org.apache.spark.sql.cassandra</code> library as the <code>source</code> argument. That is what provides the mapping Spark can use to make sense of the data source.</p>
<pre class="sourceCode r"><code class="sourceCode r">con &lt;-<span class="st"> </span><span class="kw">spark_config</span>()
conf<span class="op">$</span>sparklyr.defaultPackages &lt;-<span class="st"> &quot;datastax:spark-cassandra-connector:2.0.0-RC1-s_2.11&quot;</span>
sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>, <span class="dt">config =</span> conf)
<span class="kw">spark_read_source</span>(
  sc, 
  <span class="dt">name =</span> <span class="st">&quot;emp&quot;</span>,
  <span class="dt">source =</span> <span class="st">&quot;org.apache.spark.sql.cassandra&quot;</span>,
  <span class="dt">options =</span> <span class="kw">list</span>(<span class="dt">keyspace =</span> <span class="st">&quot;dev&quot;</span>, <span class="dt">table =</span> <span class="st">&quot;emp&quot;</span>)
  )</code></pre>
</div>
<div id="file-system-protocols" class="section level3">
<h3><span class="header-section-number">7.1.3</span> File system protocols</h3>
<p>Spark will default to the file protocol that it is running on. This means that if it is running in a YARN managed cluster, the default protocol will be HDFS. For example, a given path of “/home/user/file.csv” will be read from cluster’s HDFS folders, and not the Linux files. The Operating System’s file system will be accessed for other deployments, such as Stand Alone, and <code>sparklyr</code>’s local.</p>
<p>The file system protocol can be overridden. This is done in the <code>path</code> argument of the <code>sparklyr</code> function that will read or write. Passing a full path of “<a href="file://home/user/file.csv" class="uri">file://home/user/file.csv</a>” will force the search to be done inside the local Operating System’s file system.</p>
<p>There are other file system protocols. An example is Amazon’s S3 service. Spark is does not know how to read the S3 protocol. Accessing the “s3a” protocol involves adding a package to the <code>defaultPackages</code> configuration variable passed at connection time.</p>
<pre class="sourceCode r"><code class="sourceCode r">conf &lt;-<span class="st"> </span><span class="kw">spark_config</span>()
conf<span class="op">$</span>sparklyr.defaultPackages &lt;-<span class="st"> &quot;org.apache.hadoop:hadoop-aws:2.7.7&quot;</span>
sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>, <span class="dt">config =</span> conf)
my_file &lt;-<span class="st"> </span><span class="kw">spark_read_csv</span>(sc, <span class="st">&quot;my-file&quot;</span>, <span class="dt">path =</span>  <span class="st">&quot;s3a://my-bucket/my-file.csv&quot;</span>)</code></pre>
<p>Currently, only “<a href="file://" class="uri">file://</a>” and “hdfs://” file protocols are supported when used in their respective environments. Accessing a different file protocol may require loading a default package. In some cases the vendor providing the Spark environment could already be loading the package for you.</p>
</div>
</div>
<div id="reading-data" class="section level2">
<h2><span class="header-section-number">7.2</span> Reading data</h2>
<div id="folders-as-a-table" class="section level3">
<h3><span class="header-section-number">7.2.1</span> Folders as a table</h3>
<p>Loading multiple files into a single table is a common scenario. In R, we typically use a loop or functional programming to load the files individually into the table.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">lapply</span>(<span class="kw">c</span>(<span class="st">&quot;data-folder/file1.csv&quot;</span>, <span class="st">&quot;data-folder/file2.csv&quot;</span>), read.csv)</code></pre>
<p>In Spark, there is the notion of a folder as a table. Instead of ennumerating each file, simply pass the path the containing folder’s path. Spark assumes that every file in that folder is part of the same table. This implies that the target folder should only be used for data purposes.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">spark_read_csv</span>(sc, <span class="st">&quot;my_data&quot;</span>, <span class="dt">path =</span> <span class="st">&quot;data-folder&quot;</span>)</code></pre>
<p>The folder as a table notion is also found in other open source technologies. Under the hood, Hive tables work the same way. When quering a Hive table, the mapping is done over multiple files inside the same folder. The folder’s name usually match the name of the table visible to the user. Kafka uses this same idea, see the Streaming chapter.</p>
</div>
<div id="file-layout" class="section level3">
<h3><span class="header-section-number">7.2.2</span> File layout</h3>
<p>When reading data, Spark is able to determine the data source’s column names and types. This comes at a cost. To determine the type Spark has to do a first pass to the data, and then assign a type. For large data, this may add a significant amount of time to the data ingestion process. Even for medium size data loads, this also becomes costly. If the files are read over and over again, even small additional load time adds up over time.</p>
<p>Spark allows the user to provide a column layout. If provided, Spark will bypass the step that ut uses to determine the file’s layout. In <code>sparklyr</code>, the <code>column</code> argument is used to take advantage of this functionality. The <code>infer_schema</code> argument also needs to be set to <code>FALSE</code>. This arguments is the switch that indicates if the <code>column</code> argument should be used.</p>
<p>For example, a file called <em>test.csv</em> is going to be loaded to Spark. This is its layout:</p>
<pre><code>&quot;x&quot;,&quot;y&quot;
&quot;a&quot;,1
&quot;b&quot;,2
&quot;c&quot;,3
&quot;d&quot;,4
&quot;e&quot;,5</code></pre>
<p>The column spec is started with a vector containing the column types. The vector’s values are named to match the field names. The accepted variable types are: <code>integer</code>, <code>character</code>, <code>logical</code>, <code>double</code>, <code>numeric</code>, <code>factor</code>, <code>Date</code>, <code>POSIXct</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">col_spec_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;character&quot;</span>, <span class="st">&quot;numeric&quot;</span>)
<span class="kw">names</span>(col_spec_<span class="dv">1</span>) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;x&quot;</span>, <span class="st">&quot;y&quot;</span>)
col_spec_<span class="dv">1</span></code></pre>
<pre><code>##           x           y 
## &quot;character&quot;   &quot;numeric&quot; </code></pre>
<p>In <code>spark_read_csv()</code>, <code>col_spec_1</code> is passed to the <code>columns</code> argument, and <code>infer_schema</code> is set to <code>FALSE</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>)
test_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">spark_read_csv</span>(sc, <span class="st">&quot;test1&quot;</span>,<span class="st">&quot;test.csv&quot;</span>, 
                         <span class="dt">columns =</span> col_spec_<span class="dv">1</span>, 
                         <span class="dt">infer_schema =</span> <span class="ot">FALSE</span>)
test_<span class="dv">1</span></code></pre>
<pre><code>## # Source: spark&lt;test1&gt; [?? x 2]
##    x         y
##    &lt;chr&gt; &lt;dbl&gt;
##  1 a         1
##  2 b         2
##  3 c         3
##  4 d         4
##  5 e         5</code></pre>
<p>In the example we tried to match the names and types of the original file. The ability to pass a column spec provides additional flexibility. The following example shows how we can set the field type to something different. The new field type needs a compatible type. For example, a <code>character</code> field could not be set as <code>numeric</code>. The field names are also set to something different that what is on the file.</p>
<pre class="sourceCode r"><code class="sourceCode r">col_spec_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;character&quot;</span>, <span class="st">&quot;character&quot;</span>)
<span class="kw">names</span>(col_spec_<span class="dv">2</span>) &lt;-<span class="st"> </span><span class="kw">c</span>(<span class="st">&quot;my_letter&quot;</span>, <span class="st">&quot;my_number&quot;</span>)

test_<span class="dv">2</span> &lt;-<span class="st"> </span><span class="kw">spark_read_csv</span>(sc, <span class="st">&quot;test2&quot;</span>,<span class="st">&quot;test.csv&quot;</span>, 
                         <span class="dt">columns =</span> col_spec_<span class="dv">2</span>, 
                         <span class="dt">infer_schema =</span> <span class="ot">FALSE</span>)
test_<span class="dv">2</span></code></pre>
<pre><code># Source: spark&lt;test2&gt; [?? x 2]
   my_letter my_number
   &lt;chr&gt;     &lt;chr&gt;    
 1 a         1        
 2 b         2        
 3 c         3        
 4 d         4        
 5 e         5    </code></pre>
<p>The ability to change the field type can be very useful. Malformed entries can cause error during reading. This is common in non-character fields. The practical approach is to import the field as a character field, and then use <code>dplyr</code> to coerce the field’s conversion.</p>
</div>
<div id="spark-memory" class="section level3">
<h3><span class="header-section-number">7.2.3</span> Spark memory</h3>
<p>Spark copies the data into its distributed memory. This makes analyses and other processes very fast. There are cases where loading all of the data may not be practical, or necessary. Spark can then just “map” the files without copying their data into memory. This creates a sort of “virtual” table in Spark memory. The implication is that when a query runs against that table, Spark has to read the data from the files at that time. Any consecutive read after that will do the same. In effect, Spark is a pass-through for the data.</p>
<p>The advantage of this method is that there is almost no up-front time cost to reading the file. The mapping process is comparitively fast.</p>
<p>In <code>sparklyr</code>, that is controlled by the <code>memory</code> argument of its read functions. Setting it to <code>FALSE</code> prevents the data copy. It defaults to <code>TRUE</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">mapped_test &lt;-<span class="st"> </span><span class="kw">spark_read_csv</span>(sc, <span class="st">&quot;test&quot;</span>,<span class="st">&quot;test.csv&quot;</span>, <span class="dt">memory =</span> <span class="ot">FALSE</span>)</code></pre>
<p>There are good use cases for this method. One of them is when not all columns of a table are needed. Take a very large file, or files, that contain many columns. This may not be the first time we interact with this data. We know what columns are needed for the analysis. The files can be read using <code>memory = FALSE</code>, and then select the needed columns with <code>dplyr</code>. The resulting <code>dplyr</code> variable can then be cached into memory, using the <code>compute()</code> function. This will make Spark query the file(s), pull the selected fields, and copy only that data into memory. The result is a in-memory table that took comparatively less time to prepare.</p>
<pre class="sourceCode r"><code class="sourceCode r">mapped_test <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(y) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">compute</span>(<span class="st">&quot;test&quot;</span>)</code></pre>
</div>
<div id="column-names" class="section level3">
<h3><span class="header-section-number">7.2.4</span> Column Names</h3>
<p>By default, <code>sparklyr</code> sanitizes column names. It translates characters such as <code>.</code> to <code>_</code>. This was required in Spark 1.6.X. To disable this functionality, you can run the following code:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">options</span>(<span class="dt">sparklyr.sanitize.column.names =</span> <span class="ot">FALSE</span>)
dplyr<span class="op">::</span><span class="kw">copy_to</span>(sc, iris, <span class="dt">overwrite =</span> <span class="ot">TRUE</span>)</code></pre>
<pre><code># Source:   table&lt;iris&gt; [?? x 5]
# Database: spark_connection
   Sepal.Length Sepal.Width Petal.Length Petal.Width Species
          &lt;dbl&gt;       &lt;dbl&gt;        &lt;dbl&gt;       &lt;dbl&gt; &lt;chr&gt;  
 1          5.1         3.5          1.4         0.2 setosa 
 2          4.9         3            1.4         0.2 setosa 
 3          4.7         3.2          1.3         0.2 setosa 
 4          4.6         3.1          1.5         0.2 setosa 
 5          5           3.6          1.4         0.2 setosa 
 6          5.4         3.9          1.7         0.4 setosa 
 7          4.6         3.4          1.4         0.3 setosa 
 8          5           3.4          1.5         0.2 setosa 
 9          4.4         2.9          1.4         0.2 setosa 
10          4.9         3.1          1.5         0.1 setosa 
# ... with more rows</code></pre>
</div>
</div>
<div id="date-time" class="section level2">
<h2><span class="header-section-number">7.3</span> Date &amp; time</h2>
<p>Some Spark date/time functions make timezone assumptions. For instance, the following code makes use of <code>to_date()</code>. It assumes that the timestamp will be given in the local time zone. This is not to discourage use of date/time functions. Please be aware of time zones to be handled with care.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sdf_len</span>(sc, <span class="dv">1</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">transmute</span>(
    <span class="dt">date =</span> <span class="kw">timestamp</span>(<span class="dv">1419126103</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">      </span><span class="kw">from_utc_timestamp</span>(<span class="st">&#39;UTC&#39;</span>) <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">      </span><span class="kw">to_date</span>() <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">      </span><span class="kw">as.character</span>()
  )</code></pre>
</div>
<div id="specific-types-and-protocols" class="section level2">
<h2><span class="header-section-number">7.4</span> Specific types and protocols</h2>
<div id="amazon-s3" class="section level3">
<h3><span class="header-section-number">7.4.1</span> Amazon S3</h3>
<p>Amazon Simple Storage Service, or S3, has become a common location to store file. Spark is able to directly access S3. This functionality can be used inside <code>sparklyr</code>. There are three key items to have, or use, when working with data from an S3 bucket:</p>
<ul>
<li>AWS Credentials - They are required by the S3 service, even for publicly accessible buckets.</li>
<li>Hadoop-to-AWS package - It is loaded at connection time.</li>
<li>A bucket location - The recommended file system to use is “s3a”.</li>
</ul>
<p>There are multiple ways to set the credentials to use to access the bucket. Please refer to the official documentation for more information. It is found in the Apache Spark official site <span class="citation">(“Spark Integration with Cloud Infrastructures” <a href="#ref-data-spark-cloud-integration">2019</a>)</span>.</p>
<p>The easiest way is to set the credentials using Environment variables. Choose a secure way to load the values into variables in R, and then load them into the appropriate Environment variable name. In case show below, <code>AWS_ACCESS_KEY_ID</code> and <code>AWS_SECRET_ACCESS_KEY</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">Sys.setenv</span>(<span class="dt">AWS_ACCESS_KEY_ID =</span> my_key_id)
<span class="kw">Sys.setenv</span>(<span class="dt">AWS_SECRET_ACCESS_KEY =</span> my_secret_key)</code></pre>
<p>Spark requires an integration package in order to access Amazon S3 buckets. Interestingly, the package is not a Spark package, it is a Hadoop package. This means that the selected version will be a Hadoop version. After some experiments, it seems that with Spark versions 2, only up to Hadoop 2.7.7 will work. That may change when Spark enters version 3. If using a YARN managed cluster, the package may be different. That would depend on the Hadoop vendor. The official site for Apache based project is called Maven <span class="citation">(“Maven Repository: Home Page” <a href="#ref-data-maven-home">2019</a>)</span>. Please visit that site to find alternative package versions if the recommended one does not work. The recommended search term to use would be: “hadoop-aws”.</p>
<pre class="sourceCode r"><code class="sourceCode r">conf &lt;-<span class="st"> </span><span class="kw">spark_config</span>()
conf<span class="op">$</span>sparklyr.defaultPackages &lt;-<span class="st"> &quot;org.apache.hadoop:hadoop-aws:2.7.7&quot;</span>

sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>, <span class="dt">config =</span> conf)</code></pre>
<p>For the file system prefix use “s3a”. There are other options, such as “s3” and “s3n”. As per the Hadoop documents, the “s3a” file system should be the default selection.</p>
<pre class="sourceCode r"><code class="sourceCode r">my_file &lt;-<span class="st"> </span><span class="kw">spark_read_csv</span>(sc, <span class="st">&quot;my-file&quot;</span>, <span class="dt">path =</span>  <span class="st">&quot;s3a://my-bucket/my-file.csv&quot;</span>)</code></pre>
</div>
<div id="google-storage" class="section level3">
<h3><span class="header-section-number">7.4.2</span> Google Storage</h3>
<pre class="sourceCode r"><code class="sourceCode r">conf &lt;-<span class="st"> </span><span class="kw">spark_config</span>()
conf<span class="op">$</span>sparklyr.defaultPackages &lt;-<span class="st"> &quot;com.google.cloud:google-cloud-storage:1.72.0&quot;</span>
sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>, <span class="dt">config =</span> conf)
my_file &lt;-<span class="st"> </span><span class="kw">spark_read_csv</span>(
  sc, 
  <span class="st">&quot;my-file&quot;</span>, 
  <span class="dt">path =</span> <span class="st">&quot;gs://gcp-public-data-sentinel-2/index.csv.gz-BQU6JALH41   &quot;</span>
  )

<span class="kw">spark_disconnect</span>(sc)
<span class="co">#https://storage.googleapis.com/gcp-public-data-landsat/index.csv.gz</span></code></pre>
</div>
<div id="azure-storage" class="section level3">
<h3><span class="header-section-number">7.4.3</span> Azure Storage</h3>
<p><code>wasb</code> files</p>
</div>
<div id="cassandra" class="section level3">
<h3><span class="header-section-number">7.4.4</span> Cassandra</h3>
<p>See <a href="https://blog.rstudio.com/2017/07/31/sparklyr-0-6/#external-data-sources">https://blog.rstudio.com/2017/07/31/sparklyr-0-6/#external-data-sources</a>.</p>
</div>
<div id="databases" class="section level3">
<h3><span class="header-section-number">7.4.5</span> Databases</h3>
<p>See <a href="https://blog.rstudio.com/2017/07/31/sparklyr-0-6/#external-data-sources">https://blog.rstudio.com/2017/07/31/sparklyr-0-6/#external-data-sources</a>.</p>
<div id="switching" class="section level4">
<h4><span class="header-section-number">7.4.5.1</span> Switching</h4>
<p>You can query multiple databases registered in Spark using the <code>.</code> syntax, as in:</p>
<pre class="sourceCode r"><code class="sourceCode r">DBI<span class="op">::</span><span class="kw">dbSendQuery</span>(<span class="st">&quot;SELECT * FROM databasename.table&quot;</span>)</code></pre>
<p>However, if you prefer to switch to a particular database and make it the default, you can run:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">tbl_change_db</span>(sc, <span class="st">&quot;db_name&quot;</span>)</code></pre>
<p>which an alias over <code>DBI::dbGetQuery(sc, "use db_name”)</code>.</p>
<div id="schemas" class="section level5">
<h5><span class="header-section-number">7.4.5.1.1</span> Schemas</h5>
<pre><code>in_schema(&quot;database&quot;, &quot;table&quot;)</code></pre>
</div>
</div>
</div>
<div id="hbase" class="section level3">
<h3><span class="header-section-number">7.4.6</span> HBase</h3>
</div>
<div id="nested-data" class="section level3">
<h3><span class="header-section-number">7.4.7</span> Nested Data</h3>
<p>See <a href="extensions.html#extensions-nested-data">nested data extension</a>.</p>
</div>
</div>
<div id="troubleshooting" class="section level2">
<h2><span class="header-section-number">7.5</span> Troubleshooting</h2>
<div id="troubleshoot-csvs" class="section level3">
<h3><span class="header-section-number">7.5.1</span> Troubleshoot CSVs</h3>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">writeLines</span>(<span class="kw">c</span>(<span class="st">&quot;bad&quot;</span>, <span class="dv">1</span>, <span class="dv">2</span>, <span class="dv">3</span>, <span class="st">&quot;broken&quot;</span>), <span class="st">&quot;tmp/bad.csv&quot;</span>)</code></pre>
<p>There are a couple modes that can help troubleshoot parsing issues:
- <strong>PERMISSIVE</strong>: <code>NULL</code>s are inserted for missing tokens.
- <strong>DROPMALFORMED</strong>: Drops lines which are malformed.
- <strong>FAILFAST</strong>: Aborts if encounters any malformed line.</p>
<p>Which can be used as follows:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">spark_read_csv</span>(
  sc,
  <span class="st">&quot;bad&quot;</span>,
  <span class="st">&quot;tmp/bad.csv&quot;</span>,
  <span class="dt">columns =</span> <span class="kw">list</span>(<span class="dt">foo =</span> <span class="st">&quot;integer&quot;</span>),
  <span class="dt">infer_schema =</span> <span class="ot">FALSE</span>,
  <span class="dt">options =</span> <span class="kw">list</span>(<span class="dt">mode =</span> <span class="st">&quot;DROPMALFORMED&quot;</span>))</code></pre>
<pre><code># Source:   table&lt;bad&gt; [?? x 1]
# Database: spark_connection
    foo
  &lt;int&gt;
1     1
2     2
3     3</code></pre>
<p>In Spark 2.X, there is also a secret column <code>_corrupt_record</code> that can be used to output those incorrect records:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">spark_read_csv</span>(
  sc,
  <span class="st">&quot;decimals&quot;</span>,
  <span class="st">&quot;tmp/bad.csv&quot;</span>,
  <span class="dt">columns =</span> <span class="kw">list</span>(<span class="dt">foo =</span> <span class="st">&quot;integer&quot;</span>, <span class="st">&quot;_corrupt_record&quot;</span> =<span class="st"> &quot;character&quot;</span>),
  <span class="dt">infer_schema =</span> <span class="ot">FALSE</span>,
  <span class="dt">options =</span> <span class="kw">list</span>(<span class="dt">mode =</span> <span class="st">&quot;PERMISIVE&quot;</span>)
)</code></pre>
<pre><code># Source:   table&lt;decimals&gt; [?? x 2]
# Database: spark_connection
    foo `_corrupt_record`   
  &lt;int&gt; &lt;chr&gt;               
1     1 NA                  
2     2 NA                  
3     3 NA                  
4    NA sdfsdfds            
5    NA 2.16027303300001e+31</code></pre>
</div>
</div>
<div id="recap-3" class="section level2">
<h2><span class="header-section-number">7.6</span> Recap</h2>
<p>In the next chapter, <a href="tuning.html#tuning">Tuning</a>, you will learn in-detail how Spark works and use this knowledge to optimize it’s resource usage and performance.</p>

</div>
</div>
<h3> References</h3>
<div id="refs" class="references">
<div id="ref-data-spark-cloud-integration">
<p>“Spark Integration with Cloud Infrastructures.” 2019. <a href="https://spark.apache.org/docs/latest/cloud-integration.html">https://spark.apache.org/docs/latest/cloud-integration.html</a>.</p>
</div>
<div id="ref-data-maven-home">
<p>“Maven Repository: Home Page.” 2019. <a href="https://mvnrepository.com">https://mvnrepository.com</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="connections.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="tuning.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
