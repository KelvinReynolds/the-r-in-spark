<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 7 Connections | Mastering Spark with R</title>
  <meta name="description" content="The Complete Guide to Large-Scale Analysis and Modeling." />
  <meta name="generator" content="bookdown 0.13 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 7 Connections | Mastering Spark with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The Complete Guide to Large-Scale Analysis and Modeling." />
  <meta name="github-repo" content="r-spark/the-r-in-spark" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 7 Connections | Mastering Spark with R" />
  
  <meta name="twitter:description" content="The Complete Guide to Large-Scale Analysis and Modeling." />
  

<meta name="author" content="Javier Luraschi, Kevin Kuo, Edgar Ruiz" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="clusters.html"/>
<link rel="next" href="data.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-119986300-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-119986300-1');
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./">Mastering Spark with R</a></strong></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#intro-background"><i class="fa fa-check"></i><b>1.1</b> Overview</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#intro-hadoop"><i class="fa fa-check"></i><b>1.2</b> Hadoop</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#intro-spark"><i class="fa fa-check"></i><b>1.3</b> Spark</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#intro-r"><i class="fa fa-check"></i><b>1.4</b> R</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#intro-sparklyr"><i class="fa fa-check"></i><b>1.5</b> sparklyr</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#intro-recap"><i class="fa fa-check"></i><b>1.6</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="starting.html"><a href="starting.html"><i class="fa fa-check"></i><b>2</b> Getting Started</a><ul>
<li class="chapter" data-level="2.1" data-path="starting.html"><a href="starting.html#overview"><i class="fa fa-check"></i><b>2.1</b> Overview</a></li>
<li class="chapter" data-level="2.2" data-path="starting.html"><a href="starting.html#starting-prerequisites"><i class="fa fa-check"></i><b>2.2</b> Prerequisites</a><ul>
<li class="chapter" data-level="2.2.1" data-path="starting.html"><a href="starting.html#starting-install-sparklyr"><i class="fa fa-check"></i><b>2.2.1</b> Installing sparklyr</a></li>
<li class="chapter" data-level="2.2.2" data-path="starting.html"><a href="starting.html#starting-installing-spark"><i class="fa fa-check"></i><b>2.2.2</b> Installing Spark</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="starting.html"><a href="starting.html#starting-connect-to-spark"><i class="fa fa-check"></i><b>2.3</b> Connecting</a></li>
<li class="chapter" data-level="2.4" data-path="starting.html"><a href="starting.html#starting-sparklyr-hello-world"><i class="fa fa-check"></i><b>2.4</b> Using Spark</a><ul>
<li class="chapter" data-level="2.4.1" data-path="starting.html"><a href="starting.html#starting-spark-web-interface"><i class="fa fa-check"></i><b>2.4.1</b> Web Interface</a></li>
<li class="chapter" data-level="2.4.2" data-path="starting.html"><a href="starting.html#starting-analysis"><i class="fa fa-check"></i><b>2.4.2</b> Analysis</a></li>
<li class="chapter" data-level="2.4.3" data-path="starting.html"><a href="starting.html#starting-modeling"><i class="fa fa-check"></i><b>2.4.3</b> Modeling</a></li>
<li class="chapter" data-level="2.4.4" data-path="starting.html"><a href="starting.html#starting-data"><i class="fa fa-check"></i><b>2.4.4</b> Data</a></li>
<li class="chapter" data-level="2.4.5" data-path="starting.html"><a href="starting.html#starting-extensions"><i class="fa fa-check"></i><b>2.4.5</b> Extensions</a></li>
<li class="chapter" data-level="2.4.6" data-path="starting.html"><a href="starting.html#starting-distributed-r"><i class="fa fa-check"></i><b>2.4.6</b> Distributed R</a></li>
<li class="chapter" data-level="2.4.7" data-path="starting.html"><a href="starting.html#starting-streaming"><i class="fa fa-check"></i><b>2.4.7</b> Streaming</a></li>
<li class="chapter" data-level="2.4.8" data-path="starting.html"><a href="starting.html#starting-logs"><i class="fa fa-check"></i><b>2.4.8</b> Logs</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="starting.html"><a href="starting.html#starting-disconnecting"><i class="fa fa-check"></i><b>2.5</b> Disconnecting</a></li>
<li class="chapter" data-level="2.6" data-path="starting.html"><a href="starting.html#starting-using-spark-from-rstudio"><i class="fa fa-check"></i><b>2.6</b> Using RStudio</a></li>
<li class="chapter" data-level="2.7" data-path="starting.html"><a href="starting.html#starting-resources"><i class="fa fa-check"></i><b>2.7</b> Resources</a></li>
<li class="chapter" data-level="2.8" data-path="starting.html"><a href="starting.html#starting-recap"><i class="fa fa-check"></i><b>2.8</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="analysis.html"><a href="analysis.html"><i class="fa fa-check"></i><b>3</b> Analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="analysis.html"><a href="analysis.html#analysis-overview"><i class="fa fa-check"></i><b>3.1</b> Overview</a></li>
<li class="chapter" data-level="3.2" data-path="analysis.html"><a href="analysis.html#import"><i class="fa fa-check"></i><b>3.2</b> Import</a></li>
<li class="chapter" data-level="3.3" data-path="analysis.html"><a href="analysis.html#wrangle"><i class="fa fa-check"></i><b>3.3</b> Wrangle</a><ul>
<li class="chapter" data-level="3.3.1" data-path="analysis.html"><a href="analysis.html#built-in-functions"><i class="fa fa-check"></i><b>3.3.1</b> Built-in Functions</a></li>
<li class="chapter" data-level="3.3.2" data-path="analysis.html"><a href="analysis.html#correlations"><i class="fa fa-check"></i><b>3.3.2</b> Correlations</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="analysis.html"><a href="analysis.html#visualize"><i class="fa fa-check"></i><b>3.4</b> Visualize</a><ul>
<li class="chapter" data-level="3.4.1" data-path="analysis.html"><a href="analysis.html#using-ggplot2"><i class="fa fa-check"></i><b>3.4.1</b> Using ggplot2</a></li>
<li class="chapter" data-level="3.4.2" data-path="analysis.html"><a href="analysis.html#using-dbplot"><i class="fa fa-check"></i><b>3.4.2</b> Using dbplot</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="analysis.html"><a href="analysis.html#model"><i class="fa fa-check"></i><b>3.5</b> Model</a><ul>
<li class="chapter" data-level="3.5.1" data-path="analysis.html"><a href="analysis.html#caching"><i class="fa fa-check"></i><b>3.5.1</b> Caching</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="analysis.html"><a href="analysis.html#communicate"><i class="fa fa-check"></i><b>3.6</b> Communicate</a></li>
<li class="chapter" data-level="3.7" data-path="analysis.html"><a href="analysis.html#recap"><i class="fa fa-check"></i><b>3.7</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="modeling.html"><a href="modeling.html"><i class="fa fa-check"></i><b>4</b> Modeling</a><ul>
<li class="chapter" data-level="4.1" data-path="modeling.html"><a href="modeling.html#overview-1"><i class="fa fa-check"></i><b>4.1</b> Overview</a></li>
<li class="chapter" data-level="4.2" data-path="modeling.html"><a href="modeling.html#exploratory-data-analysis"><i class="fa fa-check"></i><b>4.2</b> Exploratory Data Analysis</a></li>
<li class="chapter" data-level="4.3" data-path="modeling.html"><a href="modeling.html#feature-engineering"><i class="fa fa-check"></i><b>4.3</b> Feature Engineering</a></li>
<li class="chapter" data-level="4.4" data-path="modeling.html"><a href="modeling.html#supervised-learning"><i class="fa fa-check"></i><b>4.4</b> Supervised Learning</a><ul>
<li class="chapter" data-level="4.4.1" data-path="modeling.html"><a href="modeling.html#generalized-linear-regression"><i class="fa fa-check"></i><b>4.4.1</b> Generalized Linear Regression</a></li>
<li class="chapter" data-level="4.4.2" data-path="modeling.html"><a href="modeling.html#other-models"><i class="fa fa-check"></i><b>4.4.2</b> Other Models</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="modeling.html"><a href="modeling.html#unsupervised-learning"><i class="fa fa-check"></i><b>4.5</b> Unsupervised Learning</a><ul>
<li class="chapter" data-level="4.5.1" data-path="modeling.html"><a href="modeling.html#data-preparation"><i class="fa fa-check"></i><b>4.5.1</b> Data Preparation</a></li>
<li class="chapter" data-level="4.5.2" data-path="modeling.html"><a href="modeling.html#topic-modeling"><i class="fa fa-check"></i><b>4.5.2</b> Topic Modeling</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="modeling.html"><a href="modeling.html#recap-1"><i class="fa fa-check"></i><b>4.6</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="pipelines.html"><a href="pipelines.html"><i class="fa fa-check"></i><b>5</b> Pipelines</a><ul>
<li class="chapter" data-level="5.1" data-path="pipelines.html"><a href="pipelines.html#overview-2"><i class="fa fa-check"></i><b>5.1</b> Overview</a></li>
<li class="chapter" data-level="5.2" data-path="pipelines.html"><a href="pipelines.html#creation"><i class="fa fa-check"></i><b>5.2</b> Creation</a></li>
<li class="chapter" data-level="5.3" data-path="pipelines.html"><a href="pipelines.html#use-cases"><i class="fa fa-check"></i><b>5.3</b> Use Cases</a><ul>
<li class="chapter" data-level="5.3.1" data-path="pipelines.html"><a href="pipelines.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>5.3.1</b> Hyperparameter Tuning</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="pipelines.html"><a href="pipelines.html#operating-modes"><i class="fa fa-check"></i><b>5.4</b> Operating Modes</a></li>
<li class="chapter" data-level="5.5" data-path="pipelines.html"><a href="pipelines.html#interoperability"><i class="fa fa-check"></i><b>5.5</b> Interoperability</a></li>
<li class="chapter" data-level="5.6" data-path="pipelines.html"><a href="pipelines.html#deployment"><i class="fa fa-check"></i><b>5.6</b> Deployment</a><ul>
<li class="chapter" data-level="5.6.1" data-path="pipelines.html"><a href="pipelines.html#batch-scoring"><i class="fa fa-check"></i><b>5.6.1</b> Batch Scoring</a></li>
<li class="chapter" data-level="5.6.2" data-path="pipelines.html"><a href="pipelines.html#real-time-scoring"><i class="fa fa-check"></i><b>5.6.2</b> Real-Time Scoring</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="pipelines.html"><a href="pipelines.html#recap-2"><i class="fa fa-check"></i><b>5.7</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="clusters.html"><a href="clusters.html"><i class="fa fa-check"></i><b>6</b> Clusters</a><ul>
<li class="chapter" data-level="6.1" data-path="clusters.html"><a href="clusters.html#clusters-overview"><i class="fa fa-check"></i><b>6.1</b> Overview</a></li>
<li class="chapter" data-level="6.2" data-path="clusters.html"><a href="clusters.html#on-premise"><i class="fa fa-check"></i><b>6.2</b> On-Premise</a><ul>
<li class="chapter" data-level="6.2.1" data-path="clusters.html"><a href="clusters.html#clusters-manager"><i class="fa fa-check"></i><b>6.2.1</b> Managers</a></li>
<li class="chapter" data-level="6.2.2" data-path="clusters.html"><a href="clusters.html#distributions"><i class="fa fa-check"></i><b>6.2.2</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="clusters.html"><a href="clusters.html#cloud"><i class="fa fa-check"></i><b>6.3</b> Cloud</a><ul>
<li class="chapter" data-level="6.3.1" data-path="clusters.html"><a href="clusters.html#clusters-amazon-emr"><i class="fa fa-check"></i><b>6.3.1</b> Amazon</a></li>
<li class="chapter" data-level="6.3.2" data-path="clusters.html"><a href="clusters.html#databricks"><i class="fa fa-check"></i><b>6.3.2</b> Databricks</a></li>
<li class="chapter" data-level="6.3.3" data-path="clusters.html"><a href="clusters.html#google"><i class="fa fa-check"></i><b>6.3.3</b> Google</a></li>
<li class="chapter" data-level="6.3.4" data-path="clusters.html"><a href="clusters.html#ibm"><i class="fa fa-check"></i><b>6.3.4</b> IBM</a></li>
<li class="chapter" data-level="6.3.5" data-path="clusters.html"><a href="clusters.html#microsoft"><i class="fa fa-check"></i><b>6.3.5</b> Microsoft</a></li>
<li class="chapter" data-level="6.3.6" data-path="clusters.html"><a href="clusters.html#qubole"><i class="fa fa-check"></i><b>6.3.6</b> Qubole</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="clusters.html"><a href="clusters.html#kubernetes"><i class="fa fa-check"></i><b>6.4</b> Kubernetes</a></li>
<li class="chapter" data-level="6.5" data-path="clusters.html"><a href="clusters.html#tools"><i class="fa fa-check"></i><b>6.5</b> Tools</a><ul>
<li class="chapter" data-level="6.5.1" data-path="clusters.html"><a href="clusters.html#rstudio"><i class="fa fa-check"></i><b>6.5.1</b> RStudio</a></li>
<li class="chapter" data-level="6.5.2" data-path="clusters.html"><a href="clusters.html#jupyter"><i class="fa fa-check"></i><b>6.5.2</b> Jupyter</a></li>
<li class="chapter" data-level="6.5.3" data-path="clusters.html"><a href="clusters.html#clusters-livy"><i class="fa fa-check"></i><b>6.5.3</b> Livy</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="clusters.html"><a href="clusters.html#recap-3"><i class="fa fa-check"></i><b>6.6</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="connections.html"><a href="connections.html"><i class="fa fa-check"></i><b>7</b> Connections</a><ul>
<li class="chapter" data-level="7.1" data-path="connections.html"><a href="connections.html#connections-overview"><i class="fa fa-check"></i><b>7.1</b> Overview</a><ul>
<li class="chapter" data-level="7.1.1" data-path="connections.html"><a href="connections.html#connections-spark-edge-nodes"><i class="fa fa-check"></i><b>7.1.1</b> Edge Nodes</a></li>
<li class="chapter" data-level="7.1.2" data-path="connections.html"><a href="connections.html#connections-spark-home"><i class="fa fa-check"></i><b>7.1.2</b> Spark Home</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="connections.html"><a href="connections.html#connections-local"><i class="fa fa-check"></i><b>7.2</b> Local</a></li>
<li class="chapter" data-level="7.3" data-path="connections.html"><a href="connections.html#connections-standalone"><i class="fa fa-check"></i><b>7.3</b> Standalone</a></li>
<li class="chapter" data-level="7.4" data-path="connections.html"><a href="connections.html#connections-yarn"><i class="fa fa-check"></i><b>7.4</b> Yarn</a><ul>
<li class="chapter" data-level="7.4.1" data-path="connections.html"><a href="connections.html#connections-yarn-client"><i class="fa fa-check"></i><b>7.4.1</b> Yarn Client</a></li>
<li class="chapter" data-level="7.4.2" data-path="connections.html"><a href="connections.html#connections-yarn-cluster"><i class="fa fa-check"></i><b>7.4.2</b> Yarn Cluster</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="connections.html"><a href="connections.html#connections-livy"><i class="fa fa-check"></i><b>7.5</b> Livy</a></li>
<li class="chapter" data-level="7.6" data-path="connections.html"><a href="connections.html#connections-mesos"><i class="fa fa-check"></i><b>7.6</b> Mesos</a></li>
<li class="chapter" data-level="7.7" data-path="connections.html"><a href="connections.html#connections-kubernetes"><i class="fa fa-check"></i><b>7.7</b> Kubernetes</a></li>
<li class="chapter" data-level="7.8" data-path="connections.html"><a href="connections.html#cloud-1"><i class="fa fa-check"></i><b>7.8</b> Cloud</a></li>
<li class="chapter" data-level="7.9" data-path="connections.html"><a href="connections.html#batches"><i class="fa fa-check"></i><b>7.9</b> Batches</a></li>
<li class="chapter" data-level="7.10" data-path="connections.html"><a href="connections.html#tools-1"><i class="fa fa-check"></i><b>7.10</b> Tools</a></li>
<li class="chapter" data-level="7.11" data-path="connections.html"><a href="connections.html#multiple"><i class="fa fa-check"></i><b>7.11</b> Multiple</a></li>
<li class="chapter" data-level="7.12" data-path="connections.html"><a href="connections.html#connections-troubleshooting"><i class="fa fa-check"></i><b>7.12</b> Troubleshooting</a><ul>
<li class="chapter" data-level="7.12.1" data-path="connections.html"><a href="connections.html#logging"><i class="fa fa-check"></i><b>7.12.1</b> Logging</a></li>
<li class="chapter" data-level="7.12.2" data-path="connections.html"><a href="connections.html#troubleshoot-spark-submit"><i class="fa fa-check"></i><b>7.12.2</b> Spark Submit</a></li>
<li class="chapter" data-level="7.12.3" data-path="connections.html"><a href="connections.html#windows"><i class="fa fa-check"></i><b>7.12.3</b> Windows</a></li>
</ul></li>
<li class="chapter" data-level="7.13" data-path="connections.html"><a href="connections.html#recap-4"><i class="fa fa-check"></i><b>7.13</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>8</b> Data</a><ul>
<li class="chapter" data-level="8.1" data-path="data.html"><a href="data.html#overview-3"><i class="fa fa-check"></i><b>8.1</b> Overview</a></li>
<li class="chapter" data-level="8.2" data-path="data.html"><a href="data.html#read"><i class="fa fa-check"></i><b>8.2</b> Read</a><ul>
<li class="chapter" data-level="8.2.1" data-path="data.html"><a href="data.html#paths"><i class="fa fa-check"></i><b>8.2.1</b> Paths</a></li>
<li class="chapter" data-level="8.2.2" data-path="data.html"><a href="data.html#schema"><i class="fa fa-check"></i><b>8.2.2</b> Schema</a></li>
<li class="chapter" data-level="8.2.3" data-path="data.html"><a href="data.html#memory"><i class="fa fa-check"></i><b>8.2.3</b> Memory</a></li>
<li class="chapter" data-level="8.2.4" data-path="data.html"><a href="data.html#columns"><i class="fa fa-check"></i><b>8.2.4</b> Columns</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="data.html"><a href="data.html#write"><i class="fa fa-check"></i><b>8.3</b> Write</a></li>
<li class="chapter" data-level="8.4" data-path="data.html"><a href="data.html#copy"><i class="fa fa-check"></i><b>8.4</b> Copy</a></li>
<li class="chapter" data-level="8.5" data-path="data.html"><a href="data.html#data-file-formats"><i class="fa fa-check"></i><b>8.5</b> File Formats</a><ul>
<li class="chapter" data-level="8.5.1" data-path="data.html"><a href="data.html#csv"><i class="fa fa-check"></i><b>8.5.1</b> CSV</a></li>
<li class="chapter" data-level="8.5.2" data-path="data.html"><a href="data.html#json"><i class="fa fa-check"></i><b>8.5.2</b> JSON</a></li>
<li class="chapter" data-level="8.5.3" data-path="data.html"><a href="data.html#parquet"><i class="fa fa-check"></i><b>8.5.3</b> Parquet</a></li>
<li class="chapter" data-level="8.5.4" data-path="data.html"><a href="data.html#others"><i class="fa fa-check"></i><b>8.5.4</b> Others</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="data.html"><a href="data.html#data-file-systems"><i class="fa fa-check"></i><b>8.6</b> File Systems</a></li>
<li class="chapter" data-level="8.7" data-path="data.html"><a href="data.html#data-storage-systems"><i class="fa fa-check"></i><b>8.7</b> Storage Systems</a><ul>
<li class="chapter" data-level="8.7.1" data-path="data.html"><a href="data.html#hive"><i class="fa fa-check"></i><b>8.7.1</b> Hive</a></li>
<li class="chapter" data-level="8.7.2" data-path="data.html"><a href="data.html#cassandra"><i class="fa fa-check"></i><b>8.7.2</b> Cassandra</a></li>
<li class="chapter" data-level="8.7.3" data-path="data.html"><a href="data.html#jdbc"><i class="fa fa-check"></i><b>8.7.3</b> JDBC</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="data.html"><a href="data.html#recap-5"><i class="fa fa-check"></i><b>8.8</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="tuning.html"><a href="tuning.html"><i class="fa fa-check"></i><b>9</b> Tuning</a><ul>
<li class="chapter" data-level="9.1" data-path="tuning.html"><a href="tuning.html#overview-4"><i class="fa fa-check"></i><b>9.1</b> Overview</a><ul>
<li class="chapter" data-level="9.1.1" data-path="tuning.html"><a href="tuning.html#tuning-graph-visualization"><i class="fa fa-check"></i><b>9.1.1</b> Graph</a></li>
<li class="chapter" data-level="9.1.2" data-path="tuning.html"><a href="tuning.html#tuning-event-timeline"><i class="fa fa-check"></i><b>9.1.2</b> Timeline</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="tuning.html"><a href="tuning.html#tuning-configuring"><i class="fa fa-check"></i><b>9.2</b> Configuring</a><ul>
<li class="chapter" data-level="9.2.1" data-path="tuning.html"><a href="tuning.html#connect-settings"><i class="fa fa-check"></i><b>9.2.1</b> Connect Settings</a></li>
<li class="chapter" data-level="9.2.2" data-path="tuning.html"><a href="tuning.html#submit-settings"><i class="fa fa-check"></i><b>9.2.2</b> Submit Settings</a></li>
<li class="chapter" data-level="9.2.3" data-path="tuning.html"><a href="tuning.html#runtime-settings"><i class="fa fa-check"></i><b>9.2.3</b> Runtime Settings</a></li>
<li class="chapter" data-level="9.2.4" data-path="tuning.html"><a href="tuning.html#sparklyr-settings"><i class="fa fa-check"></i><b>9.2.4</b> sparklyr Settings</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="tuning.html"><a href="tuning.html#tuning-partitioning"><i class="fa fa-check"></i><b>9.3</b> Partitioning</a><ul>
<li class="chapter" data-level="9.3.1" data-path="tuning.html"><a href="tuning.html#implicit"><i class="fa fa-check"></i><b>9.3.1</b> Implicit</a></li>
<li class="chapter" data-level="9.3.2" data-path="tuning.html"><a href="tuning.html#explicit"><i class="fa fa-check"></i><b>9.3.2</b> Explicit</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="tuning.html"><a href="tuning.html#tuning-caching"><i class="fa fa-check"></i><b>9.4</b> Caching</a><ul>
<li class="chapter" data-level="9.4.1" data-path="tuning.html"><a href="tuning.html#checkpointing"><i class="fa fa-check"></i><b>9.4.1</b> Checkpointing</a></li>
<li class="chapter" data-level="9.4.2" data-path="tuning.html"><a href="tuning.html#tuning-memory"><i class="fa fa-check"></i><b>9.4.2</b> Memory</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="tuning.html"><a href="tuning.html#tuning-shuffling"><i class="fa fa-check"></i><b>9.5</b> Shuffling</a></li>
<li class="chapter" data-level="9.6" data-path="tuning.html"><a href="tuning.html#tuning-serialization"><i class="fa fa-check"></i><b>9.6</b> Serialization</a></li>
<li class="chapter" data-level="9.7" data-path="tuning.html"><a href="tuning.html#configuration-files"><i class="fa fa-check"></i><b>9.7</b> Configuration Files</a></li>
<li class="chapter" data-level="9.8" data-path="tuning.html"><a href="tuning.html#recap-6"><i class="fa fa-check"></i><b>9.8</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="extensions.html"><a href="extensions.html"><i class="fa fa-check"></i><b>10</b> Extensions</a><ul>
<li class="chapter" data-level="10.1" data-path="extensions.html"><a href="extensions.html#overview-5"><i class="fa fa-check"></i><b>10.1</b> Overview</a></li>
<li class="chapter" data-level="10.2" data-path="extensions.html"><a href="extensions.html#h2o"><i class="fa fa-check"></i><b>10.2</b> H2O</a></li>
<li class="chapter" data-level="10.3" data-path="extensions.html"><a href="extensions.html#graphs"><i class="fa fa-check"></i><b>10.3</b> Graphs</a></li>
<li class="chapter" data-level="10.4" data-path="extensions.html"><a href="extensions.html#xgboost"><i class="fa fa-check"></i><b>10.4</b> XGBoost</a></li>
<li class="chapter" data-level="10.5" data-path="extensions.html"><a href="extensions.html#deep-learning"><i class="fa fa-check"></i><b>10.5</b> Deep Learning</a></li>
<li class="chapter" data-level="10.6" data-path="extensions.html"><a href="extensions.html#genomics"><i class="fa fa-check"></i><b>10.6</b> Genomics</a></li>
<li class="chapter" data-level="10.7" data-path="extensions.html"><a href="extensions.html#spatial"><i class="fa fa-check"></i><b>10.7</b> Spatial</a></li>
<li class="chapter" data-level="10.8" data-path="extensions.html"><a href="extensions.html#troubleshooting"><i class="fa fa-check"></i><b>10.8</b> Troubleshooting</a></li>
<li class="chapter" data-level="10.9" data-path="extensions.html"><a href="extensions.html#recap-7"><i class="fa fa-check"></i><b>10.9</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="distributed.html"><a href="distributed.html"><i class="fa fa-check"></i><b>11</b> Distributed R</a><ul>
<li class="chapter" data-level="11.1" data-path="distributed.html"><a href="distributed.html#overview-6"><i class="fa fa-check"></i><b>11.1</b> Overview</a></li>
<li class="chapter" data-level="11.2" data-path="distributed.html"><a href="distributed.html#use-cases-1"><i class="fa fa-check"></i><b>11.2</b> Use Cases</a><ul>
<li class="chapter" data-level="11.2.1" data-path="distributed.html"><a href="distributed.html#custom-parsers"><i class="fa fa-check"></i><b>11.2.1</b> Custom Parsers</a></li>
<li class="chapter" data-level="11.2.2" data-path="distributed.html"><a href="distributed.html#partitioned-modeling"><i class="fa fa-check"></i><b>11.2.2</b> Partitioned Modeling</a></li>
<li class="chapter" data-level="11.2.3" data-path="distributed.html"><a href="distributed.html#distributed-grid-search"><i class="fa fa-check"></i><b>11.2.3</b> Grid Search</a></li>
<li class="chapter" data-level="11.2.4" data-path="distributed.html"><a href="distributed.html#web-apis"><i class="fa fa-check"></i><b>11.2.4</b> Web APIs</a></li>
<li class="chapter" data-level="11.2.5" data-path="distributed.html"><a href="distributed.html#simulations"><i class="fa fa-check"></i><b>11.2.5</b> Simulations</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="distributed.html"><a href="distributed.html#partitions"><i class="fa fa-check"></i><b>11.3</b> Partitions</a></li>
<li class="chapter" data-level="11.4" data-path="distributed.html"><a href="distributed.html#grouping"><i class="fa fa-check"></i><b>11.4</b> Grouping</a></li>
<li class="chapter" data-level="11.5" data-path="distributed.html"><a href="distributed.html#columns-1"><i class="fa fa-check"></i><b>11.5</b> Columns</a></li>
<li class="chapter" data-level="11.6" data-path="distributed.html"><a href="distributed.html#context"><i class="fa fa-check"></i><b>11.6</b> Context</a></li>
<li class="chapter" data-level="11.7" data-path="distributed.html"><a href="distributed.html#functions"><i class="fa fa-check"></i><b>11.7</b> Functions</a></li>
<li class="chapter" data-level="11.8" data-path="distributed.html"><a href="distributed.html#packages"><i class="fa fa-check"></i><b>11.8</b> Packages</a></li>
<li class="chapter" data-level="11.9" data-path="distributed.html"><a href="distributed.html#cluster-requirements"><i class="fa fa-check"></i><b>11.9</b> Cluster Requirements</a><ul>
<li class="chapter" data-level="11.9.1" data-path="distributed.html"><a href="distributed.html#installing-r"><i class="fa fa-check"></i><b>11.9.1</b> Installing R</a></li>
<li class="chapter" data-level="11.9.2" data-path="distributed.html"><a href="distributed.html#apache-arrow"><i class="fa fa-check"></i><b>11.9.2</b> Apache Arrow</a></li>
</ul></li>
<li class="chapter" data-level="11.10" data-path="distributed.html"><a href="distributed.html#troubleshooting-1"><i class="fa fa-check"></i><b>11.10</b> Troubleshooting</a><ul>
<li class="chapter" data-level="11.10.1" data-path="distributed.html"><a href="distributed.html#worker-logs"><i class="fa fa-check"></i><b>11.10.1</b> Worker Logs</a></li>
<li class="chapter" data-level="11.10.2" data-path="distributed.html"><a href="distributed.html#resolving-timeouts"><i class="fa fa-check"></i><b>11.10.2</b> Resolving Timeouts</a></li>
<li class="chapter" data-level="11.10.3" data-path="distributed.html"><a href="distributed.html#inspecting-partition"><i class="fa fa-check"></i><b>11.10.3</b> Inspecting Partition</a></li>
<li class="chapter" data-level="11.10.4" data-path="distributed.html"><a href="distributed.html#debugging-workers"><i class="fa fa-check"></i><b>11.10.4</b> Debugging Workers</a></li>
</ul></li>
<li class="chapter" data-level="11.11" data-path="distributed.html"><a href="distributed.html#recap-8"><i class="fa fa-check"></i><b>11.11</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="streaming.html"><a href="streaming.html"><i class="fa fa-check"></i><b>12</b> Streaming</a><ul>
<li class="chapter" data-level="12.1" data-path="streaming.html"><a href="streaming.html#overview-7"><i class="fa fa-check"></i><b>12.1</b> Overview</a></li>
<li class="chapter" data-level="12.2" data-path="streaming.html"><a href="streaming.html#transformations"><i class="fa fa-check"></i><b>12.2</b> Transformations</a><ul>
<li class="chapter" data-level="12.2.1" data-path="streaming.html"><a href="streaming.html#analysis-1"><i class="fa fa-check"></i><b>12.2.1</b> Analysis</a></li>
<li class="chapter" data-level="12.2.2" data-path="streaming.html"><a href="streaming.html#modeling-1"><i class="fa fa-check"></i><b>12.2.2</b> Modeling</a></li>
<li class="chapter" data-level="12.2.3" data-path="streaming.html"><a href="streaming.html#pipelines-1"><i class="fa fa-check"></i><b>12.2.3</b> Pipelines</a></li>
<li class="chapter" data-level="12.2.4" data-path="streaming.html"><a href="streaming.html#distributed-r-streaming-r-code"><i class="fa fa-check"></i><b>12.2.4</b> Distributed R {streaming-r-code}</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="streaming.html"><a href="streaming.html#kafka"><i class="fa fa-check"></i><b>12.3</b> Kafka</a></li>
<li class="chapter" data-level="12.4" data-path="streaming.html"><a href="streaming.html#shiny"><i class="fa fa-check"></i><b>12.4</b> Shiny</a></li>
<li class="chapter" data-level="12.5" data-path="streaming.html"><a href="streaming.html#recap-9"><i class="fa fa-check"></i><b>12.5</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="contributing.html"><a href="contributing.html"><i class="fa fa-check"></i><b>13</b> Contributing</a><ul>
<li class="chapter" data-level="13.1" data-path="contributing.html"><a href="contributing.html#contributing-overview"><i class="fa fa-check"></i><b>13.1</b> Overview</a></li>
<li class="chapter" data-level="13.2" data-path="contributing.html"><a href="contributing.html#contributing-spark-api"><i class="fa fa-check"></i><b>13.2</b> Spark API</a></li>
<li class="chapter" data-level="13.3" data-path="contributing.html"><a href="contributing.html#spark-extensions"><i class="fa fa-check"></i><b>13.3</b> Spark Extensions</a></li>
<li class="chapter" data-level="13.4" data-path="contributing.html"><a href="contributing.html#scala-code"><i class="fa fa-check"></i><b>13.4</b> Scala Code</a></li>
<li class="chapter" data-level="13.5" data-path="contributing.html"><a href="contributing.html#recap-10"><i class="fa fa-check"></i><b>13.5</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>14</b> Appendix</a><ul>
<li class="chapter" data-level="14.1" data-path="appendix.html"><a href="appendix.html#appendix-preface"><i class="fa fa-check"></i><b>14.1</b> Preface</a><ul>
<li class="chapter" data-level="14.1.1" data-path="appendix.html"><a href="appendix.html#appendix-ggplot2-theme"><i class="fa fa-check"></i><b>14.1.1</b> Formatting</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="appendix.html"><a href="appendix.html#appendix-intro"><i class="fa fa-check"></i><b>14.2</b> Introduction</a><ul>
<li class="chapter" data-level="14.2.1" data-path="appendix.html"><a href="appendix.html#appendix-storage-capacity"><i class="fa fa-check"></i><b>14.2.1</b> Worlds Store Capacity</a></li>
<li class="chapter" data-level="14.2.2" data-path="appendix.html"><a href="appendix.html#appendix-cran-downloads"><i class="fa fa-check"></i><b>14.2.2</b> Daily downloads of CRAN packages</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="appendix.html"><a href="appendix.html#appendix-starting"><i class="fa fa-check"></i><b>14.3</b> Getting Started</a><ul>
<li class="chapter" data-level="14.3.1" data-path="appendix.html"><a href="appendix.html#appendix-prerequisites"><i class="fa fa-check"></i><b>14.3.1</b> Prerequisites</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="appendix.html"><a href="appendix.html#appendix-analysis"><i class="fa fa-check"></i><b>14.4</b> Analysis</a><ul>
<li class="chapter" data-level="14.4.1" data-path="appendix.html"><a href="appendix.html#hive-functions"><i class="fa fa-check"></i><b>14.4.1</b> Hive Functions</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="appendix.html"><a href="appendix.html#appendix-modeling"><i class="fa fa-check"></i><b>14.5</b> Modeling</a><ul>
<li class="chapter" data-level="14.5.1" data-path="appendix.html"><a href="appendix.html#appendix-ml-functionlist"><i class="fa fa-check"></i><b>14.5.1</b> MLlib Functions</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="appendix.html"><a href="appendix.html#appendix-clusters"><i class="fa fa-check"></i><b>14.6</b> Clusters</a><ul>
<li class="chapter" data-level="14.6.1" data-path="appendix.html"><a href="appendix.html#appendix-cluster-trends"><i class="fa fa-check"></i><b>14.6.1</b> Google trends for mainframes, cloud computing and kubernetes</a></li>
</ul></li>
<li class="chapter" data-level="14.7" data-path="appendix.html"><a href="appendix.html#appendix-streaming"><i class="fa fa-check"></i><b>14.7</b> Streaming</a><ul>
<li class="chapter" data-level="14.7.1" data-path="appendix.html"><a href="appendix.html#appendix-streaming-generator"><i class="fa fa-check"></i><b>14.7.1</b> Stream Generator</a></li>
<li class="chapter" data-level="14.7.2" data-path="appendix.html"><a href="appendix.html#appendix-streaming-kafka"><i class="fa fa-check"></i><b>14.7.2</b> Installing Kafka</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>15</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Mastering Spark with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="connections" class="section level1">
<h1><span class="header-section-number">Chapter 7</span> Connections</h1>
<blockquote>
<p>“They don’t get to choose.”</p>
<p>— Daenerys Targaryen</p>
</blockquote>
<p>The previous chapter, Clusters, presented the major cluster computing trends, cluster managers, distributions and cloud service providers to help you choose the Spark cluster that best suits your needs. In contrast, this chapter presents the internal components of a Spark cluster and how to connect to a particular Spark cluster.</p>
<p>When reading this chapter, don’t try to execute every line of code; this would be quite hard since you would have to prepare different Spark environments. Instead, if you already have a Spark cluster or if the previous chapter get you motivated enough to sign-up for an on-demand cluster; now is the time to learn how to connect to it. This chapter will help you connect to your cluster, which you should have already chosen by now. Without a cluster, we recommend you learn the concepts and come back to execute code later on.</p>
<p>In addition, this chapter provides various troubleshooting connection techniques. While we hope you won’t need to use them, this chapter will prepare you to use them as effective techniques to resolve connectivity issues.</p>
<p>While this chapter might feel a bit dry – connecting and troubleshooting connections is definitely not the most exciting part of large-scale computing – it will introduce the components of a Spark cluster and how they interact, often known as the architecture of Apache Spark. This chapter, in addition to the Data and Tuning chapters, will provide a detailed view of how Spark works, which will help you move towards becoming an intermediate Spark user that can truly understand the exciting world of distributed computing, using Apache Spark.</p>
<div id="connections-overview" class="section level2">
<h2><span class="header-section-number">7.1</span> Overview</h2>
<p>The overall connection architecture for a Spark cluster is composed of three types of compute instances: the <em>driver node</em>, the <em>worker nodes</em> and the <em>cluster manager</em>. A cluster manager is a service that allows Spark to be executed in the cluster, this was detailed in the previous chapter under the <a href="clusters.html#clusters-manager">cluster managers</a> section. The <em>worker nodes</em> (also referred to as <em>executors</em>) execute compute tasks over partitioned data and communicate intermediate results to other workers or back to the driver node. The <em>driver node</em> is tasked with delegating work to the worker nodes, but also for aggregating their results and controlling computation flow. For the most part, aggregation happens in the worker nodes; however, even after the nodes aggregate data, it is often the case that the driver node would have to collect the worker’s results. Therefore, the driver node usually has at least, but often much more, compute resources (memory, CPUs, local storage, etc.) than the worker node.</p>
<p>Strictly speaking, the driver node and worker nodes are just names assigned to machines with particular roles, while the actual computation in the driver node is performed by the <em>spark context</em>. The Spark context is the main entry point for Spark functionality <span class="citation">(“Azure Wikipedia” <a href="#ref-connections-spark-context-scala">2019</a>)</span> since it’s tasked with scheduling tasks, managing storage, tracking execution status, access configuration settings, canceling jobs and so on. In the worker nodes, the actual computation is performed under a <em>spark executor</em>, which is a Spark component tasked with executing subtasks against specific data partition.</p>
<p>We can illustrate this concept in Figure <a href="connections.html#fig:connections-architecture">7.1</a>, where the driver node orchestrates worker’s work through the cluster manager.</p>
<div class="figure" style="text-align: center"><span id="fig:connections-architecture"></span>
<img src="the-r-in-spark_files/figure-html/connections-architecture-1.png" alt="Apache Spark Architecture" width="auto" height="280pt" />
<p class="caption">
FIGURE 7.1: Apache Spark Architecture
</p>
</div>
<p>If you already have a Spark cluster in your organization, you should request the connection information to this cluster from your cluster administrator, read their usage policies carefully and follow their advice. Since a cluster may be shared among many users, you want to make sure you only request the compute resources you need – we will cover how to request resources in the Tuning chapter. Your system administrator will describe if it’s an <em>on-premise</em> vs <em>cloud</em> cluster, the <em>cluster manager</em> being used, supported <em>connections</em> and supported <em>tools</em>. You can use this information to jump directly to <a href="connections.html#connections-local">Local</a>, <a href="connections.html#connections-standalone">Standalone</a>, <a href="connections.html#connections-yarn">YARN</a>, <a href="connections.html#connections-mesos">Mesos</a>, <a href="connections.html#connections-livy">Livy</a> or <a href="connections.html#connections-kubernetes">Kubernetes</a> based on the information provided to you.</p>
<p><strong>Note:</strong> Once connected is performed with <code>spark_connect()</code>, you can use all the techniques described in previous chapters using the <code>sc</code> connection; for instance, you can do data analysis or modeling with the same code previous chapters presented.</p>
<div id="connections-spark-edge-nodes" class="section level3">
<h3><span class="header-section-number">7.1.1</span> Edge Nodes</h3>
<p>Computing clusters are configured to enable high-bandwidth and fast network connectivity between nodes. To optimize network connectivity, the nodes in the cluster are configured to trust each other and to disable security features. This improves performance but requires the cluster to be secured by closing all external network communication, making the entire cluster secure as a whole. Except for a few cluster machines that are carefully configured to accept connections from outside the cluster; conceptually, these machines are located in the “edge” of the cluster and are known as <em>edge nodes</em>.</p>
<p>Therefore, before connecting to Apache Spark, it is likely you will first have to connect to an edge node in your cluster. There are two methods to connect:</p>
<dl>
<dt>Terminal</dt>
<dd>Using a <a href="https://en.wikipedia.org/wiki/Computer_terminal">computer terminal</a> application, one can use a <a href="https://en.wikipedia.org/wiki/Secure_Shell">secure shell</a> to establish a remote connection into the cluster, once you connect into the cluster, you can launch R and then use <code>sparklyr</code>. However, a terminal can be cumbersome for some tasks, like exploratory data analysis, so it’s often only used while configuring the cluster or troubleshooting issues.
</dd>
<dt>Web Browser</dt>
<dd>While using <code>sparklyr</code> from a terminal is possible, it is usually more productive to install a <em>web server</em> in an edge node that provides access to run R with <code>sparklyr</code> from a web browser. Most likely, you will want to consider using <a href="RStudio%20Server">RStudio</a> or Jupyter rather than connecting from the terminal.
</dd>
</dl>
<p>Figure <a href="connections.html#fig:connections-spark-edge">7.2</a> explains these concepts visually. The left block is usually your web browser, the right block is the edge node, client and edge nodes communicate over HTTP when using a web browser or SSH when using the terminal.</p>
<div class="figure" style="text-align: center"><span id="fig:connections-spark-edge"></span>
<img src="the-r-in-spark_files/figure-html/connections-spark-edge-1.png" alt="Connecting to Sparks Edge Node" width="auto" height="240pt" />
<p class="caption">
FIGURE 7.2: Connecting to Sparks Edge Node
</p>
</div>
</div>
<div id="connections-spark-home" class="section level3">
<h3><span class="header-section-number">7.1.2</span> Spark Home</h3>
<p>After you connect to an edge node, the next step is to find out where Spark is installed, this location is known as the <code>SPARK_HOME</code>. In most cases, your cluster administrator will have already set the <code>SPARK_HOME</code> environment variable to the correct installation path. If not, you will need to find out the correct <code>SPARK_HOME</code> path. The <code>SPARK_HOME</code> path must be specified as an environment variable or explicitly when running <code>spark_connect()</code> using the <code>spark_home</code> parameter.</p>
<p>If your cluster provider or cluster administrator already provided <code>SPARK_HOME</code> for you, the following code should return a path instead of an empty string.</p>
<div class="sourceCode" id="cb215"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb215-1" title="1"><span class="kw">Sys.getenv</span>(<span class="st">&quot;SPARK_HOME&quot;</span>)</a></code></pre></div>
<p>If the code above returns an empty string, this would mean the <code>SPARK_HOME</code> environment variable is not set in your cluster, so you will have to specify <code>SPARK_HOME</code> while using <code>spark_connect()</code> as follows:</p>
<div class="sourceCode" id="cb216"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb216-1" title="1">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;&lt;master&gt;&quot;</span>, <span class="dt">spark_home =</span> <span class="st">&quot;local/path/to/spark&quot;</span>)</a></code></pre></div>
<p>Where <code>&lt;master&gt;</code> is set to the correct cluster manager master for <a href="connections.html#connections-standalone">Spark Standalone</a>, <a href="connections.html#connections-yarn">YARN</a>, <a href="connections.html#connections-mesos">Mesos</a>, <a href="connections.html#connections-kubernetes">Kubernetes</a> or <a href="connections.html#connections-livy">Livy</a>.</p>
</div>
</div>
<div id="connections-local" class="section level2">
<h2><span class="header-section-number">7.2</span> Local</h2>
<p>When connecting to Spark in local mode, Spark starts a single process which runs most of the cluster components like the Spark context and a single executor. This is ideal to learn Spark, work offline, troubleshoot issues or to test code before you run it over a large compute cluster. A local connection to Spark is represented in Figure <a href="connections.html#fig:connections-local-diagram">7.3</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:connections-local-diagram"></span>
<img src="the-r-in-spark_files/figure-html/connections-local-diagram-1.png" alt="Local Connection Diagram" width="auto" height="260pt" />
<p class="caption">
FIGURE 7.3: Local Connection Diagram
</p>
</div>
<p>Notice that in the local connections diagram, there is no cluster manager nor worker process since, in local mode, everything runs inside the driver application. It’s also worth noting that <code>sparklyr</code> starts the Spark Context through <code>spark-submit</code>, a script available in every Spark installation to enable users to submit custom application to Spark which, <code>sparklyr</code> makes use of to submit itself to Spark. For the curious reader, the Contributing chapter explains the internal processes that takes place in <code>sparklyr</code> to submit this application and connect properly from R.</p>
<p>To perform this local connection, we can connect with the following familiar code used in previous chapters:</p>
<div class="sourceCode" id="cb217"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb217-1" title="1"><span class="co"># Connect to local Spark instance</span></a>
<a class="sourceLine" id="cb217-2" title="2">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>)</a></code></pre></div>
</div>
<div id="connections-standalone" class="section level2">
<h2><span class="header-section-number">7.3</span> Standalone</h2>
<p>Connecting to a Spark Standalone cluster requires the location of the cluster manager’s master instance, which can be found in the cluster manager web interface as described in the <a href="clusters.html#clusters-standalone">Standalone Clusters</a> section. You can find this location by looking for a URL starting with <code>spark://</code>.</p>
<p>A connection in standalone mode starts from <code>sparklyr</code>, which launches <code>spark-submit</code>, which then submits the <code>sparklyr</code> application, and creates the Spark Context, which requests executors from the Spark Standalone instance running under the given <code>master</code> address.</p>
<p>Visually, this is described in Figure <a href="connections.html#fig:connections-standalone-diagram">7.4</a>, which is quite similar to the overall connection architecture from Figure <a href="connections.html#fig:connections-architecture">7.1</a> but, with additional details that are particular to standalone clusters and <code>sparklyr</code>.</p>
<div class="figure" style="text-align: center"><span id="fig:connections-standalone-diagram"></span>
<img src="the-r-in-spark_files/figure-html/connections-standalone-diagram-1.png" alt="Spark Standalone Connection Diagram" width="auto" height="280pt" />
<p class="caption">
FIGURE 7.4: Spark Standalone Connection Diagram
</p>
</div>
<p>In order to connect, use <code>master = "spark://hostname:port"</code> in <code>spark_connect()</code> as follows:</p>
<div class="sourceCode" id="cb218"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb218-1" title="1">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;spark://hostname:port&quot;</span>)</a></code></pre></div>
</div>
<div id="connections-yarn" class="section level2">
<h2><span class="header-section-number">7.4</span> Yarn</h2>
<p>Hadoop YARN is the cluster manager from the Hadoop project, it’s the most common cluster manager which you are likely to find in clusters that started out as Hadoop clusters; with Cloudera, Hortonworks and MapR distributions as when using Amazon EMR. YARN supports two connection modes: YARN Client and YARN Cluster. However, YARN Client mode is much more common that YARN Cluster since it’s more efficient and easier to set up.</p>
<div id="connections-yarn-client" class="section level3">
<h3><span class="header-section-number">7.4.1</span> Yarn Client</h3>
<p>When connecting in YARN Client mode, the driver instance runs R, sparklyr and the Spark Context which requests worker nodes from YARN to run Spark executors as shown in Figure <a href="connections.html#fig:connections-yarn-client-diagram">7.5</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:connections-yarn-client-diagram"></span>
<img src="the-r-in-spark_files/figure-html/connections-yarn-client-diagram-1.png" alt="YARN Client Connection Diagram" width="auto" height="280pt" />
<p class="caption">
FIGURE 7.5: YARN Client Connection Diagram
</p>
</div>
<p>To connect, one can simply run with <code>master = "yarn"</code> as follows:</p>
<div class="sourceCode" id="cb219"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb219-1" title="1">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;yarn&quot;</span>)</a></code></pre></div>
<p>Behind the scenes, when running YARN in client mode, the cluster manager will do what you would expect a cluster manager would do; it will allocate resources from the cluster and assign them to your Spark application, which the Spark Context will manage for you. The important piece to notice in Figure <a href="#fig:connections-yarn"><strong>??</strong></a> is that, the Spark Context resides in the same machine where you run R code, this is different when running YARN in cluster mode.</p>
</div>
<div id="connections-yarn-cluster" class="section level3">
<h3><span class="header-section-number">7.4.2</span> Yarn Cluster</h3>
<p>The main difference between YARN in cluster mode and running YARN in client mode is that, in cluster mode, the driver node is not required to be the node where R and sparklyr were launched; instead, the driver node remains the designated driver node which is usually a different node than the edge node where R is running. It can be helpful to consider using cluster mode when the edge node has too many concurrent users, when is lacking computing resources, or where tools (like RStudio or Jupyter) need to be managed independently of other cluster resources.</p>
<p>Figure <a href="connections.html#fig:connections-yarn-cluster-diagram">7.6</a> shows how the different components become decoupled when running in cluster mode. Notice there is still a line connecting the client with the cluster manager since, first of all, resources still need to be allocated from the cluster manager; however, once allocated, the client communicates directly with the driver node which will then communicate with the worker nodes. From this diagram, you might think that cluster mode looks much more complicated than the client mode diagram – this would be a correct assessment; therefore, it’s best to avoid cluster mode when possible due to additional configuration overhead that is best to avoid, if possible.</p>
<div class="figure" style="text-align: center"><span id="fig:connections-yarn-cluster-diagram"></span>
<img src="the-r-in-spark_files/figure-html/connections-yarn-cluster-diagram-1.png" alt="YARN Cluster Connection Diagram" width="auto" height="280pt" />
<p class="caption">
FIGURE 7.6: YARN Cluster Connection Diagram
</p>
</div>
<p>To connect in YARN Cluster mode, we can simply run:</p>
<div class="sourceCode" id="cb220"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb220-1" title="1">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;yarn-cluster&quot;</span>)</a></code></pre></div>
<p>Cluster mode assumes that the node running <code>spark_connect()</code> is properly configured, meaning that, <code>yarn-site.xml</code> exists and the <code>YARN_CONF_DIR</code> environment variable is properly set. When using Hadoop as a file system, you will also need the <code>HADOOP_CONF_DIR</code> environment variable properly configured. In addition, you would need to have proper network connectivity between the client and the driver node, not just with sufficient bandwidth but also making sure both machines are reachable and no intermediate. This configuration is usually provided by your system administrator and is not something that you would have to manually configure.</p>
</div>
</div>
<div id="connections-livy" class="section level2">
<h2><span class="header-section-number">7.5</span> Livy</h2>
<p>As opposed to other connection methods which require using an edge node in the cluster, <a href="clusters.html#clusters-livy">Livy</a> provides a <em>Web API</em> that makes the Spark cluster accessible from outside the cluster and does not require a Spark installation in the client. Once connected through the Web API, the <em>Livy Service</em> starts the Spark context by requesting resources from the cluster manager and distributing work as usual. Figure <a href="connections.html#fig:connections-livy-diagram">7.7</a> illustrates a Livy connection, notice that the client connects remotely to the driver through a web API.</p>
<div class="figure" style="text-align: center"><span id="fig:connections-livy-diagram"></span>
<img src="the-r-in-spark_files/figure-html/connections-livy-diagram-1.png" alt="Livy Connection Diagram" width="auto" height="280pt" />
<p class="caption">
FIGURE 7.7: Livy Connection Diagram
</p>
</div>
<p>Connecting through Livy requires the URL to the Livy service which should be similar to <code>https://hostname:port/livy</code>. Since remote connections are allowed, connections usually requires, at the very least, basic authentication:</p>
<div class="sourceCode" id="cb221"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb221-1" title="1">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(</a>
<a class="sourceLine" id="cb221-2" title="2">  <span class="dt">master =</span> <span class="st">&quot;https://hostname:port/livy&quot;</span>,</a>
<a class="sourceLine" id="cb221-3" title="3">  <span class="dt">method =</span> <span class="st">&quot;livy&quot;</span>, <span class="dt">config =</span> <span class="kw">livy_config</span>(</a>
<a class="sourceLine" id="cb221-4" title="4">    <span class="dt">spark_version =</span> <span class="st">&quot;2.4.0&quot;</span>,</a>
<a class="sourceLine" id="cb221-5" title="5">    <span class="dt">username =</span> <span class="st">&quot;&lt;username&gt;&quot;</span>,</a>
<a class="sourceLine" id="cb221-6" title="6">    <span class="dt">password =</span> <span class="st">&quot;&lt;password&gt;&quot;</span></a>
<a class="sourceLine" id="cb221-7" title="7">  ))</a></code></pre></div>
<p>To try out Livy in your local machine, you can install and run a Livy service as described under the <a href="clusters.html#clusters-livy">Livy Clusters</a> section and then, connect as follows:</p>
<div class="sourceCode" id="cb222"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb222-1" title="1">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(</a>
<a class="sourceLine" id="cb222-2" title="2">  <span class="dt">master =</span> <span class="st">&quot;http://localhost:8998&quot;</span>,</a>
<a class="sourceLine" id="cb222-3" title="3">  <span class="dt">method =</span> <span class="st">&quot;livy&quot;</span>,</a>
<a class="sourceLine" id="cb222-4" title="4">  <span class="dt">version =</span> <span class="st">&quot;2.4.0&quot;</span>)</a></code></pre></div>
<p>Once connected through Livy, you can make use of any <code>sparklyr</code> feature; however, Livy is not suitable for exploratory data analysis, since executing commands has a significant performance cost. That said, while running long running computations, this overhead could be considered irrelevant. In general, it is preferred to avoid using Livy and work directly within an edge node in the cluster; when this is not feasible, using Livy could be a reasonable approach.</p>
<p><strong>Note:</strong> Specifying the Spark version through the <code>spark_version</code> parameter is optional; however, when the version is specified, performance is significantly improved by deploying precompiled Java binaries compatible with the given version. Therefore, it is a best practice to specify the Spark version when connecting to Spark using Livy.</p>
</div>
<div id="connections-mesos" class="section level2">
<h2><span class="header-section-number">7.6</span> Mesos</h2>
<p>Similar to YARN, Mesos supports client mode and a cluster mode; however – <code>sparklyr</code> currently only supports client mode under Mesos. Therefore, the diagram from Figure <a href="connections.html#fig:connections-mesos-diagram">7.8</a>, is equivalent to YARN Client’s diagram with only the cluster manager changed from YARN to Mesos.</p>
<div class="figure" style="text-align: center"><span id="fig:connections-mesos-diagram"></span>
<img src="the-r-in-spark_files/figure-html/connections-mesos-diagram-1.png" alt="Mesos Connection Diagram" width="auto" height="280pt" />
<p class="caption">
FIGURE 7.8: Mesos Connection Diagram
</p>
</div>
<p>Connecting requires the address to the Mesos master node, usually in the form of <code>mesos://host:port</code> or <code>mesos://zk://host1:2181,host2:2181,host3:2181/mesos</code> for Mesos using ZooKeeper.</p>
<div class="sourceCode" id="cb223"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb223-1" title="1">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;mesos://host:port&quot;</span>)</a></code></pre></div>
<p>The <code>MESOS_NATIVE_JAVA_LIBRARY</code> environment variable needs to be set by your system administrator, or manually set when running mesos on your local machine. For instance, in OS X, you can install and initialize Mesos from a terminal, followed by manually setting the mesos library and connecting with <code>spark_connect()</code>:</p>
<div class="sourceCode" id="cb224"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb224-1" title="1"><span class="ex">brew</span> install mesos</a>
<a class="sourceLine" id="cb224-2" title="2"><span class="ex">/usr/local/Cellar/mesos/1.6.1/sbin/mesos-master</span> --registry=in_memory</a>
<a class="sourceLine" id="cb224-3" title="3">  <span class="ex">--ip</span>=127.0.0.1 MESOS_WORK_DIR=. /usr/local/Cellar/mesos/1.6.1/sbin/mesos-slave</a>
<a class="sourceLine" id="cb224-4" title="4">  <span class="ex">--master</span>=127.0.0.1:5050</a></code></pre></div>
<div class="sourceCode" id="cb225"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb225-1" title="1"><span class="kw">Sys.setenv</span>(<span class="dt">MESOS_NATIVE_JAVA_LIBRARY =</span> </a>
<a class="sourceLine" id="cb225-2" title="2">             <span class="st">&quot;/usr/local/Cellar/mesos/1.6.1/lib/libmesos.dylib&quot;</span>)</a>
<a class="sourceLine" id="cb225-3" title="3"></a>
<a class="sourceLine" id="cb225-4" title="4">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;mesos://localhost:5050&quot;</span>,</a>
<a class="sourceLine" id="cb225-5" title="5">                    <span class="dt">spark_home =</span> <span class="kw">spark_home_dir</span>())</a></code></pre></div>
</div>
<div id="connections-kubernetes" class="section level2">
<h2><span class="header-section-number">7.7</span> Kubernetes</h2>
<p>Kubernetes cluster do not support client modes like Mesos or YARN; instead, the connection model is similar to YARN Cluster, where the driver node is assigned by Kubernetes. This is illustrated in Figure <a href="connections.html#fig:connections-kubernetes-diagram">7.9</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:connections-kubernetes-diagram"></span>
<img src="the-r-in-spark_files/figure-html/connections-kubernetes-diagram-1.png" alt="Kubernetes Connection Diagram" width="auto" height="280pt" />
<p class="caption">
FIGURE 7.9: Kubernetes Connection Diagram
</p>
</div>
<p>To use Kubernetes, you will need to prepare a virtual machine with Spark installed and properly configured; however, it is beyond the scope of this book to present how to create one. Once created, connecting to Kubernetes works as follows:</p>
<div class="sourceCode" id="cb226"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb226-1" title="1"><span class="kw">library</span>(sparklyr)</a>
<a class="sourceLine" id="cb226-2" title="2">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">config =</span> <span class="kw">spark_config_kubernetes</span>(</a>
<a class="sourceLine" id="cb226-3" title="3">  <span class="st">&quot;k8s://https://&lt;apiserver-host&gt;:&lt;apiserver-port&gt;&quot;</span>,</a>
<a class="sourceLine" id="cb226-4" title="4">  <span class="dt">account =</span> <span class="st">&quot;default&quot;</span>,</a>
<a class="sourceLine" id="cb226-5" title="5">  <span class="dt">image =</span> <span class="st">&quot;docker.io/owner/repo:version&quot;</span>,</a>
<a class="sourceLine" id="cb226-6" title="6">  <span class="dt">version =</span> <span class="st">&quot;2.3.1&quot;</span>))</a></code></pre></div>
<p>If your computer is already configured to use a Kubernetes cluster, you can use the following command to find the <code>apiserver-host</code> and <code>apiserver-port</code>:</p>
<div class="sourceCode" id="cb227"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb227-1" title="1"><span class="kw">system2</span>(<span class="st">&quot;kubectl&quot;</span>, <span class="st">&quot;cluster-info&quot;</span>)</a></code></pre></div>
</div>
<div id="cloud-1" class="section level2">
<h2><span class="header-section-number">7.8</span> Cloud</h2>
<p>When working with cloud providers, there are a few connection differences. For instance, connecting from <em>Databricks</em> requires the following connection method:</p>
<div class="sourceCode" id="cb228"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb228-1" title="1">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">method =</span> <span class="st">&quot;databricks&quot;</span>)</a></code></pre></div>
<p>Since Amazon <em>EMR</em> makes use of YARN, you can connect using <code>master = "yarn"</code>:</p>
<div class="sourceCode" id="cb229"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb229-1" title="1">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;yarn&quot;</span>)</a></code></pre></div>
<p>Connections to Spark when using IBM’s <em>Watson Studio</em> requires you to retrieve a configuration object through a <code>load_spark_kernels()</code> function IBM provides:</p>
<div class="sourceCode" id="cb230"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb230-1" title="1">kernels &lt;-<span class="st"> </span><span class="kw">load_spark_kernels</span>()</a>
<a class="sourceLine" id="cb230-2" title="2">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">config =</span> kernels[<span class="dv">2</span>])</a></code></pre></div>
<p>Under Microsoft Azure <em>HDInsights</em> and when using ML Services (R Server), a Spark connection gets initialized through:</p>
<div class="sourceCode" id="cb231"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb231-1" title="1"><span class="kw">library</span>(RevoScaleR)</a>
<a class="sourceLine" id="cb231-2" title="2">cc &lt;-<span class="st"> </span><span class="kw">rxSparkConnect</span>(<span class="dt">reset =</span> <span class="ot">TRUE</span>, <span class="dt">interop =</span> <span class="st">&quot;sparklyr&quot;</span>)</a>
<a class="sourceLine" id="cb231-3" title="3">sc &lt;-<span class="st"> </span><span class="kw">rxGetSparklyrConnection</span>(cc)</a></code></pre></div>
<p>Connecting from <em>Qubole</em> requires using the <code>qubole</code> connection method:</p>
<div class="sourceCode" id="cb232"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb232-1" title="1">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">method =</span> <span class="st">&quot;qubole&quot;</span>)</a></code></pre></div>
<p>Please reference your cloud provider documentation and their support channels if assistance is needed.</p>
</div>
<div id="batches" class="section level2">
<h2><span class="header-section-number">7.9</span> Batches</h2>
<p>Most of the time, <code>sparklyr</code> used interactively; as in, you explicitly connect with <code>spark_connect()</code> and then execute commands to analyze and model large-scale data. However, you can also automate processes by scheduling Spark jobs that use <code>sparklyr</code>. Spark does not provide tools to schedule data processing tasks; so instead, you would use other workflow management tools. This can be useful useful to transform data, prepare a model and score data overnight or to make use of Spark by other systems.</p>
<p>As an example, you can create a file named <code>batch.R</code> with contents:</p>
<div class="sourceCode" id="cb233"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb233-1" title="1"><span class="kw">library</span>(sparklyr)</a>
<a class="sourceLine" id="cb233-2" title="2"></a>
<a class="sourceLine" id="cb233-3" title="3">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>)</a>
<a class="sourceLine" id="cb233-4" title="4"></a>
<a class="sourceLine" id="cb233-5" title="5"><span class="kw">sdf_len</span>(sc, <span class="dv">10</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">spark_write_csv</span>(<span class="st">&quot;batch.csv&quot;</span>)</a>
<a class="sourceLine" id="cb233-6" title="6"></a>
<a class="sourceLine" id="cb233-7" title="7"><span class="kw">spark_disconnect</span>(sc)</a></code></pre></div>
<p>You can then submit this application to Spark in batch mode using <code>spark_submit()</code>, the <code>master</code> parameter should be set to the appropriately.</p>
<div class="sourceCode" id="cb234"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb234-1" title="1"><span class="kw">spark_submit</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>, <span class="st">&quot;batch.R&quot;</span>)</a></code></pre></div>
<p>You can also invoke <code>spark-submit</code> from the shell directly through:</p>
<div class="sourceCode" id="cb235"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb235-1" title="1"><span class="ex">/spark-home-path/spark-submit</span></a>
<a class="sourceLine" id="cb235-2" title="2">  <span class="ex">--class</span> sparklyr.Shell <span class="st">&#39;/spark-jars-path/sparklyr-2.3-2.11.jar&#39;</span></a>
<a class="sourceLine" id="cb235-3" title="3">  <span class="ex">8880</span> 12345 --batch /path/to/batch.R</a></code></pre></div>
<p>The last parameters represent the port number <code>8880</code> and the session number, <code>12345</code>, which can be set to any unique numeric identifier. You can use the following R code to get the correct paths:</p>
<div class="sourceCode" id="cb236"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb236-1" title="1"><span class="co"># Retrieve spark-home-path</span></a>
<a class="sourceLine" id="cb236-2" title="2"><span class="kw">spark_home_dir</span>()</a>
<a class="sourceLine" id="cb236-3" title="3"></a>
<a class="sourceLine" id="cb236-4" title="4"><span class="co"># Retrieve spark-jars-path</span></a>
<a class="sourceLine" id="cb236-5" title="5"><span class="kw">system.file</span>(<span class="st">&quot;java&quot;</span>, <span class="dt">package =</span> <span class="st">&quot;sparklyr&quot;</span>)</a></code></pre></div>
<p>You can customize your script by passing additional command line arguments to <code>spark-submit</code> and then reading them back in R using <code>commandArgs()</code></p>
</div>
<div id="tools-1" class="section level2">
<h2><span class="header-section-number">7.10</span> Tools</h2>
<p>When connecting to a Spark Cluster using tools like Jupyter and RStudio, you can run the same connection parameters presented in this chapter. However, since many cloud providers make use of a web proxy to secure Spark’s web interface, in order to use <code>spark_web()</code> or the RStudio connections pane extension, you will need to properly configure the <code>sparklyr.web.spark</code> setting which you would then pass to <code>spark_config()</code> through the <code>config</code> parameter.</p>
<p>For instance, when using Amazon <em>EMR</em>, you can configure <code>sparklyr.web.spark</code> and <code>sparklyr.web.yarn</code> by dynamically retrieving the YARN application and building the EMR proxy URL:</p>
<div class="sourceCode" id="cb237"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb237-1" title="1">domain &lt;-<span class="st"> &quot;http://ec2-12-345-678-9.us-west-2.compute.amazonaws.com&quot;</span></a>
<a class="sourceLine" id="cb237-2" title="2">config &lt;-<span class="st"> </span><span class="kw">spark_config</span>()</a>
<a class="sourceLine" id="cb237-3" title="3">config<span class="op">$</span>sparklyr.web.spark &lt;-<span class="st"> </span><span class="er">~</span><span class="kw">paste0</span>(</a>
<a class="sourceLine" id="cb237-4" title="4">  domain, <span class="st">&quot;:20888/proxy/&quot;</span>, <span class="kw">invoke</span>(<span class="kw">spark_context</span>(sc), <span class="st">&quot;applicationId&quot;</span>))</a>
<a class="sourceLine" id="cb237-5" title="5">config<span class="op">$</span>sparklyr.web.yarn &lt;-<span class="st"> </span><span class="kw">paste0</span>(domain, <span class="st">&quot;:8088&quot;</span>)</a>
<a class="sourceLine" id="cb237-6" title="6"></a>
<a class="sourceLine" id="cb237-7" title="7">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;yarn&quot;</span>, <span class="dt">config =</span> config)</a></code></pre></div>
</div>
<div id="multiple" class="section level2">
<h2><span class="header-section-number">7.11</span> Multiple</h2>
<p>It is common to connect once, and only once, to Spark. However, you can also open multiple connections to Spark by connecting to different clusters or by specifying the <code>app_name</code> parameter. This can be helpful to compare Spark versions or validate your analysis before submitting to the cluster. The following example opens connections to Spark 1.6.3, 2.3.0 and Spark Standalone:</p>
<div class="sourceCode" id="cb238"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb238-1" title="1"><span class="co"># Connect to local Spark 1.6.3</span></a>
<a class="sourceLine" id="cb238-2" title="2">sc_<span class="dv">16</span> &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>, <span class="dt">version =</span> <span class="st">&quot;1.6&quot;</span>)</a>
<a class="sourceLine" id="cb238-3" title="3"></a>
<a class="sourceLine" id="cb238-4" title="4"><span class="co"># Connect to local Spark 2.3.0</span></a>
<a class="sourceLine" id="cb238-5" title="5">sc_<span class="dv">23</span> &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>, <span class="dt">version =</span> <span class="st">&quot;2.3&quot;</span>, <span class="dt">appName =</span> <span class="st">&quot;Spark23&quot;</span>)</a>
<a class="sourceLine" id="cb238-6" title="6"></a>
<a class="sourceLine" id="cb238-7" title="7"><span class="co"># Connect to local Spark Standalone</span></a>
<a class="sourceLine" id="cb238-8" title="8">sc_standalone &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;spark://host:port&quot;</span>)</a></code></pre></div>
<p>Finally, we can disconnect from each connection:</p>
<div class="sourceCode" id="cb239"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb239-1" title="1"><span class="kw">spark_disconnect</span>(sc_<span class="dv">1</span>_<span class="dv">6</span>_<span class="dv">3</span>)</a>
<a class="sourceLine" id="cb239-2" title="2"><span class="kw">spark_disconnect</span>(sc_<span class="dv">2</span>_<span class="dv">3</span>_<span class="dv">0</span>)</a>
<a class="sourceLine" id="cb239-3" title="3"><span class="kw">spark_disconnect</span>(sc_standalone)</a></code></pre></div>
<p>Alternatively, you can disconnect from all connections at once:</p>
<div class="sourceCode" id="cb240"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb240-1" title="1"><span class="kw">spark_disconnect_all</span>()</a></code></pre></div>
</div>
<div id="connections-troubleshooting" class="section level2">
<h2><span class="header-section-number">7.12</span> Troubleshooting</h2>
<p>Last but not least, we will introduce the following troubleshooting techniques: <em>Logging</em>, <em>Spark Submit</em> and <em>Windows</em>. When in doubt of where to start, start with the Windows section when using Windows systems, followed by Logging and closing with Spark Submit. This techniques are useful when running <code>spark_connect()</code> fails with an error message.</p>
<div id="logging" class="section level3">
<h3><span class="header-section-number">7.12.1</span> Logging</h3>
<p>The first technique to troubleshoot connections is to print Spark logs directly to the console to help you spot additional error messages:</p>
<div class="sourceCode" id="cb241"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb241-1" title="1">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>, <span class="dt">log =</span> <span class="st">&quot;console&quot;</span>)</a></code></pre></div>
<p>In addition, you can enable verbose logging by setting the <code>sparklyr.verbose</code> option when connecting:</p>
<div class="sourceCode" id="cb242"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb242-1" title="1">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>, <span class="dt">log =</span> <span class="st">&quot;console&quot;</span>,</a>
<a class="sourceLine" id="cb242-2" title="2">                    <span class="dt">config =</span> <span class="kw">list</span>(<span class="dt">sparklyr.verbose =</span> <span class="ot">TRUE</span>))</a></code></pre></div>
</div>
<div id="troubleshoot-spark-submit" class="section level3">
<h3><span class="header-section-number">7.12.2</span> Spark Submit</h3>
<p>You can diagnose if a connection issue is specific to R or Spark in general. This can be accomplished by running an example job through <code>spark-submit</code> and validating that no errors are thrown:</p>
<div class="sourceCode" id="cb243"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb243-1" title="1"><span class="co"># Find the spark directory using an environment variable</span></a>
<a class="sourceLine" id="cb243-2" title="2">spark_home &lt;-<span class="st"> </span><span class="kw">Sys.getenv</span>(<span class="st">&quot;SPARK_HOME&quot;</span>)</a>
<a class="sourceLine" id="cb243-3" title="3"></a>
<a class="sourceLine" id="cb243-4" title="4"><span class="co"># Or by getting the local spark installation</span></a>
<a class="sourceLine" id="cb243-5" title="5">spark_home &lt;-<span class="st"> </span>sparklyr<span class="op">::</span><span class="kw">spark_home_dir</span>()</a></code></pre></div>
<p>Then execute the sample compute Pi example by replacing <code>"local"</code> with the correct master parameter you are troubleshooting:</p>
<div class="sourceCode" id="cb244"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb244-1" title="1"><span class="co"># Launching a sample application to compute Pi</span></a>
<a class="sourceLine" id="cb244-2" title="2"><span class="kw">system2</span>(</a>
<a class="sourceLine" id="cb244-3" title="3">  <span class="kw">file.path</span>(spark_home, <span class="st">&quot;bin&quot;</span>, <span class="st">&quot;spark-submit&quot;</span>),</a>
<a class="sourceLine" id="cb244-4" title="4">  <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb244-5" title="5">    <span class="st">&quot;--master&quot;</span>, <span class="st">&quot;local&quot;</span>,</a>
<a class="sourceLine" id="cb244-6" title="6">    <span class="st">&quot;--class&quot;</span>, <span class="st">&quot;org.apache.spark.examples.SparkPi&quot;</span>,</a>
<a class="sourceLine" id="cb244-7" title="7">    <span class="kw">dir</span>(<span class="kw">file.path</span>(spark_home, <span class="st">&quot;examples&quot;</span>, <span class="st">&quot;jars&quot;</span>), </a>
<a class="sourceLine" id="cb244-8" title="8">        <span class="dt">pattern =</span> <span class="st">&quot;spark-examples&quot;</span>, <span class="dt">full.names =</span> <span class="ot">TRUE</span>),</a>
<a class="sourceLine" id="cb244-9" title="9">    <span class="dv">100</span>),</a>
<a class="sourceLine" id="cb244-10" title="10">  <span class="dt">stderr =</span> <span class="ot">FALSE</span></a>
<a class="sourceLine" id="cb244-11" title="11">)</a></code></pre></div>
<pre><code>Pi is roughly 3.1415503141550314</code></pre>
<p>If the above message is not displayed, you will have to investigate why your Spark cluster is not properly configured, which is beyond the scope of this book. As a start, rerun the Pi example but remove <code>stderr = FALSE</code>, this will print errors to the console which you can then use to investigate what the problem might be. When using a cloud provider or a Spark distribution, you can contact their support team to help you troubleshoot this further; otherwise, StackOverflow is a good place to start.</p>
<p>If you do see the message above, this means your Spark cluster is properly configured but somehow, R is not being able to use Spark, so you will have to troubleshoot in-detail as we will explain next.</p>
<div id="detailed" class="section level4">
<h4><span class="header-section-number">7.12.2.1</span> Detailed</h4>
<p>To troubleshoot the connection process in detail, you can manually replicate the two-step connection process, which is often very helpful to diagnose connection issues. Connecting to Spark is performed in two steps; first, <code>spark-submit</code> is triggered from R which submits the application to Spark, second, R connects to the running Spark application.</p>
<p>First, <a href="troubleshoot-spark-submit">identify the Spark installation directory</a> and the path to the correct <code>sparklyr*.jar</code> by running:</p>
<div class="sourceCode" id="cb246"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb246-1" title="1"><span class="kw">dir</span>(<span class="kw">system.file</span>(<span class="st">&quot;java&quot;</span>, <span class="dt">package =</span> <span class="st">&quot;sparklyr&quot;</span>),</a>
<a class="sourceLine" id="cb246-2" title="2">    <span class="dt">pattern =</span> <span class="st">&quot;sparklyr&quot;</span>, <span class="dt">full.names =</span> T)</a></code></pre></div>
<p>Make sure you identify the correct version that matches your Spark cluster, for instance <code>sparklyr-2.1-2.11.jar</code> for Spark 2.1.</p>
<p>Then, from the terminal, run:</p>
<div class="sourceCode" id="cb247"><pre class="sourceCode bash"><code class="sourceCode bash"><a class="sourceLine" id="cb247-1" title="1"><span class="va">$SPARK_HOME</span><span class="ex">/bin/spark-submit</span> --class sparklyr.Shell <span class="va">$PATH_TO_SPARKLYR_JAR</span> 8880 42</a></code></pre></div>
<pre><code>18/06/11 12:13:53 INFO sparklyr: Session (42) found port 8880 is available
18/06/11 12:13:53 INFO sparklyr: Gateway (42) is waiting for sparklyr client
                                 to connect to port 8880</code></pre>
<p>The parameter <code>8880</code> represents the default port to use in <code>sparklyr</code> while <code>42 the session number, this is a cryptographically secure number generated by</code>sparklyr<code>, but for troubleshooting purposes can be as simple as</code>42`.</p>
<p>If this first connection step fails, it means that the cluster can’t accept the application. This usually means that there are not enough resources, there are permission restrictions, etc.</p>
<p>The second step is to connect from R as follows – notice that there is a 60 seconds timeout, so you’ll have to run the R command after running the terminal command. If needed, this timeout can be configured as described in the Tuning chapter.</p>
<div class="sourceCode" id="cb249"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb249-1" title="1"><span class="kw">library</span>(sparklyr)</a>
<a class="sourceLine" id="cb249-2" title="2">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;sparklyr://localhost:8880/42&quot;</span>, <span class="dt">version =</span> <span class="st">&quot;2.3&quot;</span>)</a></code></pre></div>
<p>If this second connection step fails, it usually means that there is a connectivity problem between R and the driver node. You can try using a different connection port, for instance.</p>
</div>
</div>
<div id="windows" class="section level3">
<h3><span class="header-section-number">7.12.3</span> Windows</h3>
<p>Connecting from Windows is, in most cases, as straightforward as connecting from Linux and OS X. However, there are a few common connection issues you should be aware of:</p>
<ul>
<li>Firewalls and antivirus software might block ports for your connection. The default port used by <code>sparklyr</code> is <code>8880</code>; double check this port is not being blocked.</li>
<li>Long path names can cause issues, especially in older Windows systems like Windows 7. When using these systems, try connecting with Spark installed with all folders using at most eight characters and no spaces in their names.</li>
</ul>
</div>
</div>
<div id="recap-4" class="section level2">
<h2><span class="header-section-number">7.13</span> Recap</h2>
<p>This chapter presented an overview of Spark’s architecture, connection concepts and examples to connect in local mode, standalone, YARN, Mesos, Kubernetes and Livy. It also presented edge nodes and their role while connecting to Spark clusters. This should have provided you with enough information to successfully connect to any Apache Spark cluster.</p>
<p>To troubleshoot connection problems beyond the techniques described in this chapter, it is recommended to search for the connection problem in StackOverflow, the <a href="https://github.com/rstudio/sparklyr/issues">sparklyr GitHub issues</a> and, if needed, open a <a href="https://github.com/rstudio/sparklyr/issues/new">new GitHub issue in sparklyr</a> to assist further.</p>
<p>In the next chapter, <a href="data.html#data">Data</a>, we will cover how to user Spark to read and write from a variety of data sources and formats which allows you to be more agile when adding new data sources for data analysis – what used to take days, weeks or even months, can now be completed in hours by embracing data lakes.</p>

</div>
</div>
<h3> References</h3>
<div id="refs" class="references">
<div id="ref-connections-spark-context-scala">
<p>“Azure Wikipedia.” 2019. <a href="https://spark.apache.org/docs/2.1.0/api/scala/index.html">https://spark.apache.org/docs/2.1.0/api/scala/index.html</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="clusters.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="data.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
