<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 3 Analysis | Mastering Apache Spark with R</title>
  <meta name="description" content="The complete guide to large-scale analysis and modeling.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 3 Analysis | Mastering Apache Spark with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The complete guide to large-scale analysis and modeling." />
  <meta name="github-repo" content="javierluraschi/the-r-in-spark" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Analysis | Mastering Apache Spark with R" />
  
  <meta name="twitter:description" content="The complete guide to large-scale analysis and modeling." />
  

<meta name="author" content="Javier Luraschi, Kevin Kuo, Edgar Ruiz">


<meta name="date" content="2019-06-10">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="starting.html">
<link rel="next" href="modeling.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.3/htmlwidgets.js"></script>
<script src="libs/dagre-0.0.1/dagre.min.js"></script>
<script src="libs/lodash-3.7.0/lodash.js"></script>
<script src="libs/nomnoml-0.2.0/nomnoml.js"></script>
<script src="libs/nomnoml-binding-0.1.0/nomnoml.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-119986300-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-119986300-1');
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./">Learning Apache Spark with R</a></strong></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#intro-background"><i class="fa fa-check"></i><b>1.1</b> Information</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#intro-hadoop"><i class="fa fa-check"></i><b>1.2</b> Hadoop</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#intro-spark"><i class="fa fa-check"></i><b>1.3</b> Spark</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#intro-r"><i class="fa fa-check"></i><b>1.4</b> R</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#intro-sparklyr"><i class="fa fa-check"></i><b>1.5</b> sparklyr</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#intro-recap"><i class="fa fa-check"></i><b>1.6</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="starting.html"><a href="starting.html"><i class="fa fa-check"></i><b>2</b> Getting Started</a><ul>
<li class="chapter" data-level="2.1" data-path="starting.html"><a href="starting.html#starting-prerequisites"><i class="fa fa-check"></i><b>2.1</b> Prerequisites</a></li>
<li class="chapter" data-level="2.2" data-path="starting.html"><a href="starting.html#starting-install-sparklyr"><i class="fa fa-check"></i><b>2.2</b> Installing sparklyr</a></li>
<li class="chapter" data-level="2.3" data-path="starting.html"><a href="starting.html#starting-installing-spark"><i class="fa fa-check"></i><b>2.3</b> Installing Spark</a></li>
<li class="chapter" data-level="2.4" data-path="starting.html"><a href="starting.html#starting-connect-to-spark"><i class="fa fa-check"></i><b>2.4</b> Connecting to Spark</a></li>
<li class="chapter" data-level="2.5" data-path="starting.html"><a href="starting.html#starting-sparklyr-hello-world"><i class="fa fa-check"></i><b>2.5</b> Using Spark</a><ul>
<li class="chapter" data-level="2.5.1" data-path="starting.html"><a href="starting.html#starting-spark-web-interface"><i class="fa fa-check"></i><b>2.5.1</b> Web Interface</a></li>
<li class="chapter" data-level="2.5.2" data-path="starting.html"><a href="starting.html#starting-analysis"><i class="fa fa-check"></i><b>2.5.2</b> Analysis</a></li>
<li class="chapter" data-level="2.5.3" data-path="starting.html"><a href="starting.html#starting-modeling"><i class="fa fa-check"></i><b>2.5.3</b> Modeling</a></li>
<li class="chapter" data-level="2.5.4" data-path="starting.html"><a href="starting.html#starting-data"><i class="fa fa-check"></i><b>2.5.4</b> Data</a></li>
<li class="chapter" data-level="2.5.5" data-path="starting.html"><a href="starting.html#starting-extensions"><i class="fa fa-check"></i><b>2.5.5</b> Extensions</a></li>
<li class="chapter" data-level="2.5.6" data-path="starting.html"><a href="starting.html#starting-distributed-r"><i class="fa fa-check"></i><b>2.5.6</b> Distributed R</a></li>
<li class="chapter" data-level="2.5.7" data-path="starting.html"><a href="starting.html#starting-streaming"><i class="fa fa-check"></i><b>2.5.7</b> Streaming</a></li>
<li class="chapter" data-level="2.5.8" data-path="starting.html"><a href="starting.html#starting-logs"><i class="fa fa-check"></i><b>2.5.8</b> Logs</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="starting.html"><a href="starting.html#starting-disconnecting"><i class="fa fa-check"></i><b>2.6</b> Disconnecting</a></li>
<li class="chapter" data-level="2.7" data-path="starting.html"><a href="starting.html#starting-using-spark-from-rstudio"><i class="fa fa-check"></i><b>2.7</b> Using RStudio</a></li>
<li class="chapter" data-level="2.8" data-path="starting.html"><a href="starting.html#starting-resources"><i class="fa fa-check"></i><b>2.8</b> Resources</a></li>
<li class="chapter" data-level="2.9" data-path="starting.html"><a href="starting.html#starting-recap"><i class="fa fa-check"></i><b>2.9</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="analysis.html"><a href="analysis.html"><i class="fa fa-check"></i><b>3</b> Analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="analysis.html"><a href="analysis.html#r-as-an-interface-to-spark"><i class="fa fa-check"></i><b>3.1</b> R as an Interface to Spark</a><ul>
<li class="chapter" data-level="3.1.1" data-path="analysis.html"><a href="analysis.html#exercise"><i class="fa fa-check"></i><b>3.1.1</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="analysis.html"><a href="analysis.html#import-access"><i class="fa fa-check"></i><b>3.2</b> Import / Access</a></li>
<li class="chapter" data-level="3.3" data-path="analysis.html"><a href="analysis.html#wrangle"><i class="fa fa-check"></i><b>3.3</b> Wrangle</a><ul>
<li class="chapter" data-level="3.3.1" data-path="analysis.html"><a href="analysis.html#correlations"><i class="fa fa-check"></i><b>3.3.1</b> Correlations</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="analysis.html"><a href="analysis.html#visualize"><i class="fa fa-check"></i><b>3.4</b> Visualize</a><ul>
<li class="chapter" data-level="3.4.1" data-path="analysis.html"><a href="analysis.html#recommended-approach"><i class="fa fa-check"></i><b>3.4.1</b> Recommended approach</a></li>
<li class="chapter" data-level="3.4.2" data-path="analysis.html"><a href="analysis.html#simple-plots"><i class="fa fa-check"></i><b>3.4.2</b> Simple Plots</a></li>
<li class="chapter" data-level="3.4.3" data-path="analysis.html"><a href="analysis.html#histograms"><i class="fa fa-check"></i><b>3.4.3</b> Histograms</a></li>
<li class="chapter" data-level="3.4.4" data-path="analysis.html"><a href="analysis.html#scatter-vs-raster-plots"><i class="fa fa-check"></i><b>3.4.4</b> Scatter vs Raster Plots</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="analysis.html"><a href="analysis.html#model"><i class="fa fa-check"></i><b>3.5</b> Model</a><ul>
<li class="chapter" data-level="3.5.1" data-path="analysis.html"><a href="analysis.html#cache-model-data"><i class="fa fa-check"></i><b>3.5.1</b> Cache model data</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="analysis.html"><a href="analysis.html#communicate"><i class="fa fa-check"></i><b>3.6</b> Communicate</a><ul>
<li class="chapter" data-level="3.6.1" data-path="analysis.html"><a href="analysis.html#reports"><i class="fa fa-check"></i><b>3.6.1</b> Reports</a></li>
<li class="chapter" data-level="3.6.2" data-path="analysis.html"><a href="analysis.html#presentation-decks"><i class="fa fa-check"></i><b>3.6.2</b> Presentation decks</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="analysis.html"><a href="analysis.html#recap"><i class="fa fa-check"></i><b>3.7</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="modeling.html"><a href="modeling.html"><i class="fa fa-check"></i><b>4</b> Modeling</a><ul>
<li class="chapter" data-level="4.1" data-path="modeling.html"><a href="modeling.html#overview"><i class="fa fa-check"></i><b>4.1</b> Overview</a></li>
<li class="chapter" data-level="4.2" data-path="modeling.html"><a href="modeling.html#the-data"><i class="fa fa-check"></i><b>4.2</b> The Data</a></li>
<li class="chapter" data-level="4.3" data-path="modeling.html"><a href="modeling.html#exploratory-data-analysis"><i class="fa fa-check"></i><b>4.3</b> Exploratory Data Analysis</a></li>
<li class="chapter" data-level="4.4" data-path="modeling.html"><a href="modeling.html#feature-engineering"><i class="fa fa-check"></i><b>4.4</b> Feature Engineering</a></li>
<li class="chapter" data-level="4.5" data-path="modeling.html"><a href="modeling.html#model-building"><i class="fa fa-check"></i><b>4.5</b> Model Building</a><ul>
<li class="chapter" data-level="4.5.1" data-path="modeling.html"><a href="modeling.html#logistic-regression-as-a-generalized-linear-regression"><i class="fa fa-check"></i><b>4.5.1</b> Logistic Regression as a Generalized Linear Regression</a></li>
<li class="chapter" data-level="4.5.2" data-path="modeling.html"><a href="modeling.html#more-machine-learning-algorithms"><i class="fa fa-check"></i><b>4.5.2</b> More Machine Learning Algorithms</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="modeling.html"><a href="modeling.html#working-with-textual-data"><i class="fa fa-check"></i><b>4.6</b> Working with Textual Data</a><ul>
<li class="chapter" data-level="4.6.1" data-path="modeling.html"><a href="modeling.html#data-prep"><i class="fa fa-check"></i><b>4.6.1</b> Data Prep</a></li>
<li class="chapter" data-level="4.6.2" data-path="modeling.html"><a href="modeling.html#topic-modeling"><i class="fa fa-check"></i><b>4.6.2</b> Topic Modeling</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="modeling.html"><a href="modeling.html#conclusion"><i class="fa fa-check"></i><b>4.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="pipelines.html"><a href="pipelines.html"><i class="fa fa-check"></i><b>5</b> Pipelines</a><ul>
<li class="chapter" data-level="5.1" data-path="pipelines.html"><a href="pipelines.html#estimators-and-transformers"><i class="fa fa-check"></i><b>5.1</b> Estimators and Transformers</a></li>
<li class="chapter" data-level="5.2" data-path="pipelines.html"><a href="pipelines.html#pipelines-and-pipeline-models"><i class="fa fa-check"></i><b>5.2</b> Pipelines and Pipeline Models</a></li>
<li class="chapter" data-level="5.3" data-path="pipelines.html"><a href="pipelines.html#applying-pipelines-to-okcupid-data"><i class="fa fa-check"></i><b>5.3</b> Applying Pipelines to OKCupid Data</a><ul>
<li class="chapter" data-level="5.3.1" data-path="pipelines.html"><a href="pipelines.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>5.3.1</b> Hyperparameter Tuning</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="pipelines.html"><a href="pipelines.html#operating-modes-of-pipelines-functions"><i class="fa fa-check"></i><b>5.4</b> Operating Modes of Pipelines Functions</a></li>
<li class="chapter" data-level="5.5" data-path="pipelines.html"><a href="pipelines.html#model-persistence-and-interoperability"><i class="fa fa-check"></i><b>5.5</b> Model Persistence and Interoperability</a></li>
<li class="chapter" data-level="5.6" data-path="pipelines.html"><a href="pipelines.html#model-deployment"><i class="fa fa-check"></i><b>5.6</b> Model Deployment</a><ul>
<li class="chapter" data-level="5.6.1" data-path="pipelines.html"><a href="pipelines.html#batch-scoring-with-ml-pipelines"><i class="fa fa-check"></i><b>5.6.1</b> Batch Scoring With ML Pipelines</a></li>
<li class="chapter" data-level="5.6.2" data-path="pipelines.html"><a href="pipelines.html#real-time-scoring-with-mleap"><i class="fa fa-check"></i><b>5.6.2</b> Real-Time Scoring with MLeap</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="pipelines.html"><a href="pipelines.html#conclusion-1"><i class="fa fa-check"></i><b>5.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="clusters.html"><a href="clusters.html"><i class="fa fa-check"></i><b>6</b> Clusters</a><ul>
<li class="chapter" data-level="6.1" data-path="clusters.html"><a href="clusters.html#clusters-overview"><i class="fa fa-check"></i><b>6.1</b> Overview</a></li>
<li class="chapter" data-level="6.2" data-path="clusters.html"><a href="clusters.html#on-premise"><i class="fa fa-check"></i><b>6.2</b> On-Premise</a><ul>
<li class="chapter" data-level="6.2.1" data-path="clusters.html"><a href="clusters.html#clusters-manager"><i class="fa fa-check"></i><b>6.2.1</b> Managers</a></li>
<li class="chapter" data-level="6.2.2" data-path="clusters.html"><a href="clusters.html#distributions"><i class="fa fa-check"></i><b>6.2.2</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="clusters.html"><a href="clusters.html#cloud"><i class="fa fa-check"></i><b>6.3</b> Cloud</a><ul>
<li class="chapter" data-level="6.3.1" data-path="clusters.html"><a href="clusters.html#clusters-amazon-emr"><i class="fa fa-check"></i><b>6.3.1</b> Amazon</a></li>
<li class="chapter" data-level="6.3.2" data-path="clusters.html"><a href="clusters.html#databricks"><i class="fa fa-check"></i><b>6.3.2</b> Databricks</a></li>
<li class="chapter" data-level="6.3.3" data-path="clusters.html"><a href="clusters.html#google"><i class="fa fa-check"></i><b>6.3.3</b> Google</a></li>
<li class="chapter" data-level="6.3.4" data-path="clusters.html"><a href="clusters.html#ibm"><i class="fa fa-check"></i><b>6.3.4</b> IBM</a></li>
<li class="chapter" data-level="6.3.5" data-path="clusters.html"><a href="clusters.html#microsoft"><i class="fa fa-check"></i><b>6.3.5</b> Microsoft</a></li>
<li class="chapter" data-level="6.3.6" data-path="clusters.html"><a href="clusters.html#qubole"><i class="fa fa-check"></i><b>6.3.6</b> Qubole</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="clusters.html"><a href="clusters.html#kubernetes"><i class="fa fa-check"></i><b>6.4</b> Kubernetes</a></li>
<li class="chapter" data-level="6.5" data-path="clusters.html"><a href="clusters.html#tools"><i class="fa fa-check"></i><b>6.5</b> Tools</a><ul>
<li class="chapter" data-level="6.5.1" data-path="clusters.html"><a href="clusters.html#rstudio"><i class="fa fa-check"></i><b>6.5.1</b> RStudio</a></li>
<li class="chapter" data-level="6.5.2" data-path="clusters.html"><a href="clusters.html#jupyter"><i class="fa fa-check"></i><b>6.5.2</b> Jupyter</a></li>
<li class="chapter" data-level="6.5.3" data-path="clusters.html"><a href="clusters.html#clusters-livy"><i class="fa fa-check"></i><b>6.5.3</b> Livy</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="clusters.html"><a href="clusters.html#recap-1"><i class="fa fa-check"></i><b>6.6</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="connections.html"><a href="connections.html"><i class="fa fa-check"></i><b>7</b> Connections</a><ul>
<li class="chapter" data-level="7.1" data-path="connections.html"><a href="connections.html#connections-overview"><i class="fa fa-check"></i><b>7.1</b> Overview</a><ul>
<li class="chapter" data-level="7.1.1" data-path="connections.html"><a href="connections.html#connections-spark-edge-nodes"><i class="fa fa-check"></i><b>7.1.1</b> Edge Nodes</a></li>
<li class="chapter" data-level="7.1.2" data-path="connections.html"><a href="connections.html#connections-spark-home"><i class="fa fa-check"></i><b>7.1.2</b> Spark Home</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="connections.html"><a href="connections.html#connections-local"><i class="fa fa-check"></i><b>7.2</b> Local</a></li>
<li class="chapter" data-level="7.3" data-path="connections.html"><a href="connections.html#connections-standalone"><i class="fa fa-check"></i><b>7.3</b> Standalone</a></li>
<li class="chapter" data-level="7.4" data-path="connections.html"><a href="connections.html#connections-yarn"><i class="fa fa-check"></i><b>7.4</b> Yarn</a><ul>
<li class="chapter" data-level="7.4.1" data-path="connections.html"><a href="connections.html#connections-yarn-client"><i class="fa fa-check"></i><b>7.4.1</b> Yarn Client</a></li>
<li class="chapter" data-level="7.4.2" data-path="connections.html"><a href="connections.html#connections-yarn-cluster"><i class="fa fa-check"></i><b>7.4.2</b> Yarn Cluster</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="connections.html"><a href="connections.html#connections-livy"><i class="fa fa-check"></i><b>7.5</b> Livy</a></li>
<li class="chapter" data-level="7.6" data-path="connections.html"><a href="connections.html#connections-mesos"><i class="fa fa-check"></i><b>7.6</b> Mesos</a></li>
<li class="chapter" data-level="7.7" data-path="connections.html"><a href="connections.html#connections-kubernetes"><i class="fa fa-check"></i><b>7.7</b> Kubernetes</a></li>
<li class="chapter" data-level="7.8" data-path="connections.html"><a href="connections.html#cloud-1"><i class="fa fa-check"></i><b>7.8</b> Cloud</a></li>
<li class="chapter" data-level="7.9" data-path="connections.html"><a href="connections.html#batches"><i class="fa fa-check"></i><b>7.9</b> Batches</a></li>
<li class="chapter" data-level="7.10" data-path="connections.html"><a href="connections.html#tools-1"><i class="fa fa-check"></i><b>7.10</b> Tools</a></li>
<li class="chapter" data-level="7.11" data-path="connections.html"><a href="connections.html#multiple"><i class="fa fa-check"></i><b>7.11</b> Multiple</a></li>
<li class="chapter" data-level="7.12" data-path="connections.html"><a href="connections.html#connections-troubleshooting"><i class="fa fa-check"></i><b>7.12</b> Troubleshooting</a><ul>
<li class="chapter" data-level="7.12.1" data-path="connections.html"><a href="connections.html#logging"><i class="fa fa-check"></i><b>7.12.1</b> Logging</a></li>
<li class="chapter" data-level="7.12.2" data-path="connections.html"><a href="connections.html#troubleshoot-spark-submit"><i class="fa fa-check"></i><b>7.12.2</b> Spark Submit</a></li>
<li class="chapter" data-level="7.12.3" data-path="connections.html"><a href="connections.html#windows"><i class="fa fa-check"></i><b>7.12.3</b> Windows</a></li>
</ul></li>
<li class="chapter" data-level="7.13" data-path="connections.html"><a href="connections.html#recap-2"><i class="fa fa-check"></i><b>7.13</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>8</b> Data</a><ul>
<li class="chapter" data-level="8.1" data-path="data.html"><a href="data.html#source-types-and-file-systems"><i class="fa fa-check"></i><b>8.1</b> Source types and file systems</a><ul>
<li class="chapter" data-level="8.1.1" data-path="data.html"><a href="data.html#default-packages"><i class="fa fa-check"></i><b>8.1.1</b> Default packages</a></li>
<li class="chapter" data-level="8.1.2" data-path="data.html"><a href="data.html#source-types"><i class="fa fa-check"></i><b>8.1.2</b> Source types</a></li>
<li class="chapter" data-level="8.1.3" data-path="data.html"><a href="data.html#file-systems"><i class="fa fa-check"></i><b>8.1.3</b> File systems</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="data.html"><a href="data.html#reading-data"><i class="fa fa-check"></i><b>8.2</b> Reading data</a><ul>
<li class="chapter" data-level="8.2.1" data-path="data.html"><a href="data.html#folders-as-a-table"><i class="fa fa-check"></i><b>8.2.1</b> Folders as a table</a></li>
<li class="chapter" data-level="8.2.2" data-path="data.html"><a href="data.html#file-layout"><i class="fa fa-check"></i><b>8.2.2</b> File layout</a></li>
<li class="chapter" data-level="8.2.3" data-path="data.html"><a href="data.html#spark-memory"><i class="fa fa-check"></i><b>8.2.3</b> Spark memory</a></li>
<li class="chapter" data-level="8.2.4" data-path="data.html"><a href="data.html#column-names"><i class="fa fa-check"></i><b>8.2.4</b> Column Names</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="data.html"><a href="data.html#writing-data"><i class="fa fa-check"></i><b>8.3</b> Writing Data</a><ul>
<li class="chapter" data-level="8.3.1" data-path="data.html"><a href="data.html#spark-not-r-as-pass-through"><i class="fa fa-check"></i><b>8.3.1</b> Spark, not R, as pass-through</a></li>
<li class="chapter" data-level="8.3.2" data-path="data.html"><a href="data.html#practical-approach"><i class="fa fa-check"></i><b>8.3.2</b> Practical approach</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="data.html"><a href="data.html#date-time"><i class="fa fa-check"></i><b>8.4</b> Date &amp; time</a></li>
<li class="chapter" data-level="8.5" data-path="data.html"><a href="data.html#specific-types-and-protocols"><i class="fa fa-check"></i><b>8.5</b> Specific types and protocols</a><ul>
<li class="chapter" data-level="8.5.1" data-path="data.html"><a href="data.html#amazon-s3"><i class="fa fa-check"></i><b>8.5.1</b> Amazon S3</a></li>
<li class="chapter" data-level="8.5.2" data-path="data.html"><a href="data.html#sql"><i class="fa fa-check"></i><b>8.5.2</b> SQL</a></li>
<li class="chapter" data-level="8.5.3" data-path="data.html"><a href="data.html#hive"><i class="fa fa-check"></i><b>8.5.3</b> Hive</a></li>
<li class="chapter" data-level="8.5.4" data-path="data.html"><a href="data.html#comma-delimited-values-csv"><i class="fa fa-check"></i><b>8.5.4</b> Comma Delimited Values (CSV)</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="data.html"><a href="data.html#recap-3"><i class="fa fa-check"></i><b>8.6</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="tuning.html"><a href="tuning.html"><i class="fa fa-check"></i><b>9</b> Tuning</a><ul>
<li class="chapter" data-level="9.1" data-path="tuning.html"><a href="tuning.html#overview-1"><i class="fa fa-check"></i><b>9.1</b> Overview</a><ul>
<li class="chapter" data-level="9.1.1" data-path="tuning.html"><a href="tuning.html#tuning-graph-visualization"><i class="fa fa-check"></i><b>9.1.1</b> Graph</a></li>
<li class="chapter" data-level="9.1.2" data-path="tuning.html"><a href="tuning.html#tuning-event-timeline"><i class="fa fa-check"></i><b>9.1.2</b> Timeline</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="tuning.html"><a href="tuning.html#tuning-configuring"><i class="fa fa-check"></i><b>9.2</b> Configuring</a><ul>
<li class="chapter" data-level="9.2.1" data-path="tuning.html"><a href="tuning.html#connect-settings"><i class="fa fa-check"></i><b>9.2.1</b> Connect Settings</a></li>
<li class="chapter" data-level="9.2.2" data-path="tuning.html"><a href="tuning.html#submit-settings"><i class="fa fa-check"></i><b>9.2.2</b> Submit Settings</a></li>
<li class="chapter" data-level="9.2.3" data-path="tuning.html"><a href="tuning.html#runtime-settings"><i class="fa fa-check"></i><b>9.2.3</b> Runtime Settings</a></li>
<li class="chapter" data-level="9.2.4" data-path="tuning.html"><a href="tuning.html#sparklyr-settings"><i class="fa fa-check"></i><b>9.2.4</b> sparklyr Settings</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="tuning.html"><a href="tuning.html#tuning-partitioning"><i class="fa fa-check"></i><b>9.3</b> Partitioning</a><ul>
<li class="chapter" data-level="9.3.1" data-path="tuning.html"><a href="tuning.html#implicit"><i class="fa fa-check"></i><b>9.3.1</b> Implicit</a></li>
<li class="chapter" data-level="9.3.2" data-path="tuning.html"><a href="tuning.html#explicit"><i class="fa fa-check"></i><b>9.3.2</b> Explicit</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="tuning.html"><a href="tuning.html#tuning-caching"><i class="fa fa-check"></i><b>9.4</b> Caching</a><ul>
<li class="chapter" data-level="9.4.1" data-path="tuning.html"><a href="tuning.html#checkpointing"><i class="fa fa-check"></i><b>9.4.1</b> Checkpointing</a></li>
<li class="chapter" data-level="9.4.2" data-path="tuning.html"><a href="tuning.html#tuning-memory"><i class="fa fa-check"></i><b>9.4.2</b> Memory</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="tuning.html"><a href="tuning.html#tuning-shuffling"><i class="fa fa-check"></i><b>9.5</b> Shuffling</a></li>
<li class="chapter" data-level="9.6" data-path="tuning.html"><a href="tuning.html#tuning-serialization"><i class="fa fa-check"></i><b>9.6</b> Serialization</a></li>
<li class="chapter" data-level="9.7" data-path="tuning.html"><a href="tuning.html#configuration-files"><i class="fa fa-check"></i><b>9.7</b> Configuration Files</a></li>
<li class="chapter" data-level="9.8" data-path="tuning.html"><a href="tuning.html#recap-4"><i class="fa fa-check"></i><b>9.8</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="extensions.html"><a href="extensions.html"><i class="fa fa-check"></i><b>10</b> Extensions</a><ul>
<li class="chapter" data-level="10.1" data-path="extensions.html"><a href="extensions.html#overview-2"><i class="fa fa-check"></i><b>10.1</b> Overview</a></li>
<li class="chapter" data-level="10.2" data-path="extensions.html"><a href="extensions.html#h2o"><i class="fa fa-check"></i><b>10.2</b> H2O</a></li>
<li class="chapter" data-level="10.3" data-path="extensions.html"><a href="extensions.html#graphs"><i class="fa fa-check"></i><b>10.3</b> Graphs</a></li>
<li class="chapter" data-level="10.4" data-path="extensions.html"><a href="extensions.html#xgboost"><i class="fa fa-check"></i><b>10.4</b> XGBoost</a></li>
<li class="chapter" data-level="10.5" data-path="extensions.html"><a href="extensions.html#deep-learning"><i class="fa fa-check"></i><b>10.5</b> Deep Learning</a></li>
<li class="chapter" data-level="10.6" data-path="extensions.html"><a href="extensions.html#genomics"><i class="fa fa-check"></i><b>10.6</b> Genomics</a></li>
<li class="chapter" data-level="10.7" data-path="extensions.html"><a href="extensions.html#spatial"><i class="fa fa-check"></i><b>10.7</b> Spatial</a></li>
<li class="chapter" data-level="10.8" data-path="extensions.html"><a href="extensions.html#troubleshooting"><i class="fa fa-check"></i><b>10.8</b> Troubleshooting</a></li>
<li class="chapter" data-level="10.9" data-path="extensions.html"><a href="extensions.html#recap-5"><i class="fa fa-check"></i><b>10.9</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="distributed.html"><a href="distributed.html"><i class="fa fa-check"></i><b>11</b> Distributed R</a><ul>
<li class="chapter" data-level="11.1" data-path="distributed.html"><a href="distributed.html#overview-3"><i class="fa fa-check"></i><b>11.1</b> Overview</a></li>
<li class="chapter" data-level="11.2" data-path="distributed.html"><a href="distributed.html#use-cases"><i class="fa fa-check"></i><b>11.2</b> Use Cases</a><ul>
<li class="chapter" data-level="11.2.1" data-path="distributed.html"><a href="distributed.html#custom-parsers"><i class="fa fa-check"></i><b>11.2.1</b> Custom Parsers</a></li>
<li class="chapter" data-level="11.2.2" data-path="distributed.html"><a href="distributed.html#partitioned-modeling"><i class="fa fa-check"></i><b>11.2.2</b> Partitioned Modeling</a></li>
<li class="chapter" data-level="11.2.3" data-path="distributed.html"><a href="distributed.html#distributed-grid-search"><i class="fa fa-check"></i><b>11.2.3</b> Grid Search</a></li>
<li class="chapter" data-level="11.2.4" data-path="distributed.html"><a href="distributed.html#web-apis"><i class="fa fa-check"></i><b>11.2.4</b> Web APIs</a></li>
<li class="chapter" data-level="11.2.5" data-path="distributed.html"><a href="distributed.html#intensive-computations"><i class="fa fa-check"></i><b>11.2.5</b> Intensive Computations</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="distributed.html"><a href="distributed.html#partitions"><i class="fa fa-check"></i><b>11.3</b> Partitions</a></li>
<li class="chapter" data-level="11.4" data-path="distributed.html"><a href="distributed.html#grouping"><i class="fa fa-check"></i><b>11.4</b> Grouping</a></li>
<li class="chapter" data-level="11.5" data-path="distributed.html"><a href="distributed.html#columns"><i class="fa fa-check"></i><b>11.5</b> Columns</a></li>
<li class="chapter" data-level="11.6" data-path="distributed.html"><a href="distributed.html#context"><i class="fa fa-check"></i><b>11.6</b> Context</a></li>
<li class="chapter" data-level="11.7" data-path="distributed.html"><a href="distributed.html#functions"><i class="fa fa-check"></i><b>11.7</b> Functions</a></li>
<li class="chapter" data-level="11.8" data-path="distributed.html"><a href="distributed.html#packages"><i class="fa fa-check"></i><b>11.8</b> Packages</a></li>
<li class="chapter" data-level="11.9" data-path="distributed.html"><a href="distributed.html#cluster-requirements"><i class="fa fa-check"></i><b>11.9</b> Cluster Requirements</a><ul>
<li class="chapter" data-level="11.9.1" data-path="distributed.html"><a href="distributed.html#installing-r"><i class="fa fa-check"></i><b>11.9.1</b> Installing R</a></li>
<li class="chapter" data-level="11.9.2" data-path="distributed.html"><a href="distributed.html#apache-arrow"><i class="fa fa-check"></i><b>11.9.2</b> Apache Arrow</a></li>
</ul></li>
<li class="chapter" data-level="11.10" data-path="distributed.html"><a href="distributed.html#troubleshooting-1"><i class="fa fa-check"></i><b>11.10</b> Troubleshooting</a><ul>
<li class="chapter" data-level="11.10.1" data-path="distributed.html"><a href="distributed.html#worker-logs"><i class="fa fa-check"></i><b>11.10.1</b> Worker Logs</a></li>
<li class="chapter" data-level="11.10.2" data-path="distributed.html"><a href="distributed.html#resolving-timeouts"><i class="fa fa-check"></i><b>11.10.2</b> Resolving Timeouts</a></li>
<li class="chapter" data-level="11.10.3" data-path="distributed.html"><a href="distributed.html#inspecting-partition"><i class="fa fa-check"></i><b>11.10.3</b> Inspecting Partition</a></li>
<li class="chapter" data-level="11.10.4" data-path="distributed.html"><a href="distributed.html#debugging-workers"><i class="fa fa-check"></i><b>11.10.4</b> Debugging Workers</a></li>
</ul></li>
<li class="chapter" data-level="11.11" data-path="distributed.html"><a href="distributed.html#recap-6"><i class="fa fa-check"></i><b>11.11</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="streaming.html"><a href="streaming.html"><i class="fa fa-check"></i><b>12</b> Streaming</a><ul>
<li class="chapter" data-level="12.1" data-path="streaming.html"><a href="streaming.html#spark-streaming"><i class="fa fa-check"></i><b>12.1</b> Spark Streaming</a></li>
<li class="chapter" data-level="12.2" data-path="streaming.html"><a href="streaming.html#working-with-spark-streams"><i class="fa fa-check"></i><b>12.2</b> Working with Spark Streams</a></li>
<li class="chapter" data-level="12.3" data-path="streaming.html"><a href="streaming.html#sparklyr-extras"><i class="fa fa-check"></i><b>12.3</b> <code>sparklyr</code> extras</a><ul>
<li class="chapter" data-level="12.3.1" data-path="streaming.html"><a href="streaming.html#stream-monitor"><i class="fa fa-check"></i><b>12.3.1</b> Stream monitor</a></li>
<li class="chapter" data-level="12.3.2" data-path="streaming.html"><a href="streaming.html#stream-generator"><i class="fa fa-check"></i><b>12.3.2</b> Stream generator</a></li>
<li class="chapter" data-level="12.3.3" data-path="streaming.html"><a href="streaming.html#shiny-reactive"><i class="fa fa-check"></i><b>12.3.3</b> Shiny reactive</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="streaming.html"><a href="streaming.html#intro-example"><i class="fa fa-check"></i><b>12.4</b> Intro example</a></li>
<li class="chapter" data-level="12.5" data-path="streaming.html"><a href="streaming.html#transformations"><i class="fa fa-check"></i><b>12.5</b> Transformations</a><ul>
<li class="chapter" data-level="12.5.1" data-path="streaming.html"><a href="streaming.html#dplyr"><i class="fa fa-check"></i><b>12.5.1</b> dplyr</a></li>
<li class="chapter" data-level="12.5.2" data-path="streaming.html"><a href="streaming.html#transformer-functions"><i class="fa fa-check"></i><b>12.5.2</b> Transformer functions</a></li>
<li class="chapter" data-level="12.5.3" data-path="streaming.html"><a href="streaming.html#r-code"><i class="fa fa-check"></i><b>12.5.3</b> R code</a></li>
<li class="chapter" data-level="12.5.4" data-path="streaming.html"><a href="streaming.html#ml-pipelines"><i class="fa fa-check"></i><b>12.5.4</b> ML Pipelines</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="streaming.html"><a href="streaming.html#shiny-integration"><i class="fa fa-check"></i><b>12.6</b> Shiny integration</a></li>
<li class="chapter" data-level="12.7" data-path="streaming.html"><a href="streaming.html#kafka"><i class="fa fa-check"></i><b>12.7</b> Kafka</a><ul>
<li class="chapter" data-level="12.7.1" data-path="streaming.html"><a href="streaming.html#workflow"><i class="fa fa-check"></i><b>12.7.1</b> Workflow</a></li>
<li class="chapter" data-level="12.7.2" data-path="streaming.html"><a href="streaming.html#spark-integration"><i class="fa fa-check"></i><b>12.7.2</b> Spark integration</a></li>
<li class="chapter" data-level="12.7.3" data-path="streaming.html"><a href="streaming.html#r-integration"><i class="fa fa-check"></i><b>12.7.3</b> R integration</a></li>
<li class="chapter" data-level="12.7.4" data-path="streaming.html"><a href="streaming.html#example"><i class="fa fa-check"></i><b>12.7.4</b> Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="contributing.html"><a href="contributing.html"><i class="fa fa-check"></i><b>13</b> Contributing</a><ul>
<li class="chapter" data-level="13.1" data-path="contributing.html"><a href="contributing.html#contributing-overview"><i class="fa fa-check"></i><b>13.1</b> Overview</a></li>
<li class="chapter" data-level="13.2" data-path="contributing.html"><a href="contributing.html#contributing-spark-api"><i class="fa fa-check"></i><b>13.2</b> Spark API</a></li>
<li class="chapter" data-level="13.3" data-path="contributing.html"><a href="contributing.html#spark-extensions"><i class="fa fa-check"></i><b>13.3</b> Spark Extensions</a></li>
<li class="chapter" data-level="13.4" data-path="contributing.html"><a href="contributing.html#scala-code"><i class="fa fa-check"></i><b>13.4</b> Scala Code</a></li>
<li class="chapter" data-level="13.5" data-path="contributing.html"><a href="contributing.html#recap-7"><i class="fa fa-check"></i><b>13.5</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>14</b> Appendix</a><ul>
<li class="chapter" data-level="14.1" data-path="appendix.html"><a href="appendix.html#appendix-prerequisites"><i class="fa fa-check"></i><b>14.1</b> Prerequisites</a><ul>
<li class="chapter" data-level="14.1.1" data-path="appendix.html"><a href="appendix.html#appendix-install-r"><i class="fa fa-check"></i><b>14.1.1</b> Installing R</a></li>
<li class="chapter" data-level="14.1.2" data-path="appendix.html"><a href="appendix.html#appendix-install-java"><i class="fa fa-check"></i><b>14.1.2</b> Installing Java</a></li>
<li class="chapter" data-level="14.1.3" data-path="appendix.html"><a href="appendix.html#appendix-install-rstudio"><i class="fa fa-check"></i><b>14.1.3</b> Installing RStudio</a></li>
<li class="chapter" data-level="14.1.4" data-path="appendix.html"><a href="appendix.html#appendix-using-rstudio"><i class="fa fa-check"></i><b>14.1.4</b> Using RStudio</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="appendix.html"><a href="appendix.html#diagrams"><i class="fa fa-check"></i><b>14.2</b> Diagrams</a><ul>
<li class="chapter" data-level="14.2.1" data-path="appendix.html"><a href="appendix.html#appendix-storage-capacity"><i class="fa fa-check"></i><b>14.2.1</b> Worlds Store Capacity</a></li>
<li class="chapter" data-level="14.2.2" data-path="appendix.html"><a href="appendix.html#appendix-cran-downloads"><i class="fa fa-check"></i><b>14.2.2</b> Daily downloads of CRAN packages</a></li>
<li class="chapter" data-level="14.2.3" data-path="appendix.html"><a href="appendix.html#appendix-cluster-trends"><i class="fa fa-check"></i><b>14.2.3</b> Google trends for mainframes, cloud computing and kubernetes</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="appendix.html"><a href="appendix.html#appendix-ggplot2-theme"><i class="fa fa-check"></i><b>14.3</b> Formatting</a></li>
<li class="chapter" data-level="14.4" data-path="appendix.html"><a href="appendix.html#ml-functionlist"><i class="fa fa-check"></i><b>14.4</b> List of ML Functions</a><ul>
<li class="chapter" data-level="14.4.1" data-path="appendix.html"><a href="appendix.html#classification"><i class="fa fa-check"></i><b>14.4.1</b> Classification</a></li>
<li class="chapter" data-level="14.4.2" data-path="appendix.html"><a href="appendix.html#regression"><i class="fa fa-check"></i><b>14.4.2</b> Regression</a></li>
<li class="chapter" data-level="14.4.3" data-path="appendix.html"><a href="appendix.html#clustering"><i class="fa fa-check"></i><b>14.4.3</b> Clustering</a></li>
<li class="chapter" data-level="14.4.4" data-path="appendix.html"><a href="appendix.html#recommendation"><i class="fa fa-check"></i><b>14.4.4</b> Recommendation</a></li>
<li class="chapter" data-level="14.4.5" data-path="appendix.html"><a href="appendix.html#frequent-pattern-mining"><i class="fa fa-check"></i><b>14.4.5</b> Frequent Pattern Mining</a></li>
<li class="chapter" data-level="14.4.6" data-path="appendix.html"><a href="appendix.html#feature-transformers"><i class="fa fa-check"></i><b>14.4.6</b> Feature Transformers</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="appendix.html"><a href="appendix.html#kafka-1"><i class="fa fa-check"></i><b>14.5</b> Kafka</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>15</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Mastering Apache Spark with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="analysis" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Analysis</h1>
<p>Previous chapters focused on introducing Spark and R. They also focused on helping you get started with the tools you need throughout this book. In this chapter, you will learn how to perform data analysis in Spark from R.</p>
<p>In a data analysis project, the main goal is to understand what the data is trying to “tell us”. The hope is that the data provides an answer to a specific question. The question is typically poised by stakeholders of the analysis. Most data analysis projects follow a set of steps outlined in Figure <a href="analysis.html#fig:analysis-steps">3.1</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:analysis-steps"></span>
<div id="htmlwidget-56fd3c47f957a147d67d" style="width:100%;height:220pt;" class="nomnoml html-widget"></div>
<script type="application/json" data-for="htmlwidget-56fd3c47f957a147d67d">{"x":{"code":"\n  #fill: #FEFEFF\n  #lineWidth: 2\n  #zoom: 4\n  #direction: right\n   \n\n#direction: right\n#edgeMargin: 4\n#padding: 25\n#fontSize: 18\n\n[Import] -> [Understand]\n[Understand |\n  [Wrangle] -> [Visualize] \n  [Visualize] -> [Model]\n  [Model] -> [Wrangle]\n]\n[Understand] -> [Communicate]\n","className":null,"svg":false},"evals":[],"jsHooks":[]}</script>
<p class="caption">
FIGURE 3.1: General steps of a data analysis
</p>
</div>
<p>As the diagram illustrates, the data is <strong>imported</strong> into our analysis stem, then <strong>wrangled</strong> by trying different data transformations, such as aggregations, and then <strong>visualized</strong> to help us perceive relationships and trends. In order to get deeper insight, one or multiple statistical <strong>models</strong> can be fitted against sample data. This will help in finding out if the patterns hold true when new data is applied to them. And lastly, the results are <strong>communicated</strong> to stakeholders.</p>
<p>Commonly, in R all of the steps are performed in local memory. But that approach has to change when the analysis adds the use of Spark. The next section will introduce the main concept of how to best integrate R and Spark for data analysis.</p>
<div id="r-as-an-interface-to-spark" class="section level2">
<h2><span class="header-section-number">3.1</span> R as an Interface to Spark</h2>
<p>For data analysis, the ideal approach is to let Spark do what its good at. Spark is a parallel computation engine that works at a large scale. Spark includes a SQL Engine and Machine Learning libraries. These can be used to perform most of the same operations R performs. Such operations include data selection, transformation, and modeling. Additionally, Spark include Graph analysis and Streaming libraries that extend the R user’s capabilities. Figure <a href="analysis.html#fig:analysis-features">3.2</a> lists the four main capabilities available to data analysts in Spark.</p>
<div class="figure" style="text-align: center"><span id="fig:analysis-features"></span>
<div id="htmlwidget-08be699a5c81669f30be" style="width:100%;height:220pt;" class="nomnoml html-widget"></div>
<script type="application/json" data-for="htmlwidget-08be699a5c81669f30be">{"x":{"code":"\n  #fill: #FEFEFF\n  #lineWidth: 2\n  #zoom: 4\n  #direction: right\n   \n\n#direction: down\n#fontSize: 18\n#padding: 30\n[Apache Spark |\n  [SQL Engine]\n  [Machine Learning]\n  [Graph analysis]\n  [Streaming]\n]\n","className":null,"svg":false},"evals":[],"jsHooks":[]}</script>
<p class="caption">
FIGURE 3.2: Spark capabilities
</p>
</div>
<p>The data <strong>import</strong>, <strong>wrangling</strong>, and <strong>modeling</strong> can be performed inside Spark. <strong>Visualization</strong> can also partly be done by Spark, we will cover that later in this chapter. The idea is to use R to tell Spark what data operations to run, and then bring only the results into R. As illustrated in Figured <a href="analysis.html#fig:analysis-approach">3.3</a>, the ideal method <strong>pushes compute</strong> to the Spark cluster, and then <strong>collects results</strong> into R.</p>
<div class="figure" style="text-align: center"><span id="fig:analysis-approach"></span>
<div id="htmlwidget-3c73f6a2a11516fef543" style="width:100%;height:220pt;" class="nomnoml html-widget"></div>
<script type="application/json" data-for="htmlwidget-3c73f6a2a11516fef543">{"x":{"code":"\n  #fill: #FEFEFF\n  #lineWidth: 2\n  #zoom: 4\n  #direction: right\n   \n\n#direction: right\n#arrowSize: 0.4\n#lineWidth: 1\n#gutter: -6\n[R]->[<label> Push Compute]\n[Push Compute]->[Spark]\n[R]<-[<label> Collect Results]\n[Collect Results]<-[Spark]\n","className":null,"svg":false},"evals":[],"jsHooks":[]}</script>
<p class="caption">
FIGURE 3.3: Spark computes, R collects results
</p>
</div>
<p>The <code>sparklyr</code> package aids in using the “push compute, collect results” principle. Most of its functions are mainly wrappers on top of Spark API calls. This allows us to take advantage of Spark’s analysis components, instead of R’s.<br />
For example, an analyst needs to fit a Linear Regression model. The data is available in Spark. Instead of using the familiar <code>lm()</code> function, the analyst would use <code>ml_linear_regression()</code>. This R function will run the Scala code that uses the Spark’s API model. This specific example is illustrated in Figure <a href="analysis.html#fig:analysis-scala">3.4</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:analysis-scala"></span>
<div id="htmlwidget-e7cb114015f9b1e4aa1e" style="width:100%;height:220pt;" class="nomnoml html-widget"></div>
<script type="application/json" data-for="htmlwidget-e7cb114015f9b1e4aa1e">{"x":{"code":"\n  #fill: #FEFEFF\n  #lineWidth: 2\n  #zoom: 4\n  #direction: right\n   \n\n#direction: right\n#padding: 12\n#lineWidth: 1\n#arrowSize: 0.4\n[R | [<note>ml_linear_regression()]]->[<label>Invokes]\n[Invokes]->[Scala | [<note>var lr = new LinearRegression()\n                     var lrModel = lr.fit(training)]]\n","className":null,"svg":false},"evals":[],"jsHooks":[]}</script>
<p class="caption">
FIGURE 3.4: R functions call Scala functions
</p>
</div>
<p>For more common data manipulation tasks, <code>sparklyr</code> provides a back-end for <code>dplyr</code>. This means that already familiar <code>dplyr</code> verbs can be used in R, and then <code>sparklyr</code> and <code>dplyr</code> will translate those actions into Spark SQL statements.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-22"></span>
<div id="htmlwidget-db573311acf7c9c922f0" style="width:100%;height:220pt;" class="nomnoml html-widget"></div>
<script type="application/json" data-for="htmlwidget-db573311acf7c9c922f0">{"x":{"code":"\n  #fill: #FEFEFF\n  #lineWidth: 2\n  #zoom: 4\n  #direction: right\n   \n\n#direction: right\n#padding: 12\n#lineWidth: 1\n#arrowSize: 0.4\n[R | [<note>filter(data, y == 1)]]->[<label>Invokes]\n[Invokes]->  [SQL | [<note>... WHERE y = 1 ...]]\n","className":null,"svg":false},"evals":[],"jsHooks":[]}</script>
<p class="caption">
FIGURE 3.5: dplyr writes SQL
</p>
</div>
<p>An example of a practical implementation of this concept will be covered in the following sections.</p>
<div id="exercise" class="section level3">
<h3><span class="header-section-number">3.1.1</span> Exercise</h3>
<p>In order to practice as you learn, the rest of this chapter’s code will use a single exercise that runs in the <strong>local</strong> Spark master. This way, the code can be replicated in your personal computer. Please, make sure to already have <code>sparklyr</code> and a local copy of Spark installed. The installation of Spark can be done by using the utility that comes with the package. For more information on how to do that please see the Local section in the <a href="conections/#connections-local">Connections</a> chapter.</p>
<p>First, load the <code>sparklyr</code> and <code>dplyr</code> packages, and open a new <strong>local</strong> connection.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(sparklyr)
sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>)</code></pre>
<p>The environment is ready to be used. The next step is to add data that we can analyze.</p>
</div>
</div>
<div id="import-access" class="section level2">
<h2><span class="header-section-number">3.2</span> Import / Access</h2>
<p>The step of importing data is to be approached differently when using Spark with R, as opposed to R alone. Usually, importing means that R will read files and save the information into memory. But when used with Spark, it is important to only focus in importing results into R. The data is either imported, or accessed by Spark. Notice how in Figure <a href="analysis.html#fig:analysis-access">3.6</a> the Data Source is connected to Spark, and not R.</p>
<div class="figure" style="text-align: center"><span id="fig:analysis-access"></span>
<div id="htmlwidget-31fae57cdcb86e5f564e" style="width:100%;height:220pt;" class="nomnoml html-widget"></div>
<script type="application/json" data-for="htmlwidget-31fae57cdcb86e5f564e">{"x":{"code":"\n  #fill: #FEFEFF\n  #lineWidth: 2\n  #zoom: 4\n  #direction: right\n   \n\n#direction: right\n#padding: 14\n#arrowSize: 0.4\n[R]->[<label>Push Compute]\n[Push Compute]->[Spark]\n[R]<-[<label>Collect Results]\n[Collect Results]<-[Spark]\n[Spark]<-[<label> Import / Access]\n[Import / Access]<-[Data Source]\n","className":null,"svg":false},"evals":[],"jsHooks":[]}</script>
<p class="caption">
FIGURE 3.6: Import Data to Spark not R
</p>
</div>
<p>Most organization will use Spark inside a YARN managed cluster. For those cases the vast majority of the necessary data is already available in the cluster’s Hadoop File System (HDFS). It is made available to users via Hive tables, or by accessing the file system directly. Because the Spark session is initiated inside the same server hosts as the rest of the cluster, it is much easier to transfer large amounts of data between the HDFS and Spark memory.</p>
<p>The decision of having Spark either access the data source, or to import data into memory is mostly a decisions based on speed and performance. Importing all of the data into the Spark session will incur an up-front cost of time. That is because we would have to wait for the data to be loaded before analyzing. Accessing the data incurs cost with every query sent to the cluster via Spark. That is because we depend on the cluster’s in-disk data, which has to be retrieved before it is analyzed. More will be covered in the <a href="tuning">Tuning</a> chapter.</p>
<p>The exercise’s Spark session does not have any data. So the next step is to prime the session with data, in this case <code>mtcars</code>. The <code>copy_to()</code> command from <code>dplyr</code> can be used for that. The mechanics of this operation is explained in the <a href="starting.html#starting-sparklyr-hello-world">Getting Started</a> chapter.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(dplyr)
cars &lt;-<span class="st"> </span><span class="kw">copy_to</span>(sc, mtcars, <span class="st">&quot;mtcars_remote&quot;</span>)</code></pre>
<p><strong>Note:</strong> In an enterprise setting, <code>copy_to()</code> should only be used to transfer small tables from R, such as a look up value table. Large data transfers should be performed by more formal data transfer or ETL tools.</p>
<p>The data is now accessible to Spark and R. Transformations can now be applied to the data. The next section will cover how to wrangle data by running transformations inside Spark.</p>
</div>
<div id="wrangle" class="section level2">
<h2><span class="header-section-number">3.3</span> Wrangle</h2>
<p>Data wrangling uses transformations to understand the data. A data <strong>transformation</strong> can be interpreted as a <strong>change</strong> performed to the data.</p>
<p>Malformed or missing values and columns with multiple attributes are common data issues. These issues prevent us from understanding the activity reflected in the data. For example, a “name” field contains the last and first name of a customer. There are two attributes (first &amp; last name) in a single column. In order to be usable, we need to <strong>transform</strong> the “name” field, by <strong>changing</strong> it into a “first_name” and “last_name” fields.</p>
<p>After the data is cleaned, there is still the task to try and understand the basics about its content. Other transformations, such as aggregations, can help with this task. For example, the result of requesting the average balance of all customers will return a single row and column. The value will be the average of all customers. That information will give us context when we see individual, or grouped, customer balances.</p>
<p>The main goal is to write the data transformations using R syntax as much as possible. This saves us from the cognitive cost of having to switch between multiple computer languages to accomplish a single task. In this case, it is better to take advantage of <code>sparklyr</code>’s <code>dplyr</code> back-end interface, instead of writing Spark SQL statements for data exploration,</p>
<p>In the R environment, <em>cars</em> can be treated as if it is a local data frame, so <code>dplyr</code> verbs can be used, and in a piped fashion.</p>
<pre class="sourceCode r"><code class="sourceCode r">cars <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(am) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">mpg_mean =</span> <span class="kw">mean</span>(mpg, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>))</code></pre>
<pre><code>## # Source: spark&lt;?&gt; [?? x 2]
##      am mpg_mean
##   &lt;dbl&gt;    &lt;dbl&gt;
## 1     0     17.1
## 2     1     24.4</code></pre>
<p>Instead of importing the <em>mtcars_remote</em> data set from Spark, and then performing the aggregation, <code>dplyr</code> converts the verbs into SQL statements that are then sent to Spark. The <code>show_query()</code> command makes it possible to peer into the SQL statement that <code>sparklyr</code> and <code>dplyr</code> created and sent to Spark.</p>
<pre class="sourceCode r"><code class="sourceCode r">cars <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(am) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">mpg_mean =</span> <span class="kw">mean</span>(mpg, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">show_query</span>()</code></pre>
<pre><code>## &lt;SQL&gt;
## SELECT `am`, AVG(`mpg`) AS `mpg_mean`
## FROM `mtcars_remote`
## GROUP BY `am`</code></pre>
<p>As it is evident, it will not be necessary to have to see the resulting query every time <code>dplyr</code> verbs are being used. The focus can remain on obtaining insights from the data, as opposed to figuring out how to express a given set of transformation in SQL.</p>
<pre class="sourceCode r"><code class="sourceCode r">cars <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(am) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarise</span>(
    <span class="dt">wt_mean =</span> <span class="kw">mean</span>(wt, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>),
    <span class="dt">mpg_mean =</span> <span class="kw">mean</span>(mpg, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)
  )</code></pre>
<pre><code>## # Source: spark&lt;?&gt; [?? x 3]
##      am wt_mean mpg_mean
##   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;
## 1     0    3.77     17.1
## 2     1    2.41     24.4</code></pre>
<p>Most of the data transformation made available by <code>dplyr</code> to work with local data frames are also available to use with a Spark connection. This means that a general approach to learning <code>dplyr</code> can be taken in order to gain more proficiency with data exploration and preparation with Spark. The chapter on Data Transformation in the R for Data Science <span class="citation">(Wickham and Grolemund <a href="#ref-intro-r-for-data-science">2016</a>)</span> book should be a great help with this. If proficiency with <code>dplyr</code> is not an issue for you, then please take some time to experiment with different <code>dplyr</code> functions against the <em>cars</em> table.</p>
<div id="correlations" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Correlations</h3>
<p>A very common exploration technique is to calculate and visualize correlations. We often calculate correlations to find out what kind of statistical relationship are between paired sets of variables. The Spark API provides an internal function that calculates correlations across the entire data set. The results are then returned to R as a <code>data.frame</code> object.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ml_corr</span>(cars) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">as_tibble</span>()</code></pre>
<pre><code>## # A tibble: 11 x 11
##       mpg    cyl   disp     hp    drat     wt    qsec
##     &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;
##  1  1     -0.852 -0.848 -0.776  0.681  -0.868  0.419 
##  2 -0.852  1      0.902  0.832 -0.700   0.782 -0.591 
##  3 -0.848  0.902  1      0.791 -0.710   0.888 -0.434 
##  4 -0.776  0.832  0.791  1     -0.449   0.659 -0.708 
##  5  0.681 -0.700 -0.710 -0.449  1      -0.712  0.0912
##  6 -0.868  0.782  0.888  0.659 -0.712   1     -0.175 
##  7  0.419 -0.591 -0.434 -0.708  0.0912 -0.175  1     
##  8  0.664 -0.811 -0.710 -0.723  0.440  -0.555  0.745 
##  9  0.600 -0.523 -0.591 -0.243  0.713  -0.692 -0.230 
## 10  0.480 -0.493 -0.556 -0.126  0.700  -0.583 -0.213 
## 11 -0.551  0.527  0.395  0.750 -0.0908  0.428 -0.656 
## # ... with 4 more variables: vs &lt;dbl&gt;, am &lt;dbl&gt;,
## #   gear &lt;dbl&gt;, carb &lt;dbl&gt;</code></pre>
<p>The <code>corrr</code> R package specializes in correlations. It contains friendly functions to prepare and visualize the results. Included inside the package is a back-end for <code>sparklyr</code> table objects, so it will not return an error if a non local table is passed to it. In the background, the <code>correlate()</code> function runs <code>ml_corr()</code>, so there is no need to collect any data into R prior running the command.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(corrr)

cars <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">correlate</span>(<span class="dt">use =</span> <span class="st">&quot;pairwise.complete.obs&quot;</span>, <span class="dt">method =</span> <span class="st">&quot;pearson&quot;</span>) </code></pre>
<pre><code>## # A tibble: 11 x 12
##    rowname     mpg     cyl    disp      hp     drat      wt
##    &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;
##  1 mpg      NA      -0.852  -0.848  -0.776   0.681   -0.868
##  2 cyl      -0.852  NA       0.902   0.832  -0.700    0.782
##  3 disp     -0.848   0.902  NA       0.791  -0.710    0.888
##  4 hp       -0.776   0.832   0.791  NA      -0.449    0.659
##  5 drat      0.681  -0.700  -0.710  -0.449  NA       -0.712
##  6 wt       -0.868   0.782   0.888   0.659  -0.712   NA    
##  7 qsec      0.419  -0.591  -0.434  -0.708   0.0912  -0.175
##  8 vs        0.664  -0.811  -0.710  -0.723   0.440   -0.555
##  9 am        0.600  -0.523  -0.591  -0.243   0.713   -0.692
## 10 gear      0.480  -0.493  -0.556  -0.126   0.700   -0.583
## 11 carb     -0.551   0.527   0.395   0.750  -0.0908   0.428
## # ... with 5 more variables: qsec &lt;dbl&gt;, vs &lt;dbl&gt;,
## #   am &lt;dbl&gt;, gear &lt;dbl&gt;, carb &lt;dbl&gt;</code></pre>
<p>The <code>correlate()</code> function returns a local R object that <code>corrr</code> recognizes. This way, it is easy to perform more functions on top of the results. In this case, the <code>shave()</code> command turns all of the duplicated results into <code>NA</code>’s</p>
<pre class="sourceCode r"><code class="sourceCode r">cars <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">correlate</span>(<span class="dt">use =</span> <span class="st">&quot;pairwise.complete.obs&quot;</span>, <span class="dt">method =</span> <span class="st">&quot;pearson&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">shave</span>()</code></pre>
<pre><code>## # A tibble: 11 x 12
##    rowname     mpg     cyl    disp      hp     drat      wt
##    &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;
##  1 mpg      NA      NA      NA      NA      NA       NA    
##  2 cyl      -0.852  NA      NA      NA      NA       NA    
##  3 disp     -0.848   0.902  NA      NA      NA       NA    
##  4 hp       -0.776   0.832   0.791  NA      NA       NA    
##  5 drat      0.681  -0.700  -0.710  -0.449  NA       NA    
##  6 wt       -0.868   0.782   0.888   0.659  -0.712   NA    
##  7 qsec      0.419  -0.591  -0.434  -0.708   0.0912  -0.175
##  8 vs        0.664  -0.811  -0.710  -0.723   0.440   -0.555
##  9 am        0.600  -0.523  -0.591  -0.243   0.713   -0.692
## 10 gear      0.480  -0.493  -0.556  -0.126   0.700   -0.583
## 11 carb     -0.551   0.527   0.395   0.750  -0.0908   0.428
## # ... with 5 more variables: qsec &lt;dbl&gt;, vs &lt;dbl&gt;,
## #   am &lt;dbl&gt;, gear &lt;dbl&gt;, carb &lt;dbl&gt;</code></pre>
<p>Finally, the results can be easily visualized using <code>rplot()</code>. This function returns a <code>ggplot</code> object. Figure <a href="analysis.html#fig:analysis-rplot">3.7</a> is an example of what <code>rplot()</code> returns.</p>
<pre class="sourceCode r"><code class="sourceCode r">cars <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">correlate</span>(<span class="dt">use =</span> <span class="st">&quot;pairwise.complete.obs&quot;</span>, <span class="dt">method =</span> <span class="st">&quot;pearson&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">shave</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">rplot</span>()</code></pre>
<div class="figure" style="text-align: center"><span id="fig:analysis-rplot"></span>
<img src="images/analysis-wrangling-1-resized.png" alt="Using rplot() to visualize correlations" width="800pt" height="400pt" />
<p class="caption">
FIGURE 3.7: Using rplot() to visualize correlations
</p>
</div>
<p>It is much easier to see which relationships are positive or negative. Positive relationships are in blue, and negative relationships are in red. The size of the circle indicates how significant their relationship is. The power of a visualizing data is in how much easier it makes it for us to understand results. The next section will expand on this step of the process.</p>
</div>
</div>
<div id="visualize" class="section level2">
<h2><span class="header-section-number">3.4</span> Visualize</h2>
<p>Visualizations are a vital tool to help us find patterns in the data. It is easier for us to identify outliers in a data set of 1,000 observations when plotted in a graph, as opposed to reading them from a list.</p>
<p>R is great at data visualizations. Its capabilities for creating plots is extended by the many R packages that focus on this analysis step. Unfortunately, the vast majority of R functions that create plots depend on the data already being in local memory within R, so they fail when using a remote table inside Spark.</p>
<p>It is possible to create visualizations in R from data source from Spark. To understand how to do this, let’s first break down how computer programs build plots: It takes the raw data and performs some sort of transformation. The transformed data is then mapped to a set of coordinates. Finally, the mapped values are drawn in a plot. Figure <a href="analysis.html#fig:analysis-plot">3.8</a> summarizes each of the steps.</p>
<div class="figure" style="text-align: center"><span id="fig:analysis-plot"></span>
<div id="htmlwidget-da8b5f3435f5ac444f78" style="width:100%;height:100pt;" class="nomnoml html-widget"></div>
<script type="application/json" data-for="htmlwidget-da8b5f3435f5ac444f78">{"x":{"code":"\n  #fill: #FEFEFF\n  #lineWidth: 2\n  #zoom: 4\n  #direction: right\n   \n\n#direction: right\n#edgeMargin: 4\n#padding: 25\n#fontSize: 18\n\n[Raw data] -> [Transformed Data]\n[Transformed Data] -> [Map data to coordinates]\n[Map data to coordinates] -> [Draw plot]\n","className":null,"svg":false},"evals":[],"jsHooks":[]}</script>
<p class="caption">
FIGURE 3.8: Stages of a plot
</p>
</div>
<p>For example, to create a bar plot in R, we simply call a function:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(ggplot2)
<span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="kw">as.factor</span>(cyl), mpg), <span class="dt">data =</span> mtcars) <span class="op">+</span><span class="st"> </span><span class="kw">geom_col</span>()</code></pre>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-32"></span>
<img src="the-r-in-spark_files/figure-html/unnamed-chunk-32-1.png" alt="Plotting inside R" width="500pt" height="400pt" />
<p class="caption">
FIGURE 3.9: Plotting inside R
</p>
</div>
<p>In this case, the <code>mtcars</code> raw data was <strong>automatically</strong> transformed into three discrete aggregated numbers, then each result was mapped into an <code>x</code> and <code>y</code> plane, and then the plot was drawn, see figure <a href="analysis.html#fig:analysis-mtcars-plot">3.10</a>. As R users, all of the stages of building the plot are conveniently abstracted for us.</p>
<div class="figure" style="text-align: center"><span id="fig:analysis-mtcars-plot"></span>
<div id="htmlwidget-ef1fc5e5c80af3b9478f" style="width:100%;height:200pt;" class="nomnoml html-widget"></div>
<script type="application/json" data-for="htmlwidget-ef1fc5e5c80af3b9478f">{"x":{"code":"\n  #fill: #FEFEFF\n  #lineWidth: 2\n  #zoom: 4\n  #direction: right\n   \n\n#direction: right\n#edgeMargin: 4\n#padding: 25\n#fontSize: 18\n\n[R |\n  [mtcars] -> [Total mpg for each cyl]\n  [Total mpg for each cyl] -> [x = cyl, y = mpg]\n  [x = cyl, y = mpg] -> [Draw plot]\n]\n","className":null,"svg":false},"evals":[],"jsHooks":[]}</script>
<p class="caption">
FIGURE 3.10: R plotting function
</p>
</div>
<div id="recommended-approach" class="section level3">
<h3><span class="header-section-number">3.4.1</span> Recommended approach</h3>
<p>In essence, the approach for visualizing is the same as in wrangling, push the computation to Spark, and then collect the results in R for plotting. As illustrated in figure <a href="analysis.html#fig:analysis-spark-plot">3.11</a>, the heavy lifting of preparing the data, such as in aggregating the data by groups or bins, can be done inside Spark, and then collect the much smaller data set into R. Inside R, the plot becomes a more basic operation. For example, to plot a histogram, the bins are calculated in Spark, and then in R, use a simple column plot, as opposed to a histogram plot, because there is no need for R to re-calculate the bins.</p>
<div class="figure" style="text-align: center"><span id="fig:analysis-spark-plot"></span>
<div id="htmlwidget-6e7c8027494ebf83b8e4" style="width:100%;height:200pt;" class="nomnoml html-widget"></div>
<script type="application/json" data-for="htmlwidget-6e7c8027494ebf83b8e4">{"x":{"code":"\n  #fill: #FEFEFF\n  #lineWidth: 2\n  #zoom: 4\n  #direction: right\n   \n\n#direction: right\n#edgeMargin: 4\n#padding: 25\n#fontSize: 18\n[Spark |\n  [Raw data] -> [Transformed Data]\n]\n[R |\n  [Map to coordinates] -> [Draw plot]\n]\n[Spark] -> [R]\n","className":null,"svg":false},"evals":[],"jsHooks":[]}</script>
<p class="caption">
FIGURE 3.11: Plotting with Spark and R
</p>
</div>
<p>The next section will cover how to use plot simple, but useful, plots.</p>
</div>
<div id="simple-plots" class="section level3">
<h3><span class="header-section-number">3.4.2</span> Simple Plots</h3>
<p>There are a couple of key steps when codifying the “push compute, collect results” approach. First, ensure that the transformation operations happen inside Spark. In the example below, <code>group_by()</code> and <code>summarise()</code> will run as SQL inside the Spark session. The second is to bring the results back into R after the data has been transformed. Make sure to transform and then collect, in that order, because if <code>collect()</code> is run first, then all R will try to ingest the entire data set from Spark. Depending on the size of the data, collecting all of the data will slow down or may even bring down your system.</p>
<pre class="sourceCode r"><code class="sourceCode r">car_group &lt;-<span class="st"> </span>cars <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(cyl) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">mpg =</span> <span class="kw">sum</span>(mpg, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">collect</span>()

car_group</code></pre>
<pre><code>## # A tibble: 3 x 2
##     cyl   mpg
##   &lt;dbl&gt; &lt;dbl&gt;
## 1     6  138.
## 2     4  293.
## 3     8  211.</code></pre>
<p>In this example, now that the data has been pre-aggregated and collected into R, only three records are passed to the plotting function. Figure <a href="analysis.html#fig:analysis-viz1">3.12</a> shows the resulting plot.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="kw">as.factor</span>(cyl), mpg), <span class="dt">data =</span> car_group) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_col</span>() </code></pre>
<div class="figure" style="text-align: center"><span id="fig:analysis-viz1"></span>
<img src="images/analysis-visualizations-1-resized.png" alt="Plot from Spark" width="1500" />
<p class="caption">
FIGURE 3.12: Plot from Spark
</p>
</div>
<p>Thanks to the consistency among the <code>tidyverse</code> packages, the entire operation can be written in a single piped code segment:</p>
<pre class="sourceCode r"><code class="sourceCode r">cars <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(cyl) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">mpg =</span> <span class="kw">sum</span>(mpg, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">collect</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="kw">as.factor</span>(cyl), mpg)) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_col</span>()</code></pre>
<p>The resulting plot is in figure <a href="analysis.html#fig:analysis-viz1b">3.13</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:analysis-viz1b"></span>
<img src="images/analysis-visualizations-1-resized.png" alt="Plot from Spark" width="1500" />
<p class="caption">
FIGURE 3.13: Plot from Spark
</p>
</div>
<p>Using this approach, most visualizations can be easily produced. More complex plots require a bit more work, the next section will breakdown how to create a Histogram.</p>
</div>
<div id="histograms" class="section level3">
<h3><span class="header-section-number">3.4.3</span> Histograms</h3>
<p>There are plots that are both useful, and commonly used. They provide a clear idea of the distribution of values in a single variable. Their calculations are not easily reproducible. For example, producing a histogram that runs over the entire large data set has not been typically feasible, because the data needs to be imported into R first.</p>
<p>The following code breaks down the creation of bins 3 mpg wide into a combination of the most basic aggregation functions. It can run easily inside the <code>mutate()</code> command. It creates as many discrete bins size 3 mpg, and passes the minimum value as the “label” of that bin.</p>
<pre class="sourceCode r"><code class="sourceCode r">mtcars <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">fx    =</span> <span class="kw">floor</span>((mpg <span class="op">-</span><span class="st"> </span><span class="kw">min</span>(mpg, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>))<span class="op">/</span><span class="dv">3</span>),
    <span class="dt">min_x =</span> <span class="kw">min</span>(mpg, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>),
    <span class="dt">max_x =</span> <span class="kw">max</span>(mpg, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>),
    <span class="dt">bins  =</span> (<span class="dv">3</span> <span class="op">*</span><span class="st"> </span><span class="kw">ifelse</span>(fx <span class="op">==</span><span class="st"> </span>(max_x <span class="op">-</span><span class="st"> </span>min_x)<span class="op">/</span><span class="dv">3</span>, fx <span class="op">-</span><span class="st"> </span><span class="dv">1</span>, fx)) <span class="op">+</span><span class="st"> </span>min_x
  ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">select</span>(mpg, bins) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">head</span>()</code></pre>
<pre><code>##    mpg bins
## 1 21.0 19.4
## 2 21.0 19.4
## 3 22.8 22.4
## 4 21.4 19.4
## 5 18.7 16.4
## 6 18.1 16.4</code></pre>
<p>The same R code can run inside Spark, <code>dplyr</code> will translate the R code into a SQL statement. The results can then be collected into R for visualizing.</p>
<pre class="sourceCode r"><code class="sourceCode r">bins &lt;-<span class="st"> </span>cars <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(
    <span class="dt">fx    =</span> <span class="kw">floor</span>((mpg <span class="op">-</span><span class="st"> </span><span class="kw">min</span>(mpg, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>))<span class="op">/</span><span class="dv">3</span>),
    <span class="dt">min_x =</span> <span class="kw">min</span>(mpg, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>),
    <span class="dt">max_x =</span> <span class="kw">max</span>(mpg, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>),
    <span class="dt">bins  =</span> (<span class="dv">3</span> <span class="op">*</span><span class="st"> </span><span class="kw">ifelse</span>(fx <span class="op">==</span><span class="st"> </span>(max_x <span class="op">-</span><span class="st"> </span>min_x)<span class="op">/</span><span class="dv">3</span>, fx <span class="op">-</span><span class="st"> </span><span class="dv">1</span>, fx)) <span class="op">+</span><span class="st"> </span>min_x
  ) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">count</span>(bins)<span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">collect</span>()

bins</code></pre>
<pre><code>## # A tibble: 7 x 2
##    bins     n
##   &lt;dbl&gt; &lt;dbl&gt;
## 1  19.4     6
## 2  22.4     3
## 3  13.4     8
## 4  10.4     3
## 5  16.4     6
## 6  28.4     4
## 7  25.4     2</code></pre>
<p>The bins and counts have been pre-calculated, so a simple column plot is used. The resulting plot is in figure <a href="analysis.html#fig:analysis-viz2">3.14</a>.</p>
<pre class="sourceCode r"><code class="sourceCode r">bins <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ggplot</span>() <span class="op">+</span>
<span class="st">  </span><span class="kw">geom_col</span>(<span class="kw">aes</span>(bins, n))</code></pre>
<div class="figure" style="text-align: center"><span id="fig:analysis-viz2"></span>
<img src="images/analysis-visualizations-2-resized.png" alt="Plot from Spark" width="1500" />
<p class="caption">
FIGURE 3.14: Plot from Spark
</p>
</div>
<p>There is an R package that can make plotting histograms and other plots easier. The next section will cover that.</p>
<div id="using-dbplot" class="section level4">
<h4><span class="header-section-number">3.4.3.1</span> Using dbplot</h4>
<p>The <code>dbplot</code> package provides helper function for plotting with remote data. The R code <code>dbplot</code> uses to transform the data is written so that it can be translated by <code>sparklyr</code> into SQL. It then uses the results to generate a <code>ggplot2</code> plot.</p>
<p>The <code>dbplot_histogram()</code> function will have Spark calculate the bins and the count per bin, and then outputs a <code>ggplot</code> object. It accepts a <code>binwidth</code> argument. The resulting plot is in figure <a href="analysis.html#fig:analysis-viz3">3.15</a>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(dbplot)

cars <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">dbplot_histogram</span>(mpg, <span class="dt">binwidth =</span> <span class="dv">3</span>)</code></pre>
<div class="figure" style="text-align: center"><span id="fig:analysis-viz3"></span>
<img src="images/analysis-visualizations-3-resized.png" alt="Histogram created by dbplot" width="1500" />
<p class="caption">
FIGURE 3.15: Histogram created by dbplot
</p>
</div>
<p>Because it is a <code>ggplot</code> object, it can be further refined by adding more steps to the plot object. In the example on figure <a href="analysis.html#fig:analysis-viz4">3.16</a>, we added a title to the plot, after it was created by <code>dbplot</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">cars <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">dbplot_histogram</span>(mpg, <span class="dt">binwidth =</span> <span class="dv">3</span>) <span class="op">+</span>
<span class="st">  </span><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;Histogram of Miles Per Galon&quot;</span>)</code></pre>
<div class="figure" style="text-align: center"><span id="fig:analysis-viz4"></span>
<img src="images/analysis-visualizations-4-resized.png" alt="Refined Histogram" width="1500" />
<p class="caption">
FIGURE 3.16: Refined Histogram
</p>
</div>
<p>The package also provides a way to obtain the raw results via the <code>db_compute_bins()</code> package.</p>
<pre class="sourceCode r"><code class="sourceCode r">cars <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">db_compute_bins</span>(mpg, <span class="dt">binwidth =</span> <span class="dv">3</span>) </code></pre>
<pre><code>## # A tibble: 7 x 2
##     mpg count
##   &lt;dbl&gt; &lt;dbl&gt;
## 1  19.4     6
## 2  22.4     3
## 3  13.4     8
## 4  10.4     3
## 5  16.4     6
## 6  28.4     4
## 7  25.4     2</code></pre>
<p>Histograms provide a great way to analyze a single variable. To analyze two variables, a scatter or raster plot is commonly used. The next section will discuss how to do that with <code>dbplot</code>.</p>
</div>
</div>
<div id="scatter-vs-raster-plots" class="section level3">
<h3><span class="header-section-number">3.4.4</span> Scatter vs Raster Plots</h3>
<p>Scatter plots are used to compare the relationship between two continuous variables. For example, a scatter plot will display the relationship between the weight of a car and its gas consumption. The plot will show that the higher the weight, the higher the gas consumption because the dots clump together into almost a line that goes from the top left towards the bottom right. See figure <a href="analysis.html#fig:analysis-point">3.17</a> for an example of the plot.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">ggplot</span>(<span class="kw">aes</span>(mpg, wt), <span class="dt">data =</span> mtcars) <span class="op">+</span><span class="st"> </span>
<span class="st">  </span><span class="kw">geom_point</span>()</code></pre>
<div class="figure" style="text-align: center"><span id="fig:analysis-point"></span>
<img src="images/analysis-point-resized.png" alt="Scatter plot example" width="1500" />
<p class="caption">
FIGURE 3.17: Scatter plot example
</p>
</div>
<p>The problems that arise when trying to use this visualization with a large amount of data are:</p>
<dl>
<dt>Performance problems</dt>
<dd>Too many single dots have to be calculated and drawn.
</dd>
<dt>Perception problems</dt>
<dd>It becomes hard to see the true amount of dots in a single area.
</dd>
</dl>
<p>No amount of “pushing the computation” to Spark will help with this problem because the data has to be plotted in individual dots.</p>
<p>The best alternative is to find a plot type that represents the x/y relationship and concentration in a way that it is easy to perceive and to “physically” plot. The <em>raster</em> plot may be the best answer. It returns a grid of x/y positions and the results of a given aggregation usually represented by the color of the square.</p>
<p>The <code>dbplot</code> package provides functionality that helps with this kind of plotting. The first one is <code>dbplot_raster()</code>. It is the way to quickly run a visualization of this kind.</p>
<pre class="sourceCode r"><code class="sourceCode r">cars <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">dbplot_raster</span>(mpg, wt, <span class="dt">resolution =</span> <span class="dv">5</span>)</code></pre>
<div class="figure" style="text-align: center"><span id="fig:analysis-raster1"></span>
<img src="images/analysis-visualizations-5-resized.png" alt="Using dbplot_raster()" width="1500" />
<p class="caption">
FIGURE 3.18: Using dbplot_raster()
</p>
</div>
<p>As shown in figure <a href="analysis.html#fig:analysis-raster1">3.18</a>, the plot returns a grid no bigger than 5x5. This limits the number of records that need to be collected into R to 25.</p>
<p>If the user prefers to either visualize, or simply obtain the result data, the <code>db_compute_raster()</code> and <code>db_compute_raster2()</code> functions return the data. The <code>db_compute_raster2()</code> includes the upper and lower bounds of each square.</p>
<pre class="sourceCode r"><code class="sourceCode r">cars <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">db_compute_raster2</span>(mpg, wt, <span class="dt">resolution =</span> <span class="dv">5</span>)</code></pre>
<pre><code>## # A tibble: 9 x 5
##     mpg    wt `n()` mpg_2  wt_2
##   &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
## 1  19.8  2.30     5  24.5  3.08
## 2  19.8  3.08     3  24.5  3.86
## 3  15.1  3.08    10  19.8  3.86
## 4  10.4  3.08     3  15.1  3.86
## 5  15.1  3.86     1  19.8  4.64
## 6  10.4  4.64     3  15.1  5.42
## 7  29.2  1.51     4  33.9  2.30
## 8  24.5  1.51     2  29.2  2.30
## 9  15.1  2.30     1  19.8  3.08</code></pre>
</div>
</div>
<div id="model" class="section level2">
<h2><span class="header-section-number">3.5</span> Model</h2>
<p>An analysis project focuses on going through as many transformations and models to find the answer. The ideal data analysis framework enables the user to quickly complete each iteration. The transition from an analysis to a deployment project occurs after the model is selected, and findings are presented to the stakeholders. The <a href="modeling">Modeling</a> chapter will dive deeper into how to prepare and run models. The focus of this section will be how to properly, and easily, transition from wrangling to modeling.</p>
<p>There are some steps needed for Spark to run even the most simple models. The modeling functions in <code>sparklyr</code> already cover those steps in order to make them easier to use. Transitioning from data wrangling to model prototyping is as easy as piping the transformed code right into the a <code>sparklyr</code> modeling function. Consider the following example:</p>
<pre class="sourceCode r"><code class="sourceCode r">cars <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">cyl =</span> <span class="kw">paste0</span>(<span class="st">&quot;cyl_&quot;</span>, cyl)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ml_linear_regression</span>(wt <span class="op">~</span><span class="st"> </span>.) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summary</span>()</code></pre>
<pre><code>## Deviance Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.36805 -0.18275 -0.01221  0.11616  0.56768 
## 
## Coefficients:
##  (Intercept)          mpg  cyl_cyl_8.0  cyl_cyl_4.0         disp 
## -0.241536145 -0.046030695  0.085662905  0.193862495  0.006621045 
##           hp         drat         qsec           vs           am 
## -0.004654348 -0.126435269  0.189135280  0.017628206  0.037101380 
##         gear         carb 
## -0.081473252  0.281287273 </code></pre>
<p>The <em>cyl</em> field is converted into a character variable, and then the <code>ml_linear_regression()</code> function is applied to the resulting data set. This is a very straight forward thing to do inside R, but it is not so in Spark. Firstly, Spark does not run models without a Spark PipelineModel. Secondly, it does not create Dummy Variables by default. The <code>ml_linear_regression()</code> already encases all of the needed Scala code to complete those steps and run the Pipeline model, which is then returned looking as a standard fitted model in R.</p>
<p>At this point it is very easy to experiment with different formulas, as shows in the code below:</p>
<pre class="sourceCode r"><code class="sourceCode r">cars <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ml_linear_regression</span>(wt <span class="op">~</span><span class="st"> </span>mpg) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summary</span>()</code></pre>
<pre><code>## Deviance Residuals:
##     Min      1Q  Median      3Q     Max 
## -0.6516 -0.3490 -0.1381  0.3190  1.3684 
## 
## Coefficients:
## (Intercept)         mpg 
##    6.047255   -0.140862 
## 
## R-Squared: 0.7528
## Root Mean Squared Error: 0.4788</code></pre>
<p>Additionally, it is also very easy to try out other kinds of models. The following code shows a Generalized Linear model using the <code>ml_generalized_linear_regression()</code> function:</p>
<pre class="sourceCode r"><code class="sourceCode r">cars <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ml_generalized_linear_regression</span>(am <span class="op">~</span><span class="st"> </span>.) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summary</span>()</code></pre>
<pre><code>## Deviance Residuals:
##      Min       1Q   Median       3Q      Max 
## -0.46909 -0.16762 -0.00578  0.18601  0.35635 
## 
## Coefficients:
##   (Intercept)           mpg           cyl          disp            hp          drat
##  1.6345144901  0.0264792061 -0.1207285158 -0.0005715857  0.0010378347  0.0952244996
##            wt          qsec            vs          gear          carb 
##  0.0172804532 -0.1125888981 -0.2087047973  0.1952457664 -0.0180502642 
## 
## (Dispersion paramter for gaussian family taken to be 0.0737941)
## 
##    Null  deviance: 7.71875 on 31 degress of freedom
## Residual deviance: 1.54968 on 21 degrees of freedom
## AIC: 17.926</code></pre>
<p>It is recommended that data used to fit a model be inside Spark memory. The reasons and explanation will be covered in the next section.</p>
<div id="cache-model-data" class="section level3">
<h3><span class="header-section-number">3.5.1</span> Cache model data</h3>
<p>The examples in this chapter are built using a very small data set. In real-life scenarios, large amounts of data are used for models. If the data needs to be transformed first, the volume of the data could exact a heavy toll on the Spark session. Before fitting the models, it is a good idea to save the results of all the transformations in a new table inside Spark memory.</p>
<p>The <code>compute()</code> command can take the end of a <code>dplyr</code> piped command set and save the results to Spark memory.</p>
<pre class="sourceCode r"><code class="sourceCode r">cached_cars &lt;-<span class="st"> </span>cars <span class="op">%&gt;%</span><span class="st"> </span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">cyl =</span> <span class="kw">paste0</span>(<span class="st">&quot;cyl_&quot;</span>, cyl)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">compute</span>(<span class="st">&quot;cached_cars&quot;</span>)</code></pre>
<pre class="sourceCode r"><code class="sourceCode r">cached_cars <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ml_linear_regression</span>(mpg <span class="op">~</span><span class="st"> </span>.) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summary</span>()</code></pre>
<pre><code>## Deviance Residuals:
##      Min       1Q   Median       3Q      Max 
## -3.47339 -1.37936 -0.06554  1.05105  4.39057 
## 
## Coefficients:
## (Intercept) cyl_cyl_8.0 cyl_cyl_4.0        disp          hp        drat
## 16.15953652  3.29774653  1.66030673  0.01391241 -0.04612835  0.02635025
##           wt        qsec          vs          am       gear        carb 
##  -3.80624757  0.64695710  1.74738689  2.61726546 0.76402917  0.50935118  
## 
## R-Squared: 0.8816
## Root Mean Squared Error: 2.041</code></pre>
<p>As more insights are gained from the data, more questions may be raised. That is why we expect to iterate through data <strong>wrangle</strong>, <strong>visualize</strong>, and <strong>model</strong> multiple times. Each iteration should provide incremental insights of what the data is “telling us”. There will be a point where we reach a satisfactory level of understanding. It is at this point that we will be ready to share the results with the stakeholders of the analysis. How to do this is the topic of the next section.</p>
</div>
</div>
<div id="communicate" class="section level2">
<h2><span class="header-section-number">3.6</span> Communicate</h2>
<p>It is important to clearly communicate the analysis results to the stakeholders. It is as important as the analysis work itself. The stakeholders need to understand what you found out, and how you found it out.</p>
<p>To communicate effectively we need to use artifacts, such as reports and presentation decks. These are common output formats that we can create in R, using R Markdown.</p>
<p>R Markdown documents allow weave narrative text and code together. The amount of output formats provides a very compelling reason to learn and use R Markdown. The available output formats include:</p>
<ul>
<li>Hypertext Markup Language (HTML)</li>
<li>Portable Document Format (PDF)</li>
<li>Microsoft PowerPoint</li>
<li>Microsoft Word</li>
<li>Presentation slides</li>
<li>Websites</li>
<li>Books</li>
</ul>
<p>Most of these outputs are available in the core R packages of R Markdown: <code>knitr</code> and <code>rmarkdown</code>. R Markdown can be extended by other R packages. For example, this book was written using R Markdown thanks to an extension provided by the <code>bookdown</code> package. The best resource to delve deeper into R Markdown is the official book <span class="citation">(Xie <a href="#ref-r-markdown-the-definite-guide">2018</a>)</span>.</p>
<p>In R Markdown, one type of artifact could potentially be rendered in different formats. For example, the same report could be rendered in HTML, or as a PDF file by changing a setting within the report itself. Conversely, multiple types of artifacts could be rendered as the same output. For example, a presentation deck and a report could be rendered in HTML.</p>
<div id="reports" class="section level3">
<h3><span class="header-section-number">3.6.1</span> Reports</h3>
<p>Creating a new R Markdown report is easy. At the top, R Markdown expect a YAML header. The first and last line are three consecutive dashes (<code>---</code>). The content in between the dashes vary depending on the type of document. The only required one is the <code>output</code> value. R Markdown needs to know what kind of output it needs to render your report into. This YAML header is called Front Matter. Following the Front Matter are sections of code, called code chunks. These code chunks can be interlaced with the narratives.</p>
<p>The following example shows how easy it is to create a fully reproducible report. The narrative, code and, most importantly, the output of the code is recorded inside the resulting HTML file. You can copy and paste this following code in a file. Save the file with a <code>.Rmd</code> extension, and choose whatever name you would like.</p>
<pre class="sourceCode markdown"><code class="sourceCode markdown">---
title: &quot;mtcars analysis&quot;
output: html_document
---
<span class="bn">```{r, setup, include = FALSE}</span>
<span class="bn">library(sparklyr)</span>
<span class="bn">conf &lt;- spark_config()</span>
<span class="bn">conf$`sparklyr.cores.local` &lt;- 4</span>
<span class="bn">conf$`sparklyr.shell.driver-memory` &lt;- &quot;1G&quot;</span>
<span class="bn">conf$spark.memory.fraction &lt;- 0.9</span>
<span class="bn">```</span>

<span class="fu">## Setup Spark environment</span>
A local Spark environment is setup and the <span class="bn">`mtcars`</span> data set preloaded
<span class="bn">```{r, eval = FALSE}</span>
<span class="bn">sc &lt;- spark_connect(master = &quot;local&quot;, config = conf)</span>
<span class="bn">cars &lt;- copy_to(sc, mtcars, &quot;mtcars_remote&quot;)</span>
<span class="bn">cars</span>
<span class="bn">```</span>

<span class="fu">## Visualize</span>
A histogram was run over the <span class="bn">`mpg`</span> data
<span class="bn">```{r, eval = FALSE}</span>
<span class="bn">library(dbplot)</span>
<span class="bn">cars %&gt;%</span>
<span class="bn">  dbplot_histogram(mpg, binwidth = 3)</span>
<span class="bn">```</span>

<span class="fu">## Model</span>
The selected model was a simple linear regression that 
uses the weight as the predictor of MPG

<span class="bn">```{r, eval = FALSE}</span>
<span class="bn">cars %&gt;%</span>
<span class="bn">  ml_linear_regression(wt ~ mpg) %&gt;%</span>
<span class="bn">  summary()</span>
<span class="bn">```</span>
<span class="bn">```{r, include = FALSE}</span>
<span class="bn">spark_disconnect(sc)</span>
<span class="bn">```</span></code></pre>
<p>If using the RStudio IDE, click on the <strong>Knit</strong> button at the top of the document to run it. The HTML output should look like this:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-56"></span>
<img src="images/analysis-rmarkdown-resized.png" alt="R Markdown HTML output" width="1500" />
<p class="caption">
FIGURE 3.19: R Markdown HTML output
</p>
</div>
<p>This report can now be sent to stakeholders. The stakeholders will not need Spark or even R to be able to read the report. All of the needed output from your Spark session was captured in the HTML document.</p>
</div>
<div id="presentation-decks" class="section level3">
<h3><span class="header-section-number">3.6.2</span> Presentation decks</h3>
<p>It is common to distill insights of a long report into a presentation deck. This artifact is sometimes easier to digest than a long form because it segments the information into discrete slides.</p>
<p>In R Markdown, switching from an HTML output, to a PowerPoint output is as easy as changing a single option. In the top Front Matter, change the <code>output</code> option to <code>powerpoint_presentation</code>.</p>
<pre><code>---
title: &quot;mtcars analysis&quot;
output: powerpoint_presentation
---</code></pre>
<p>The result will be a PowerPoint presentation with all of the same information that was displayed in the HTML report. There will be a need to edit the PowerPoint template or the output of the code chunks. This minimal example show how easy it is to go from one format to another. Of course, it will take some more editing on the R user side to make sure the slides contain only the pertinent information. The main point is to highlight that it does not require to learn a different markup, or code conventions, to switch from one artifact to another.</p>
<p>The <code>xaringan</code> package enhances R Markdown presentaions. It provides a self contained HTML5 presentation deck. This may be a better option when not everyone in the organization has PowerPoint. Converting the original R Markdown report to a <code>xaringan</code> deck is easy. Only make sure to include a tripple dash needs to indicate where a new slide begins, and update the <code>output</code> value of in the Front Matter to <code>xaringan::moon_reader</code>.</p>
<pre class="sourceCode markdown"><code class="sourceCode markdown">---
title: &quot;mtcars analysis&quot;
output: xaringan::moon_reader
---
<span class="bn">```{r, setup, include = FALSE}</span>
<span class="bn">library(sparklyr)</span>
<span class="bn">conf &lt;- spark_config()</span>
<span class="bn">conf$`sparklyr.cores.local` &lt;- 4</span>
<span class="bn">conf$`sparklyr.shell.driver-memory` &lt;- &quot;1G&quot;</span>
<span class="bn">conf$spark.memory.fraction &lt;- 0.9</span>
<span class="bn">```</span>

<span class="fu">## Setup Spark environment</span>
A local Spark environment is setup and the <span class="bn">`mtcars`</span> dataset preloaded
<span class="bn">```{r, eval = FALSE}</span>
<span class="bn">sc &lt;- spark_connect(master = &quot;local&quot;, config = conf)</span>
<span class="bn">cars &lt;- copy_to(sc, mtcars, &quot;mtcars_remote&quot;)</span>
<span class="bn">cars</span>
<span class="bn">```</span>
---
<span class="fu">## Visualize</span>
A histogram was run over the <span class="bn">`mpg`</span> data
<span class="bn">```{r, fig.height = 6}</span>
<span class="bn">library(dbplot)</span>
<span class="bn">cars %&gt;%</span>
<span class="bn">  dbplot_histogram(mpg, binwidth = 3)</span>
<span class="bn">```</span>
<span class="bn">```{r, include = FALSE}</span>
<span class="bn">spark_disconnect(sc)</span>
<span class="bn">```</span></code></pre>
<p>Here is what the first full slide should look like:</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-57"></span>
<img src="images/analysis-xaringan-resized.png" alt="R Markdown HTML output" width="1500" />
<p class="caption">
FIGURE 3.20: R Markdown HTML output
</p>
</div>
</div>
</div>
<div id="recap" class="section level2">
<h2><span class="header-section-number">3.7</span> Recap</h2>
<p>R and Spark are a very powerful combination. Being able to use a powerful computing platform, along with an incredibly robust ecosystem of packages makes up for an ideal analysis platform. Remember to push computation to Spark, and focus on collecting results in R. The result can then be used for further data manipulation or for plotting. The results can then be shared with stakeholders in a variety of outputs. A a learner of R, hopefully this chapter also encouraged you to learn more about the <code>tidyverse</code> as well as R Markdown.</p>
<p>The next chapter will dive deeper into how to expand the models created during analysis into Machine Learning workflows that can be used in Production.</p>

</div>
</div>
<h3> References</h3>
<div id="refs" class="references">
<div id="ref-intro-r-for-data-science">
<p>Wickham, Hadley, and Garrett Grolemund. 2016. <em>R for Data Science: Import, Tidy, Transform, Visualize, and Model Data</em>. O’Reilly Media, Inc.</p>
</div>
<div id="ref-r-markdown-the-definite-guide">
<p>Xie, Grolemund, Allaire. 2018. <em>R Markdown: The Definite Guide</em>. 1st ed. CRC Press.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="starting.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="modeling.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
