<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 3 Analysis | Mastering Apache Spark with R</title>
  <meta name="description" content="The complete guide to large-scale analysis and modeling." />
  <meta name="generator" content="bookdown 0.13 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 3 Analysis | Mastering Apache Spark with R" />
  <meta property="og:type" content="book" />


  <meta property="og:description" content="The complete guide to large-scale analysis and modeling." />
  <meta name="github-repo" content="javierluraschi/the-r-in-spark" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 3 Analysis | Mastering Apache Spark with R" />

  <meta name="twitter:description" content="The complete guide to large-scale analysis and modeling." />


<meta name="author" content="Javier Luraschi, Kevin Kuo, Edgar Ruiz" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />


<link rel="prev" href="starting.html"/>
<link rel="next" href="modeling.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-119986300-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-119986300-1');
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./">Mastering Apache Spark with R</a></strong></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#intro-background"><i class="fa fa-check"></i><b>1.1</b> Overview</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#intro-hadoop"><i class="fa fa-check"></i><b>1.2</b> Hadoop</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#intro-spark"><i class="fa fa-check"></i><b>1.3</b> Spark</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#intro-r"><i class="fa fa-check"></i><b>1.4</b> R</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#intro-sparklyr"><i class="fa fa-check"></i><b>1.5</b> sparklyr</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#intro-recap"><i class="fa fa-check"></i><b>1.6</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="starting.html"><a href="starting.html"><i class="fa fa-check"></i><b>2</b> Getting Started</a><ul>
<li class="chapter" data-level="2.1" data-path="starting.html"><a href="starting.html#overview"><i class="fa fa-check"></i><b>2.1</b> Overview</a></li>
<li class="chapter" data-level="2.2" data-path="starting.html"><a href="starting.html#starting-prerequisites"><i class="fa fa-check"></i><b>2.2</b> Prerequisites</a><ul>
<li class="chapter" data-level="2.2.1" data-path="starting.html"><a href="starting.html#starting-install-sparklyr"><i class="fa fa-check"></i><b>2.2.1</b> Installing sparklyr</a></li>
<li class="chapter" data-level="2.2.2" data-path="starting.html"><a href="starting.html#starting-installing-spark"><i class="fa fa-check"></i><b>2.2.2</b> Installing Spark</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="starting.html"><a href="starting.html#starting-connect-to-spark"><i class="fa fa-check"></i><b>2.3</b> Connecting</a></li>
<li class="chapter" data-level="2.4" data-path="starting.html"><a href="starting.html#starting-sparklyr-hello-world"><i class="fa fa-check"></i><b>2.4</b> Using Spark</a><ul>
<li class="chapter" data-level="2.4.1" data-path="starting.html"><a href="starting.html#starting-spark-web-interface"><i class="fa fa-check"></i><b>2.4.1</b> Web Interface</a></li>
<li class="chapter" data-level="2.4.2" data-path="starting.html"><a href="starting.html#starting-analysis"><i class="fa fa-check"></i><b>2.4.2</b> Analysis</a></li>
<li class="chapter" data-level="2.4.3" data-path="starting.html"><a href="starting.html#starting-modeling"><i class="fa fa-check"></i><b>2.4.3</b> Modeling</a></li>
<li class="chapter" data-level="2.4.4" data-path="starting.html"><a href="starting.html#starting-data"><i class="fa fa-check"></i><b>2.4.4</b> Data</a></li>
<li class="chapter" data-level="2.4.5" data-path="starting.html"><a href="starting.html#starting-extensions"><i class="fa fa-check"></i><b>2.4.5</b> Extensions</a></li>
<li class="chapter" data-level="2.4.6" data-path="starting.html"><a href="starting.html#starting-distributed-r"><i class="fa fa-check"></i><b>2.4.6</b> Distributed R</a></li>
<li class="chapter" data-level="2.4.7" data-path="starting.html"><a href="starting.html#starting-streaming"><i class="fa fa-check"></i><b>2.4.7</b> Streaming</a></li>
<li class="chapter" data-level="2.4.8" data-path="starting.html"><a href="starting.html#starting-logs"><i class="fa fa-check"></i><b>2.4.8</b> Logs</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="starting.html"><a href="starting.html#starting-disconnecting"><i class="fa fa-check"></i><b>2.5</b> Disconnecting</a></li>
<li class="chapter" data-level="2.6" data-path="starting.html"><a href="starting.html#starting-using-spark-from-rstudio"><i class="fa fa-check"></i><b>2.6</b> Using RStudio</a></li>
<li class="chapter" data-level="2.7" data-path="starting.html"><a href="starting.html#starting-resources"><i class="fa fa-check"></i><b>2.7</b> Resources</a></li>
<li class="chapter" data-level="2.8" data-path="starting.html"><a href="starting.html#starting-recap"><i class="fa fa-check"></i><b>2.8</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="analysis.html"><a href="analysis.html"><i class="fa fa-check"></i><b>3</b> Analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="analysis.html"><a href="analysis.html#analysis-overview"><i class="fa fa-check"></i><b>3.1</b> Overview</a></li>
<li class="chapter" data-level="3.2" data-path="analysis.html"><a href="analysis.html#import"><i class="fa fa-check"></i><b>3.2</b> Import</a></li>
<li class="chapter" data-level="3.3" data-path="analysis.html"><a href="analysis.html#wrangle"><i class="fa fa-check"></i><b>3.3</b> Wrangle</a><ul>
<li class="chapter" data-level="3.3.1" data-path="analysis.html"><a href="analysis.html#built-in-functions"><i class="fa fa-check"></i><b>3.3.1</b> Built-in Functions</a></li>
<li class="chapter" data-level="3.3.2" data-path="analysis.html"><a href="analysis.html#correlations"><i class="fa fa-check"></i><b>3.3.2</b> Correlations</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="analysis.html"><a href="analysis.html#visualize"><i class="fa fa-check"></i><b>3.4</b> Visualize</a><ul>
<li class="chapter" data-level="3.4.1" data-path="analysis.html"><a href="analysis.html#using-ggplot2"><i class="fa fa-check"></i><b>3.4.1</b> Using ggplot2</a></li>
<li class="chapter" data-level="3.4.2" data-path="analysis.html"><a href="analysis.html#using-dbplot"><i class="fa fa-check"></i><b>3.4.2</b> Using dbplot</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="analysis.html"><a href="analysis.html#model"><i class="fa fa-check"></i><b>3.5</b> Model</a><ul>
<li class="chapter" data-level="3.5.1" data-path="analysis.html"><a href="analysis.html#caching"><i class="fa fa-check"></i><b>3.5.1</b> Caching</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="analysis.html"><a href="analysis.html#communicate"><i class="fa fa-check"></i><b>3.6</b> Communicate</a></li>
<li class="chapter" data-level="3.7" data-path="analysis.html"><a href="analysis.html#recap"><i class="fa fa-check"></i><b>3.7</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="modeling.html"><a href="modeling.html"><i class="fa fa-check"></i><b>4</b> Modeling</a><ul>
<li class="chapter" data-level="4.1" data-path="modeling.html"><a href="modeling.html#overview-1"><i class="fa fa-check"></i><b>4.1</b> Overview</a></li>
<li class="chapter" data-level="4.2" data-path="modeling.html"><a href="modeling.html#exploratory-data-analysis"><i class="fa fa-check"></i><b>4.2</b> Exploratory Data Analysis</a></li>
<li class="chapter" data-level="4.3" data-path="modeling.html"><a href="modeling.html#feature-engineering"><i class="fa fa-check"></i><b>4.3</b> Feature Engineering</a></li>
<li class="chapter" data-level="4.4" data-path="modeling.html"><a href="modeling.html#supervised-learning"><i class="fa fa-check"></i><b>4.4</b> Supervised Learning</a><ul>
<li class="chapter" data-level="4.4.1" data-path="modeling.html"><a href="modeling.html#generalized-linear-regression"><i class="fa fa-check"></i><b>4.4.1</b> Generalized Linear Regression</a></li>
<li class="chapter" data-level="4.4.2" data-path="modeling.html"><a href="modeling.html#other-models"><i class="fa fa-check"></i><b>4.4.2</b> Other Models</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="modeling.html"><a href="modeling.html#unsupervised-learning"><i class="fa fa-check"></i><b>4.5</b> Unsupervised Learning</a><ul>
<li class="chapter" data-level="4.5.1" data-path="modeling.html"><a href="modeling.html#data-preparation"><i class="fa fa-check"></i><b>4.5.1</b> Data Preparation</a></li>
<li class="chapter" data-level="4.5.2" data-path="modeling.html"><a href="modeling.html#topic-modeling"><i class="fa fa-check"></i><b>4.5.2</b> Topic Modeling</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="modeling.html"><a href="modeling.html#recap-1"><i class="fa fa-check"></i><b>4.6</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="pipelines.html"><a href="pipelines.html"><i class="fa fa-check"></i><b>5</b> Pipelines</a><ul>
<li class="chapter" data-level="5.1" data-path="pipelines.html"><a href="pipelines.html#overview-2"><i class="fa fa-check"></i><b>5.1</b> Overview</a></li>
<li class="chapter" data-level="5.2" data-path="pipelines.html"><a href="pipelines.html#creation"><i class="fa fa-check"></i><b>5.2</b> Creation</a></li>
<li class="chapter" data-level="5.3" data-path="pipelines.html"><a href="pipelines.html#use-cases"><i class="fa fa-check"></i><b>5.3</b> Use Cases</a><ul>
<li class="chapter" data-level="5.3.1" data-path="pipelines.html"><a href="pipelines.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>5.3.1</b> Hyperparameter Tuning</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="pipelines.html"><a href="pipelines.html#operating-modes"><i class="fa fa-check"></i><b>5.4</b> Operating Modes</a></li>
<li class="chapter" data-level="5.5" data-path="pipelines.html"><a href="pipelines.html#interoperability"><i class="fa fa-check"></i><b>5.5</b> Interoperability</a></li>
<li class="chapter" data-level="5.6" data-path="pipelines.html"><a href="pipelines.html#deployment"><i class="fa fa-check"></i><b>5.6</b> Deployment</a><ul>
<li class="chapter" data-level="5.6.1" data-path="pipelines.html"><a href="pipelines.html#batch-scoring"><i class="fa fa-check"></i><b>5.6.1</b> Batch Scoring</a></li>
<li class="chapter" data-level="5.6.2" data-path="pipelines.html"><a href="pipelines.html#real-time-scoring"><i class="fa fa-check"></i><b>5.6.2</b> Real-Time Scoring</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="pipelines.html"><a href="pipelines.html#recap-2"><i class="fa fa-check"></i><b>5.7</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="clusters.html"><a href="clusters.html"><i class="fa fa-check"></i><b>6</b> Clusters</a><ul>
<li class="chapter" data-level="6.1" data-path="clusters.html"><a href="clusters.html#clusters-overview"><i class="fa fa-check"></i><b>6.1</b> Overview</a></li>
<li class="chapter" data-level="6.2" data-path="clusters.html"><a href="clusters.html#on-premise"><i class="fa fa-check"></i><b>6.2</b> On-Premise</a><ul>
<li class="chapter" data-level="6.2.1" data-path="clusters.html"><a href="clusters.html#clusters-manager"><i class="fa fa-check"></i><b>6.2.1</b> Managers</a></li>
<li class="chapter" data-level="6.2.2" data-path="clusters.html"><a href="clusters.html#distributions"><i class="fa fa-check"></i><b>6.2.2</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="clusters.html"><a href="clusters.html#cloud"><i class="fa fa-check"></i><b>6.3</b> Cloud</a><ul>
<li class="chapter" data-level="6.3.1" data-path="clusters.html"><a href="clusters.html#clusters-amazon-emr"><i class="fa fa-check"></i><b>6.3.1</b> Amazon</a></li>
<li class="chapter" data-level="6.3.2" data-path="clusters.html"><a href="clusters.html#databricks"><i class="fa fa-check"></i><b>6.3.2</b> Databricks</a></li>
<li class="chapter" data-level="6.3.3" data-path="clusters.html"><a href="clusters.html#google"><i class="fa fa-check"></i><b>6.3.3</b> Google</a></li>
<li class="chapter" data-level="6.3.4" data-path="clusters.html"><a href="clusters.html#ibm"><i class="fa fa-check"></i><b>6.3.4</b> IBM</a></li>
<li class="chapter" data-level="6.3.5" data-path="clusters.html"><a href="clusters.html#microsoft"><i class="fa fa-check"></i><b>6.3.5</b> Microsoft</a></li>
<li class="chapter" data-level="6.3.6" data-path="clusters.html"><a href="clusters.html#qubole"><i class="fa fa-check"></i><b>6.3.6</b> Qubole</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="clusters.html"><a href="clusters.html#kubernetes"><i class="fa fa-check"></i><b>6.4</b> Kubernetes</a></li>
<li class="chapter" data-level="6.5" data-path="clusters.html"><a href="clusters.html#tools"><i class="fa fa-check"></i><b>6.5</b> Tools</a><ul>
<li class="chapter" data-level="6.5.1" data-path="clusters.html"><a href="clusters.html#rstudio"><i class="fa fa-check"></i><b>6.5.1</b> RStudio</a></li>
<li class="chapter" data-level="6.5.2" data-path="clusters.html"><a href="clusters.html#jupyter"><i class="fa fa-check"></i><b>6.5.2</b> Jupyter</a></li>
<li class="chapter" data-level="6.5.3" data-path="clusters.html"><a href="clusters.html#clusters-livy"><i class="fa fa-check"></i><b>6.5.3</b> Livy</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="clusters.html"><a href="clusters.html#recap-3"><i class="fa fa-check"></i><b>6.6</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="connections.html"><a href="connections.html"><i class="fa fa-check"></i><b>7</b> Connections</a><ul>
<li class="chapter" data-level="7.1" data-path="connections.html"><a href="connections.html#connections-overview"><i class="fa fa-check"></i><b>7.1</b> Overview</a><ul>
<li class="chapter" data-level="7.1.1" data-path="connections.html"><a href="connections.html#connections-spark-edge-nodes"><i class="fa fa-check"></i><b>7.1.1</b> Edge Nodes</a></li>
<li class="chapter" data-level="7.1.2" data-path="connections.html"><a href="connections.html#connections-spark-home"><i class="fa fa-check"></i><b>7.1.2</b> Spark Home</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="connections.html"><a href="connections.html#connections-local"><i class="fa fa-check"></i><b>7.2</b> Local</a></li>
<li class="chapter" data-level="7.3" data-path="connections.html"><a href="connections.html#connections-standalone"><i class="fa fa-check"></i><b>7.3</b> Standalone</a></li>
<li class="chapter" data-level="7.4" data-path="connections.html"><a href="connections.html#connections-yarn"><i class="fa fa-check"></i><b>7.4</b> Yarn</a><ul>
<li class="chapter" data-level="7.4.1" data-path="connections.html"><a href="connections.html#connections-yarn-client"><i class="fa fa-check"></i><b>7.4.1</b> Yarn Client</a></li>
<li class="chapter" data-level="7.4.2" data-path="connections.html"><a href="connections.html#connections-yarn-cluster"><i class="fa fa-check"></i><b>7.4.2</b> Yarn Cluster</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="connections.html"><a href="connections.html#connections-livy"><i class="fa fa-check"></i><b>7.5</b> Livy</a></li>
<li class="chapter" data-level="7.6" data-path="connections.html"><a href="connections.html#connections-mesos"><i class="fa fa-check"></i><b>7.6</b> Mesos</a></li>
<li class="chapter" data-level="7.7" data-path="connections.html"><a href="connections.html#connections-kubernetes"><i class="fa fa-check"></i><b>7.7</b> Kubernetes</a></li>
<li class="chapter" data-level="7.8" data-path="connections.html"><a href="connections.html#cloud-1"><i class="fa fa-check"></i><b>7.8</b> Cloud</a></li>
<li class="chapter" data-level="7.9" data-path="connections.html"><a href="connections.html#batches"><i class="fa fa-check"></i><b>7.9</b> Batches</a></li>
<li class="chapter" data-level="7.10" data-path="connections.html"><a href="connections.html#tools-1"><i class="fa fa-check"></i><b>7.10</b> Tools</a></li>
<li class="chapter" data-level="7.11" data-path="connections.html"><a href="connections.html#multiple"><i class="fa fa-check"></i><b>7.11</b> Multiple</a></li>
<li class="chapter" data-level="7.12" data-path="connections.html"><a href="connections.html#connections-troubleshooting"><i class="fa fa-check"></i><b>7.12</b> Troubleshooting</a><ul>
<li class="chapter" data-level="7.12.1" data-path="connections.html"><a href="connections.html#logging"><i class="fa fa-check"></i><b>7.12.1</b> Logging</a></li>
<li class="chapter" data-level="7.12.2" data-path="connections.html"><a href="connections.html#troubleshoot-spark-submit"><i class="fa fa-check"></i><b>7.12.2</b> Spark Submit</a></li>
<li class="chapter" data-level="7.12.3" data-path="connections.html"><a href="connections.html#windows"><i class="fa fa-check"></i><b>7.12.3</b> Windows</a></li>
</ul></li>
<li class="chapter" data-level="7.13" data-path="connections.html"><a href="connections.html#recap-4"><i class="fa fa-check"></i><b>7.13</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>8</b> Data</a><ul>
<li class="chapter" data-level="8.1" data-path="data.html"><a href="data.html#overview-3"><i class="fa fa-check"></i><b>8.1</b> Overview</a></li>
<li class="chapter" data-level="8.2" data-path="data.html"><a href="data.html#read"><i class="fa fa-check"></i><b>8.2</b> Read</a><ul>
<li class="chapter" data-level="8.2.1" data-path="data.html"><a href="data.html#paths"><i class="fa fa-check"></i><b>8.2.1</b> Paths</a></li>
<li class="chapter" data-level="8.2.2" data-path="data.html"><a href="data.html#schema"><i class="fa fa-check"></i><b>8.2.2</b> Schema</a></li>
<li class="chapter" data-level="8.2.3" data-path="data.html"><a href="data.html#memory"><i class="fa fa-check"></i><b>8.2.3</b> Memory</a></li>
<li class="chapter" data-level="8.2.4" data-path="data.html"><a href="data.html#columns"><i class="fa fa-check"></i><b>8.2.4</b> Columns</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="data.html"><a href="data.html#write"><i class="fa fa-check"></i><b>8.3</b> Write</a></li>
<li class="chapter" data-level="8.4" data-path="data.html"><a href="data.html#copy"><i class="fa fa-check"></i><b>8.4</b> Copy</a></li>
<li class="chapter" data-level="8.5" data-path="data.html"><a href="data.html#data-file-formats"><i class="fa fa-check"></i><b>8.5</b> File Formats</a><ul>
<li class="chapter" data-level="8.5.1" data-path="data.html"><a href="data.html#csv"><i class="fa fa-check"></i><b>8.5.1</b> CSV</a></li>
<li class="chapter" data-level="8.5.2" data-path="data.html"><a href="data.html#json"><i class="fa fa-check"></i><b>8.5.2</b> JSON</a></li>
<li class="chapter" data-level="8.5.3" data-path="data.html"><a href="data.html#parquet"><i class="fa fa-check"></i><b>8.5.3</b> Parquet</a></li>
<li class="chapter" data-level="8.5.4" data-path="data.html"><a href="data.html#others"><i class="fa fa-check"></i><b>8.5.4</b> Others</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="data.html"><a href="data.html#data-file-systems"><i class="fa fa-check"></i><b>8.6</b> File Systems</a></li>
<li class="chapter" data-level="8.7" data-path="data.html"><a href="data.html#data-storage-systems"><i class="fa fa-check"></i><b>8.7</b> Storage Systems</a><ul>
<li class="chapter" data-level="8.7.1" data-path="data.html"><a href="data.html#hive"><i class="fa fa-check"></i><b>8.7.1</b> Hive</a></li>
<li class="chapter" data-level="8.7.2" data-path="data.html"><a href="data.html#cassandra"><i class="fa fa-check"></i><b>8.7.2</b> Cassandra</a></li>
<li class="chapter" data-level="8.7.3" data-path="data.html"><a href="data.html#jdbc"><i class="fa fa-check"></i><b>8.7.3</b> JDBC</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="data.html"><a href="data.html#recap-5"><i class="fa fa-check"></i><b>8.8</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="tuning.html"><a href="tuning.html"><i class="fa fa-check"></i><b>9</b> Tuning</a><ul>
<li class="chapter" data-level="9.1" data-path="tuning.html"><a href="tuning.html#overview-4"><i class="fa fa-check"></i><b>9.1</b> Overview</a><ul>
<li class="chapter" data-level="9.1.1" data-path="tuning.html"><a href="tuning.html#tuning-graph-visualization"><i class="fa fa-check"></i><b>9.1.1</b> Graph</a></li>
<li class="chapter" data-level="9.1.2" data-path="tuning.html"><a href="tuning.html#tuning-event-timeline"><i class="fa fa-check"></i><b>9.1.2</b> Timeline</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="tuning.html"><a href="tuning.html#tuning-configuring"><i class="fa fa-check"></i><b>9.2</b> Configuring</a><ul>
<li class="chapter" data-level="9.2.1" data-path="tuning.html"><a href="tuning.html#connect-settings"><i class="fa fa-check"></i><b>9.2.1</b> Connect Settings</a></li>
<li class="chapter" data-level="9.2.2" data-path="tuning.html"><a href="tuning.html#submit-settings"><i class="fa fa-check"></i><b>9.2.2</b> Submit Settings</a></li>
<li class="chapter" data-level="9.2.3" data-path="tuning.html"><a href="tuning.html#runtime-settings"><i class="fa fa-check"></i><b>9.2.3</b> Runtime Settings</a></li>
<li class="chapter" data-level="9.2.4" data-path="tuning.html"><a href="tuning.html#sparklyr-settings"><i class="fa fa-check"></i><b>9.2.4</b> sparklyr Settings</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="tuning.html"><a href="tuning.html#tuning-partitioning"><i class="fa fa-check"></i><b>9.3</b> Partitioning</a><ul>
<li class="chapter" data-level="9.3.1" data-path="tuning.html"><a href="tuning.html#implicit"><i class="fa fa-check"></i><b>9.3.1</b> Implicit</a></li>
<li class="chapter" data-level="9.3.2" data-path="tuning.html"><a href="tuning.html#explicit"><i class="fa fa-check"></i><b>9.3.2</b> Explicit</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="tuning.html"><a href="tuning.html#tuning-caching"><i class="fa fa-check"></i><b>9.4</b> Caching</a><ul>
<li class="chapter" data-level="9.4.1" data-path="tuning.html"><a href="tuning.html#checkpointing"><i class="fa fa-check"></i><b>9.4.1</b> Checkpointing</a></li>
<li class="chapter" data-level="9.4.2" data-path="tuning.html"><a href="tuning.html#tuning-memory"><i class="fa fa-check"></i><b>9.4.2</b> Memory</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="tuning.html"><a href="tuning.html#tuning-shuffling"><i class="fa fa-check"></i><b>9.5</b> Shuffling</a></li>
<li class="chapter" data-level="9.6" data-path="tuning.html"><a href="tuning.html#tuning-serialization"><i class="fa fa-check"></i><b>9.6</b> Serialization</a></li>
<li class="chapter" data-level="9.7" data-path="tuning.html"><a href="tuning.html#configuration-files"><i class="fa fa-check"></i><b>9.7</b> Configuration Files</a></li>
<li class="chapter" data-level="9.8" data-path="tuning.html"><a href="tuning.html#recap-6"><i class="fa fa-check"></i><b>9.8</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="extensions.html"><a href="extensions.html"><i class="fa fa-check"></i><b>10</b> Extensions</a><ul>
<li class="chapter" data-level="10.1" data-path="extensions.html"><a href="extensions.html#overview-5"><i class="fa fa-check"></i><b>10.1</b> Overview</a></li>
<li class="chapter" data-level="10.2" data-path="extensions.html"><a href="extensions.html#h2o"><i class="fa fa-check"></i><b>10.2</b> H2O</a></li>
<li class="chapter" data-level="10.3" data-path="extensions.html"><a href="extensions.html#graphs"><i class="fa fa-check"></i><b>10.3</b> Graphs</a></li>
<li class="chapter" data-level="10.4" data-path="extensions.html"><a href="extensions.html#xgboost"><i class="fa fa-check"></i><b>10.4</b> XGBoost</a></li>
<li class="chapter" data-level="10.5" data-path="extensions.html"><a href="extensions.html#deep-learning"><i class="fa fa-check"></i><b>10.5</b> Deep Learning</a></li>
<li class="chapter" data-level="10.6" data-path="extensions.html"><a href="extensions.html#genomics"><i class="fa fa-check"></i><b>10.6</b> Genomics</a></li>
<li class="chapter" data-level="10.7" data-path="extensions.html"><a href="extensions.html#spatial"><i class="fa fa-check"></i><b>10.7</b> Spatial</a></li>
<li class="chapter" data-level="10.8" data-path="extensions.html"><a href="extensions.html#troubleshooting"><i class="fa fa-check"></i><b>10.8</b> Troubleshooting</a></li>
<li class="chapter" data-level="10.9" data-path="extensions.html"><a href="extensions.html#recap-7"><i class="fa fa-check"></i><b>10.9</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="distributed.html"><a href="distributed.html"><i class="fa fa-check"></i><b>11</b> Distributed R</a><ul>
<li class="chapter" data-level="11.1" data-path="distributed.html"><a href="distributed.html#overview-6"><i class="fa fa-check"></i><b>11.1</b> Overview</a></li>
<li class="chapter" data-level="11.2" data-path="distributed.html"><a href="distributed.html#use-cases-1"><i class="fa fa-check"></i><b>11.2</b> Use Cases</a><ul>
<li class="chapter" data-level="11.2.1" data-path="distributed.html"><a href="distributed.html#custom-parsers"><i class="fa fa-check"></i><b>11.2.1</b> Custom Parsers</a></li>
<li class="chapter" data-level="11.2.2" data-path="distributed.html"><a href="distributed.html#partitioned-modeling"><i class="fa fa-check"></i><b>11.2.2</b> Partitioned Modeling</a></li>
<li class="chapter" data-level="11.2.3" data-path="distributed.html"><a href="distributed.html#distributed-grid-search"><i class="fa fa-check"></i><b>11.2.3</b> Grid Search</a></li>
<li class="chapter" data-level="11.2.4" data-path="distributed.html"><a href="distributed.html#web-apis"><i class="fa fa-check"></i><b>11.2.4</b> Web APIs</a></li>
<li class="chapter" data-level="11.2.5" data-path="distributed.html"><a href="distributed.html#simulations"><i class="fa fa-check"></i><b>11.2.5</b> Simulations</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="distributed.html"><a href="distributed.html#partitions"><i class="fa fa-check"></i><b>11.3</b> Partitions</a></li>
<li class="chapter" data-level="11.4" data-path="distributed.html"><a href="distributed.html#grouping"><i class="fa fa-check"></i><b>11.4</b> Grouping</a></li>
<li class="chapter" data-level="11.5" data-path="distributed.html"><a href="distributed.html#columns-1"><i class="fa fa-check"></i><b>11.5</b> Columns</a></li>
<li class="chapter" data-level="11.6" data-path="distributed.html"><a href="distributed.html#context"><i class="fa fa-check"></i><b>11.6</b> Context</a></li>
<li class="chapter" data-level="11.7" data-path="distributed.html"><a href="distributed.html#functions"><i class="fa fa-check"></i><b>11.7</b> Functions</a></li>
<li class="chapter" data-level="11.8" data-path="distributed.html"><a href="distributed.html#packages"><i class="fa fa-check"></i><b>11.8</b> Packages</a></li>
<li class="chapter" data-level="11.9" data-path="distributed.html"><a href="distributed.html#cluster-requirements"><i class="fa fa-check"></i><b>11.9</b> Cluster Requirements</a><ul>
<li class="chapter" data-level="11.9.1" data-path="distributed.html"><a href="distributed.html#installing-r"><i class="fa fa-check"></i><b>11.9.1</b> Installing R</a></li>
<li class="chapter" data-level="11.9.2" data-path="distributed.html"><a href="distributed.html#apache-arrow"><i class="fa fa-check"></i><b>11.9.2</b> Apache Arrow</a></li>
</ul></li>
<li class="chapter" data-level="11.10" data-path="distributed.html"><a href="distributed.html#troubleshooting-1"><i class="fa fa-check"></i><b>11.10</b> Troubleshooting</a><ul>
<li class="chapter" data-level="11.10.1" data-path="distributed.html"><a href="distributed.html#worker-logs"><i class="fa fa-check"></i><b>11.10.1</b> Worker Logs</a></li>
<li class="chapter" data-level="11.10.2" data-path="distributed.html"><a href="distributed.html#resolving-timeouts"><i class="fa fa-check"></i><b>11.10.2</b> Resolving Timeouts</a></li>
<li class="chapter" data-level="11.10.3" data-path="distributed.html"><a href="distributed.html#inspecting-partition"><i class="fa fa-check"></i><b>11.10.3</b> Inspecting Partition</a></li>
<li class="chapter" data-level="11.10.4" data-path="distributed.html"><a href="distributed.html#debugging-workers"><i class="fa fa-check"></i><b>11.10.4</b> Debugging Workers</a></li>
</ul></li>
<li class="chapter" data-level="11.11" data-path="distributed.html"><a href="distributed.html#recap-8"><i class="fa fa-check"></i><b>11.11</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="streaming.html"><a href="streaming.html"><i class="fa fa-check"></i><b>12</b> Streaming</a><ul>
<li class="chapter" data-level="12.1" data-path="streaming.html"><a href="streaming.html#overview-7"><i class="fa fa-check"></i><b>12.1</b> Overview</a></li>
<li class="chapter" data-level="12.2" data-path="streaming.html"><a href="streaming.html#transformations"><i class="fa fa-check"></i><b>12.2</b> Transformations</a><ul>
<li class="chapter" data-level="12.2.1" data-path="streaming.html"><a href="streaming.html#analysis-1"><i class="fa fa-check"></i><b>12.2.1</b> Analysis</a></li>
<li class="chapter" data-level="12.2.2" data-path="streaming.html"><a href="streaming.html#modeling-1"><i class="fa fa-check"></i><b>12.2.2</b> Modeling</a></li>
<li class="chapter" data-level="12.2.3" data-path="streaming.html"><a href="streaming.html#pipelines-1"><i class="fa fa-check"></i><b>12.2.3</b> Pipelines</a></li>
<li class="chapter" data-level="12.2.4" data-path="streaming.html"><a href="streaming.html#distributed-r-streaming-r-code"><i class="fa fa-check"></i><b>12.2.4</b> Distributed R {streaming-r-code}</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="streaming.html"><a href="streaming.html#kafka"><i class="fa fa-check"></i><b>12.3</b> Kafka</a></li>
<li class="chapter" data-level="12.4" data-path="streaming.html"><a href="streaming.html#shiny"><i class="fa fa-check"></i><b>12.4</b> Shiny</a></li>
<li class="chapter" data-level="12.5" data-path="streaming.html"><a href="streaming.html#recap-9"><i class="fa fa-check"></i><b>12.5</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="contributing.html"><a href="contributing.html"><i class="fa fa-check"></i><b>13</b> Contributing</a><ul>
<li class="chapter" data-level="13.1" data-path="contributing.html"><a href="contributing.html#contributing-overview"><i class="fa fa-check"></i><b>13.1</b> Overview</a></li>
<li class="chapter" data-level="13.2" data-path="contributing.html"><a href="contributing.html#contributing-spark-api"><i class="fa fa-check"></i><b>13.2</b> Spark API</a></li>
<li class="chapter" data-level="13.3" data-path="contributing.html"><a href="contributing.html#spark-extensions"><i class="fa fa-check"></i><b>13.3</b> Spark Extensions</a></li>
<li class="chapter" data-level="13.4" data-path="contributing.html"><a href="contributing.html#scala-code"><i class="fa fa-check"></i><b>13.4</b> Scala Code</a></li>
<li class="chapter" data-level="13.5" data-path="contributing.html"><a href="contributing.html#recap-10"><i class="fa fa-check"></i><b>13.5</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>14</b> Appendix</a><ul>
<li class="chapter" data-level="14.1" data-path="appendix.html"><a href="appendix.html#appendix-preface"><i class="fa fa-check"></i><b>14.1</b> Preface</a><ul>
<li class="chapter" data-level="14.1.1" data-path="appendix.html"><a href="appendix.html#appendix-ggplot2-theme"><i class="fa fa-check"></i><b>14.1.1</b> Formatting</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="appendix.html"><a href="appendix.html#appendix-intro"><i class="fa fa-check"></i><b>14.2</b> Introduction</a><ul>
<li class="chapter" data-level="14.2.1" data-path="appendix.html"><a href="appendix.html#appendix-storage-capacity"><i class="fa fa-check"></i><b>14.2.1</b> Worlds Store Capacity</a></li>
<li class="chapter" data-level="14.2.2" data-path="appendix.html"><a href="appendix.html#appendix-cran-downloads"><i class="fa fa-check"></i><b>14.2.2</b> Daily downloads of CRAN packages</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="appendix.html"><a href="appendix.html#appendix-starting"><i class="fa fa-check"></i><b>14.3</b> Getting Started</a><ul>
<li class="chapter" data-level="14.3.1" data-path="appendix.html"><a href="appendix.html#appendix-prerequisites"><i class="fa fa-check"></i><b>14.3.1</b> Prerequisites</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="appendix.html"><a href="appendix.html#appendix-analysis"><i class="fa fa-check"></i><b>14.4</b> Analysis</a><ul>
<li class="chapter" data-level="14.4.1" data-path="appendix.html"><a href="appendix.html#hive-functions"><i class="fa fa-check"></i><b>14.4.1</b> Hive Functions</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="appendix.html"><a href="appendix.html#appendix-modeling"><i class="fa fa-check"></i><b>14.5</b> Modeling</a><ul>
<li class="chapter" data-level="14.5.1" data-path="appendix.html"><a href="appendix.html#appendix-ml-functionlist"><i class="fa fa-check"></i><b>14.5.1</b> MLlib Functions</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="appendix.html"><a href="appendix.html#appendix-clusters"><i class="fa fa-check"></i><b>14.6</b> Clusters</a><ul>
<li class="chapter" data-level="14.6.1" data-path="appendix.html"><a href="appendix.html#appendix-cluster-trends"><i class="fa fa-check"></i><b>14.6.1</b> Google trends for mainframes, cloud computing and kubernetes</a></li>
</ul></li>
<li class="chapter" data-level="14.7" data-path="appendix.html"><a href="appendix.html#appendix-streaming"><i class="fa fa-check"></i><b>14.7</b> Streaming</a><ul>
<li class="chapter" data-level="14.7.1" data-path="appendix.html"><a href="appendix.html#appendix-streaming-generator"><i class="fa fa-check"></i><b>14.7.1</b> Stream Generator</a></li>
<li class="chapter" data-level="14.7.2" data-path="appendix.html"><a href="appendix.html#appendix-streaming-kafka"><i class="fa fa-check"></i><b>14.7.2</b> Installing Kafka</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>15</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Mastering Apache Spark with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="analysis" class="section level1">
<h1><span class="header-section-number">Chapter 3</span> Analysis</h1>
<blockquote>
<p>“First lesson, stick them with the pointy end.”</p>
<p>— Jon Snow</p>
</blockquote>
<p>Previous chapters focused on introducing Spark with R, they got you up to speed and encouraged you to try basic data analysis workflows. However, they have not properly introduced what such data analysis means, especially while running in Spark. They presented the tools you need throughout this book, to help you spend more time learning and less time troubleshooting.</p>
<p>This chapter will introduce tools and concepts to perform data analysis in Spark from R; which, spoiler alert, are the same tools you use when using plain R! This is not an accidental coincidence; but rather, we want data scientist to live in a world where technology is hidden from them, where you can use the R packages you know and love, and where they simply happen to just work in Spark! Now, we are not quite there yet, but we are also not that far. In this chapter you will learn widely used R packages and practices to pereform data analysis like: <code>dplyr</code>, <code>ggplot2</code>, formulas, <code>rmarkdown</code> and so on – which also happen to work in Spark!</p>
<p>The next chapter, Modeling, will focus on creating statistical models to predict, estimate and describe datasets; but first, let’s get started with analysis!</p>
<div id="analysis-overview" class="section level2">
<h2><span class="header-section-number">3.1</span> Overview</h2>
<p>In a data analysis project, the main goal is to understand what the data is trying to “tell us”, hoping that it provides an answer to a specific question. Most data analysis projects follow a set of steps outlined in Figure <a href="analysis.html#fig:analysis-steps">3.1</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:analysis-steps"></span>
<img src="the-r-in-spark_files/figure-html/analysis-steps-1.png" alt="General steps of a data analysis" width="100%" height="220pt" />
<p class="caption">
FIGURE 3.1: General steps of a data analysis
</p>
</div>
<p>As the diagram illustrates, we first <em>import</em> data into our analysis stem, then <em>wrangle</em> by trying different data transformations, such as aggregations, and then <em>visualize</em> to help us perceive relationships and trends. In order to get deeper insight, one or multiple statistical <em>models</em> can be fitted against sample data. This will help in finding out if the patterns hold true when new data is applied to them. Lastly, the results are <em>communicated</em> publicly or privately to colleagues and stakeholders.</p>
<p>When working with not-large-scale datasets, as in datasets that fit in memory; we can perform all those steps from R, without using Spark. However, when data does not fit in memory or computation is simply too-slow, we can slightly modify this approach by incorporating Spark, but how?</p>
<p>For data analysis, the ideal approach is to let Spark do what its good at. Spark is a parallel computation engine that works at a large-scale and provides a SQL engine and modeling libraries. These can be used to perform most of the same operations R performs. Such operations include data selection, transformation, and modeling. Additionally, Spark includes tools for performing specialized computational work like graph analysis, stream processing, and many others. For now, we will skip those non-rectangular datasets and present them in later chapters.</p>
<p>Data <em>import</em>, <em>wrangling</em>, and <em>modeling</em> can be performed inside Spark. <em>Visualization</em> can also partly be done by Spark, and we will cover that later in this chapter. The relationship between Spark, R, and your data is an important concept to grasp. The goal is to use R to tell Spark what data operations to run. The only data that should come through R directly are the final results of those Spark operations. As illustrated in Figure <a href="analysis.html#fig:analysis-approach">3.2</a>, the ideal method is to <em>push compute</em> to the Spark cluster, and then <em>collect results</em> into R.</p>
<div class="figure" style="text-align: center"><span id="fig:analysis-approach"></span>
<img src="the-r-in-spark_files/figure-html/analysis-approach-1.png" alt="Spark computes while R collects results" width="100%" height="220pt" />
<p class="caption">
FIGURE 3.2: Spark computes while R collects results
</p>
</div>
<p>The <code>sparklyr</code> package aids in using the “push compute, collect results” principle. Most of its functions are wrappers on top of Spark API calls. This allows us to take advantage of Spark’s analysis components, instead of R’s. For example, when you need to fit a linear regression model; instead of using R’s familiar <code>lm()</code> function, you would use Spark’s <code>ml_linear_regression()</code>. This R function then calls Spark to create this model, this specific example is illustrated in Figure <a href="analysis.html#fig:analysis-scala">3.3</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:analysis-scala"></span>
<img src="the-r-in-spark_files/figure-html/analysis-scala-1.png" alt="R functions call Spark functionality" width="100%" height="220pt" />
<p class="caption">
FIGURE 3.3: R functions call Spark functionality
</p>
</div>
<p>For more common data manipulation tasks, <code>sparklyr</code> provides a back-end for <code>dplyr</code>. This means that already familiar <code>dplyr</code> verbs can be used in R, and then <code>sparklyr</code> and <code>dplyr</code> will translate those actions into Spark SQL statements, which are generally more compact and easier to read than SQL statements. So, if you are already familiar with R and <code>dplyr</code>, there is nothing new to learn! This might feel a bit anticlimactic, it is indeed, but it’s also great since you can focus that energy on learning other skills required to do large-scale computing.</p>
<div class="figure" style="text-align: center"><span id="fig:unnamed-chunk-23"></span>
<img src="the-r-in-spark_files/figure-html/unnamed-chunk-23-1.png" alt="dplyr writes SQL" width="100%" height="220pt" />
<p class="caption">
FIGURE 3.4: dplyr writes SQL
</p>
</div>
<p>In order to practice as you learn, the rest of this chapter’s code will use a single exercise that runs in the <em>local</em> Spark master. This way, the code can be replicated in your personal computer. Please, make sure to already have <code>sparklyr</code> working, this should already be the case if you completed the Getting Started chapter.</p>
<p>This chapter will make use of packages that you might not have installed; so first, make sure the following packages are installed by running:</p>
<div class="sourceCode" id="cb48"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb48-1" title="1"><span class="kw">install.packages</span>(<span class="st">&quot;ggplot2&quot;</span>)</a>
<a class="sourceLine" id="cb48-2" title="2"><span class="kw">install.packages</span>(<span class="st">&quot;corrr&quot;</span>)</a>
<a class="sourceLine" id="cb48-3" title="3"><span class="kw">install.packages</span>(<span class="st">&quot;dbplot&quot;</span>)</a>
<a class="sourceLine" id="cb48-4" title="4"><span class="kw">install.packages</span>(<span class="st">&quot;rmarkdown&quot;</span>)</a></code></pre></div>
<p>First, load the <code>sparklyr</code> and <code>dplyr</code> packages, and open a new <em>local</em> connection.</p>
<div class="sourceCode" id="cb49"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb49-1" title="1"><span class="kw">library</span>(sparklyr)</a>
<a class="sourceLine" id="cb49-2" title="2"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb49-3" title="3"></a>
<a class="sourceLine" id="cb49-4" title="4">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>, <span class="dt">version =</span> <span class="st">&quot;2.3&quot;</span>)</a></code></pre></div>
<p>The environment is ready to be used, so our next task is to import data that we can later analyze.</p>
</div>
<div id="import" class="section level2">
<h2><span class="header-section-number">3.2</span> Import</h2>
<p>Importing data is to be approached differently when using Spark with R. Usually, importing means that R will read files and load them into memory; when using Spark, the data is imported into Spark, not R. Notice how in Figure <a href="analysis.html#fig:analysis-access">3.5</a> the data source is connected to Spark instead of being connected to R.</p>
<div class="figure" style="text-align: center"><span id="fig:analysis-access"></span>
<img src="the-r-in-spark_files/figure-html/analysis-access-1.png" alt="Import Data to Spark not R" width="100%" height="220pt" />
<p class="caption">
FIGURE 3.5: Import Data to Spark not R
</p>
</div>
<p><strong>Note:</strong> When you are doing analysis over large-scale datasets, the vast majority of the necessary data will be already available in your Spark cluster (which is usually made available to users via Hive tables, or by accessing the file system directly), the Data chapter will cover this extensively.</p>
<p>Rather than importing all data into Spark, you can request Spark to access the data source without importing it – this is a decision you should make based on speed and performance. Importing all of the data into the Spark session will incur a up-front cost, once; since Spark needs to wait for the data to be loaded before analyzing it. If the data is not imported, you usually incur a cost with every Spark operation since Spark needs to retrieve a subset from the cluster’s storage, which is usually disk drives that happen to be much slower than reading from Spark’s memory. More will be covered in the Tuning chapter.</p>
<p>Let’s prime the session with some data by importing <code>mtcars</code> into Spark using <code>copy_to()</code>; you can also import data from distributed files in many different file formats, which you’ll learn in the Data chapter.</p>
<div class="sourceCode" id="cb50"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb50-1" title="1">cars &lt;-<span class="st"> </span><span class="kw">copy_to</span>(sc, mtcars)</a></code></pre></div>
<p><strong>Note:</strong> In an enterprise setting, <code>copy_to()</code> should only be used to transfer small tables from R, large data transfers should be performed with specialized data transfer tools.</p>
<p>The data is now accessible to Spark and transformations can now be applied with ease; the next section will cover how to wrangle data by running transformations inside Spark, using <code>dplyr</code>.</p>
</div>
<div id="wrangle" class="section level2">
<h2><span class="header-section-number">3.3</span> Wrangle</h2>
<p>Data wrangling uses transformations to understand the data, it is often referred to as the process of transforming data from one “raw” data form into another format with the intent of making it more appropriate for data analysis.</p>
<p>Malformed or missing values and columns with multiple attributes are common data problems you might need to fix, since they prevent you from understanding your dataset. For example, a “name” field contains the last and first name of a customer. There are two attributes (first and last name) in a single column. In order to be usable, we need to <em>transform</em> the “name” field, by <em>changing</em> it into “first_name” and “last_name” fields.</p>
<p>After the data is cleaned, you still need to understand the basics about its content. Other transformations, such as aggregations, can help with this task. For example, the result of requesting the average balance of all customers will return a single row and column. The value will be the average of all customers. That information will give us context when we see individual, or grouped, customer balances.</p>
<p>The main goal is to write the data transformations using R syntax as much as possible. This saves us from the cognitive cost of having to switch between multiple computer technologies to accomplish a single task. In this case, it is better to take advantage of <code>dplyr</code>, instead of writing Spark SQL statements for data exploration.</p>
<p>In the R environment, <em>cars</em> can be treated as if it is a local data frame, so <code>dplyr</code> verbs can be used. For instance, we can find out the mean of all columns as with <code>summarise_all()</code>:</p>
<div class="sourceCode" id="cb51"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb51-1" title="1"><span class="kw">summarize_all</span>(cars, mean)</a></code></pre></div>
<pre><code># Source: spark&lt;?&gt; [?? x 11]
    mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb
  &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
1  20.1  6.19  231.  147.  3.60  3.22  17.8 0.438 0.406  3.69  2.81</code></pre>
<p>While this code is exactly the same as the code you would run when using <code>dplyr</code> without Spark, a lot is happening under the hood! The data is NOT being imported into R; instead,<code>dplyr</code> converts this task into SQL statements that are then sent to Spark. The <code>show_query()</code> command makes it possible to peer into the SQL statement that <code>sparklyr</code> and <code>dplyr</code> created and sent to Spark. We can also use this time to introduce the pipe (<code>%&gt;%</code>) operator, a custom operator from the <code>magrittr</code> package that takes pipes a computation into the first argument of the next function, making your data analysis much easier to read.</p>
<div class="sourceCode" id="cb53"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb53-1" title="1"><span class="kw">summarize_all</span>(cars, mean) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb53-2" title="2"><span class="st">  </span><span class="kw">show_query</span>()</a></code></pre></div>
<pre><code>&lt;SQL&gt;
SELECT AVG(`mpg`) AS `mpg`, AVG(`cyl`) AS `cyl`, AVG(`disp`) AS `disp`,
       AVG(`hp`) AS `hp`, AVG(`drat`) AS `drat`, AVG(`wt`) AS `wt`,
       AVG(`qsec`) AS `qsec`, AVG(`vs`) AS `vs`, AVG(`am`) AS `am`,
       AVG(`gear`) AS `gear`, AVG(`carb`) AS `carb`
FROM `mtcars`</code></pre>
<p>As it is evident, <code>dplyr</code> is much more concise than SQL; but rest assured, you will not have to see nor understand SQL when using <code>dplyr</code>. Your focus can remain on obtaining insights from the data, as opposed to figuring out how to express a given set of transformations in SQL. Here is another example that groups the cars dataset by “transmission” type.</p>
<div class="sourceCode" id="cb55"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb55-1" title="1">cars <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb55-2" title="2"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">transmission =</span> <span class="kw">ifelse</span>(am <span class="op">==</span><span class="st"> </span><span class="dv">0</span>, <span class="st">&quot;automatic&quot;</span>, <span class="st">&quot;manual&quot;</span>)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb55-3" title="3"><span class="st">  </span><span class="kw">group_by</span>(transmission) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb55-4" title="4"><span class="st">  </span><span class="kw">summarise_all</span>(mean)</a></code></pre></div>
<pre><code># Source: spark&lt;?&gt; [?? x 12]
  transmission   mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb
  &lt;chr&gt;        &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt;
1 automatic     17.1  6.95  290.  160.  3.29  3.77  18.2 0.368     0  3.21  2.74
2 manmual       24.4  5.08  144.  127.  4.05  2.41  17.4 0.538     1  4.38  2.92</code></pre>
<p>Most of the data transformation operations made available by <code>dplyr</code> to work with local data frames are also available to use with a Spark connection. This means that you can focus on learning <code>dplyr</code> first, and then reuse that skill when working with Spark. The Data Transformation chapter from the “R for Data Science” <span class="citation">(Wickham and Grolemund <a href="#ref-intro-r-for-data-science">2016</a>)</span> book is a great resource to learn <code>dplyr</code> in-depth. If proficiency with <code>dplyr</code> is not an issue for you, then please take some time to experiment with different <code>dplyr</code> functions against the <em>cars</em> table.</p>
<p>Sometimes we may need to perform an operation not yet available through <code>dplyr</code> and <code>sparklyr</code>. Instead of downloading the data into R, there is usually a Hive function within Spark to accomplish what we need. The next section will cover this scenario.</p>
<div id="built-in-functions" class="section level3">
<h3><span class="header-section-number">3.3.1</span> Built-in Functions</h3>
<p>Spark SQL is based on Hive’s SQL conventions and functions and it is possible to call all these functions using <code>dplyr</code> as well. This means that we can use any Spark SQL functions to accomplish operations that may not be available via <code>dplyr</code>. The functions can be accessed by calling them as if they were R functions. Instead of failing, <code>dplyr</code> passes functions it does not recognize “as-is” to the query engine. This gives us a lot of flexibility on the function we can use!</p>
<p>For instance, the <em>percentile</em> function returns the exact percentile of a column in a group. The function expects a column name, and either a single percentile value, or an array of multiple percentile values. We can use this Spark SQL function from <code>dplyr</code> as follows:</p>
<div class="sourceCode" id="cb57"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb57-1" title="1"><span class="kw">summarise</span>(cars, <span class="dt">mpg_percentile =</span> <span class="kw">percentile</span>(mpg, <span class="fl">0.25</span>))</a></code></pre></div>
<pre><code># Source: spark&lt;?&gt; [?? x 1]
  mpg_percentile
           &lt;dbl&gt;
1           15.4</code></pre>
<p>There is no <code>percentile()</code> function in R, so <code>dplyr</code> passes the that portion of the code, “as-is”, to the resulting SQL query.</p>
<div class="sourceCode" id="cb59"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb59-1" title="1"><span class="kw">summarise</span>(cars, <span class="dt">mpg_percentile =</span> <span class="kw">percentile</span>(mpg, <span class="fl">0.25</span>)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb59-2" title="2"><span class="st">  </span><span class="kw">show_query</span>()</a></code></pre></div>
<pre><code>&lt;SQL&gt;
SELECT percentile(`mpg`, 0.25) AS `mpg_percentile`
FROM `mtcars_remote`</code></pre>
<p>To pass multiple values to <em>percentile</em>, we can call another Hive function called <em>array</em>. In this case, <em>array</em> would work similarly to R’s <code>list()</code> function. We can pass multiple values separated by commas. The output from Spark is an array variable, which is imported into R as a list variable column.</p>
<div class="sourceCode" id="cb61"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb61-1" title="1"><span class="kw">summarise</span>(cars, <span class="dt">mpg_percentile =</span> <span class="kw">percentile</span>(mpg, <span class="kw">array</span>(<span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>)))</a></code></pre></div>
<pre><code># Source: spark&lt;?&gt; [?? x 1]
  mpg_percentile
  &lt;list&gt;
1 &lt;list [3]&gt;   </code></pre>
<p>The <em>explode</em> function can be used to separate the Spark’s array value results into their own record. To do this, use <em>explode</em> within a <code>mutate()</code> command, and pass the variable containing the results of the percentile operation.</p>
<div class="sourceCode" id="cb63"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb63-1" title="1"><span class="kw">summarise</span>(cars, <span class="dt">mpg_percentile =</span> <span class="kw">percentile</span>(mpg, <span class="kw">array</span>(<span class="fl">0.25</span>, <span class="fl">0.5</span>, <span class="fl">0.75</span>))) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb63-2" title="2"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">mpg_percentile =</span> <span class="kw">explode</span>(mpg_percentile))</a></code></pre></div>
<pre><code># Source: spark&lt;?&gt; [?? x 1]
  mpg_percentile
           &lt;dbl&gt;
1           15.4
2           19.2
3           22.8</code></pre>
<p>We have included a comprehensive list of all the Hive functions in the Appendix under <a href="appendix.html#hive-functions">Hive functions</a>, make sure you glance over them to get a sense of the wide range of operations you can accomplish with them.</p>
</div>
<div id="correlations" class="section level3">
<h3><span class="header-section-number">3.3.2</span> Correlations</h3>
<p>A very common exploration technique is to calculate and visualize correlations, which we often calculate to find out what kind of statistical relationship exists between paired sets of variables. Spark provides functions to calculate correlations across the entire data set and returns the results to R as a data frame object.</p>
<div class="sourceCode" id="cb65"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb65-1" title="1"><span class="kw">ml_corr</span>(cars)</a></code></pre></div>
<pre><code># A tibble: 11 x 11
      mpg    cyl   disp     hp    drat     wt    qsec
    &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;  &lt;dbl&gt;   &lt;dbl&gt;
 1  1     -0.852 -0.848 -0.776  0.681  -0.868  0.419
 2 -0.852  1      0.902  0.832 -0.700   0.782 -0.591
 3 -0.848  0.902  1      0.791 -0.710   0.888 -0.434
 4 -0.776  0.832  0.791  1     -0.449   0.659 -0.708
 5  0.681 -0.700 -0.710 -0.449  1      -0.712  0.0912
 6 -0.868  0.782  0.888  0.659 -0.712   1     -0.175
 7  0.419 -0.591 -0.434 -0.708  0.0912 -0.175  1
 8  0.664 -0.811 -0.710 -0.723  0.440  -0.555  0.745
 9  0.600 -0.523 -0.591 -0.243  0.713  -0.692 -0.230
10  0.480 -0.493 -0.556 -0.126  0.700  -0.583 -0.213
11 -0.551  0.527  0.395  0.750 -0.0908  0.428 -0.656
# ... with 4 more variables: vs &lt;dbl&gt;, am &lt;dbl&gt;,
#   gear &lt;dbl&gt;, carb &lt;dbl&gt;</code></pre>
<p>The <code>corrr</code> R package specializes in correlations. It contains friendly functions to prepare and visualize the results. Included inside the package is a back-end for Spark, so when a Spark object is used in <code>corrr</code> the actual computation also happens in Spark. In the background, the <code>correlate()</code> function runs <code>sparklyr::ml_corr()</code>, so there is no need to collect any data into R prior running the command.</p>
<div class="sourceCode" id="cb67"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb67-1" title="1"><span class="kw">library</span>(corrr)</a>
<a class="sourceLine" id="cb67-2" title="2"><span class="kw">correlate</span>(cars, <span class="dt">use =</span> <span class="st">&quot;pairwise.complete.obs&quot;</span>, <span class="dt">method =</span> <span class="st">&quot;pearson&quot;</span>) </a></code></pre></div>
<pre><code># A tibble: 11 x 12
   rowname     mpg     cyl    disp      hp     drat      wt
   &lt;chr&gt;     &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;   &lt;dbl&gt;    &lt;dbl&gt;   &lt;dbl&gt;
 1 mpg      NA      -0.852  -0.848  -0.776   0.681   -0.868
 2 cyl      -0.852  NA       0.902   0.832  -0.700    0.782
 3 disp     -0.848   0.902  NA       0.791  -0.710    0.888
 4 hp       -0.776   0.832   0.791  NA      -0.449    0.659
 5 drat      0.681  -0.700  -0.710  -0.449  NA       -0.712
 6 wt       -0.868   0.782   0.888   0.659  -0.712   NA
 7 qsec      0.419  -0.591  -0.434  -0.708   0.0912  -0.175
 8 vs        0.664  -0.811  -0.710  -0.723   0.440   -0.555
 9 am        0.600  -0.523  -0.591  -0.243   0.713   -0.692
10 gear      0.480  -0.493  -0.556  -0.126   0.700   -0.583
11 carb     -0.551   0.527   0.395   0.750  -0.0908   0.428
# ... with 5 more variables: qsec &lt;dbl&gt;, vs &lt;dbl&gt;,
#   am &lt;dbl&gt;, gear &lt;dbl&gt;, carb &lt;dbl&gt;</code></pre>
<p>We can pipe the results to other <code>corrr</code> functions. For example, the <code>shave()</code> functions turns all of the duplicated results into <code>NA</code>’s. Again, while this feels like standard R code using existing R packages, Spark is being used under the hood to perform the correlation!</p>
<p>Additionally, as shown in Figure <a href="analysis.html#fig:analysis-corrr-rplot">3.6</a>, the results can be easily visualized using the <code>rplot()</code> function.</p>
<div class="sourceCode" id="cb69"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb69-1" title="1"><span class="kw">correlate</span>(cars, <span class="dt">use =</span> <span class="st">&quot;pairwise.complete.obs&quot;</span>, <span class="dt">method =</span> <span class="st">&quot;pearson&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb69-2" title="2"><span class="st">  </span><span class="kw">shave</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb69-3" title="3"><span class="st">  </span><span class="kw">rplot</span>()</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:analysis-corrr-rplot"></span>
<img src="images/analysis-corrr-rplot-resized.png" alt="Using rplot() to visualize correlations" width="800pt" height="400pt" />
<p class="caption">
FIGURE 3.6: Using rplot() to visualize correlations
</p>
</div>
<p>It is much easier to see which relationships are positive or negative. Positive relationships are in grey, and negative relationships are black. The size of the circle indicates how significant their relationship is. The power of visualizing data is in how much easier it makes it for us to understand results. The next section will expand on this step of the process.</p>
</div>
</div>
<div id="visualize" class="section level2">
<h2><span class="header-section-number">3.4</span> Visualize</h2>
<p>Visualizations are a vital tool to help us find patterns in the data. It is easier for us to identify outliers in a data set of 1,000 observations when plotted in a graph, as opposed to reading them from a list.</p>
<p>R is great at data visualizations. Its capabilities for creating plots are extended by the many R packages that focus on this analysis step. Unfortunately, the vast majority of R functions that create plots depend on the data already being in local memory within R, so they fail when using a remote table inside Spark.</p>
<p>It is possible to create visualizations in R from data sources that exist in Spark. To understand how to do this, let’s first break down how computer programs build plots: It takes the raw data and performs some sort of transformation. The transformed data is then mapped to a set of coordinates. Finally, the mapped values are drawn in a plot. Figure <a href="analysis.html#fig:analysis-plot">3.7</a> summarizes each of the steps.</p>
<div class="figure" style="text-align: center"><span id="fig:analysis-plot"></span>
<img src="the-r-in-spark_files/figure-html/analysis-plot-1.png" alt="Stages of an R plot" width="100%" height="100pt" />
<p class="caption">
FIGURE 3.7: Stages of an R plot
</p>
</div>
<p>In essence, the approach for visualizing is the same as in wrangling, push the computation to Spark, then collect the results in R for plotting. As illustrated in Figure <a href="analysis.html#fig:analysis-spark-plot">3.8</a>, the heavy lifting of preparing the data, such as aggregating the data by groups or bins, can be done inside Spark, then collected as a much smaller data set into R. Inside R, the plot becomes a more basic operation. For example, to plot a histogram, bins are first calculated in Spark. Once the result is collected to R, we can use a simple column plot, as opposed to a histogram plot, because there is no need for R to re-calculate the bins.</p>
<div class="figure" style="text-align: center"><span id="fig:analysis-spark-plot"></span>
<img src="the-r-in-spark_files/figure-html/analysis-spark-plot-1.png" alt="Plotting with Spark and R" width="100%" height="200pt" />
<p class="caption">
FIGURE 3.8: Plotting with Spark and R
</p>
</div>
<p>Using this conceptual model, let’s apply this when using <code>ggplot2</code>.</p>
<div id="using-ggplot2" class="section level3">
<h3><span class="header-section-number">3.4.1</span> Using ggplot2</h3>
<p>To create a bar plot using <code>ggplot2</code>, we simply call a function:</p>
<div class="sourceCode" id="cb70"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb70-1" title="1"><span class="kw">library</span>(ggplot2)</a></code></pre></div>
<pre><code>## Warning: package &#39;ggplot2&#39; was built under R version 3.5.2</code></pre>
<div class="sourceCode" id="cb72"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb72-1" title="1"><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="kw">as.factor</span>(cyl), mpg), <span class="dt">data =</span> mtcars) <span class="op">+</span><span class="st"> </span><span class="kw">geom_col</span>()</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:analysis-ggplot2-simple"></span>
<img src="the-r-in-spark_files/figure-html/analysis-ggplot2-simple-1.png" alt="Plotting inside R" width="500pt" height="400pt" />
<p class="caption">
FIGURE 3.9: Plotting inside R
</p>
</div>
<p>In this case, the <code>mtcars</code> raw data was <em>automatically</em> transformed into three discrete aggregated numbers, then each result was mapped into an <code>x</code> and <code>y</code> plane, and then the plot was drawn. As R users, all of the stages of building the plot are conveniently abstracted for us.</p>
<p>In Spark, there are a couple of key steps when codifying the “push compute, collect results” approach. First, ensure that the transformation operations happen inside Spark. In the example below, <code>group_by()</code> and <code>summarise()</code> will run inside Spark. Second, bring the results back into R after the data has been transformed. Make sure to transform and then collect, in that order; if <code>collect()</code> is run first, then R will try to ingest the entire data set from Spark. Depending on the size of the data, collecting all of the data will slow down or may even bring down your system.</p>
<div class="sourceCode" id="cb73"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb73-1" title="1">car_group &lt;-<span class="st"> </span>cars <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb73-2" title="2"><span class="st">  </span><span class="kw">group_by</span>(cyl) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb73-3" title="3"><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">mpg =</span> <span class="kw">sum</span>(mpg, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb73-4" title="4"><span class="st">  </span><span class="kw">collect</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb73-5" title="5"><span class="st">  </span><span class="kw">print</span>()</a></code></pre></div>
<pre><code># A tibble: 3 x 2
    cyl   mpg
  &lt;dbl&gt; &lt;dbl&gt;
1     6  138.
2     4  293.
3     8  211.</code></pre>
<p>In this example, now that the data has been pre-aggregated and collected into R, only three records are passed to the plotting function. Figure <a href="analysis.html#fig:analysis-viz1">3.10</a> shows the resulting plot.</p>
<div class="sourceCode" id="cb75"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb75-1" title="1"><span class="kw">ggplot</span>(<span class="kw">aes</span>(<span class="kw">as.factor</span>(cyl), mpg), <span class="dt">data =</span> car_group) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb75-2" title="2"><span class="st">  </span><span class="kw">geom_col</span>(<span class="dt">fill =</span> <span class="st">&quot;#999999&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">coord_flip</span>()</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:analysis-viz1"></span>
<img src="images/analysis-visualizations-1.png" alt="Plot with aggregation in Spark" width="1500" />
<p class="caption">
FIGURE 3.10: Plot with aggregation in Spark
</p>
</div>
<p>Any other <code>ggplot2</code> visualization can be made to work using this approach; however, it is beyond the scope of the book to teach this. Instead, we recommend you use “R graphics cookbook: practical recipes for visualizing data” <span class="citation">(Chang <a href="#ref-r-graphics-cookbook">2012</a>)</span> to learn additional visualization techniques applicable to Spark. Now, to ease this transformation step before visualizing, the <code>dbplot</code> package provides a few ready-to-use visualizations that automate aggregation in Spark.</p>
</div>
<div id="using-dbplot" class="section level3">
<h3><span class="header-section-number">3.4.2</span> Using dbplot</h3>
<p>The <code>dbplot</code> package provides helper functions for plotting with remote data. The R code <code>dbplot</code> uses to transform the data is written so that it can be translated into Spark. It then uses those results to create a graph using the <code>ggplot2</code> package where data transformation and plotting are both triggered by a single function.</p>
<p>The <code>dbplot_histogram()</code> function makes Spark calculate the bins and the count per bin and outputs a <code>ggplot</code> object which can be further refined by adding more steps to the plot object. <code>dbplot_histogram()</code> also accepts a <code>binwidth</code> argument to control the range used to compute the bins. The resulting plot is in Figure <a href="analysis.html#fig:analysis-visualizations-histogram">3.11</a>.</p>
<div class="sourceCode" id="cb76"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb76-1" title="1"><span class="kw">library</span>(dbplot)</a>
<a class="sourceLine" id="cb76-2" title="2"></a>
<a class="sourceLine" id="cb76-3" title="3">cars <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb76-4" title="4"><span class="kw">dbplot_histogram</span>(mpg, <span class="dt">binwidth =</span> <span class="dv">3</span>) <span class="op">+</span></a>
<a class="sourceLine" id="cb76-5" title="5"><span class="kw">labs</span>(<span class="dt">title =</span> <span class="st">&quot;MPG Distribution&quot;</span>,</a>
<a class="sourceLine" id="cb76-6" title="6">     <span class="dt">subtitle =</span> <span class="st">&quot;Histogram over miles per gallon&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:analysis-visualizations-histogram"></span>
<img src="images/analysis-visualizations-histogram-resized.png" alt="Histogram created by dbplot" width="1500" />
<p class="caption">
FIGURE 3.11: Histogram created by dbplot
</p>
</div>
<p>Histograms provide a great way to analyze a single variable. To analyze two variables, a scatter or raster plot is commonly used.</p>
<p>Scatter plots are used to compare the relationship between two continuous variables. For example, a scatter plot will display the relationship between the weight of a car and its gas consumption. The plot will show that the higher the weight, the higher the gas consumption because the dots clump together into almost a line that goes from the top left towards the bottom right. See Figure <a href="analysis.html#fig:analysis-point">3.12</a> for an example of the plot.</p>
<div class="sourceCode" id="cb77"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb77-1" title="1"><span class="kw">ggplot</span>(<span class="kw">aes</span>(mpg, wt), <span class="dt">data =</span> mtcars) <span class="op">+</span><span class="st"> </span></a>
<a class="sourceLine" id="cb77-2" title="2"><span class="st">  </span><span class="kw">geom_point</span>()</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:analysis-point"></span>
<img src="images/analysis-point-resized.png" alt="Scatter plot example in Spark" width="1500" />
<p class="caption">
FIGURE 3.12: Scatter plot example in Spark
</p>
</div>
<p>However, for scatter plots, no amount of “pushing the computation” to Spark will help with this problem because the data has to be plotted in individual dots.</p>
<p>The best alternative is to find a plot type that represents the x/y relationship and concentration in a way that it is easy to perceive and to “physically” plot. The <em>raster</em> plot may be the best answer. It returns a grid of x/y positions and the results of a given aggregation usually represented by the color of the square.</p>
<p>You can use <code>dbplot_raster()</code> to create a scatter-like plot in Spark, while only retrieving (collecting) a small subset of the remote dataset:</p>
<div class="sourceCode" id="cb78"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb78-1" title="1"><span class="kw">dbplot_raster</span>(cars, mpg, wt, <span class="dt">resolution =</span> <span class="dv">16</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:analysis-visualizations-raster"></span>
<img src="images/analysis-visualizations-raster-resized.png" alt="A raster plot using Spark" width="1500" />
<p class="caption">
FIGURE 3.13: A raster plot using Spark
</p>
</div>
<p>As shown in Figure <a href="analysis.html#fig:analysis-visualizations-raster">3.13</a>, the plot returns a grid no bigger than 5x5. This limits the number of records that need to be collected into R to 25.</p>
<p><strong>Tip:</strong> You can also use <code>dbplot</code> to retrieve the raw data and visualize by other means; to retrieve the aggregates but not the plots, use: <code>db_compute_bins()</code>, <code>db_compute_count()</code>, <code>db_compute_raster()</code> and <code>db_compute_boxplot()</code>.</p>
<p>While visualizations are indispensable, you can complement data analysis using statistical models to gain even deeper insights into our data. The next section will present how we can prepare data for modeling with Spark.</p>
</div>
</div>
<div id="model" class="section level2">
<h2><span class="header-section-number">3.5</span> Model</h2>
<p>The next two chapters will focus entirely on modeling, so rather than introducing modeling with too much detail in this chapter, we want to present how to interact with models while doing data analysis.</p>
<p>First, an analysis project goes through as many transformations and models to find the answer. That’s why the first data analysis diagram we introduced in Figure <a href="analysis.html#fig:analysis-approach">3.2</a>, illustrates a cycle between: visualizing, wrangling and modeling – we know you don’t end with modeling, not in R and neither when using Spark.</p>
<p>Therefore, the ideal data analysis language enables you to quickly adjust over each wrangle-visualize-model iteration. Fortunately, this is the case when using Spark and R.</p>
<p>To illustrate how easy it is to iterate over wrangling and modeling in Spark, consider the following example. We will start by performing a linear regression against all features and predict MPG:</p>
<div class="sourceCode" id="cb79"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb79-1" title="1">cars <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb79-2" title="2"><span class="st">  </span><span class="kw">ml_linear_regression</span>(mpg <span class="op">~</span><span class="st"> </span>.) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb79-3" title="3"><span class="st">  </span><span class="kw">summary</span>()</a></code></pre></div>
<pre><code>Deviance Residuals:
    Min      1Q  Median      3Q     Max
-3.4506 -1.6044 -0.1196  1.2193  4.6271

Coefficients:
(Intercept)         cyl        disp          hp        drat          wt
12.30337416 -0.11144048  0.01333524 -0.02148212  0.78711097 -3.71530393
      qsec          vs          am        gear        carb
0.82104075  0.31776281  2.52022689  0.65541302 -0.19941925

R-Squared: 0.869
Root Mean Squared Error: 2.147</code></pre>
<p>At this point it is very easy to experiment with different features, we can simply change the R formula from <code>mpg ~ .</code> to say <code>mpg ~ hp + cyl</code> to only use HP and cylinders as features:</p>
<div class="sourceCode" id="cb81"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb81-1" title="1">cars <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb81-2" title="2"><span class="st">  </span><span class="kw">ml_linear_regression</span>(mpg <span class="op">~</span><span class="st"> </span>hp <span class="op">+</span><span class="st"> </span>cyl) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb81-3" title="3"><span class="st">  </span><span class="kw">summary</span>()</a></code></pre></div>
<pre><code>Deviance Residuals:
    Min      1Q  Median      3Q     Max
-4.4948 -2.4901 -0.1828  1.9777  7.2934

Coefficients:
(Intercept)          hp         cyl
 36.9083305  -0.0191217  -2.2646936

R-Squared: 0.7407
Root Mean Squared Error: 3.021</code></pre>
<p>Additionally, it is also very easy to iterate with other kinds of models. The following one replaces the linear model with a generalized linear model:</p>
<div class="sourceCode" id="cb83"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb83-1" title="1">cars <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb83-2" title="2"><span class="st">  </span><span class="kw">ml_generalized_linear_regression</span>(mpg <span class="op">~</span><span class="st"> </span>hp <span class="op">+</span><span class="st"> </span>cyl) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb83-3" title="3"><span class="st">  </span><span class="kw">summary</span>()</a></code></pre></div>
<pre><code>Deviance Residuals:
    Min      1Q  Median      3Q     Max
-4.4948 -2.4901 -0.1828  1.9777  7.2934

Coefficients:
(Intercept)          hp         cyl
 36.9083305  -0.0191217  -2.2646936

(Dispersion parameter for gaussian family taken to be 10.06809)

   Null  deviance: 1126.05 on 31 degress of freedom
Residual deviance: 291.975 on 29 degrees of freedom
AIC: 169.56</code></pre>
<p>Usually, before fitting a model you would have to use multiple <code>dplyr</code> transformations to get it ready to be consumed by a model. To make sure the model can be fitted as efficiently as possible, you should cache your dataset before fitting it.</p>
<div id="caching" class="section level3">
<h3><span class="header-section-number">3.5.1</span> Caching</h3>
<p>The examples in this chapter are built using a very small data set. In real-life scenarios, large amounts of data are used for models. If the data needs to be transformed first, the volume of the data could exact a heavy toll on the Spark session. Before fitting the models, it is a good idea to save the results of all the transformations in a new table inside Spark memory.</p>
<p>The <code>compute()</code> command can take the end of a <code>dplyr</code> piped command set and save the results to Spark memory.</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb85-1" title="1">cached_cars &lt;-<span class="st"> </span>cars <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb85-2" title="2"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">cyl =</span> <span class="kw">paste0</span>(<span class="st">&quot;cyl_&quot;</span>, cyl)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb85-3" title="3"><span class="st">  </span><span class="kw">compute</span>(<span class="st">&quot;cached_cars&quot;</span>)</a></code></pre></div>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb86-1" title="1">cached_cars <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb86-2" title="2"><span class="st">  </span><span class="kw">ml_linear_regression</span>(mpg <span class="op">~</span><span class="st"> </span>.) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb86-3" title="3"><span class="st">  </span><span class="kw">summary</span>()</a></code></pre></div>
<pre><code>Deviance Residuals:
     Min       1Q   Median       3Q      Max
-3.47339 -1.37936 -0.06554  1.05105  4.39057

Coefficients:
(Intercept) cyl_cyl_8.0 cyl_cyl_4.0        disp          hp        drat
16.15953652  3.29774653  1.66030673  0.01391241 -0.04612835  0.02635025
          wt        qsec          vs          am       gear        carb
 -3.80624757  0.64695710  1.74738689  2.61726546 0.76402917  0.50935118

R-Squared: 0.8816
Root Mean Squared Error: 2.041</code></pre>
<p>As more insights are gained from the data, more questions may be raised. That is why we expect to iterate through data <em>wrangle</em>, <em>visualize</em>, and <em>model</em> multiple times. Each iteration should provide incremental insights of what the data is “telling us”. There will be a point where we reach a satisfactory level of understanding. It is at this point that we will be ready to share the results of the analysis, this is the topic of the next section.</p>
</div>
</div>
<div id="communicate" class="section level2">
<h2><span class="header-section-number">3.6</span> Communicate</h2>
<p>It is important to clearly communicate the analysis results – as important as the analysis work itself! The public, colleagues or stakeholders need to understand what you found out and how.</p>
<p>To communicate effectively we need to use artifacts, such as reports and presentations; these are common output formats that we can create in R, using R Markdown.</p>
<p>R Markdown documents allow us to weave narrative text and code together. Once created, R Markdown documents can be turned into a variety of output formats such as HTML, PDF, PowerPoint, Word, web slides, Websites, books and so on.</p>
<p>Most of these outputs are available in the core R packages of R Markdown: <code>knitr</code> and <code>rmarkdown</code>. R Markdown can be extended by other R packages. For example, this book was written using R Markdown thanks to an extension provided by the <code>bookdown</code> package. The best resource to delve deeper into R Markdown is the official book. <span class="citation">(Xie <a href="#ref-r-markdown-the-definite-guide">2018</a>)</span></p>
<p>In R Markdown, one singular artifact could potentially be rendered into many different formats. For example, the same report could be rendered in HTML, or as a PDF file by changing a setting within the report itself. Conversely, multiple types of artifacts could be rendered as the same output. For example, a presentation deck and a report could be rendered in HTML.</p>
<p>Creating a new R Markdown report which uses Spark as a compute engine is easy! At the top, R Markdown expects a YAML header. The first and last line are three consecutive dashes (<code>---</code>). The content in between the dashes will vary depending on the type of document. The only required attribute is the <code>output</code> value. R Markdown needs to know what kind of output it needs to render your report into. This YAML header is called Front Matter. Following the Front Matter are sections of code, called code chunks. These code chunks can be interlaced with the plain text narratives. There is nothing particularly interesting to note when using Spark with R Markdown, except that it is just business as usual.</p>
<p>Since an R Markdown document is self-contained and meant to be reproducible, before rendering documents we should first disconnect from Spark to free resources:</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb88-1" title="1"><span class="kw">spark_disconnect</span>(sc)</a></code></pre></div>
<p>The following example shows how easy it is to create a fully reproducible report that uses Spark to process large-scale datasets. The narrative, code and, most importantly, the output of the code is recorded inside the resulting HTML file. You can copy and paste the following code into a file.</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode markdown"><code class="sourceCode markdown"><a class="sourceLine" id="cb89-1" title="1">---</a>
<a class="sourceLine" id="cb89-2" title="2">title: &quot;mtcars analysis&quot;</a>
<a class="sourceLine" id="cb89-3" title="3">output:</a>
<a class="sourceLine" id="cb89-4" title="4">  html_document: </a>
<a class="sourceLine" id="cb89-5" title="5"><span class="bn">    fig_width: 6 </span></a>
<a class="sourceLine" id="cb89-6" title="6"><span class="bn">    fig_height: 3</span></a>
<a class="sourceLine" id="cb89-7" title="7">---</a>
<a class="sourceLine" id="cb89-8" title="8"><span class="bn">```{r, setup, include = FALSE}</span></a>
<a class="sourceLine" id="cb89-9" title="9"><span class="bn">library(sparklyr)</span></a>
<a class="sourceLine" id="cb89-10" title="10"><span class="bn">library(dplyr)</span></a>
<a class="sourceLine" id="cb89-11" title="11"></a>
<a class="sourceLine" id="cb89-12" title="12"><span class="bn">sc &lt;- spark_connect(master = &quot;local&quot;, version = &quot;2.3&quot;)</span></a>
<a class="sourceLine" id="cb89-13" title="13"><span class="bn">cars &lt;- copy_to(sc, mtcars)</span></a>
<a class="sourceLine" id="cb89-14" title="14"><span class="bn">```</span></a>
<a class="sourceLine" id="cb89-15" title="15"></a>
<a class="sourceLine" id="cb89-16" title="16"><span class="fu">## Visualize</span></a>
<a class="sourceLine" id="cb89-17" title="17">Aggregate data in Spark, visualize in R.</a>
<a class="sourceLine" id="cb89-18" title="18"><span class="bn">```{r  fig.align=&#39;center&#39;, warning=FALSE}</span></a>
<a class="sourceLine" id="cb89-19" title="19"><span class="bn">library(ggplot2)</span></a>
<a class="sourceLine" id="cb89-20" title="20"><span class="bn">cars %&gt;%</span></a>
<a class="sourceLine" id="cb89-21" title="21"><span class="bn">  group_by(cyl) %&gt;% summarise(mpg = mean(mpg)) %&gt;%</span></a>
<a class="sourceLine" id="cb89-22" title="22"><span class="bn">  ggplot(aes(cyl, mpg)) + geom_bar(stat=&quot;identity&quot;)</span></a>
<a class="sourceLine" id="cb89-23" title="23"><span class="bn">```</span></a>
<a class="sourceLine" id="cb89-24" title="24"></a>
<a class="sourceLine" id="cb89-25" title="25"><span class="fu">## Model</span></a>
<a class="sourceLine" id="cb89-26" title="26">The selected model was a simple linear regression that </a>
<a class="sourceLine" id="cb89-27" title="27">uses the weight as the predictor of MPG</a>
<a class="sourceLine" id="cb89-28" title="28"></a>
<a class="sourceLine" id="cb89-29" title="29"><span class="bn">```{r}</span></a>
<a class="sourceLine" id="cb89-30" title="30"><span class="bn">cars %&gt;%</span></a>
<a class="sourceLine" id="cb89-31" title="31"><span class="bn">  ml_linear_regression(wt ~ mpg) %&gt;%</span></a>
<a class="sourceLine" id="cb89-32" title="32"><span class="bn">  summary()</span></a>
<a class="sourceLine" id="cb89-33" title="33"><span class="bn">```</span></a>
<a class="sourceLine" id="cb89-34" title="34"><span class="bn">```{r, include = FALSE}</span></a>
<a class="sourceLine" id="cb89-35" title="35"><span class="bn">spark_disconnect(sc)</span></a>
<a class="sourceLine" id="cb89-36" title="36"><span class="bn">```</span></a></code></pre></div>
<p>To knit this report, save the file with a <code>.Rmd</code> extension such as: <code>report.Rmd</code>, and run <code>render()</code> from R. The output should look like the one in Figure <a href="analysis.html#fig:visualize-analysis-communicate">3.14</a>.</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb90-1" title="1">rmarkdown<span class="op">::</span><span class="kw">render</span>(<span class="st">&quot;report.Rmd&quot;</span>)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:visualize-analysis-communicate"></span>
<img src="images/analysis-rmarkdown-resized.png" alt="R Markdown HTML output" width="1500" />
<p class="caption">
FIGURE 3.14: R Markdown HTML output
</p>
</div>
<p>This report can now be easily shared. Viewers of this report won’t need Spark or R to read and consume its contents; it’s just a self-contained HTML file, trivial to open in any browser.</p>
<p>It is also common to distill insights of a report into many other output formats. Switching is quite easy, in the top Front Matter, change the <code>output</code> option to <code>powerpoint_presentation</code>, <code>pdf_document</code>, <code>word_document</code>, etc. You can even produce multiple output formats from the same report:</p>
<pre><code>---
title: &quot;mtcars analysis&quot;
output:
  word_document: default
  pdf_document: default
  powerpoint_presentation: default
---</code></pre>
<p>The result will be a PowerPoint presentation, a Word document and a PDF! All of the same information that was displayed in the original HTML report, computed in Spark and rendered in R.</p>
<p>There will be a need to edit the PowerPoint template or the output of the code chunks. This minimal example shows how easy it is to go from one format to another. Of course, it will take some more editing on the R user’s side to make sure the slides contain only the pertinent information. The main point is to highlight that it does not require learning a different markup, or code conventions, to switch from one artifact to another.</p>
</div>
<div id="recap" class="section level2">
<h2><span class="header-section-number">3.7</span> Recap</h2>
<p>This chapter presented a solid introduction to data analysis with R and Spark. Many of the techniques presented looked quite similar to using just R and no Spark; which while anticlimactic, is the right design to help users already familiar with R, easily transition to Spark. For users unfamiliar with R, this chapter also served as a very brief introduction to some of the most popular (and useful!) packages available in R.</p>
<p>It should now be quite obvious that together, R and Spark, are a powerful combination – a large-scale computing platform, along with an incredibly robust ecosystem of R packages, makes up for an ideal analysis platform.</p>
<p>While doing analysis in Spark with R, remember to push computation to Spark, and focus on collecting results in R. This paradigm should set up a successful approach to data manipulation, visualization and communication through sharing your results in a variety of outputs.</p>
<p>The next chapter, Modeling, will dive deeper into how to build statistical models in Spark using a much more interesting dataset, what’s more interesting than dating datasets? You will also learn many more techniques we did not even mention in the brief modeling section from this chapter.</p>

</div>
</div>
<h3> References</h3>
<div id="refs" class="references">
<div id="ref-r-graphics-cookbook">
<p>Chang, Winston. 2012. <em>R Graphics Cookbook: Practical Recipes for Visualizing Data</em>. O’Reilly Media, Inc.</p>
</div>
<div id="ref-intro-r-for-data-science">
<p>Wickham, Hadley, and Garrett Grolemund. 2016. <em>R for Data Science: Import, Tidy, Transform, Visualize, and Model Data</em>. O’Reilly Media, Inc.</p>
</div>
<div id="ref-r-markdown-the-definite-guide">
<p>Xie, Grolemund, Allaire. 2018. <em>R Markdown: The Definite Guide</em>. 1st ed. CRC Press.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="starting.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="modeling.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>

<script src="hide-controls.js"></script>
</body>

</html>
