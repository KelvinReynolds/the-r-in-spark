<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>The R in Spark: Learning Apache Spark with R</title>
  <meta name="description" content="A book to learn Apache Spark with R using the sparklyr R package.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="The R in Spark: Learning Apache Spark with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A book to learn Apache Spark with R using the sparklyr R package." />
  <meta name="github-repo" content="javierluraschi/the-r-in-spark" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="The R in Spark: Learning Apache Spark with R" />
  
  <meta name="twitter:description" content="A book to learn Apache Spark with R using the sparklyr R package." />
  



<meta name="date" content="2019-02-15">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="data.html">
<link rel="next" href="extensions.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.3/htmlwidgets.js"></script>
<script src="libs/dagre-0.0.1/dagre.min.js"></script>
<script src="libs/lodash-3.7.0/lodash.js"></script>
<script src="libs/nomnoml-0.2.0/nomnoml.js"></script>
<script src="libs/nomnoml-binding-0.1.0/nomnoml.js"></script>
<script src="libs/r2d3-render-0.1.0/r2d3-render.js"></script>
<script src="libs/webcomponents-2.0.0/webcomponents.js"></script>
<script src="libs/r2d3-binding-0.2.3/r2d3.js"></script>
<script src="libs/d3v5-5.0.0/d3.min.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-119986300-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-119986300-1');
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./">Learning Apache Spark with R</a></strong></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#authors"><i class="fa fa-check"></i>Authors</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#formatting"><i class="fa fa-check"></i>Formatting</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#intro-background"><i class="fa fa-check"></i><b>1.1</b> Information</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#intro-hadoop"><i class="fa fa-check"></i><b>1.2</b> Hadoop</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#intro-spark"><i class="fa fa-check"></i><b>1.3</b> Spark</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#intro-r"><i class="fa fa-check"></i><b>1.4</b> R</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#intro-sparklyr"><i class="fa fa-check"></i><b>1.5</b> sparklyr</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#intro-recap"><i class="fa fa-check"></i><b>1.6</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="starting.html"><a href="starting.html"><i class="fa fa-check"></i><b>2</b> Getting Started</a><ul>
<li class="chapter" data-level="2.1" data-path="starting.html"><a href="starting.html#starting-prerequisites"><i class="fa fa-check"></i><b>2.1</b> Prerequisites</a></li>
<li class="chapter" data-level="2.2" data-path="starting.html"><a href="starting.html#starting-install-sparklyr"><i class="fa fa-check"></i><b>2.2</b> Installing sparklyr</a></li>
<li class="chapter" data-level="2.3" data-path="starting.html"><a href="starting.html#starting-installing-spark"><i class="fa fa-check"></i><b>2.3</b> Installing Spark</a></li>
<li class="chapter" data-level="2.4" data-path="starting.html"><a href="starting.html#starting-connect-to-spark"><i class="fa fa-check"></i><b>2.4</b> Connecting to Spark</a></li>
<li class="chapter" data-level="2.5" data-path="starting.html"><a href="starting.html#starting-sparklyr-hello-world"><i class="fa fa-check"></i><b>2.5</b> Using Spark</a><ul>
<li class="chapter" data-level="2.5.1" data-path="starting.html"><a href="starting.html#starting-spark-web-interface"><i class="fa fa-check"></i><b>2.5.1</b> Web Interface</a></li>
<li class="chapter" data-level="2.5.2" data-path="starting.html"><a href="starting.html#starting-analysis"><i class="fa fa-check"></i><b>2.5.2</b> Analysis</a></li>
<li class="chapter" data-level="2.5.3" data-path="starting.html"><a href="starting.html#starting-modeling"><i class="fa fa-check"></i><b>2.5.3</b> Modeling</a></li>
<li class="chapter" data-level="2.5.4" data-path="starting.html"><a href="starting.html#starting-data"><i class="fa fa-check"></i><b>2.5.4</b> Data</a></li>
<li class="chapter" data-level="2.5.5" data-path="starting.html"><a href="starting.html#starting-extensions"><i class="fa fa-check"></i><b>2.5.5</b> Extensions</a></li>
<li class="chapter" data-level="2.5.6" data-path="starting.html"><a href="starting.html#starting-distributed-r"><i class="fa fa-check"></i><b>2.5.6</b> Distributed R</a></li>
<li class="chapter" data-level="2.5.7" data-path="starting.html"><a href="starting.html#starting-streaming"><i class="fa fa-check"></i><b>2.5.7</b> Streaming</a></li>
<li class="chapter" data-level="2.5.8" data-path="starting.html"><a href="starting.html#starting-logs"><i class="fa fa-check"></i><b>2.5.8</b> Logs</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="starting.html"><a href="starting.html#starting-disconnecting"><i class="fa fa-check"></i><b>2.6</b> Disconnecting</a></li>
<li class="chapter" data-level="2.7" data-path="starting.html"><a href="starting.html#starting-using-spark-from-rstudio"><i class="fa fa-check"></i><b>2.7</b> Using RStudio</a></li>
<li class="chapter" data-level="2.8" data-path="starting.html"><a href="starting.html#starting-resources"><i class="fa fa-check"></i><b>2.8</b> Resources</a></li>
<li class="chapter" data-level="2.9" data-path="starting.html"><a href="starting.html#starting-recap"><i class="fa fa-check"></i><b>2.9</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="analysis.html"><a href="analysis.html"><i class="fa fa-check"></i><b>3</b> Analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="analysis.html"><a href="analysis.html#background"><i class="fa fa-check"></i><b>3.1</b> Background</a><ul>
<li class="chapter" data-level="3.1.1" data-path="analysis.html"><a href="analysis.html#working-with-big-data"><i class="fa fa-check"></i><b>3.1.1</b> Working with Big Data</a></li>
<li class="chapter" data-level="3.1.2" data-path="analysis.html"><a href="analysis.html#avoid-running-r-inside-spark"><i class="fa fa-check"></i><b>3.1.2</b> Avoid running R inside Spark</a></li>
<li class="chapter" data-level="3.1.3" data-path="analysis.html"><a href="analysis.html#r-under-the-hood"><i class="fa fa-check"></i><b>3.1.3</b> R, under the hood</a></li>
<li class="chapter" data-level="3.1.4" data-path="analysis.html"><a href="analysis.html#use-r-as-an-interface-to-spark"><i class="fa fa-check"></i><b>3.1.4</b> Use R as an interface to Spark</a></li>
<li class="chapter" data-level="3.1.5" data-path="analysis.html"><a href="analysis.html#using-sparklyr-for-analysis"><i class="fa fa-check"></i><b>3.1.5</b> Using <code>sparklyr</code> for analysis</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="analysis.html"><a href="analysis.html#basics"><i class="fa fa-check"></i><b>3.2</b> Basics</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="modeling.html"><a href="modeling.html"><i class="fa fa-check"></i><b>4</b> Modeling</a><ul>
<li class="chapter" data-level="4.1" data-path="modeling.html"><a href="modeling.html#overview"><i class="fa fa-check"></i><b>4.1</b> Overview</a></li>
<li class="chapter" data-level="4.2" data-path="modeling.html"><a href="modeling.html#supervised"><i class="fa fa-check"></i><b>4.2</b> Supervised</a></li>
<li class="chapter" data-level="4.3" data-path="modeling.html"><a href="modeling.html#unsupervised"><i class="fa fa-check"></i><b>4.3</b> Unsupervised</a><ul>
<li class="chapter" data-level="4.3.1" data-path="modeling.html"><a href="modeling.html#k-means-clustering"><i class="fa fa-check"></i><b>4.3.1</b> K-Means Clustering</a></li>
<li class="chapter" data-level="4.3.2" data-path="modeling.html"><a href="modeling.html#gaussian-mixture-clustering"><i class="fa fa-check"></i><b>4.3.2</b> Gaussian Mixture Clustering</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="modeling.html"><a href="modeling.html#broom"><i class="fa fa-check"></i><b>4.4</b> Broom</a></li>
<li class="chapter" data-level="4.5" data-path="modeling.html"><a href="modeling.html#pipelines"><i class="fa fa-check"></i><b>4.5</b> Pipelines</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="clusters.html"><a href="clusters.html"><i class="fa fa-check"></i><b>5</b> Clusters</a><ul>
<li class="chapter" data-level="5.1" data-path="clusters.html"><a href="clusters.html#clusters-overview"><i class="fa fa-check"></i><b>5.1</b> Overview</a></li>
<li class="chapter" data-level="5.2" data-path="clusters.html"><a href="clusters.html#clusters-manager"><i class="fa fa-check"></i><b>5.2</b> Managers</a><ul>
<li class="chapter" data-level="5.2.1" data-path="clusters.html"><a href="clusters.html#clusters-standalone"><i class="fa fa-check"></i><b>5.2.1</b> Standalone</a></li>
<li class="chapter" data-level="5.2.2" data-path="clusters.html"><a href="clusters.html#yarn"><i class="fa fa-check"></i><b>5.2.2</b> Yarn</a></li>
<li class="chapter" data-level="5.2.3" data-path="clusters.html"><a href="clusters.html#mesos"><i class="fa fa-check"></i><b>5.2.3</b> Mesos</a></li>
<li class="chapter" data-level="5.2.4" data-path="clusters.html"><a href="clusters.html#kubernetes"><i class="fa fa-check"></i><b>5.2.4</b> Kubernetes</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="clusters.html"><a href="clusters.html#on-premise"><i class="fa fa-check"></i><b>5.3</b> On-Premise</a><ul>
<li class="chapter" data-level="5.3.1" data-path="clusters.html"><a href="clusters.html#cloudera"><i class="fa fa-check"></i><b>5.3.1</b> Cloudera</a></li>
<li class="chapter" data-level="5.3.2" data-path="clusters.html"><a href="clusters.html#hortonworks"><i class="fa fa-check"></i><b>5.3.2</b> Hortonworks</a></li>
<li class="chapter" data-level="5.3.3" data-path="clusters.html"><a href="clusters.html#mapr"><i class="fa fa-check"></i><b>5.3.3</b> MapR</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="clusters.html"><a href="clusters.html#cloud"><i class="fa fa-check"></i><b>5.4</b> Cloud</a><ul>
<li class="chapter" data-level="5.4.1" data-path="clusters.html"><a href="clusters.html#clusters-amazon-emr"><i class="fa fa-check"></i><b>5.4.1</b> Amazon</a></li>
<li class="chapter" data-level="5.4.2" data-path="clusters.html"><a href="clusters.html#databricks"><i class="fa fa-check"></i><b>5.4.2</b> Databricks</a></li>
<li class="chapter" data-level="5.4.3" data-path="clusters.html"><a href="clusters.html#google"><i class="fa fa-check"></i><b>5.4.3</b> Google</a></li>
<li class="chapter" data-level="5.4.4" data-path="clusters.html"><a href="clusters.html#ibm"><i class="fa fa-check"></i><b>5.4.4</b> IBM</a></li>
<li class="chapter" data-level="5.4.5" data-path="clusters.html"><a href="clusters.html#microsoft"><i class="fa fa-check"></i><b>5.4.5</b> Microsoft</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="clusters.html"><a href="clusters.html#tools"><i class="fa fa-check"></i><b>5.5</b> Tools</a><ul>
<li class="chapter" data-level="5.5.1" data-path="clusters.html"><a href="clusters.html#rstudio"><i class="fa fa-check"></i><b>5.5.1</b> RStudio</a></li>
<li class="chapter" data-level="5.5.2" data-path="clusters.html"><a href="clusters.html#jupyter"><i class="fa fa-check"></i><b>5.5.2</b> Jupyter</a></li>
<li class="chapter" data-level="5.5.3" data-path="clusters.html"><a href="clusters.html#clusters-livy"><i class="fa fa-check"></i><b>5.5.3</b> Livy</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="clusters.html"><a href="clusters.html#recap"><i class="fa fa-check"></i><b>5.6</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="connections.html"><a href="connections.html"><i class="fa fa-check"></i><b>6</b> Connections</a><ul>
<li class="chapter" data-level="6.1" data-path="connections.html"><a href="connections.html#connections-overview"><i class="fa fa-check"></i><b>6.1</b> Overview</a><ul>
<li class="chapter" data-level="6.1.1" data-path="connections.html"><a href="connections.html#connections-spark-edge-nodes"><i class="fa fa-check"></i><b>6.1.1</b> Edge Nodes</a></li>
<li class="chapter" data-level="6.1.2" data-path="connections.html"><a href="connections.html#connections-spark-home"><i class="fa fa-check"></i><b>6.1.2</b> Spark Home</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="connections.html"><a href="connections.html#connections-local"><i class="fa fa-check"></i><b>6.2</b> Local</a></li>
<li class="chapter" data-level="6.3" data-path="connections.html"><a href="connections.html#connections-standalone"><i class="fa fa-check"></i><b>6.3</b> Standalone</a></li>
<li class="chapter" data-level="6.4" data-path="connections.html"><a href="connections.html#connections-yarn"><i class="fa fa-check"></i><b>6.4</b> Yarn</a><ul>
<li class="chapter" data-level="6.4.1" data-path="connections.html"><a href="connections.html#connections-yarn-client"><i class="fa fa-check"></i><b>6.4.1</b> Yarn Client</a></li>
<li class="chapter" data-level="6.4.2" data-path="connections.html"><a href="connections.html#connections-yarn-cluster"><i class="fa fa-check"></i><b>6.4.2</b> Yarn Cluster</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="connections.html"><a href="connections.html#connections-livy"><i class="fa fa-check"></i><b>6.5</b> Livy</a></li>
<li class="chapter" data-level="6.6" data-path="connections.html"><a href="connections.html#connections-mesos"><i class="fa fa-check"></i><b>6.6</b> Mesos</a></li>
<li class="chapter" data-level="6.7" data-path="connections.html"><a href="connections.html#connections-kubernetes"><i class="fa fa-check"></i><b>6.7</b> Kubernetes</a></li>
<li class="chapter" data-level="6.8" data-path="connections.html"><a href="connections.html#cloud-1"><i class="fa fa-check"></i><b>6.8</b> Cloud</a></li>
<li class="chapter" data-level="6.9" data-path="connections.html"><a href="connections.html#multiple"><i class="fa fa-check"></i><b>6.9</b> Multiple</a></li>
<li class="chapter" data-level="6.10" data-path="connections.html"><a href="connections.html#connections-troubleshooting"><i class="fa fa-check"></i><b>6.10</b> Troubleshooting</a><ul>
<li class="chapter" data-level="6.10.1" data-path="connections.html"><a href="connections.html#logging"><i class="fa fa-check"></i><b>6.10.1</b> Logging</a></li>
<li class="chapter" data-level="6.10.2" data-path="connections.html"><a href="connections.html#troubleshoot-spark-submit"><i class="fa fa-check"></i><b>6.10.2</b> Spark Submit</a></li>
<li class="chapter" data-level="6.10.3" data-path="connections.html"><a href="connections.html#windows"><i class="fa fa-check"></i><b>6.10.3</b> Windows</a></li>
</ul></li>
<li class="chapter" data-level="6.11" data-path="connections.html"><a href="connections.html#recap-1"><i class="fa fa-check"></i><b>6.11</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>7</b> Data</a><ul>
<li class="chapter" data-level="7.1" data-path="data.html"><a href="data.html#overview-1"><i class="fa fa-check"></i><b>7.1</b> Overview</a></li>
<li class="chapter" data-level="7.2" data-path="data.html"><a href="data.html#data-frames"><i class="fa fa-check"></i><b>7.2</b> Data Frames</a><ul>
<li class="chapter" data-level="7.2.1" data-path="data.html"><a href="data.html#data-sdf-functions"><i class="fa fa-check"></i><b>7.2.1</b> Functions</a></li>
<li class="chapter" data-level="7.2.2" data-path="data.html"><a href="data.html#pivoting"><i class="fa fa-check"></i><b>7.2.2</b> Pivoting</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="data.html"><a href="data.html#formats"><i class="fa fa-check"></i><b>7.3</b> Formats</a></li>
<li class="chapter" data-level="7.4" data-path="data.html"><a href="data.html#data-types"><i class="fa fa-check"></i><b>7.4</b> Data Types</a><ul>
<li class="chapter" data-level="7.4.1" data-path="data.html"><a href="data.html#dates"><i class="fa fa-check"></i><b>7.4.1</b> Dates</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="data.html"><a href="data.html#sources"><i class="fa fa-check"></i><b>7.5</b> Sources</a><ul>
<li class="chapter" data-level="7.5.1" data-path="data.html"><a href="data.html#amazon-s3"><i class="fa fa-check"></i><b>7.5.1</b> Amazon S3</a></li>
<li class="chapter" data-level="7.5.2" data-path="data.html"><a href="data.html#azure-storage"><i class="fa fa-check"></i><b>7.5.2</b> Azure Storage</a></li>
<li class="chapter" data-level="7.5.3" data-path="data.html"><a href="data.html#cassandra"><i class="fa fa-check"></i><b>7.5.3</b> Cassandra</a></li>
<li class="chapter" data-level="7.5.4" data-path="data.html"><a href="data.html#databases"><i class="fa fa-check"></i><b>7.5.4</b> Databases</a></li>
<li class="chapter" data-level="7.5.5" data-path="data.html"><a href="data.html#hbase"><i class="fa fa-check"></i><b>7.5.5</b> HBase</a></li>
<li class="chapter" data-level="7.5.6" data-path="data.html"><a href="data.html#nested-data"><i class="fa fa-check"></i><b>7.5.6</b> Nested Data</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="data.html"><a href="data.html#troubleshooting"><i class="fa fa-check"></i><b>7.6</b> Troubleshooting</a><ul>
<li class="chapter" data-level="7.6.1" data-path="data.html"><a href="data.html#troubleshoot-csvs"><i class="fa fa-check"></i><b>7.6.1</b> Troubleshoot CSVs</a></li>
<li class="chapter" data-level="7.6.2" data-path="data.html"><a href="data.html#column-names"><i class="fa fa-check"></i><b>7.6.2</b> Column Names</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="data.html"><a href="data.html#recap-2"><i class="fa fa-check"></i><b>7.7</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="tuning.html"><a href="tuning.html"><i class="fa fa-check"></i><b>8</b> Tuning</a><ul>
<li class="chapter" data-level="8.1" data-path="tuning.html"><a href="tuning.html#overview-2"><i class="fa fa-check"></i><b>8.1</b> Overview</a><ul>
<li class="chapter" data-level="8.1.1" data-path="tuning.html"><a href="tuning.html#tuning-graph-visualization"><i class="fa fa-check"></i><b>8.1.1</b> Graph Visualization</a></li>
<li class="chapter" data-level="8.1.2" data-path="tuning.html"><a href="tuning.html#tuning-event-timeline"><i class="fa fa-check"></i><b>8.1.2</b> Event Timeline</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="tuning.html"><a href="tuning.html#tuning-configuring"><i class="fa fa-check"></i><b>8.2</b> Configuring</a><ul>
<li class="chapter" data-level="8.2.1" data-path="tuning.html"><a href="tuning.html#connect-settings"><i class="fa fa-check"></i><b>8.2.1</b> Connect Settings</a></li>
<li class="chapter" data-level="8.2.2" data-path="tuning.html"><a href="tuning.html#submit-settings"><i class="fa fa-check"></i><b>8.2.2</b> Submit Settings</a></li>
<li class="chapter" data-level="8.2.3" data-path="tuning.html"><a href="tuning.html#runtime-settings"><i class="fa fa-check"></i><b>8.2.3</b> Runtime Settings</a></li>
<li class="chapter" data-level="8.2.4" data-path="tuning.html"><a href="tuning.html#sparklyr-settings"><i class="fa fa-check"></i><b>8.2.4</b> sparklyr Settings</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="tuning.html"><a href="tuning.html#tuning-partitioning"><i class="fa fa-check"></i><b>8.3</b> Partitioning</a><ul>
<li class="chapter" data-level="8.3.1" data-path="tuning.html"><a href="tuning.html#implicit"><i class="fa fa-check"></i><b>8.3.1</b> Implicit</a></li>
<li class="chapter" data-level="8.3.2" data-path="tuning.html"><a href="tuning.html#explicit"><i class="fa fa-check"></i><b>8.3.2</b> Explicit</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="tuning.html"><a href="tuning.html#tuning-caching"><i class="fa fa-check"></i><b>8.4</b> Caching</a><ul>
<li class="chapter" data-level="8.4.1" data-path="tuning.html"><a href="tuning.html#checkpointing"><i class="fa fa-check"></i><b>8.4.1</b> Checkpointing</a></li>
<li class="chapter" data-level="8.4.2" data-path="tuning.html"><a href="tuning.html#tuning-memory"><i class="fa fa-check"></i><b>8.4.2</b> Memory</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="tuning.html"><a href="tuning.html#tuning-shuffling"><i class="fa fa-check"></i><b>8.5</b> Shuffling</a></li>
<li class="chapter" data-level="8.6" data-path="tuning.html"><a href="tuning.html#tuning-serialization"><i class="fa fa-check"></i><b>8.6</b> Serialization</a></li>
<li class="chapter" data-level="8.7" data-path="tuning.html"><a href="tuning.html#configuration-files"><i class="fa fa-check"></i><b>8.7</b> Configuration Files</a></li>
<li class="chapter" data-level="8.8" data-path="tuning.html"><a href="tuning.html#recap-3"><i class="fa fa-check"></i><b>8.8</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="extensions.html"><a href="extensions.html"><i class="fa fa-check"></i><b>9</b> Extensions</a><ul>
<li class="chapter" data-level="9.1" data-path="extensions.html"><a href="extensions.html#rsparkling"><i class="fa fa-check"></i><b>9.1</b> RSparkling</a><ul>
<li class="chapter" data-level="9.1.1" data-path="extensions.html"><a href="extensions.html#troubleshooting-1"><i class="fa fa-check"></i><b>9.1.1</b> Troubleshooting</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="extensions.html"><a href="extensions.html#graphframes"><i class="fa fa-check"></i><b>9.2</b> GraphFrames</a></li>
<li class="chapter" data-level="9.3" data-path="extensions.html"><a href="extensions.html#mleap"><i class="fa fa-check"></i><b>9.3</b> Mleap</a></li>
<li class="chapter" data-level="9.4" data-path="extensions.html"><a href="extensions.html#extensions-nested-data"><i class="fa fa-check"></i><b>9.4</b> Nested Data</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="distributed.html"><a href="distributed.html"><i class="fa fa-check"></i><b>10</b> Distributed R</a><ul>
<li class="chapter" data-level="10.1" data-path="distributed.html"><a href="distributed.html#overview-3"><i class="fa fa-check"></i><b>10.1</b> Overview</a></li>
<li class="chapter" data-level="10.2" data-path="distributed.html"><a href="distributed.html#use-cases"><i class="fa fa-check"></i><b>10.2</b> Use Cases</a><ul>
<li class="chapter" data-level="10.2.1" data-path="distributed.html"><a href="distributed.html#custom-parsers"><i class="fa fa-check"></i><b>10.2.1</b> Custom Parsers</a></li>
<li class="chapter" data-level="10.2.2" data-path="distributed.html"><a href="distributed.html#partitioned-modeling"><i class="fa fa-check"></i><b>10.2.2</b> Partitioned Modeling</a></li>
<li class="chapter" data-level="10.2.3" data-path="distributed.html"><a href="distributed.html#grid-search"><i class="fa fa-check"></i><b>10.2.3</b> Grid Search</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="distributed.html"><a href="distributed.html#partitions"><i class="fa fa-check"></i><b>10.3</b> Partitions</a></li>
<li class="chapter" data-level="10.4" data-path="distributed.html"><a href="distributed.html#grouping"><i class="fa fa-check"></i><b>10.4</b> Grouping</a></li>
<li class="chapter" data-level="10.5" data-path="distributed.html"><a href="distributed.html#columns"><i class="fa fa-check"></i><b>10.5</b> Columns</a></li>
<li class="chapter" data-level="10.6" data-path="distributed.html"><a href="distributed.html#context"><i class="fa fa-check"></i><b>10.6</b> Context</a></li>
<li class="chapter" data-level="10.7" data-path="distributed.html"><a href="distributed.html#packages"><i class="fa fa-check"></i><b>10.7</b> Packages</a></li>
<li class="chapter" data-level="10.8" data-path="distributed.html"><a href="distributed.html#requirements"><i class="fa fa-check"></i><b>10.8</b> Requirements</a></li>
<li class="chapter" data-level="10.9" data-path="distributed.html"><a href="distributed.html#limitations"><i class="fa fa-check"></i><b>10.9</b> Limitations</a><ul>
<li class="chapter" data-level="10.9.1" data-path="distributed.html"><a href="distributed.html#functions"><i class="fa fa-check"></i><b>10.9.1</b> Functions</a></li>
<li class="chapter" data-level="10.9.2" data-path="distributed.html"><a href="distributed.html#livy"><i class="fa fa-check"></i><b>10.9.2</b> Livy</a></li>
<li class="chapter" data-level="10.9.3" data-path="distributed.html"><a href="distributed.html#grouping-1"><i class="fa fa-check"></i><b>10.9.3</b> Grouping</a></li>
<li class="chapter" data-level="10.9.4" data-path="distributed.html"><a href="distributed.html#packages-1"><i class="fa fa-check"></i><b>10.9.4</b> Packages</a></li>
</ul></li>
<li class="chapter" data-level="10.10" data-path="distributed.html"><a href="distributed.html#troubleshooting-2"><i class="fa fa-check"></i><b>10.10</b> Troubleshooting</a><ul>
<li class="chapter" data-level="10.10.1" data-path="distributed.html"><a href="distributed.html#worker-logs"><i class="fa fa-check"></i><b>10.10.1</b> Worker Logs</a></li>
<li class="chapter" data-level="10.10.2" data-path="distributed.html"><a href="distributed.html#partition-errors"><i class="fa fa-check"></i><b>10.10.2</b> Partition Errors</a></li>
<li class="chapter" data-level="10.10.3" data-path="distributed.html"><a href="distributed.html#debugging-workers"><i class="fa fa-check"></i><b>10.10.3</b> Debugging Workers</a></li>
</ul></li>
<li class="chapter" data-level="10.11" data-path="distributed.html"><a href="distributed.html#clusters-1"><i class="fa fa-check"></i><b>10.11</b> Clusters</a></li>
<li class="chapter" data-level="10.12" data-path="distributed.html"><a href="distributed.html#apache-arrow"><i class="fa fa-check"></i><b>10.12</b> Apache Arrow</a></li>
<li class="chapter" data-level="10.13" data-path="distributed.html"><a href="distributed.html#recap-4"><i class="fa fa-check"></i><b>10.13</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="streaming.html"><a href="streaming.html"><i class="fa fa-check"></i><b>11</b> Streaming</a><ul>
<li class="chapter" data-level="11.1" data-path="streaming.html"><a href="streaming.html#overview-4"><i class="fa fa-check"></i><b>11.1</b> Overview</a></li>
<li class="chapter" data-level="11.2" data-path="streaming.html"><a href="streaming.html#streaming-treansform"><i class="fa fa-check"></i><b>11.2</b> Transformations</a><ul>
<li class="chapter" data-level="11.2.1" data-path="streaming.html"><a href="streaming.html#streams-dplyr"><i class="fa fa-check"></i><b>11.2.1</b> dplyr</a></li>
<li class="chapter" data-level="11.2.2" data-path="streaming.html"><a href="streaming.html#streams-pipelines"><i class="fa fa-check"></i><b>11.2.2</b> Pipelines</a></li>
<li class="chapter" data-level="11.2.3" data-path="streaming.html"><a href="streaming.html#streams-r"><i class="fa fa-check"></i><b>11.2.3</b> R Code</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="streaming.html"><a href="streaming.html#shiny"><i class="fa fa-check"></i><b>11.3</b> Shiny</a></li>
<li class="chapter" data-level="11.4" data-path="streaming.html"><a href="streaming.html#formats-1"><i class="fa fa-check"></i><b>11.4</b> Formats</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="contributing.html"><a href="contributing.html"><i class="fa fa-check"></i><b>12</b> Contributing</a><ul>
<li class="chapter" data-level="12.1" data-path="contributing.html"><a href="contributing.html#overview-5"><i class="fa fa-check"></i><b>12.1</b> Overview</a></li>
<li class="chapter" data-level="12.2" data-path="contributing.html"><a href="contributing.html#contributing-r-extension"><i class="fa fa-check"></i><b>12.2</b> R Extensions</a></li>
<li class="chapter" data-level="12.3" data-path="contributing.html"><a href="contributing.html#scala-extensions"><i class="fa fa-check"></i><b>12.3</b> Scala Extensions</a><ul>
<li class="chapter" data-level="12.3.1" data-path="contributing.html"><a href="contributing.html#scala-extension-prereq"><i class="fa fa-check"></i><b>12.3.1</b> Prerequisites</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="contributing.html"><a href="contributing.html#spark-extensions"><i class="fa fa-check"></i><b>12.4</b> Spark Extensions</a></li>
<li class="chapter" data-level="12.5" data-path="contributing.html"><a href="contributing.html#r-packages"><i class="fa fa-check"></i><b>12.5</b> R Packages</a><ul>
<li class="chapter" data-level="12.5.1" data-path="contributing.html"><a href="contributing.html#rstudio-projects"><i class="fa fa-check"></i><b>12.5.1</b> RStudio Projects</a></li>
<li class="chapter" data-level="12.5.2" data-path="contributing.html"><a href="contributing.html#troubleshooting-3"><i class="fa fa-check"></i><b>12.5.2</b> Troubleshooting</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="contributing.html"><a href="contributing.html#contributing-sparklyr"><i class="fa fa-check"></i><b>12.6</b> sparklyr</a><ul>
<li class="chapter" data-level="12.6.1" data-path="contributing.html"><a href="contributing.html#compiling"><i class="fa fa-check"></i><b>12.6.1</b> Compiling</a></li>
<li class="chapter" data-level="12.6.2" data-path="contributing.html"><a href="contributing.html#serialization"><i class="fa fa-check"></i><b>12.6.2</b> Serialization</a></li>
<li class="chapter" data-level="12.6.3" data-path="contributing.html"><a href="contributing.html#invocations"><i class="fa fa-check"></i><b>12.6.3</b> Invocations</a></li>
<li class="chapter" data-level="12.6.4" data-path="contributing.html"><a href="contributing.html#r-packages-1"><i class="fa fa-check"></i><b>12.6.4</b> R Packages</a></li>
<li class="chapter" data-level="12.6.5" data-path="contributing.html"><a href="contributing.html#connections-1"><i class="fa fa-check"></i><b>12.6.5</b> Connections</a></li>
<li class="chapter" data-level="12.6.6" data-path="contributing.html"><a href="contributing.html#distributed-r"><i class="fa fa-check"></i><b>12.6.6</b> Distributed R</a></li>
</ul></li>
<li class="chapter" data-level="12.7" data-path="contributing.html"><a href="contributing.html#recap-5"><i class="fa fa-check"></i><b>12.7</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="12.8" data-path="appendix.html"><a href="appendix.html#appendix-prerequisites"><i class="fa fa-check"></i><b>12.8</b> Prerequisites</a><ul>
<li class="chapter" data-level="12.8.1" data-path="appendix.html"><a href="appendix.html#appendix-install-r"><i class="fa fa-check"></i><b>12.8.1</b> Installing R</a></li>
<li class="chapter" data-level="12.8.2" data-path="appendix.html"><a href="appendix.html#appendix-install-java"><i class="fa fa-check"></i><b>12.8.2</b> Installing Java</a></li>
<li class="chapter" data-level="12.8.3" data-path="appendix.html"><a href="appendix.html#appendix-install-rstudio"><i class="fa fa-check"></i><b>12.8.3</b> Installing RStudio</a></li>
<li class="chapter" data-level="12.8.4" data-path="appendix.html"><a href="appendix.html#appendix-using-rstudio"><i class="fa fa-check"></i><b>12.8.4</b> Using RStudio</a></li>
</ul></li>
<li class="chapter" data-level="12.9" data-path="appendix.html"><a href="appendix.html#diagrams"><i class="fa fa-check"></i><b>12.9</b> Diagrams</a><ul>
<li class="chapter" data-level="12.9.1" data-path="appendix.html"><a href="appendix.html#appendix-storage-capacity"><i class="fa fa-check"></i><b>12.9.1</b> Worlds Store Capacity</a></li>
<li class="chapter" data-level="12.9.2" data-path="appendix.html"><a href="appendix.html#appendix-cran-downloads"><i class="fa fa-check"></i><b>12.9.2</b> Daily downloads of CRAN packages</a></li>
<li class="chapter" data-level="12.9.3" data-path="appendix.html"><a href="appendix.html#appendix-cluster-trends"><i class="fa fa-check"></i><b>12.9.3</b> Google trends for mainframes, cloud computing and kubernetes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>13</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The R in Spark: Learning Apache Spark with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="tuning" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Tuning</h1>
<p>In previous chapters we’ve assumed that computation within a Spark cluster works efficiently. While this is true in some cases, it is often required to have some knowledge of the operations Spark runs internally to fine tune configuration settings that will make computations run efficiently. This chapter will explain how Spark computes data over large datasets and provide details on how to fine-tune its operations.</p>
<p>For instance, you will learn how to request more compute nodes and increase the amount of memory which, if you remember, defaults to only 500MB in local instances. You will learn how Spark unifies computation through partioning, shuffling and caching. As mentioned a few chapters back, this is the last chapter describing the internals of Spark; once you complete this chapter, it is our belief that you would have acquired intermediate Spark skills necessary to be productive at using Spark.</p>
<p>In subsequent chapters: Extensions, Distributed R and Streaming; you will learn exciting techniques to deal with specific scaling problems. However, we must first understand how spark performs internal computations, what pieces we can control and why.</p>
<div id="overview-2" class="section level2">
<h2><span class="header-section-number">8.1</span> Overview</h2>
<p>Spark performs distributed computation by: configuring, partitioning, executing, shuffling, caching and serializing data, tasks and resources across multiple machines:</p>
<ul>
<li><a href="tuning.html#tuning-configuring"><strong>Configuring</strong></a> requests the cluster manager for resources: total machines, memory, etc.</li>
<li><a href="tuning.html#tuning-configuring"><strong>Partitioning</strong></a> splits the data among various machines. Partitions can be either implicit or explicit.</li>
<li><a href="tuning.html#tuning-configuring"><strong>Executing</strong></a> means running an arbitrary transformation over each partition.</li>
<li><a href="tuning.html#tuning-configuring"><strong>Shuffling</strong></a> redistributes data when data to the correct machine.</li>
<li><a href="tuning.html#tuning-configuring"><strong>Caching</strong></a> preserves data in-memory across different computation cycles.</li>
<li><a href="#tuning-serializing"><strong>Serializing</strong></a> transforms data partitions or data collection to be sent over the network to other workers or back to the driver node.</li>
</ul>
<p>The following diagram shows an example on how a sorting <strong>job</strong> would conceptually work across a cluster of machines. First, Spark would <strong>configure</strong> the cluster to use three worker machines. In this example, the numbers 1-9 are partitioned across three storage instances. Since the data is already partitioned, each worker node loads this implicit <strong>partition</strong>; for instance, <code>4,9,1</code> is loaded in the first worker node. Afterwards, a custom transformation is applied to each partition in each worker node, this is denoted by <code>f(x)</code> in the diagram below and is defined as a <strong>stage</strong> in Spark terminology. In this example, <code>f(x)</code> <strong>executes</strong> a sorting operation within a partition. Since Spark is general, execution over a partition can be as simple or complex as needed. Once the execution completes, the result is <strong>shuffled</strong> to the right machine to finish the sorting operation across the entire dataset. Once the data is sorted across the cluster, the sorted results can be optionally <strong>cached</strong> in memory to avoid rerunning this computation multiple times. Finally, a small subset of the cached results is <strong>serialized</strong>, through the network connecting the cluster machines, back to the driver node to print a preview of this sorting example.</p>
<div class="figure" style="text-align: center"><span id="fig:tuning-overview-render"></span>
<img src="images/tuning-spark-overview.png" alt="Sorting Distributed Data with Apache Spark." width="707" />
<p class="caption">
FIGURE 8.1: Sorting Distributed Data with Apache Spark.
</p>
</div>
<p>Notice that while the diagram above describes a sorting operation, a similar approach applies to filtering or joining datasets and analyzing and modeling data at scale. Spark provides support to perform custom partitions, custom shuffling, etc; however, these lower level operations are not exposed in <code>sparklyr</code>, instead, <code>sparklyr</code> makes those operations available through higher level commands provided by the data <a href="analysis.html#analysis">analysis</a> tools like <code>dplyr</code> or <code>DBI</code>, <a href="modeling.html#modeling">modeling</a> and by using many <a href="#using-extensions">community extensions</a>. For advanced use cases, one can always use the Spark’s Scala API through an <code>sparklyr</code> <a href="#r-extension">custom extensions</a> or run custom <a href="distributed.html#distributed">distributed R</a> code.</p>
<p>In order to effectively tune Spark computations, there are two tools that are useful to understand: the <a href="tuning.html#tuning-graph-visualization"><strong>graph visualization</strong></a> and the <a href="tuning.html#tuning-event-timeline"><strong>event timeline</strong></a>. Both tools are accessible through the <a href="starting.html#starting-spark-web-interface">Spark Web Interface</a> and then selecting a particular job and a particular state under this job.</p>
<div id="tuning-graph-visualization" class="section level3">
<h3><span class="header-section-number">8.1.1</span> Graph Visualization</h3>
<p>This <strong>graph visualization</strong> is found under each stage by expanding “DAG Visualization”. DAG stands for Directed Acyclic Graph, since all computations in Spark move computation forward without repeating previous steps, this helps Spark optimize computations effectively.</p>
<p>What you will see in this visualization is a breakdown of the operations that Spark had to perform (or is performing if the stage is still active) to execute your computation. It’s hard to understand what they mean the first time you see them, but as you execute more Spark jobs, this graph will become more familiar and will help you identify unexpected steps to investigate further.</p>
<p>The following graph represents the stage from ordering a dataset:</p>
<div class="figure" style="text-align: center"><span id="fig:tuning-graph-render"></span>
<img src="images/tuning-spark-graph-visualization.png" alt="Spark Graph Visualization." width="981" />
<p class="caption">
FIGURE 8.2: Spark Graph Visualization.
</p>
</div>
</div>
<div id="tuning-event-timeline" class="section level3">
<h3><span class="header-section-number">8.1.2</span> Event Timeline</h3>
<p>The <strong>event timeline</strong> is one of the best ways to optimize your Spark jobs is to use the Spark’s <a href="starting.html#starting-spark-web-interface">web interface</a>, it’s also available for each Spark stage and gives you a great summary of how Spark is spending computation cycles. In general, you want to see a lot of CPU usage since the other tasks can be considered overhead. You also want to see one event lane per CPU allocated from the cluster to your job so ensure you are fully utilizing your Spark cluster.</p>
<p>Lets the take a look at the event timeline for the ordering a data frame by a given column using three partitions:</p>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb119-1" title="1"><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb119-2" title="2"><span class="st">  </span><span class="kw">copy_to</span>(iris, <span class="dt">repartition =</span> <span class="dv">3</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb119-3" title="3"><span class="st">  </span><span class="kw">arrange</span>(Sepal_Width)</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:tuning-timeline-shot"></span>
<img src="images/tuning-spark-event-timeline.png" alt="Spark Event Timeline." width="90%" />
<p class="caption">
FIGURE 8.3: Spark Event Timeline.
</p>
</div>
<p>Which shows that time is spent mostly on computing time, this means that, increasing compute nodes in the cluster should shorten the computation time. In contrast, for timelines heavy on read or write shuffling, requesting more compute nodes might not shorten time and might actualy makes everything slower. There is no concrete set of rules to follow; however, as you gain experience understanding this timeline over multiple operations – you will develop insight as to how to properly optimize Spark operations.</p>
</div>
</div>
<div id="tuning-configuring" class="section level2">
<h2><span class="header-section-number">8.2</span> Configuring</h2>
<p>When tuning a Spark application, the most common resources to configure are memory and cores, specifically:</p>
<ul>
<li><strong>Memory in Driver:</strong> The amount of memory required in the driver node.</li>
<li><strong>Memory per Worker:</strong> The amount of memory required in the worker nodes.</li>
<li><strong>Cores per Worker:</strong> The number of CPUs to required in the worker nodes.</li>
<li><strong>Number of Workers:</strong> The number of workers required for this session.</li>
</ul>
<p><strong>Note:</strong> It is recommended to request significantly more memory for the driver than the memory available over each worker node. In most cases, you will want to request one core per worker.</p>
<p>In local mode, <code>spark_connect(master = "local")</code>; as mentioned in the <a href="#connection-local">local connections</a> section, there are no workers, but we can set the driver memory and cores to use through:</p>
<div class="sourceCode" id="cb120"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb120-1" title="1"><span class="co"># Initialize configuration with defaults</span></a>
<a class="sourceLine" id="cb120-2" title="2">config &lt;-<span class="st"> </span><span class="kw">spark_config</span>()</a>
<a class="sourceLine" id="cb120-3" title="3"></a>
<a class="sourceLine" id="cb120-4" title="4"><span class="co"># Memory</span></a>
<a class="sourceLine" id="cb120-5" title="5">config[<span class="st">&quot;sparklyr.shell.driver-memory&quot;</span>] &lt;-<span class="st"> &quot;2g&quot;</span></a>
<a class="sourceLine" id="cb120-6" title="6"></a>
<a class="sourceLine" id="cb120-7" title="7"><span class="co"># Cores</span></a>
<a class="sourceLine" id="cb120-8" title="8">config[<span class="st">&quot;sparklyr.connect.cores.local&quot;</span>] &lt;-<span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb120-9" title="9"></a>
<a class="sourceLine" id="cb120-10" title="10"><span class="co"># Connect to local cluster with custom configuration</span></a>
<a class="sourceLine" id="cb120-11" title="11">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>, <span class="dt">config =</span> config)</a></code></pre></div>
<p>When using the Spark Standalone and the Mesos cluster managers, all the available memory and cores are assigned by default; therefore, there is no additional configuration changes required, unless, you want to restrict resources to allow multiple users to share this cluster, in which case you can use <code>total-executor-cores</code> to restrict the total executors requested. The “Spark Standalone”<span class="citation">(“Spark Standalone Mode” <a href="#ref-tuning-spark-standalone">2018</a>)</span> and “Spark on Mesos”<span class="citation">(“Running Spark on Mesos” <a href="#ref-tuning-spark-on-mesos">2018</a>)</span> guides provided additional information when sharing clusters.</p>
<p>When running under YARN Client, you would configure memory and cores as follows:</p>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb121-1" title="1"><span class="co"># Memory in Driver</span></a>
<a class="sourceLine" id="cb121-2" title="2">config[<span class="st">&quot;sparklyr.shell.driver-memory&quot;</span>] &lt;-<span class="st"> &quot;2g&quot;</span></a>
<a class="sourceLine" id="cb121-3" title="3"></a>
<a class="sourceLine" id="cb121-4" title="4"><span class="co"># Memory per Worker</span></a>
<a class="sourceLine" id="cb121-5" title="5">config[<span class="st">&quot;sparklyr.shell.executor-memory&quot;</span>] &lt;-<span class="st"> &quot;2g&quot;</span></a>
<a class="sourceLine" id="cb121-6" title="6"></a>
<a class="sourceLine" id="cb121-7" title="7"><span class="co"># Cores per Worker</span></a>
<a class="sourceLine" id="cb121-8" title="8">config[<span class="st">&quot;sparklyr.shell.executor-cores&quot;</span>] &lt;-<span class="st"> </span><span class="dv">1</span></a>
<a class="sourceLine" id="cb121-9" title="9"></a>
<a class="sourceLine" id="cb121-10" title="10"><span class="co"># Number of Workers</span></a>
<a class="sourceLine" id="cb121-11" title="11">config[<span class="st">&quot;sparklyr.shell.num-executors&quot;</span>] &lt;-<span class="st"> </span><span class="dv">3</span></a></code></pre></div>
<p>When using YARN in Cluster mode, <code>sparklyr.shell.driver-cores</code> can be used to configure total cores requested in the driver node, the “Spark on YARN”<span class="citation">(“Running Spark on Yarn” <a href="#ref-tuning-spark-on-yarn">2018</a>)</span> guide provides additional configuration settings worth familiarizing with.</p>
<p>There are a few types of configuration settings:</p>
<ul>
<li><strong>Connect</strong> settings are set as parameters to <code>spark_connect()</code>, they are common settings used while connecting.</li>
<li><strong>Submit</strong> settings are set while <code>sparklyr</code> is being submitted to Spark through <code>spark-submit</code>, some dependent on the cluster manager being used.</li>
<li><strong>Runtime</strong> settings configure Spark when the Spark session is created, these settings are independent to the cluster manager and specific to Spark.</li>
<li><strong>sparklyr</strong> settings configure <code>sparklyr</code> behaviour, these settings are independent to the cluster manager and particular to R.</li>
</ul>
<p>The following subsections present extensive lists of all the available settings, it is not required to fully understand them all while tuning Spark, but skimming through them could prove useful in the future while troubleshooting issues. You can also consider skipping the following settings subsections and use them instead as reference material as needed.</p>
<div id="connect-settings" class="section level3">
<h3><span class="header-section-number">8.2.1</span> Connect Settings</h3>
<table>
<colgroup>
<col width="21%" />
<col width="78%" />
</colgroup>
<thead>
<tr class="header">
<th>name</th>
<th>value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>master</td>
<td>Spark cluster url to connect to. Use “local” to connect to a local instance of Spark installed via <code>spark_install()</code>.</td>
</tr>
<tr class="even">
<td>spark_home</td>
<td>The path to a Spark installation. Defaults to the path provided by the SPARK_HOME environment variable. If SPARK_HOME is defined, it will be always be used unless the version parameter is specified to force the use of a locally installed version.</td>
</tr>
<tr class="odd">
<td>method</td>
<td>The method used to connect to Spark. Default connection method is “shell” to connect using spark-submit, use “livy” to perform remote connections using HTTP, or “databricks” when using a Databricks clusters.</td>
</tr>
<tr class="even">
<td>app_name</td>
<td>The application name to be used while running in the Spark cluster.</td>
</tr>
<tr class="odd">
<td>version</td>
<td>The version of Spark to use. Only applicable to “local” Spark connections.</td>
</tr>
<tr class="even">
<td>hadoop_version</td>
<td>The version of Hadoop to use. Only applicable to “local” Spark connections.</td>
</tr>
<tr class="odd">
<td>config</td>
<td>Custom configuration for the generated Spark connection. See spark_config for details.</td>
</tr>
</tbody>
</table>
</div>
<div id="submit-settings" class="section level3">
<h3><span class="header-section-number">8.2.2</span> Submit Settings</h3>
<p>Some settings must be specified when <code>spark-submit</code> (the terminal application that launches Spark) is run. For instance, since <code>spark-submit</code> launches driver node which runs as a Java instance, choosing how much memory is allocated needs to be specified as a parameter to <code>spark-submit</code>.</p>
<p>You can list all the available <code>spark-submit</code> parameters by running:</p>
<div class="sourceCode" id="cb122"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb122-1" title="1"><span class="kw">spark_home_dir</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">file.path</span>(<span class="st">&quot;bin&quot;</span>, <span class="st">&quot;spark-submit&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">system2</span>()</a></code></pre></div>
<p>For readability, we’ve provided the output of this command in table format, replacing the <code>spark-submit</code> parameter with the appropriate <code>spark_config()</code> setting and removing the parameters that are not applicable or already presented in this chapter:</p>
<table>
<colgroup>
<col width="37%" />
<col width="62%" />
</colgroup>
<thead>
<tr class="header">
<th>name</th>
<th>value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>sparklyr.shell.jars</td>
<td>Specified as ‘jars’ parameter in ‘spark_connect()’.</td>
</tr>
<tr class="even">
<td>sparklyr.shell.packages</td>
<td>Comma-separated list of maven coordinates of jars to include on the driver and executor classpaths. Will search the local maven repo, then maven central and any additional remote repositories given by ‘sparklyr.shell.repositories’. The format for the coordinates should be groupId:artifactId:version.</td>
</tr>
<tr class="odd">
<td>sparklyr.shell.exclude-packages</td>
<td>Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in ‘sparklyr.shell.packages’ to avoid dependency conflicts.</td>
</tr>
<tr class="even">
<td>sparklyr.shell.repositories</td>
<td>Comma-separated list of additional remote repositories to search for the maven coordinates given with ‘sparklyr.shell.packages’</td>
</tr>
<tr class="odd">
<td>sparklyr.shell.files</td>
<td>Comma-separated list of files to be placed in the working directory of each executor. File paths of these files in executors can be accessed via SparkFiles.get(fileName).</td>
</tr>
<tr class="even">
<td>sparklyr.shell.conf</td>
<td>Arbitrary Spark configuration property set as PROP=VALUE.</td>
</tr>
<tr class="odd">
<td>sparklyr.shell.properties-file</td>
<td>Path to a file from which to load extra properties. If not specified, this will look for conf/spark-defaults.conf.</td>
</tr>
<tr class="even">
<td>sparklyr.shell.driver-java-options</td>
<td>Extra Java options to pass to the driver.</td>
</tr>
<tr class="odd">
<td>sparklyr.shell.driver-library-path</td>
<td>Extra library path entries to pass to the driver.</td>
</tr>
<tr class="even">
<td>sparklyr.shell.driver-class-path</td>
<td>Extra class path entries to pass to the driver. Note that jars added with ‘sparklyr.shell.jars’ are automatically included in the classpath.</td>
</tr>
<tr class="odd">
<td>sparklyr.shell.proxy-user</td>
<td>User to impersonate when submitting the application. This argument does not work with ‘sparklyr.shell.principal’ / ‘sparklyr.shell.keytab’.</td>
</tr>
<tr class="even">
<td>sparklyr.shell.verbose</td>
<td>Print additional debug output.</td>
</tr>
</tbody>
</table>
<p>The remaining ones are specific to YARN,</p>
<table>
<colgroup>
<col width="34%" />
<col width="65%" />
</colgroup>
<thead>
<tr class="header">
<th>name</th>
<th>value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>sparklyr.shell.queue</td>
<td>The YARN queue to submit to (Default: “default”).</td>
</tr>
<tr class="even">
<td>sparklyr.shell.archives</td>
<td>Comma separated list of archives to be extracted into the working directory of each executor.</td>
</tr>
<tr class="odd">
<td>sparklyr.shell.principal</td>
<td>Principal to be used to login to KDC, while running on secure HDFS.</td>
</tr>
<tr class="even">
<td>sparklyr.shell.keytab</td>
<td>The full path to the file that contains the keytab for the principal specified above. This keytab will be copied to the node running the Application Master via the Secure Distributed Cache, for renewing the login tickets and the delegation tokens periodically.</td>
</tr>
</tbody>
</table>
<p>In general, any <code>spark-submit</code> setting is configured through <code>sparklyr.shell.X</code>, where <code>X</code> is the name of the <code>spark-submit</code> parameter without the <code>--</code> prefix.</p>
</div>
<div id="runtime-settings" class="section level3">
<h3><span class="header-section-number">8.2.3</span> Runtime Settings</h3>
<p>As mentioned, some <strong>Spark</strong> settings configure the session runtime. The runtime settings are a superset of the <a href="tuning.html#submit-settings">submit settings</a> since is usually helpful to retrieve the current configuration even if a setting can’t be changed.</p>
<p>To list the Spark settings set in your current Spark session, you can run:</p>
<div class="sourceCode" id="cb123"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb123-1" title="1"><span class="kw">spark_session_config</span>(sc)</a></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left">name</th>
<th align="left">value</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">spark.master</td>
<td align="left">local[4]</td>
</tr>
<tr class="even">
<td align="left">spark.sql.shuffle.partitions</td>
<td align="left">4</td>
</tr>
<tr class="odd">
<td align="left">spark.driver.port</td>
<td align="left">62314</td>
</tr>
<tr class="even">
<td align="left">spark.submit.deployMode</td>
<td align="left">client</td>
</tr>
<tr class="odd">
<td align="left">spark.executor.id</td>
<td align="left">driver</td>
</tr>
<tr class="even">
<td align="left">spark.jars</td>
<td align="left">/Library/…/sparklyr/java/sparklyr-2.3-2.11.jar</td>
</tr>
<tr class="odd">
<td align="left">spark.app.id</td>
<td align="left">local-1545518234395</td>
</tr>
<tr class="even">
<td align="left">spark.env.SPARK_LOCAL_IP</td>
<td align="left">127.0.0.1</td>
</tr>
<tr class="odd">
<td align="left">spark.sql.catalogImplementation</td>
<td align="left">hive</td>
</tr>
<tr class="even">
<td align="left">spark.spark.port.maxRetries</td>
<td align="left">128</td>
</tr>
<tr class="odd">
<td align="left">spark.app.name</td>
<td align="left">sparklyr</td>
</tr>
<tr class="even">
<td align="left">spark.home</td>
<td align="left">/Users/…/spark/spark-2.3.2-bin-hadoop2.7</td>
</tr>
<tr class="odd">
<td align="left">spark.driver.host</td>
<td align="left">localhost</td>
</tr>
</tbody>
</table>
<p>However, there are many more configuration settings available in Spark as described in the “Spark Configuration”<span class="citation">(“Spark Configuration” <a href="#ref-tuning-spark-configuration">2018</a>)</span> guide. It is not in the scope of this book to describe them all so, if possible, take some time to identify the ones that might be of interest to your paritcular use cases.</p>
</div>
<div id="sparklyr-settings" class="section level3">
<h3><span class="header-section-number">8.2.4</span> sparklyr Settings</h3>
<p>Apart from Spark settings, there are a few settings particular to sparklyr listed below. <code>sparklyr.connect.cores</code> is useful to set the CPU cores to use in local mode; the remaining ones are not used as much while tuning, but they can prove helpful while troubleshooting other issues.</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb124-1" title="1"><span class="kw">spark_config_settings</span>()</a></code></pre></div>
<table>
<thead>
<tr class="header">
<th align="left">name</th>
<th align="left">description</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">sparklyr.apply.packages</td>
<td align="left">Configures default value for packages parameter in spark_apply().</td>
</tr>
<tr class="even">
<td align="left">sparklyr.apply.rlang</td>
<td align="left">Experimental feature. Turns on improved serialization for spark_apply().</td>
</tr>
<tr class="odd">
<td align="left">sparklyr.apply.schema.infer</td>
<td align="left">Number of rows collected to infer schema when column types specified in spark_apply().</td>
</tr>
<tr class="even">
<td align="left">sparklyr.arrow</td>
<td align="left">Use Apache Arrow to serialize data?</td>
</tr>
<tr class="odd">
<td align="left">sparklyr.backend.interval</td>
<td align="left">Total seconds sparklyr will check on a backend operation.</td>
</tr>
<tr class="even">
<td align="left">sparklyr.backend.timeout</td>
<td align="left">Total seconds before sparklyr will give up waiting for a backend operation to complete.</td>
</tr>
<tr class="odd">
<td align="left">sparklyr.collect.batch</td>
<td align="left">Total rows to collect when using batch collection, defaults to 100,000.</td>
</tr>
<tr class="even">
<td align="left">sparklyr.connect.aftersubmit</td>
<td align="left">R function to call after spark-submit executes.</td>
</tr>
<tr class="odd">
<td align="left">sparklyr.connect.app.jar</td>
<td align="left">The path to the sparklyr jar used in spark_connect().</td>
</tr>
<tr class="even">
<td align="left">sparklyr.cancellable</td>
<td align="left">Cancel spark jobs when the R session is interrupted?</td>
</tr>
<tr class="odd">
<td align="left">sparklyr.connect.cores.local</td>
<td align="left">Number of cores to use in spark_connect(master = “local”), defaults to parallel::detectCores().</td>
</tr>
<tr class="even">
<td align="left">sparklyr.connect.csv.embedded</td>
<td align="left">Regular expression to match against versions of Spark that require package extension to support CSVs.</td>
</tr>
<tr class="odd">
<td align="left">sparklyr.connect.csv.scala11</td>
<td align="left">Use Scala 2.11 jars when using embedded CSV jars in Spark 1.6.X.</td>
</tr>
<tr class="even">
<td align="left">sparklyr.connect.jars</td>
<td align="left">Additional JARs to include while submitting application to Spark.</td>
</tr>
<tr class="odd">
<td align="left">sparklyr.connect.master</td>
<td align="left">The cluster master as spark_connect() master parameter, notice that the ‘spark.master’ setting is usually preferred.</td>
</tr>
<tr class="even">
<td align="left">sparklyr.connect.packages</td>
<td align="left">Spark packages to include when connecting to Spark.</td>
</tr>
<tr class="odd">
<td align="left">sparklyr.connect.ondisconnect</td>
<td align="left">R function to call after spark_disconnect().</td>
</tr>
<tr class="even">
<td align="left">sparklyr.connect.sparksubmit</td>
<td align="left">Command executed instead of spark-submit when connecting.</td>
</tr>
<tr class="odd">
<td align="left">sparklyr.connect.timeout</td>
<td align="left">Total seconds before giving up connecting to the sparklyr gateway while initializing.</td>
</tr>
<tr class="even">
<td align="left">sparklyr.dplyr.period.splits</td>
<td align="left">Should ‘dplyr’ split column names into database and table?</td>
</tr>
<tr class="odd">
<td align="left">sparklyr.gateway.address</td>
<td align="left">The address of the driver machine.</td>
</tr>
<tr class="even">
<td align="left">sparklyr.gateway.config.retries</td>
<td align="left">Number of retries to retrieve port and address from config, useful when using functions to query port or address in kubernetes.</td>
</tr>
<tr class="odd">
<td align="left">sparklyr.gateway.interval</td>
<td align="left">Total of seconds sparkyr will check on a gateway connection.</td>
</tr>
<tr class="even">
<td align="left">sparklyr.gateway.port</td>
<td align="left">The port the sparklyr gateway uses in the driver machine.</td>
</tr>
<tr class="odd">
<td align="left">sparklyr.gateway.remote</td>
<td align="left">Should the sparklyr gateway allow remote connections? This is required in yarn cluster, etc.</td>
</tr>
<tr class="even">
<td align="left">sparklyr.gateway.routing</td>
<td align="left">Should the sparklyr gateway service route to other sessions? Consider disabling in kubernetes.</td>
</tr>
<tr class="odd">
<td align="left">sparklyr.gateway.service</td>
<td align="left">Should the sparklyr gateway be run as a service without shutting down when the last connection disconnects?</td>
</tr>
<tr class="even">
<td align="left">sparklyr.gateway.timeout</td>
<td align="left">Total seconds before giving up connecting to the sparklyr gateway after initialization.</td>
</tr>
<tr class="odd">
<td align="left">sparklyr.gateway.wait</td>
<td align="left">Total seconds to wait before retrying to contact the sparklyr gateway.</td>
</tr>
<tr class="even">
<td align="left">sparklyr.livy.auth</td>
<td align="left">Authentication method for Livy connections.</td>
</tr>
<tr class="odd">
<td align="left">sparklyr.livy.headers</td>
<td align="left">Additional HTTP headers for Livy connections.</td>
</tr>
<tr class="even">
<td align="left">sparklyr.livy.sources</td>
<td align="left">Should sparklyr sources be sourced when connecting? If false, manually register sparklyr jars.</td>
</tr>
<tr class="odd">
<td align="left">sparklyr.log.invoke</td>
<td align="left">Should every call to invoke() be printed in the console? Can be set to ‘callstack’ to log call stack.</td>
</tr>
<tr class="even">
<td align="left">sparklyr.log.console</td>
<td align="left">Should driver logs be printed in the console?</td>
</tr>
<tr class="odd">
<td align="left">sparklyr.progress</td>
<td align="left">Should job progress be reported to RStudio?</td>
</tr>
<tr class="even">
<td align="left">sparklyr.progress.interval</td>
<td align="left">Total of seconds to wait before attempting to retrieve job progress in Spark.</td>
</tr>
<tr class="odd">
<td align="left">sparklyr.sanitize.column.names</td>
<td align="left">Should partially unsupported column names be cleaned up?</td>
</tr>
<tr class="even">
<td align="left">sparklyr.stream.collect.timeout</td>
<td align="left">Total seconds before stopping collecting a stream sample in sdf_collect_stream().</td>
</tr>
<tr class="odd">
<td align="left">sparklyr.stream.validate.timeout</td>
<td align="left">Total seconds before stopping to check if stream has errors while being created.</td>
</tr>
<tr class="even">
<td align="left">sparklyr.verbose</td>
<td align="left">Use verbose logging across all sparklyr operations?</td>
</tr>
<tr class="odd">
<td align="left">sparklyr.verbose.na</td>
<td align="left">Use verbose logging when dealing with NAs?</td>
</tr>
<tr class="even">
<td align="left">sparklyr.verbose.sanitize</td>
<td align="left">Use verbose logging while sanitizing columns and other objects?</td>
</tr>
<tr class="odd">
<td align="left">sparklyr.worker.gateway.address</td>
<td align="left">The address of the worker machine, most likely localhost.</td>
</tr>
<tr class="even">
<td align="left">sparklyr.worker.gateway.port</td>
<td align="left">The port the sparklyr gateway uses in the driver machine.</td>
</tr>
<tr class="odd">
<td align="left">sparklyr.yarn.cluster.accepted.timeout</td>
<td align="left">Total seconds before giving up waiting for cluster resources in yarn cluster mode.</td>
</tr>
<tr class="even">
<td align="left">sparklyr.yarn.cluster.hostaddress.timeout</td>
<td align="left">Total seconds before giving up waiting for the cluster to assign a host address in yarn cluster mode.</td>
</tr>
<tr class="odd">
<td align="left">sparklyr.yarn.cluster.lookup.byname</td>
<td align="left">Should the current user name be used to filter yarn cluster jobs while searching for submitted one?</td>
</tr>
<tr class="even">
<td align="left">sparklyr.yarn.cluster.lookup.prefix</td>
<td align="left">Application name prefix used to filter yarn cluster jobs while searching for submitted one.</td>
</tr>
<tr class="odd">
<td align="left">sparklyr.yarn.cluster.lookup.username</td>
<td align="left">The user name used to filter yarn cluster jobs while searching for submitted one.</td>
</tr>
<tr class="even">
<td align="left">sparklyr.yarn.cluster.start.timeout</td>
<td align="left">Total seconds before giving up waiting for yarn cluster application to get registered.</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="tuning-partitioning" class="section level2">
<h2><span class="header-section-number">8.3</span> Partitioning</h2>
<p>As mentioned in the <a href="intro.html#intro-background">introduction</a> chapter, MapReduce and Spark were designed with the purpose of performing computations against data stored across many machines, the subset of the data available for computation over each compute instance is known as a <strong>partition</strong>.</p>
<p>By default, Spark will compute over each existing <strong>implicit</strong> partition since it’s more effective to run computations were the data is already located. However, there are cases where you will want to set an <strong>explicit</strong> partition to help Spark use more efficient use of your cluster resources.</p>
<div id="implicit" class="section level3">
<h3><span class="header-section-number">8.3.1</span> Implicit</h3>
<p>There is always an implicit partition for each computation that Spark will assign on its own. For instance, how your data is distributed in storage will match the default partitions Spark assigns.</p>
<p>You can get the number of partitions a computation will require through <code>sdf_num_partitions()</code>:</p>
<div class="sourceCode" id="cb125"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb125-1" title="1"><span class="kw">sdf_len</span>(sc, <span class="dv">10</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sdf_num_partitions</span>()</a></code></pre></div>
<pre><code>[1] 2</code></pre>
<p>While in most cases the default partitions works just fine, there are cases where we you will need to be explicit on the partitions you choose.</p>
</div>
<div id="explicit" class="section level3">
<h3><span class="header-section-number">8.3.2</span> Explicit</h3>
<p>There will be times when you have many more compute instances than data partitions, or much less compute instances than the number of partitions in your data. In both cases, it can help to <strong>repartition</strong> data to match your cluster resources.</p>
<p>Various <a href="data.html#data">data</a> functions, like <code>spark_read_csv()</code>, already support a <code>repartition</code> parameter to request Spark to repartition data appropriately. For instance, we can create a sequence of 10 numbers partitioned by 10 as follows:</p>
<div class="sourceCode" id="cb127"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb127-1" title="1"><span class="kw">sdf_len</span>(sc, <span class="dv">10</span>, <span class="dt">repartition =</span> <span class="dv">10</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sdf_num_partitions</span>()</a></code></pre></div>
<pre><code>[1] 10</code></pre>
<p>For datasets that are already partitioned, we can also use <code>sdf_repartition</code>:</p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb129-1" title="1"><span class="kw">sdf_len</span>(sc, <span class="dv">10</span>, <span class="dt">repartition =</span> <span class="dv">10</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb129-2" title="2"><span class="st">  </span><span class="kw">sdf_repartition</span>(<span class="dv">4</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb129-3" title="3"><span class="st">  </span><span class="kw">sdf_num_partitions</span>()</a></code></pre></div>
<pre><code>[1] 4</code></pre>
<p>The number of partitions usually changes significantly the speed and resources being used; for instance, the following example calculates the mean over 10M rows with different partition sizes.</p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb131-1" title="1"><span class="kw">library</span>(microbenchmark)</a>
<a class="sourceLine" id="cb131-2" title="2"><span class="kw">library</span>(ggplot2)</a>
<a class="sourceLine" id="cb131-3" title="3"></a>
<a class="sourceLine" id="cb131-4" title="4"><span class="kw">microbenchmark</span>(</a>
<a class="sourceLine" id="cb131-5" title="5">    <span class="st">&quot;1 Partition(s)&quot;</span> =<span class="st"> </span><span class="kw">sdf_len</span>(sc, <span class="dv">10</span><span class="op">^</span><span class="dv">7</span>, <span class="dt">repartition =</span> <span class="dv">1</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb131-6" title="6"><span class="st">      </span><span class="kw">summarise</span>(<span class="kw">mean</span>(id)) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">collect</span>(),</a>
<a class="sourceLine" id="cb131-7" title="7">    <span class="st">&quot;2 Partition(s)&quot;</span> =<span class="st"> </span><span class="kw">sdf_len</span>(sc, <span class="dv">10</span><span class="op">^</span><span class="dv">7</span>, <span class="dt">repartition =</span> <span class="dv">2</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb131-8" title="8"><span class="st">      </span><span class="kw">summarise</span>(<span class="kw">mean</span>(id)) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">collect</span>(),</a>
<a class="sourceLine" id="cb131-9" title="9">    <span class="dt">times =</span> <span class="dv">10</span></a>
<a class="sourceLine" id="cb131-10" title="10">) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">autoplot</span>() <span class="op">+</span><span class="st"> </span><span class="kw">theme_light</span>() </a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:tuning-partitioning-explicit-results"></span>
<img src="images/tuning-partition-explicit.png" alt="Computation speed when using explicit Spark partitions." width="1500" />
<p class="caption">
FIGURE 8.4: Computation speed when using explicit Spark partitions.
</p>
</div>
<p>The results show that sorting data with two partitions is almost twice as fast, this is the case since two CPUs can be used to execute this operation. However, it is not necessarily the case that higher-partitions produce faster computation; instead, partitioning data is particular to your computing cluster and the data analysis operations being performed.</p>
</div>
</div>
<div id="tuning-caching" class="section level2">
<h2><span class="header-section-number">8.4</span> Caching</h2>
<p>From the <a href="Intro">introduction</a> chapter, we know that Spark was designed to be faster than its predecessors by using memory instead of disk to store data, this is formally known as an Spark <strong>RDD</strong> and stands for resilient distributed dataset. An RDD is resilient by duplicating copies of the same data across many machines, such that, if one machine fails other can complete the task. Resiliency is important in distributed systems since, while things will usually work in one machine, when running over thousands of machines the likelihood of something failing is much higher; when a failure happens, it is preferable be fault tolerant to avoid losing the work of all the other machines. RDDs are fault tolerant by tracking data lineage information to rebuild lost data automatically on failure.</p>
<p>In <code>sparklyr</code>, you can control when an RDD gets loaded or unloaded from memory using <code>tbl_cache()</code> and <code>tbl_uncache()</code>.</p>
<p>Most sparklyr operations that retrieve a Spark DataFrame, cache the results in-memory, for instance, running <code>spark_read_parquet()</code> or <code>sdf_copy_to()</code> will provide a Spark DataFrame that is already cached in-memory. As a Spark DataFrame, this object can be used in most <code>sparklyr</code> functions, including data analysis with dplyr or machine learning.</p>
<div class="sourceCode" id="cb132"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb132-1" title="1"><span class="kw">library</span>(sparklyr)</a>
<a class="sourceLine" id="cb132-2" title="2">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>)</a></code></pre></div>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb133-1" title="1">iris_tbl &lt;-<span class="st"> </span><span class="kw">sdf_copy_to</span>(sc, iris, <span class="dt">overwrite =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<p>You can inspect which tables are cached by navigating to the Spark UI using <code>spark_web(sc)</code>, opening the storage tab, and clicking on a given RDD:</p>
<div class="figure" style="text-align: center"><span id="fig:tuning-caching-rdd-shot"></span>
<img src="images/tuning-cache-rdd-web.png" alt="Cached RDD in Spark Web Interface." width="494" />
<p class="caption">
FIGURE 8.5: Cached RDD in Spark Web Interface.
</p>
</div>
<p>Data loaded in memory will be released when the R session terminates either explicitly or implicitly with a restart or disconnection; however, to free up resources, you can use <code>tbl_uncache()</code>:</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb134-1" title="1"><span class="kw">tbl_uncache</span>(sc, <span class="st">&quot;iris&quot;</span>)</a></code></pre></div>
<div id="checkpointing" class="section level3">
<h3><span class="header-section-number">8.4.1</span> Checkpointing</h3>
<p>Checkpointing is a slightly different type of caching, while it also persists data it will, additionally, break the graph computation lineage. So for instance, if a cached partition is lost, it can be computed from the computation graph which is not possible while checkpointing since the source of computation is lost.</p>
<p>When performing expensive computation graphs, it can make sense to checkpoint to persist and break the computation lineage, this to help Spark reduce graph computation resources; otherwise, Spark might try to over-optimize a computation graph that is really not useful to optimize.</p>
<p>You can checkpoint explicitly by saving to CSV, Parquet, etc. files. Or let Spark checkpoint this for you using <code>sdf_checkpoint()</code> in <code>sparklyr</code> as follows.</p>
<p>Notice that checkpointing truncates the computation lineage graph which can speed up performance if the same intermediate result is used multiple times.</p>
</div>
<div id="tuning-memory" class="section level3">
<h3><span class="header-section-number">8.4.2</span> Memory</h3>
<p>Memory in Spark is categorized into: reserved, user, execution or storage:</p>
<ul>
<li><strong>Reserved:</strong> Reserved memory is the memory required by Spark to function and therefore, is overhead that is required and should not be configured. This value defaults to 300MB.</li>
<li><strong>User:</strong> User memory is the memory used to execute custom code, <code>sparklyr</code> only makes use of this memory indirectly when executing <code>dplyr</code> expressions or modeling a dataset.</li>
<li><strong>Execution:</strong> Execution memory is used to execute code by Spark, mostly, to process the results from the partition and perform shuffling.</li>
<li><strong>Storage:</strong> Storage memory is used to cache RDDs, for instance, when using <code>tbl_cache()</code> in <code>sparklyr</code>.</li>
</ul>
<p>As part of tuning execution, you can consider tweaking the amount of memory allocated for <strong>user</strong>, <strong>execution</strong> and <strong>storage</strong> by creating a Spark connection with different values than the defaults provided in Spark:</p>
<div class="sourceCode" id="cb135"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb135-1" title="1">config &lt;-<span class="st"> </span><span class="kw">spark_config</span>()</a>
<a class="sourceLine" id="cb135-2" title="2"></a>
<a class="sourceLine" id="cb135-3" title="3"><span class="co"># define memory available for storage and execution</span></a>
<a class="sourceLine" id="cb135-4" title="4">config<span class="op">$</span>spark.memory.fraction &lt;-<span class="st"> </span><span class="fl">0.75</span></a>
<a class="sourceLine" id="cb135-5" title="5"></a>
<a class="sourceLine" id="cb135-6" title="6"><span class="co"># define memory available for storage</span></a>
<a class="sourceLine" id="cb135-7" title="7">config<span class="op">$</span>spark.memory.storageFraction &lt;-<span class="st"> </span><span class="fl">0.5</span></a></code></pre></div>
<p>For instance, if you want to use Spark to store large amounts of data in-memory with the purpose of filtering and retrieving subsets quickly, you can expect Spark to use little execution or user memory; therefore, to maximize storage memory, one can tune Spark as follows:</p>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb136-1" title="1">config &lt;-<span class="st"> </span><span class="kw">spark_config</span>()</a>
<a class="sourceLine" id="cb136-2" title="2"></a>
<a class="sourceLine" id="cb136-3" title="3"><span class="co"># define memory available for storage and execution</span></a>
<a class="sourceLine" id="cb136-4" title="4">config<span class="op">$</span>spark.memory.fraction &lt;-<span class="st"> </span><span class="fl">0.90</span></a>
<a class="sourceLine" id="cb136-5" title="5"></a>
<a class="sourceLine" id="cb136-6" title="6"><span class="co"># define memory available for storage</span></a>
<a class="sourceLine" id="cb136-7" title="7">config<span class="op">$</span>spark.memory.storageFraction &lt;-<span class="st"> </span><span class="fl">0.90</span></a></code></pre></div>
<p>However, notice that Spark will borrow execution memory from storage and viceversa if needed and if possible; therefore, in practice, there should be little need to tune the memory settings.</p>
</div>
</div>
<div id="tuning-shuffling" class="section level2">
<h2><span class="header-section-number">8.5</span> Shuffling</h2>
<p>Shuffling, is the operation that redistributes data across machines, it is usually an expensive operation and therefore, one we try to minimize. One can easily identify is significant time is being spent shuffling by looking at the <a href="tuning.html#tuning-event-timeline">event timeline</a>. It is possible to reduce shuffling by reframing data analysis questions or hinting Spark appropriately.</p>
<p>For instance, when joining data frames that differ in size significantly, as in, one set being orders of magnitude smaller than the other one. You can consider using <code>sdf_broadcast()</code> to mark a data frame as small enough for use in broadcast joins, meaning, it pushes one of the smaller data frames to each of the worker nodes to reduce shuffling the bigger dataframe. One example for <code>sdf_broadcast()</code> follows:</p>
<div class="sourceCode" id="cb137"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb137-1" title="1"><span class="kw">sdf_len</span>(sc, <span class="dv">10000</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb137-2" title="2"><span class="st">    </span><span class="kw">sdf_broadcast</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb137-3" title="3"><span class="st">    </span><span class="kw">left_join</span>(<span class="kw">sdf_len</span>(sc, <span class="dv">100</span>))</a></code></pre></div>
</div>
<div id="tuning-serialization" class="section level2">
<h2><span class="header-section-number">8.6</span> Serialization</h2>
<p>It is not that common to have to adjust serialization when tuning Spark; however, it is worth mentioning there are alternative serialization modules like the <a href="https://github.com/EsotericSoftware/kryo">Kryo Serializer</a> that can provide performance improvements over the default <a href="http://docs.oracle.com/javase/6/docs/api/java/io/Serializable.html">Java Serializer</a>.</p>
<p>The Kryo Serializer can be enabled in <code>sparklyr</code> through:</p>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb138-1" title="1">config &lt;-<span class="st"> </span><span class="kw">spark_config</span>()</a>
<a class="sourceLine" id="cb138-2" title="2"></a>
<a class="sourceLine" id="cb138-3" title="3">config<span class="op">$</span>spark.serializer &lt;-<span class="st"> &quot;org.apache.spark.serializer.KryoSerializer&quot;</span></a>
<a class="sourceLine" id="cb138-4" title="4">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>, <span class="dt">config =</span> config)</a></code></pre></div>
</div>
<div id="configuration-files" class="section level2">
<h2><span class="header-section-number">8.7</span> Configuration Files</h2>
<p>Configuring the <code>spark_config()</code> settings before connecting is the most common approach while tuning Spark. However, once the desired connection is known, you should consider switching to use a configuration file since it will remove the clutter in your connection code and also allow you to share the configuration settings across projects and coworkers.</p>
<p>For instance, instead of connecting to Spark through:</p>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb139-1" title="1">config &lt;-<span class="st"> </span><span class="kw">spark_config</span>()</a>
<a class="sourceLine" id="cb139-2" title="2">config[<span class="st">&quot;sparklyr.shell.driver-memory&quot;</span>] &lt;-<span class="st"> &quot;2G&quot;</span></a>
<a class="sourceLine" id="cb139-3" title="3">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>, <span class="dt">config =</span> config)</a></code></pre></div>
<p>You can instead define a <code>config.yml</code> with the desired settings. This file should be located in the current working directory or in parent directories. For example, you can create the following <code>config.yml</code> file to modify the default driver memory:</p>
<div class="sourceCode" id="cb140"><pre class="sourceCode yml"><code class="sourceCode yaml"><a class="sourceLine" id="cb140-1" title="1"><span class="fu">default:</span></a>
<a class="sourceLine" id="cb140-2" title="2">  <span class="fu">sparklyr.shell.driver-memory:</span><span class="at"> 2G</span></a></code></pre></div>
<p>Then, connecting with the same configuration settings becomes much cleaner by using instead:</p>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb141-1" title="1">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>)</a></code></pre></div>
<p>You can also specify an alternate config file name or location by setting the <code>file</code> parameter in <code>spark_config()</code>. One additional benefit from using configuration files, is that a system administrator can change the default configuration file without modifying the original one and additional functionality provided by the <code>config</code> package, see <a href="https://github.com/rstudio/config">github.com/rstudio/config</a>.</p>
</div>
<div id="recap-3" class="section level2">
<h2><span class="header-section-number">8.8</span> Recap</h2>
<p>This chapter provided a broad overview of Spark internals and a detailed configuration settings to help you speed up computation and enable high computation loads, it provided the foundations to understand bottlenecks and guidance on common configuration considerations; however, fine-tuning Spark is a broad topic that would require many more chapters to cover extensively. Therefore, while troubleshooting Spark’s performance and scalability, searching the web and consulting online communities it is often necessary to fine-tune your particular environment.</p>
<p>The next chapter, <a href="extensions.html#extensions">Extensions</a>, introduces the exciting world of <code>sparklyr</code> extensions. It’s exciting since you might find out that the extensions can be much more relevant to your particular interests and needs. For instance, they can process nested data, perform graph analysis or use different modeling libraries like <code>rsparkling</code> from H20. Not only that, but the next few chapters introduce many advanced data analysis and modeling topics that are required to master large-scale computing in R.</p>

</div>
</div>
<h3> References</h3>
<div id="refs" class="references">
<div id="ref-tuning-spark-standalone">
<p>“Spark Standalone Mode.” 2018. <a href="https://spark.apache.org/docs/latest/spark-standalone.html">https://spark.apache.org/docs/latest/spark-standalone.html</a>.</p>
</div>
<div id="ref-tuning-spark-on-mesos">
<p>“Running Spark on Mesos.” 2018. <a href="https://spark.apache.org/docs/latest/running-on-mesos.html">https://spark.apache.org/docs/latest/running-on-mesos.html</a>.</p>
</div>
<div id="ref-tuning-spark-on-yarn">
<p>“Running Spark on Yarn.” 2018. <a href="https://spark.apache.org/docs/latest/running-on-yarn.html">https://spark.apache.org/docs/latest/running-on-yarn.html</a>.</p>
</div>
<div id="ref-tuning-spark-configuration">
<p>“Spark Configuration.” 2018. <a href="https://spark.apache.org/docs/latest/configuration.html">https://spark.apache.org/docs/latest/configuration.html</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="data.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="extensions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
