<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>The R in Spark: Learning Apache Spark with R</title>
  <meta name="description" content="A book to learn Apache Spark with R using the sparklyr R package.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="The R in Spark: Learning Apache Spark with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A book to learn Apache Spark with R using the sparklyr R package." />
  <meta name="github-repo" content="javierluraschi/the-r-in-spark" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="The R in Spark: Learning Apache Spark with R" />
  
  <meta name="twitter:description" content="A book to learn Apache Spark with R using the sparklyr R package." />
  



<meta name="date" content="2018-12-21">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="data.html">
<link rel="next" href="extensions.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.3/htmlwidgets.js"></script>
<script src="libs/dagre-0.0.1/dagre.min.js"></script>
<script src="libs/lodash-3.7.0/lodash.js"></script>
<script src="libs/nomnoml-0.2.0/nomnoml.js"></script>
<script src="libs/nomnoml-binding-0.1.0/nomnoml.js"></script>
<script src="libs/r2d3-render-0.1.0/r2d3-render.js"></script>
<script src="libs/webcomponents-2.0.0/webcomponents.js"></script>
<script src="libs/r2d3-binding-0.2.3/r2d3.js"></script>
<script src="libs/d3v5-5.0.0/d3.min.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-119986300-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-119986300-1');
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./">Learning Apache Spark with R</a></strong></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#structure"><i class="fa fa-check"></i>Structure</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#authors"><i class="fa fa-check"></i>Authors</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#intro-background"><i class="fa fa-check"></i><b>1.1</b> Background</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#intro-spark"><i class="fa fa-check"></i><b>1.2</b> Spark</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#intro-r"><i class="fa fa-check"></i><b>1.3</b> R</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#intro-sparklyr"><i class="fa fa-check"></i><b>1.4</b> sparklyr</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="starting.html"><a href="starting.html"><i class="fa fa-check"></i><b>2</b> Getting Started</a><ul>
<li class="chapter" data-level="2.1" data-path="starting.html"><a href="starting.html#starting-prerequisites"><i class="fa fa-check"></i><b>2.1</b> Prerequisites</a><ul>
<li class="chapter" data-level="2.1.1" data-path="starting.html"><a href="starting.html#starting-install-r"><i class="fa fa-check"></i><b>2.1.1</b> Install R</a></li>
<li class="chapter" data-level="2.1.2" data-path="starting.html"><a href="starting.html#starting-install-java"><i class="fa fa-check"></i><b>2.1.2</b> Install Java</a></li>
<li class="chapter" data-level="2.1.3" data-path="starting.html"><a href="starting.html#starting-install-rstudio"><i class="fa fa-check"></i><b>2.1.3</b> Install RStudio</a></li>
<li class="chapter" data-level="2.1.4" data-path="starting.html"><a href="starting.html#starting-install-sparklyr"><i class="fa fa-check"></i><b>2.1.4</b> Install sparklyr</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="starting.html"><a href="starting.html#starting-installing-spark"><i class="fa fa-check"></i><b>2.2</b> Installing Spark</a></li>
<li class="chapter" data-level="2.3" data-path="starting.html"><a href="starting.html#starting-connect-to-spark"><i class="fa fa-check"></i><b>2.3</b> Connecting to Spark</a></li>
<li class="chapter" data-level="2.4" data-path="starting.html"><a href="starting.html#starting-sparklyr-hello-world"><i class="fa fa-check"></i><b>2.4</b> Using Spark</a><ul>
<li class="chapter" data-level="2.4.1" data-path="starting.html"><a href="starting.html#starting-spark-web-interface"><i class="fa fa-check"></i><b>2.4.1</b> Web Interface</a></li>
<li class="chapter" data-level="2.4.2" data-path="starting.html"><a href="starting.html#starting-logs"><i class="fa fa-check"></i><b>2.4.2</b> Logs</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="starting.html"><a href="starting.html#starting-disconnecting"><i class="fa fa-check"></i><b>2.5</b> Disconnecting</a></li>
<li class="chapter" data-level="2.6" data-path="starting.html"><a href="starting.html#starting-using-spark-from-rstudio"><i class="fa fa-check"></i><b>2.6</b> RStudio</a></li>
<li class="chapter" data-level="2.7" data-path="starting.html"><a href="starting.html#starting-resources"><i class="fa fa-check"></i><b>2.7</b> Resources</a></li>
<li class="chapter" data-level="2.8" data-path="starting.html"><a href="starting.html#starting-recap"><i class="fa fa-check"></i><b>2.8</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="analysis.html"><a href="analysis.html"><i class="fa fa-check"></i><b>3</b> Analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="analysis.html"><a href="analysis.html#typical-analysis"><i class="fa fa-check"></i><b>3.1</b> Typical analysis</a></li>
<li class="chapter" data-level="3.2" data-path="analysis.html"><a href="analysis.html#working-with-big-data"><i class="fa fa-check"></i><b>3.2</b> Working with Big Data</a></li>
<li class="chapter" data-level="3.3" data-path="analysis.html"><a href="analysis.html#running-r-in-spark"><i class="fa fa-check"></i><b>3.3</b> Running R in Spark</a></li>
<li class="chapter" data-level="3.4" data-path="analysis.html"><a href="analysis.html#r-as-an-interface-to-spark"><i class="fa fa-check"></i><b>3.4</b> R as an interface to Spark</a></li>
<li class="chapter" data-level="3.5" data-path="analysis.html"><a href="analysis.html#cut-here"><i class="fa fa-check"></i><b>3.5</b> Cut here</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="modeling.html"><a href="modeling.html"><i class="fa fa-check"></i><b>4</b> Modeling</a><ul>
<li class="chapter" data-level="4.1" data-path="modeling.html"><a href="modeling.html#overview"><i class="fa fa-check"></i><b>4.1</b> Overview</a></li>
<li class="chapter" data-level="4.2" data-path="modeling.html"><a href="modeling.html#supervised"><i class="fa fa-check"></i><b>4.2</b> Supervised</a></li>
<li class="chapter" data-level="4.3" data-path="modeling.html"><a href="modeling.html#unsupervised"><i class="fa fa-check"></i><b>4.3</b> Unsupervised</a><ul>
<li class="chapter" data-level="4.3.1" data-path="modeling.html"><a href="modeling.html#k-means-clustering"><i class="fa fa-check"></i><b>4.3.1</b> K-Means Clustering</a></li>
<li class="chapter" data-level="4.3.2" data-path="modeling.html"><a href="modeling.html#gaussian-mixture-clustering"><i class="fa fa-check"></i><b>4.3.2</b> Gaussian Mixture Clustering</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="modeling.html"><a href="modeling.html#broom"><i class="fa fa-check"></i><b>4.4</b> Broom</a></li>
<li class="chapter" data-level="4.5" data-path="modeling.html"><a href="modeling.html#pipelines"><i class="fa fa-check"></i><b>4.5</b> Pipelines</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="clusters.html"><a href="clusters.html"><i class="fa fa-check"></i><b>5</b> Clusters</a><ul>
<li class="chapter" data-level="5.1" data-path="clusters.html"><a href="clusters.html#clusters-overview"><i class="fa fa-check"></i><b>5.1</b> Overview</a></li>
<li class="chapter" data-level="5.2" data-path="clusters.html"><a href="clusters.html#managers"><i class="fa fa-check"></i><b>5.2</b> Managers</a><ul>
<li class="chapter" data-level="5.2.1" data-path="clusters.html"><a href="clusters.html#clusters-standalone"><i class="fa fa-check"></i><b>5.2.1</b> Standalone</a></li>
<li class="chapter" data-level="5.2.2" data-path="clusters.html"><a href="clusters.html#yarn"><i class="fa fa-check"></i><b>5.2.2</b> Yarn</a></li>
<li class="chapter" data-level="5.2.3" data-path="clusters.html"><a href="clusters.html#mesos"><i class="fa fa-check"></i><b>5.2.3</b> Mesos</a></li>
<li class="chapter" data-level="5.2.4" data-path="clusters.html"><a href="clusters.html#kubernetes"><i class="fa fa-check"></i><b>5.2.4</b> Kubernetes</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="clusters.html"><a href="clusters.html#on-premise"><i class="fa fa-check"></i><b>5.3</b> On-Premise</a><ul>
<li class="chapter" data-level="5.3.1" data-path="clusters.html"><a href="clusters.html#cloudera"><i class="fa fa-check"></i><b>5.3.1</b> Cloudera</a></li>
<li class="chapter" data-level="5.3.2" data-path="clusters.html"><a href="clusters.html#hortonworks"><i class="fa fa-check"></i><b>5.3.2</b> Hortonworks</a></li>
<li class="chapter" data-level="5.3.3" data-path="clusters.html"><a href="clusters.html#mapr"><i class="fa fa-check"></i><b>5.3.3</b> MapR</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="clusters.html"><a href="clusters.html#cloud"><i class="fa fa-check"></i><b>5.4</b> Cloud</a><ul>
<li class="chapter" data-level="5.4.1" data-path="clusters.html"><a href="clusters.html#clusters-amazon-emr"><i class="fa fa-check"></i><b>5.4.1</b> Amazon</a></li>
<li class="chapter" data-level="5.4.2" data-path="clusters.html"><a href="clusters.html#databricks"><i class="fa fa-check"></i><b>5.4.2</b> Databricks</a></li>
<li class="chapter" data-level="5.4.3" data-path="clusters.html"><a href="clusters.html#google"><i class="fa fa-check"></i><b>5.4.3</b> Google</a></li>
<li class="chapter" data-level="5.4.4" data-path="clusters.html"><a href="clusters.html#ibm"><i class="fa fa-check"></i><b>5.4.4</b> IBM</a></li>
<li class="chapter" data-level="5.4.5" data-path="clusters.html"><a href="clusters.html#microsoft"><i class="fa fa-check"></i><b>5.4.5</b> Microsoft</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="clusters.html"><a href="clusters.html#tools"><i class="fa fa-check"></i><b>5.5</b> Tools</a><ul>
<li class="chapter" data-level="5.5.1" data-path="clusters.html"><a href="clusters.html#rstudio"><i class="fa fa-check"></i><b>5.5.1</b> RStudio</a></li>
<li class="chapter" data-level="5.5.2" data-path="clusters.html"><a href="clusters.html#jupyter"><i class="fa fa-check"></i><b>5.5.2</b> Jupyter</a></li>
<li class="chapter" data-level="5.5.3" data-path="clusters.html"><a href="clusters.html#clusters-livy"><i class="fa fa-check"></i><b>5.5.3</b> Livy</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="clusters.html"><a href="clusters.html#recap"><i class="fa fa-check"></i><b>5.6</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="connections.html"><a href="connections.html"><i class="fa fa-check"></i><b>6</b> Connections</a><ul>
<li class="chapter" data-level="6.1" data-path="connections.html"><a href="connections.html#connections-overview"><i class="fa fa-check"></i><b>6.1</b> Overview</a><ul>
<li class="chapter" data-level="6.1.1" data-path="connections.html"><a href="connections.html#connections-edge-nodes"><i class="fa fa-check"></i><b>6.1.1</b> Edge Nodes</a></li>
<li class="chapter" data-level="6.1.2" data-path="connections.html"><a href="connections.html#connections-spark-home"><i class="fa fa-check"></i><b>6.1.2</b> Spark Home</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="connections.html"><a href="connections.html#types"><i class="fa fa-check"></i><b>6.2</b> Types</a><ul>
<li class="chapter" data-level="6.2.1" data-path="connections.html"><a href="connections.html#connections-local"><i class="fa fa-check"></i><b>6.2.1</b> Local</a></li>
<li class="chapter" data-level="6.2.2" data-path="connections.html"><a href="connections.html#connections-standalone"><i class="fa fa-check"></i><b>6.2.2</b> Standalone</a></li>
<li class="chapter" data-level="6.2.3" data-path="connections.html"><a href="connections.html#connections-yarn"><i class="fa fa-check"></i><b>6.2.3</b> Yarn</a></li>
<li class="chapter" data-level="6.2.4" data-path="connections.html"><a href="connections.html#connections-livy"><i class="fa fa-check"></i><b>6.2.4</b> Livy</a></li>
<li class="chapter" data-level="6.2.5" data-path="connections.html"><a href="connections.html#connections-mesos"><i class="fa fa-check"></i><b>6.2.5</b> Mesos</a></li>
<li class="chapter" data-level="6.2.6" data-path="connections.html"><a href="connections.html#connections-kubernetes"><i class="fa fa-check"></i><b>6.2.6</b> Kubernetes</a></li>
<li class="chapter" data-level="6.2.7" data-path="connections.html"><a href="connections.html#clusters-1"><i class="fa fa-check"></i><b>6.2.7</b> Clusters</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="connections.html"><a href="connections.html#troubleshooting"><i class="fa fa-check"></i><b>6.3</b> Troubleshooting</a><ul>
<li class="chapter" data-level="6.3.1" data-path="connections.html"><a href="connections.html#logging"><i class="fa fa-check"></i><b>6.3.1</b> Logging</a></li>
<li class="chapter" data-level="6.3.2" data-path="connections.html"><a href="connections.html#troubleshoot-spark-submit"><i class="fa fa-check"></i><b>6.3.2</b> Spark Submit</a></li>
<li class="chapter" data-level="6.3.3" data-path="connections.html"><a href="connections.html#multiple"><i class="fa fa-check"></i><b>6.3.3</b> Multiple</a></li>
<li class="chapter" data-level="6.3.4" data-path="connections.html"><a href="connections.html#windows"><i class="fa fa-check"></i><b>6.3.4</b> Windows</a></li>
<li class="chapter" data-level="6.3.5" data-path="connections.html"><a href="connections.html#submit-manually"><i class="fa fa-check"></i><b>6.3.5</b> Submit Manually</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="connections.html"><a href="connections.html#recap-1"><i class="fa fa-check"></i><b>6.4</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>7</b> Data</a><ul>
<li class="chapter" data-level="7.1" data-path="data.html"><a href="data.html#overview-1"><i class="fa fa-check"></i><b>7.1</b> Overview</a></li>
<li class="chapter" data-level="7.2" data-path="data.html"><a href="data.html#dataframes"><i class="fa fa-check"></i><b>7.2</b> DataFrames</a><ul>
<li class="chapter" data-level="7.2.1" data-path="data.html"><a href="data.html#data-sdf-functions"><i class="fa fa-check"></i><b>7.2.1</b> Functions</a></li>
<li class="chapter" data-level="7.2.2" data-path="data.html"><a href="data.html#pivoting"><i class="fa fa-check"></i><b>7.2.2</b> Pivoting</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="data.html"><a href="data.html#formats"><i class="fa fa-check"></i><b>7.3</b> Formats</a></li>
<li class="chapter" data-level="7.4" data-path="data.html"><a href="data.html#data-types"><i class="fa fa-check"></i><b>7.4</b> Data Types</a><ul>
<li class="chapter" data-level="7.4.1" data-path="data.html"><a href="data.html#dates"><i class="fa fa-check"></i><b>7.4.1</b> Dates</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="data.html"><a href="data.html#sources"><i class="fa fa-check"></i><b>7.5</b> Sources</a><ul>
<li class="chapter" data-level="7.5.1" data-path="data.html"><a href="data.html#amazon-s3"><i class="fa fa-check"></i><b>7.5.1</b> Amazon S3</a></li>
<li class="chapter" data-level="7.5.2" data-path="data.html"><a href="data.html#azure-storage"><i class="fa fa-check"></i><b>7.5.2</b> Azure Storage</a></li>
<li class="chapter" data-level="7.5.3" data-path="data.html"><a href="data.html#cassandra"><i class="fa fa-check"></i><b>7.5.3</b> Cassandra</a></li>
<li class="chapter" data-level="7.5.4" data-path="data.html"><a href="data.html#databases"><i class="fa fa-check"></i><b>7.5.4</b> Databases</a></li>
<li class="chapter" data-level="7.5.5" data-path="data.html"><a href="data.html#hbase"><i class="fa fa-check"></i><b>7.5.5</b> HBase</a></li>
<li class="chapter" data-level="7.5.6" data-path="data.html"><a href="data.html#nested-data"><i class="fa fa-check"></i><b>7.5.6</b> Nested Data</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="data.html"><a href="data.html#troubleshooting-1"><i class="fa fa-check"></i><b>7.6</b> Troubleshooting</a><ul>
<li class="chapter" data-level="7.6.1" data-path="data.html"><a href="data.html#troubleshoot-csvs"><i class="fa fa-check"></i><b>7.6.1</b> Troubleshoot CSVs</a></li>
<li class="chapter" data-level="7.6.2" data-path="data.html"><a href="data.html#column-names"><i class="fa fa-check"></i><b>7.6.2</b> Column Names</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="data.html"><a href="data.html#recap-2"><i class="fa fa-check"></i><b>7.7</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="tuning.html"><a href="tuning.html"><i class="fa fa-check"></i><b>8</b> Tuning</a><ul>
<li class="chapter" data-level="8.1" data-path="tuning.html"><a href="tuning.html#overview-2"><i class="fa fa-check"></i><b>8.1</b> Overview</a><ul>
<li class="chapter" data-level="8.1.1" data-path="tuning.html"><a href="tuning.html#tunning-graph-visualization"><i class="fa fa-check"></i><b>8.1.1</b> Graph Visualization</a></li>
<li class="chapter" data-level="8.1.2" data-path="tuning.html"><a href="tuning.html#tunning-event-timeline"><i class="fa fa-check"></i><b>8.1.2</b> Event Timeline</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="tuning.html"><a href="tuning.html#tunning-configuring"><i class="fa fa-check"></i><b>8.2</b> Configuring</a><ul>
<li class="chapter" data-level="8.2.1" data-path="tuning.html"><a href="tuning.html#submit-settings"><i class="fa fa-check"></i><b>8.2.1</b> Submit Settings</a></li>
<li class="chapter" data-level="8.2.2" data-path="tuning.html"><a href="tuning.html#runtime-settings"><i class="fa fa-check"></i><b>8.2.2</b> Runtime Settings</a></li>
<li class="chapter" data-level="8.2.3" data-path="tuning.html"><a href="tuning.html#sparklyr-settings"><i class="fa fa-check"></i><b>8.2.3</b> sparklyr Settings</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="tuning.html"><a href="tuning.html#tunning-partitioning"><i class="fa fa-check"></i><b>8.3</b> Partitioning</a><ul>
<li class="chapter" data-level="8.3.1" data-path="tuning.html"><a href="tuning.html#implicit"><i class="fa fa-check"></i><b>8.3.1</b> Implicit</a></li>
<li class="chapter" data-level="8.3.2" data-path="tuning.html"><a href="tuning.html#explicit"><i class="fa fa-check"></i><b>8.3.2</b> Explicit</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="tuning.html"><a href="tuning.html#tunning-caching"><i class="fa fa-check"></i><b>8.4</b> Caching</a><ul>
<li class="chapter" data-level="8.4.1" data-path="tuning.html"><a href="tuning.html#checkpointing"><i class="fa fa-check"></i><b>8.4.1</b> Checkpointing</a></li>
<li class="chapter" data-level="8.4.2" data-path="tuning.html"><a href="tuning.html#tunning-memory"><i class="fa fa-check"></i><b>8.4.2</b> Memory</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="tuning.html"><a href="tuning.html#tunning-shuffling"><i class="fa fa-check"></i><b>8.5</b> Shuffling</a></li>
<li class="chapter" data-level="8.6" data-path="tuning.html"><a href="tuning.html#tunning-serialization"><i class="fa fa-check"></i><b>8.6</b> Serialization</a></li>
<li class="chapter" data-level="8.7" data-path="tuning.html"><a href="tuning.html#recap-3"><i class="fa fa-check"></i><b>8.7</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="extensions.html"><a href="extensions.html"><i class="fa fa-check"></i><b>9</b> Extensions</a><ul>
<li class="chapter" data-level="9.1" data-path="extensions.html"><a href="extensions.html#rsparkling"><i class="fa fa-check"></i><b>9.1</b> RSparkling</a><ul>
<li class="chapter" data-level="9.1.1" data-path="extensions.html"><a href="extensions.html#troubleshooting-2"><i class="fa fa-check"></i><b>9.1.1</b> Troubleshooting</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="extensions.html"><a href="extensions.html#graphframes"><i class="fa fa-check"></i><b>9.2</b> GraphFrames</a></li>
<li class="chapter" data-level="9.3" data-path="extensions.html"><a href="extensions.html#mleap"><i class="fa fa-check"></i><b>9.3</b> Mleap</a></li>
<li class="chapter" data-level="9.4" data-path="extensions.html"><a href="extensions.html#extensions-nested-data"><i class="fa fa-check"></i><b>9.4</b> Nested Data</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="distributed.html"><a href="distributed.html"><i class="fa fa-check"></i><b>10</b> Distributed R</a><ul>
<li class="chapter" data-level="10.1" data-path="distributed.html"><a href="distributed.html#use-cases"><i class="fa fa-check"></i><b>10.1</b> Use Cases</a><ul>
<li class="chapter" data-level="10.1.1" data-path="distributed.html"><a href="distributed.html#embarrassingly-parallel"><i class="fa fa-check"></i><b>10.1.1</b> Embarrassingly Parallel</a></li>
</ul></li>
<li class="chapter" data-level="10.2" data-path="distributed.html"><a href="distributed.html#columns"><i class="fa fa-check"></i><b>10.2</b> Columns</a><ul>
<li class="chapter" data-level="10.2.1" data-path="distributed.html"><a href="distributed.html#inference"><i class="fa fa-check"></i><b>10.2.1</b> Inference</a></li>
<li class="chapter" data-level="10.2.2" data-path="distributed.html"><a href="distributed.html#excplicit"><i class="fa fa-check"></i><b>10.2.2</b> Excplicit</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="distributed.html"><a href="distributed.html#grouping"><i class="fa fa-check"></i><b>10.3</b> Grouping</a></li>
<li class="chapter" data-level="10.4" data-path="distributed.html"><a href="distributed.html#packages"><i class="fa fa-check"></i><b>10.4</b> Packages</a></li>
<li class="chapter" data-level="10.5" data-path="distributed.html"><a href="distributed.html#context"><i class="fa fa-check"></i><b>10.5</b> Context</a></li>
<li class="chapter" data-level="10.6" data-path="distributed.html"><a href="distributed.html#restrictions"><i class="fa fa-check"></i><b>10.6</b> Restrictions</a><ul>
<li class="chapter" data-level="10.6.1" data-path="distributed.html"><a href="distributed.html#troubleshooting-3"><i class="fa fa-check"></i><b>10.6.1</b> Troubleshooting</a></li>
</ul></li>
<li class="chapter" data-level="10.7" data-path="distributed.html"><a href="distributed.html#clusters-2"><i class="fa fa-check"></i><b>10.7</b> Clusters</a></li>
<li class="chapter" data-level="10.8" data-path="distributed.html"><a href="distributed.html#apache-arrow"><i class="fa fa-check"></i><b>10.8</b> Apache Arrow</a></li>
<li class="chapter" data-level="10.9" data-path="distributed.html"><a href="distributed.html#recap-4"><i class="fa fa-check"></i><b>10.9</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="streaming.html"><a href="streaming.html"><i class="fa fa-check"></i><b>11</b> Streaming</a><ul>
<li class="chapter" data-level="11.1" data-path="streaming.html"><a href="streaming.html#overview-3"><i class="fa fa-check"></i><b>11.1</b> Overview</a></li>
<li class="chapter" data-level="11.2" data-path="streaming.html"><a href="streaming.html#streaming-treansform"><i class="fa fa-check"></i><b>11.2</b> Transformations</a><ul>
<li class="chapter" data-level="11.2.1" data-path="streaming.html"><a href="streaming.html#streams-dplyr"><i class="fa fa-check"></i><b>11.2.1</b> dplyr</a></li>
<li class="chapter" data-level="11.2.2" data-path="streaming.html"><a href="streaming.html#streams-pipelines"><i class="fa fa-check"></i><b>11.2.2</b> Pipelines</a></li>
<li class="chapter" data-level="11.2.3" data-path="streaming.html"><a href="streaming.html#streams-r"><i class="fa fa-check"></i><b>11.2.3</b> R Code</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="streaming.html"><a href="streaming.html#shiny"><i class="fa fa-check"></i><b>11.3</b> Shiny</a></li>
<li class="chapter" data-level="11.4" data-path="streaming.html"><a href="streaming.html#formats-1"><i class="fa fa-check"></i><b>11.4</b> Formats</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="contributing.html"><a href="contributing.html"><i class="fa fa-check"></i><b>12</b> Contributing</a><ul>
<li class="chapter" data-level="12.1" data-path="contributing.html"><a href="contributing.html#overview-4"><i class="fa fa-check"></i><b>12.1</b> Overview</a></li>
<li class="chapter" data-level="12.2" data-path="contributing.html"><a href="contributing.html#contributing-r-extension"><i class="fa fa-check"></i><b>12.2</b> R Extensions</a></li>
<li class="chapter" data-level="12.3" data-path="contributing.html"><a href="contributing.html#scala-extensions"><i class="fa fa-check"></i><b>12.3</b> Scala Extensions</a><ul>
<li class="chapter" data-level="12.3.1" data-path="contributing.html"><a href="contributing.html#scala-extension-prereq"><i class="fa fa-check"></i><b>12.3.1</b> Prerequisites</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="contributing.html"><a href="contributing.html#spark-extensions"><i class="fa fa-check"></i><b>12.4</b> Spark Extensions</a></li>
<li class="chapter" data-level="12.5" data-path="contributing.html"><a href="contributing.html#r-packages"><i class="fa fa-check"></i><b>12.5</b> R Packages</a><ul>
<li class="chapter" data-level="12.5.1" data-path="contributing.html"><a href="contributing.html#rstudio-projects"><i class="fa fa-check"></i><b>12.5.1</b> RStudio Projects</a></li>
<li class="chapter" data-level="12.5.2" data-path="contributing.html"><a href="contributing.html#troubleshooting-4"><i class="fa fa-check"></i><b>12.5.2</b> Troubleshooting</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="contributing.html"><a href="contributing.html#contributing-sparklyr"><i class="fa fa-check"></i><b>12.6</b> sparklyr</a><ul>
<li class="chapter" data-level="12.6.1" data-path="contributing.html"><a href="contributing.html#compiling"><i class="fa fa-check"></i><b>12.6.1</b> Compiling</a></li>
<li class="chapter" data-level="12.6.2" data-path="contributing.html"><a href="contributing.html#serialization"><i class="fa fa-check"></i><b>12.6.2</b> Serialization</a></li>
<li class="chapter" data-level="12.6.3" data-path="contributing.html"><a href="contributing.html#invocations"><i class="fa fa-check"></i><b>12.6.3</b> Invocations</a></li>
<li class="chapter" data-level="12.6.4" data-path="contributing.html"><a href="contributing.html#r-packages-1"><i class="fa fa-check"></i><b>12.6.4</b> R Packages</a></li>
<li class="chapter" data-level="12.6.5" data-path="contributing.html"><a href="contributing.html#connections-1"><i class="fa fa-check"></i><b>12.6.5</b> Connections</a></li>
<li class="chapter" data-level="12.6.6" data-path="contributing.html"><a href="contributing.html#distributed-r"><i class="fa fa-check"></i><b>12.6.6</b> Distributed R</a></li>
</ul></li>
<li class="chapter" data-level="12.7" data-path="contributing.html"><a href="contributing.html#recap-5"><i class="fa fa-check"></i><b>12.7</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>13</b> Appendix</a><ul>
<li class="chapter" data-level="13.1" data-path="appendix.html"><a href="appendix.html#diagrams"><i class="fa fa-check"></i><b>13.1</b> Diagrams</a><ul>
<li class="chapter" data-level="13.1.1" data-path="appendix.html"><a href="appendix.html#appendix-storage-capacity"><i class="fa fa-check"></i><b>13.1.1</b> Worlds Store Capacity</a></li>
<li class="chapter" data-level="13.1.2" data-path="appendix.html"><a href="appendix.html#appendix-cran-downloads"><i class="fa fa-check"></i><b>13.1.2</b> Daily downloads of CRAN packages</a></li>
<li class="chapter" data-level="13.1.3" data-path="appendix.html"><a href="appendix.html#appendix-cluster-trends"><i class="fa fa-check"></i><b>13.1.3</b> Google trends for mainframes, cloud computing and kubernetes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>14</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The R in Spark: Learning Apache Spark with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="tuning" class="section level1">
<h1><span class="header-section-number">Chapter 8</span> Tuning</h1>
<p>Previous chapters focused on installing, using and connecting to Spark clusters, we’ve assumed so far that computation in a Spark cluster works efficiently. While this is true in many cases, it is often required to have some knowledge of how Spark works internally to perform tunning operations that will make computations run efficiently. Tunning is often required to run operations over datasets that make use of all resources in the Spark cluster. This chapter will explain how Spark works and provide details on how to tune its operations.</p>
<div id="overview-2" class="section level2">
<h2><span class="header-section-number">8.1</span> Overview</h2>
<p>Spark performs distributed computation by: configuring cluster resources and partitioning, executing, shuffling, caching and serializing data across machines.</p>
<ul>
<li><a href="tuning.html#tunning-configuring"><strong>Configuring</strong></a> requests the cluster manager for resources: total machines, memory, etc.</li>
<li><a href="tuning.html#tunning-configuring"><strong>Partitioning</strong></a> splits the data among various machines. Partitions can be either implicit or explicit.</li>
<li><a href="tuning.html#tunning-configuring"><strong>Executing</strong></a> means running an arbitrary transformation over each partition.</li>
<li><a href="tuning.html#tunning-configuring"><strong>Shuffling</strong></a> redistributes data when data to the correct machine.</li>
<li><a href="tuning.html#tunning-configuring"><strong>Caching</strong></a> preserves data in-memory across different computation cycles.</li>
<li><a href="#tunning-serializing"><strong>Serializing</strong></a> transforms data partitions or data collection to be sent over the network to other workers or back to the driver node.</li>
</ul>
<p>The following diagram shows an example on how a sorting <strong>job</strong> would conceptually work across a cluster of machines. First, Spark would <strong>configure</strong> the cluster to use three worker machines. In this example, the numbers 1-9 are partitioned across three storage instances. Since the data is already partitioned, each worker node loads this implicit <strong>partition</strong>; for instance, <code>4,9,1</code> is loaded in the first worker node. Afterwards, a custom transformation is applied to each partition in each worker node, this is denoted by <code>f(x)</code> in the diagram below and is defined as a <strong>stage</strong> in Spark terminalogy. In this example, <code>f(x)</code> <strong>executes</strong> a sorting operation within a partition. Since Spark is general, execution over a partition can be as simple or complex as needed. Once the execution completes, the result is <strong>shuffled</strong> to the right machine to finish the sorting operation across the entire dataset. Once the data is sorted across the cluster, the sorted results can be optionally <strong>cached</strong> in memory to avoid rerunning this computation multiple times. Finally, a small subset of the cached results is <strong>serialized</strong>, through the network connecting the cluster machines, back to the driver node to print a preview of this sorting example.</p>
<p>Notice that while the diagram above describes a sorting operation, a similar approach applies to filtering or joining datasets and analyzing and modeling data at scale. Spark provides support to perform custom partitions, custom shuffling, etc; however, these lower level operations are not exposed in <code>sparklyr</code>, instead, <code>sparklyr</code> makes those operations available through higher level commands provided by the data <a href="analysis.html#analysis">analysis</a> tools like <a href="streaming.html#streams-dplyr">dplyr</a> or [DBI], <a href="modeling.html#modeling">modeling</a> and by using many <a href="#using-extensions">community extensions</a>. For advanced use cases, one can always use the Spark’s Scala API through an <code>sparklyr</code> <a href="#r-extension">custom extensions</a> or run custom [distributed] R code.</p>
<p>In order to effectevely tune Spark computations, there are two toolsthat are useful to understand: the <a href="tuning.html#tunning-graph-visualization"><strong>graph visualization</strong></a> and the <a href="tuning.html#tunning-event-timeline"><strong>event timeline</strong></a>. Both tools are accessible through the <a href="starting.html#starting-spark-web-interface">Spark Web Interface</a> and then selecting a particular job and a oarticular state under this job.</p>
<div id="tunning-graph-visualization" class="section level3">
<h3><span class="header-section-number">8.1.1</span> Graph Visualization</h3>
<p>This <strong>graph visualization</strong> is found under each stage by expanding “DAG Visualization”. DAG stands for Directed Acyclic Graph, since all computations in Spark move computaiton forward without repeating previous steps, this helps Spark optimize computations effectevely.</p>
<p>What you will see in this visualization is a breakdown of the operations that Spark had to perform (or is performing if the stage is still active) to execute your computation. It’s hard to understand what they mean the first time you see them, but as you execute more Spark jobs, this graph will become more familiar and will help you identify unexpected steps to investigate further.</p>
<p>The following graph represents the stage from ordering a dataset:</p>
</div>
<div id="tunning-event-timeline" class="section level3">
<h3><span class="header-section-number">8.1.2</span> Event Timeline</h3>
<p>The <strong>event timeline</strong> is one of the best ways to optimize your Spark jobs is to use the Spark’s <a href="starting.html#starting-spark-web-interface">web interface</a>, it’s also available for each Spark stage and gives you a great summary of how Spark is spending computation cycles. In general, you want to see a lot of CPU usage since the other tasks can be considered overhead. You also want to see one event lane per CPU allocated from the cluster to your job so ensure you are fully utilizing your Spark cluster.</p>
<p>Lets the take a look at the event timeline for the ordering a data frame by a given column using three partitions:</p>
<div class="sourceCode" id="cb85"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb85-1" data-line-number="1"><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb85-2" data-line-number="2"><span class="st">  </span><span class="kw">copy_to</span>(iris, <span class="dt">repartition =</span> <span class="dv">3</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb85-3" data-line-number="3"><span class="st">  </span><span class="kw">arrange</span>(Sepal_Width)</a></code></pre></div>
</div>
</div>
<div id="tunning-configuring" class="section level2">
<h2><span class="header-section-number">8.2</span> Configuring</h2>
<p>When tuning a Spark application, consider defining a configuration specification to describe the resources your application needs to successfully run at scale.</p>
<p>Some of the most obvious resources you would want to define are:</p>
<ul>
<li><strong>Memory in Driver:</strong> The amount of memory available in the driver node, it is convenient to have significantly more memory available in the driver than the worker nodes.</li>
<li><strong>Number of Workers:</strong>. The number of workers required to be configured for this session.</li>
<li><strong>Memory per Worker:</strong> The amount of memory available to the worker node.</li>
</ul>
<p>In local mode, <code>spark_connect(master = &quot;local&quot;)</code>; as mentioned in the <a href="#connection-local">local connections</a> section, there are no workers; however, but we can set the driver settings through:</p>
<div class="sourceCode" id="cb86"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb86-1" data-line-number="1"><span class="co"># Initialize configuration with defaults</span></a>
<a class="sourceLine" id="cb86-2" data-line-number="2">config &lt;-<span class="st"> </span><span class="kw">spark_config</span>()</a>
<a class="sourceLine" id="cb86-3" data-line-number="3"></a>
<a class="sourceLine" id="cb86-4" data-line-number="4"><span class="co"># Memory in Driver</span></a>
<a class="sourceLine" id="cb86-5" data-line-number="5">config[<span class="st">&quot;sparklyr.shell.driver-memory&quot;</span>] &lt;-<span class="st"> &quot;2g&quot;</span></a>
<a class="sourceLine" id="cb86-6" data-line-number="6"></a>
<a class="sourceLine" id="cb86-7" data-line-number="7"><span class="co"># Number of Workers</span></a>
<a class="sourceLine" id="cb86-8" data-line-number="8">config[<span class="st">&quot;sparklyr.connect.cores.local&quot;</span>] &lt;-<span class="st"> </span><span class="dv">2</span></a>
<a class="sourceLine" id="cb86-9" data-line-number="9"></a>
<a class="sourceLine" id="cb86-10" data-line-number="10"><span class="co"># Connect to local cluster with custom configuration</span></a>
<a class="sourceLine" id="cb86-11" data-line-number="11">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>, <span class="dt">config =</span> config)</a></code></pre></div>
<p>When Spark runs in Hadoop Yarn, <code>spark_connect(master = &quot;yarn&quot;)</code>:</p>
<div class="sourceCode" id="cb87"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb87-1" data-line-number="1"><span class="co"># Initialize configuration with defaults</span></a>
<a class="sourceLine" id="cb87-2" data-line-number="2">config &lt;-<span class="st"> </span><span class="kw">spark_config</span>()</a>
<a class="sourceLine" id="cb87-3" data-line-number="3"></a>
<a class="sourceLine" id="cb87-4" data-line-number="4"><span class="co"># Memory in Driver</span></a>
<a class="sourceLine" id="cb87-5" data-line-number="5">config[<span class="st">&quot;sparklyr.shell.driver-memory&quot;</span>] &lt;-<span class="st"> &quot;2g&quot;</span></a>
<a class="sourceLine" id="cb87-6" data-line-number="6"></a>
<a class="sourceLine" id="cb87-7" data-line-number="7"><span class="co"># Total Workers</span></a>
<a class="sourceLine" id="cb87-8" data-line-number="8">config[<span class="st">&quot;sparklyr.shell.num-executors&quot;</span>] &lt;-<span class="st"> </span><span class="dv">3</span></a>
<a class="sourceLine" id="cb87-9" data-line-number="9"></a>
<a class="sourceLine" id="cb87-10" data-line-number="10"><span class="co"># Cores per Worker</span></a>
<a class="sourceLine" id="cb87-11" data-line-number="11">config[<span class="st">&quot;sparklyr.shell.executor-cores&quot;</span>] &lt;-<span class="st"> </span><span class="dv">4</span></a>
<a class="sourceLine" id="cb87-12" data-line-number="12"></a>
<a class="sourceLine" id="cb87-13" data-line-number="13"><span class="co"># Memory per Worker</span></a>
<a class="sourceLine" id="cb87-14" data-line-number="14">config[<span class="st">&quot;sparklyr.shell.executor-memory&quot;</span>] &lt;-<span class="st"> &quot;2g&quot;</span></a>
<a class="sourceLine" id="cb87-15" data-line-number="15"></a>
<a class="sourceLine" id="cb87-16" data-line-number="16"><span class="co"># Connect to Yarn with custom configuration</span></a>
<a class="sourceLine" id="cb87-17" data-line-number="17">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;yarn&quot;</span>, <span class="dt">config =</span> config)</a></code></pre></div>
<p>Notice that some of the settings are different between <a href="clusters.html#clusters">clusters</a>, local and YARN in the examples above. Therefore, it is common to research online which settings your cluster managers expects.</p>
<p>There are a few types of configuration settings:</p>
<ul>
<li><strong>Submit</strong> settings are set while <code>sparklyr</code> is being submitted to Spark. For instance, they can configure the driver node.</li>
<li><strong>Runtime</strong> settings configure Spark when the Spark session is created. For instance, to configure worker nodes settings.</li>
<li><strong>sparklyr</strong> settings configure <code>sparklyr</code> behaviour. For instance,<code>sparklyr.verbose</code> controls how much diagnostics data is printed.</li>
</ul>
<div id="submit-settings" class="section level3">
<h3><span class="header-section-number">8.2.1</span> Submit Settings</h3>
<p>Some settings must be specified when <code>spark-submit</code> (the terminal application that launches Spark) is run. For instance, since <code>spark-submit</code> launches driver node which runs as a Java instance, choosing how much memory is allocated needs to be specified as a parameter to <code>spark-submit</code>.</p>
<p>You can list all the available <code>spark-submit</code> parameters by running:</p>
<div class="sourceCode" id="cb88"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb88-1" data-line-number="1"><span class="kw">spark_home_dir</span>() <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">file.path</span>(<span class="st">&quot;bin&quot;</span>, <span class="st">&quot;spark-submit&quot;</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">system2</span>()</a></code></pre></div>
<p>Notice for instance the <code>--driver-memory</code> parameter, which we previously configured by setting:</p>
<div class="sourceCode" id="cb89"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb89-1" data-line-number="1">config &lt;-<span class="st"> </span><span class="kw">spark_config</span>()</a>
<a class="sourceLine" id="cb89-2" data-line-number="2">config[<span class="st">&quot;sparklyr.shell.driver-memory&quot;</span>] &lt;-<span class="st"> &quot;2gb&quot;</span></a></code></pre></div>
<p>In general, any <code>spark-submit</code> setting is configured through <code>sparklyr.shell.X</code> where <code>X</code> is the name of the <code>spark-submit</code> parameter without the <code>--</code> prefix.</p>
</div>
<div id="runtime-settings" class="section level3">
<h3><span class="header-section-number">8.2.2</span> Runtime Settings</h3>
<p>As mentioned, some <strong>Spark</strong> settings configure the session runtime. The runtime settings are a superset of the <a href="tuning.html#submit-settings">submit settings</a> since is usually helpfull to retrieve the current configuration even if a setting can’t be changed.</p>
<p>To list all the Spark settings available at runtime, we can run:</p>
<div class="sourceCode" id="cb90"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb90-1" data-line-number="1"><span class="kw">spark_session_config</span>(sc)</a></code></pre></div>
<div class="sourceCode" id="cb91"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb91-1" data-line-number="1">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>)</a>
<a class="sourceLine" id="cb91-2" data-line-number="2"></a>
<a class="sourceLine" id="cb91-3" data-line-number="3">settings &lt;-<span class="st"> </span><span class="kw">spark_session_config</span>(sc)</a>
<a class="sourceLine" id="cb91-4" data-line-number="4">cleaned &lt;-<span class="st"> </span><span class="kw">gsub</span>(<span class="st">&quot;Frameworks/R.framework/Versions/3.5/Resources/library&quot;</span>, <span class="st">&quot;...&quot;</span>, settings)</a>
<a class="sourceLine" id="cb91-5" data-line-number="5">cleaned &lt;-<span class="st"> </span><span class="kw">gsub</span>(<span class="st">&quot;javierluraschi&quot;</span>, <span class="st">&quot;...&quot;</span>, cleaned)</a>
<a class="sourceLine" id="cb91-6" data-line-number="6">cleaned &lt;-<span class="st"> </span><span class="kw">gsub</span>(<span class="st">&quot;^file:&quot;</span>, <span class="st">&quot;&quot;</span>, cleaned)</a>
<a class="sourceLine" id="cb91-7" data-line-number="7">knitr<span class="op">::</span><span class="kw">kable</span>(<span class="kw">data.frame</span>(<span class="dt">name =</span> <span class="kw">names</span>(settings), <span class="dt">value =</span> <span class="kw">unlist</span>(<span class="kw">unname</span>(cleaned))))</a></code></pre></div>
<p>See also, <a href="https://spark.apache.org/docs/latest/configuration.html">spark.apache.org/docs/latest/configuration.html</a>.</p>
</div>
<div id="sparklyr-settings" class="section level3">
<h3><span class="header-section-number">8.2.3</span> sparklyr Settings</h3>
<p>Appart from Spark settings, there are a few settings particular to sparklyr listed below. <code>sparklyr.connect.cores</code> is useful to set the CPU cores to use in local mode; the remaining ones are not used as much while tuning, but they can prove helpful while troubleshooting other issues.</p>
<div class="sourceCode" id="cb92"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb92-1" data-line-number="1"><span class="kw">spark_config_settings</span>()</a></code></pre></div>
</div>
</div>
<div id="tunning-partitioning" class="section level2">
<h2><span class="header-section-number">8.3</span> Partitioning</h2>
<p>As mentioned in the <a href="intro.html#intro-background">introduction</a> chapter, MapReduce and Spark were designed with the purpuse of performing computations against data stored across many machines, the subset of the data available for computation over each compute instance is known as a <strong>partition</strong>.</p>
<p>By default, Spark will compute over each existing <strong>implicit</strong> partition since it’s more effective to run computaitons were the data is already located. However, there are cases where you will want to set an <strong>explicit</strong> partition to help Spark use more efficient use of your cluster resources.</p>
<div id="implicit" class="section level3">
<h3><span class="header-section-number">8.3.1</span> Implicit</h3>
<p>There is always an implicit partition for each Spark computation. If your data is already spread across your cluster evenly, there is usually no need to tune this further.</p>
<p>You can get the number of partitions a computation will require through <code>sdf_num_partitions()</code>:</p>
<div class="sourceCode" id="cb93"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb93-1" data-line-number="1"><span class="kw">sdf_len</span>(sc, <span class="dv">10</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sdf_num_partitions</span>()</a></code></pre></div>
</div>
<div id="explicit" class="section level3">
<h3><span class="header-section-number">8.3.2</span> Explicit</h3>
<p>There will be times when you have many more compute instances than data partitions, or much less compute instances than the number of partitions in your data. In both cases, it can help to <strong>repartition</strong> data to match your cluster resources.</p>
<p>Various <a href="data.html#data">data</a> functions, like <code>spark_read_csv()</code>, already support a <code>repartition</code> parameter to requrest Spark to repartition data appropriately. For instance, we can create a sequence of 10 numbers partitioned by 10 as follows:</p>
<div class="sourceCode" id="cb94"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb94-1" data-line-number="1"><span class="kw">sdf_len</span>(sc, <span class="dv">10</span>, <span class="dt">repartition =</span> <span class="dv">10</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sdf_num_partitions</span>()</a></code></pre></div>
<p>For datasets that are already repartitioned, we can also use <code>sdf_repartition</code>:</p>
<div class="sourceCode" id="cb95"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb95-1" data-line-number="1"><span class="kw">sdf_len</span>(sc, <span class="dv">10</span>, <span class="dt">repartition =</span> <span class="dv">10</span>) <span class="op">%&gt;%</span><span class="st"> </span></a>
<a class="sourceLine" id="cb95-2" data-line-number="2"><span class="st">  </span><span class="kw">sdf_repartition</span>(<span class="dv">4</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb95-3" data-line-number="3"><span class="st">  </span><span class="kw">sdf_num_partitions</span>()</a></code></pre></div>
<p>However, lets look at this with a practical example. Suppose that we want to sort a large dataset that does not even fit in memory, for simplicity, we will generate this dataset by generating 1 billion rows and appending a column of random values. A first attempt to sort this in Spark would be to run:</p>
<div class="sourceCode" id="cb96"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb96-1" data-line-number="1"><span class="co"># Attempt to sort 20 GB dataset in disk with one billion entries</span></a>
<a class="sourceLine" id="cb96-2" data-line-number="2"><span class="kw">sdf_len</span>(sc, <span class="dv">10</span><span class="op">^</span><span class="dv">9</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb96-3" data-line-number="3"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">x =</span> <span class="kw">rand</span>()) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb96-4" data-line-number="4"><span class="st">  </span><span class="kw">arrange</span>(x) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb96-5" data-line-number="5"><span class="st">  </span><span class="kw">spark_write_csv</span>(<span class="st">&quot;billion.csv&quot;</span>)</a></code></pre></div>
<p>However, since each partition needs to fit in memory in Spark, the code above will result in an <code>OutOfMemory</code> exception that shuts down Spark completely. Instead, we can explicitly partition the data into chunks that would fit in the default memory configureation by explicitly defining the total number of partitions to use with the <code>repartition</code> parameter set to 10,000 as follows:</p>
<div class="sourceCode" id="cb97"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb97-1" data-line-number="1"><span class="kw">library</span>(sparklyr)</a>
<a class="sourceLine" id="cb97-2" data-line-number="2"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb97-3" data-line-number="3"></a>
<a class="sourceLine" id="cb97-4" data-line-number="4">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>)</a>
<a class="sourceLine" id="cb97-5" data-line-number="5"></a>
<a class="sourceLine" id="cb97-6" data-line-number="6"><span class="co"># Sort 20 GB dataset in disk with one billion entries</span></a>
<a class="sourceLine" id="cb97-7" data-line-number="7"><span class="kw">sdf_len</span>(sc, <span class="dv">10</span><span class="op">^</span><span class="dv">9</span>, <span class="dt">repartition =</span> <span class="dv">10</span><span class="op">^</span><span class="dv">4</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb97-8" data-line-number="8"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">x =</span> <span class="kw">rand</span>()) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb97-9" data-line-number="9"><span class="st">  </span><span class="kw">arrange</span>(x) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb97-10" data-line-number="10"><span class="st">  </span><span class="kw">spark_write_csv</span>(<span class="st">&quot;billion.csv&quot;</span>)</a></code></pre></div>
</div>
</div>
<div id="tunning-caching" class="section level2">
<h2><span class="header-section-number">8.4</span> Caching</h2>
<p>From the <a href="Intro">introduction</a> chapter, we know that Spark was designed to be faster than it’s predecesors by using memory instead of disk to store data, this is formally known as an Spark <strong>RDD</strong> and stands for resilient distributed dataset. An RDD is resilient by duplicating copies of the same data across many machines, such that, if one machine fails other can complete the task. Resiliency is important in distributed systems since, while things will usually work in one machine, when running over thousands of machines the likelyhood of something failing is much higher; when a failure happens, it is prefferable be fault tolerant to avoid loosing the work of all the other machines. RDDs are fault tolerant by tracking data lineage information to rebuild lost data automatically on failure.</p>
<p>In <code>sparklyr</code>, you can control when an RDD gets loaded or unloaded from memory using <code>tbl_cache()</code> and <code>tbl_uncache()</code>.</p>
<p>Most sparklyr operations that retrieve a Spark data frame, cache the results in-memory, for instance, running <code>spark_read_parquet()</code> or <code>sdf_copy_to()</code> will provide a Spark dataframe that is already cached in-memory. As a Spark data frame, this object can be used in most sparklyr functions, including data analysis with dplyr or machine learning.</p>
<div class="sourceCode" id="cb98"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb98-1" data-line-number="1"><span class="kw">library</span>(sparklyr)</a>
<a class="sourceLine" id="cb98-2" data-line-number="2">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>)</a></code></pre></div>
<div class="sourceCode" id="cb99"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb99-1" data-line-number="1">iris_tbl &lt;-<span class="st"> </span><span class="kw">sdf_copy_to</span>(sc, iris, <span class="dt">overwrite =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<p>You can inspect which tables are cached by navigating to the Spark UI using <code>spark_web(sc)</code>, opening the storage tab, and clicking on a given RDD:</p>
<p>Data loaded in memory will be released when the R session terminates either explicitly or implicitly with a restart or disconnection; however, to free up resources, you can use <code>tbl_uncache()</code>:</p>
<div class="sourceCode" id="cb100"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb100-1" data-line-number="1"><span class="kw">tbl_uncache</span>(sc, <span class="st">&quot;iris&quot;</span>)</a></code></pre></div>
<div id="checkpointing" class="section level3">
<h3><span class="header-section-number">8.4.1</span> Checkpointing</h3>
<p>Checkpointing is a slightly different type of caching, while it also persists data it will, additionally, break the graph computation lineage. So for instance, if a cached partition is lost, it can be computed from the computation graph which is not possible while checkpointing since the source of computation is lost.</p>
<p>When performing expensive computation graphs, it can make sense to checkpoint to persist and break the computation lineage, this to help Spark reduce graph computation resources; otherwise, Spark might try to over-optimize a computation graph that is really not useful to optimize.</p>
<p>You can checkpoint explicitly by saving to CSV, Parquet, etc. files. Or let Spark checkpoint this for you using <code>sdf_checkpoint()</code> in <code>sparklyr</code> as follows.</p>
<p>Notice that checkpointing truncates the computation lineage graph which can speed up performance if the same intermediate result is used multiple times.</p>
</div>
<div id="tunning-memory" class="section level3">
<h3><span class="header-section-number">8.4.2</span> Memory</h3>
<p>Memory in Spark is categorized into: reserved, user, execution or storage:</p>
<ul>
<li><strong>Reserved:</strong> Reserved memory is the memory required by Spark to function and therefore, is overhead that is required and should not be configured. This value defaults to 300MB.</li>
<li><strong>User:</strong> User memory is the memory used to execute custom code, <code>sparklyr</code> only makes use of this memory indirectly when executing <code>dplyr</code> expressions or modeling a dataset.</li>
<li><strong>Execution:</strong> Execution memory is used to execute code by Spark, mostly, to process the results from the partition and perform shuffling.</li>
<li><strong>Storage:</strong> Storage memory is used to cache RDDs, for instance, when using <code>tbl_cache()</code> in <code>sparklyr</code>.</li>
</ul>
<p>As part of tuning execution, you can consider tweaking the amount of memory allocated for <strong>user</strong>, <strong>execution</strong> and <strong>storage</strong> by creating a Spark connection with different values than the defaults provided in Spark:</p>
<div class="sourceCode" id="cb101"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb101-1" data-line-number="1">config &lt;-<span class="st"> </span><span class="kw">spark_config</span>()</a>
<a class="sourceLine" id="cb101-2" data-line-number="2"></a>
<a class="sourceLine" id="cb101-3" data-line-number="3"><span class="co"># define memory available for storage and execution</span></a>
<a class="sourceLine" id="cb101-4" data-line-number="4">config<span class="op">$</span>spark.memory.fraction &lt;-<span class="st"> </span><span class="fl">0.75</span></a>
<a class="sourceLine" id="cb101-5" data-line-number="5"></a>
<a class="sourceLine" id="cb101-6" data-line-number="6"><span class="co"># define memory available for storage</span></a>
<a class="sourceLine" id="cb101-7" data-line-number="7">config<span class="op">$</span>spark.memory.storageFraction &lt;-<span class="st"> </span><span class="fl">0.5</span></a></code></pre></div>
<p>For instance, if you want to use Spark to store large amounts of data in-memory with the purpuse of filtering and retrieving subsets quickly, you can expect Spark to use little execution or user memory; therefore, to maximize storage memory, one can tune Spark as follows:</p>
<div class="sourceCode" id="cb102"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb102-1" data-line-number="1">config &lt;-<span class="st"> </span><span class="kw">spark_config</span>()</a>
<a class="sourceLine" id="cb102-2" data-line-number="2"></a>
<a class="sourceLine" id="cb102-3" data-line-number="3"><span class="co"># define memory available for storage and execution</span></a>
<a class="sourceLine" id="cb102-4" data-line-number="4">config<span class="op">$</span>spark.memory.fraction &lt;-<span class="st"> </span><span class="fl">0.90</span></a>
<a class="sourceLine" id="cb102-5" data-line-number="5"></a>
<a class="sourceLine" id="cb102-6" data-line-number="6"><span class="co"># define memory available for storage</span></a>
<a class="sourceLine" id="cb102-7" data-line-number="7">config<span class="op">$</span>spark.memory.storageFraction &lt;-<span class="st"> </span><span class="fl">0.90</span></a></code></pre></div>
<p>However, notice that Spark will borrow execution memory from storage and viceversa if needed and if possible; therefore, in practice, there should be little need to tune the memory settings.</p>
</div>
</div>
<div id="tunning-shuffling" class="section level2">
<h2><span class="header-section-number">8.5</span> Shuffling</h2>
<p>Shuffling, is the operation that redistributes data across machines, it is usually an expensive operation and therefore, one we try to minimize. One can easily identify is significant time is being spent shuffling by looking at the <a href="tuning.html#tunning-event-timeline">event timeline</a>. It is possible to reduce shuffling by reframing data analysis questions or hinting Spark appropriately.</p>
<p>For instance, when joining dataframes that differ in size significantly, as in, one set being orders of magnitude smaller than the other one. You can consider using <code>sdf_broadcast()</code> to mark a dataframe as small enough for use in broadcast joins, meaning, it pushes one of the smaller dataframes to each of the worker nodes to reduce shuffling the bigger dataframe. One example for <code>sdf_broadcast()</code> follows:</p>
<div class="sourceCode" id="cb103"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb103-1" data-line-number="1"><span class="kw">sdf_len</span>(sc, <span class="dv">10000</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb103-2" data-line-number="2"><span class="st">    </span><span class="kw">sdf_broadcast</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb103-3" data-line-number="3"><span class="st">    </span><span class="kw">left_join</span>(<span class="kw">sdf_len</span>(sc, <span class="dv">100</span>))</a></code></pre></div>
</div>
<div id="tunning-serialization" class="section level2">
<h2><span class="header-section-number">8.6</span> Serialization</h2>
<p>It is not that common to have to adjust serialization when tunning Spark; however, it is worth mentioning there are alternative serialization modules like the <a href="https://github.com/EsotericSoftware/kryo">Kryo Serializer</a> that can provide performance improvements over the default <a href="http://docs.oracle.com/javase/6/docs/api/java/io/Serializable.html">Java Serializer</a>.</p>
<p>The Kryo Serializer can be enabled in <code>sparklyr</code> through:</p>
<div class="sourceCode" id="cb104"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb104-1" data-line-number="1">config &lt;-<span class="st"> </span><span class="kw">spark_config</span>()</a>
<a class="sourceLine" id="cb104-2" data-line-number="2"></a>
<a class="sourceLine" id="cb104-3" data-line-number="3">config<span class="op">$</span>spark.serializer &lt;-<span class="st"> &quot;org.apache.spark.serializer.KryoSerializer&quot;</span></a>
<a class="sourceLine" id="cb104-4" data-line-number="4">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>, <span class="dt">config =</span> config)</a></code></pre></div>
</div>
<div id="recap-3" class="section level2">
<h2><span class="header-section-number">8.7</span> Recap</h2>
<p>This chapter provided a broad but also detailed overview to help you speed up and reduce resource consumption in Spark, it provided the foundations to understand bottlenecks and some common workarounds to known issues; however, fine-tunning Spark is a broad topic that would require many more chapters to cover extensively. Therefore, while troubleshooting Spark’s performance and scalability, searching the web and consulting online communities is often necessary to fine-tune your particular environment.</p>
<div class="sourceCode" id="cb105"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb105-1" data-line-number="1"><span class="kw">spark_disconnect</span>(sc)</a></code></pre></div>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="data.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="extensions.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
