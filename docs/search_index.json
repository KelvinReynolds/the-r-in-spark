[
["index.html", "The R in Spark: Learning Apache Spark with R Welcome", " The R in Spark: Learning Apache Spark with R 2018-12-02 Welcome In this book you will learn how to use Apache Spark with R using the sparklyr R package. The book intends to take someone unfamiliar with Spark or R and help them become intermediate users by teaching a set of tools, skills and practices applicable to data science. This work is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 United States License. Note: While this book is being written, the content in this website will not be accessible, once published, this website will reopen. Contact javier@rstudio.com for early access. "],
["preface.html", "Preface Structure Authors Acknowledgments", " Preface Various books have been written for learning Apache Spark; for instance, Spark: The Definitive Guide: Big Data Processing Made Simple is a comprehensive resource while Learning Spark: Lightning-Fast Big Data Analysis is an introductory book meant to help users get up an running. However, as of this writting, there is no book to learn Apache Spark using the R programming language and neither, a book specifically designed for the R user nor the aspiring R user. There are some resources online to learn Apache Spark with R, most notably, the spark.rstudio.com site and the Spark documentation site under spark.apache.org. Both sites are great online resources; however, the content is not intended to be read from start to finish and assumes the reader has some context on Apache Spark, R and cluster computing. The goal of this book is to help anyone get started with Apache Spark using R, with the first chapters being mostly introductory, but quickly ramping up to relevant data science topics, cluster computing and closing on advanced topics which should interst even the most advanced users. This book is inteded to be a useful resource for a wide range of users; from those of you curious to learn the tools used in big data and big compute, to those of you experienced in those topics seeking to understand deeper topics while working with Apache Spark from R. Structure This book has the following general outline: Introductions: In the first chapters, Introduction and Getting Started, you will learn about Apache Spark, R and the tools you will need to perform data analysis with Spark and R. Analysis: In the Analysis chapter, you will learn how to analyse, explore, transform and visualize data in Apache Spark with R. Modeling: In the Modeling chapter, you will learn how to create statistical models with the purpose of extracting information and predictions outcomes. Scaling: Up to this point, chapters will have focused on performing operations on your personal computer; the Clusters, Connections, Data and Tuning chapters, introduce distributed dcomputing techniques required to perform analysis and modeling across many machines to tackle the large-scale data and computation problems which Apache Spark was designed for. Extensions: The extension chapter describe optional components and extended functionality applicable to specific, yet relevant, use cases. You will learn about alternative modeling frameworks, graph processing at scale and model deployment topics that will be relevant to many readers at some point in time. Advanced Topics: This book closes with a set of advanced chapters, Distributed R, Streaming and Contributing, which the advanced users will be most interested in. However, by the time you reach this section, these chapters won’t seem as intimidating; instead, they will be equally relevant, useful and interesting as the previous chapters. Authors Javier Luraschi Javier is a Software Engineer with experience in technologies ranging from desktop, web, mobile and backend; to augmented reality and deep learning applications. He previously worked for Microsoft Research and SAP and holds a double degree in Mathematics and Software Engineering. Kevin Kuo Kevin is a software engineer working on open source packages for big data analytics and machine learning. He has held data science positions in a variety of industries and was a credentialed actuary. He likes mixing cocktails and studying about wine. Edgar Ruiz Edgar has a background in deploying enterprise reporting and Business Intelligence solutions. He has posted multiple articles and blog posts sharing analytics insights and server infrastructure for Data Science. He lives with his family near Biloxi, MS. Acknowledgments This project would not have been possible without the work put into building sparklyr by Javier Luraschi, Kevin Kuo, Kevin Ushey and JJ Allaire, nor without bookdown by Yihui Xie, dplyr by Hadley Wickham, DBI by Kirill Mülller nor the Apache Spark project iteself. "],
["intro.html", "Chapter 1 Introduction 1.1 Background 1.2 Spark 1.3 R 1.4 sparklyr", " Chapter 1 Introduction This chapter covers the historical background that lead to the development of Apache Spark, introduces R in the context of Spark and sparklyr as a project bridging Spark and R. 1.1 Background As humans, we have been storing, retrieving, manipulating, and communicating information since the Sumerians in Mesopotamia developed writing in about 3000 BC. Based on the storage and processing technologies employed, it is possible to distinguish four distinct phases of development: pre-mechanical (3000 BC – 1450 AD), mechanical (1450–1840), electromechanical (1840–1940), and electronic (1940–present)(Laudon, Traver, and Laudon 1996). Mathematician George Stibitz used the word digital to describe fast electric pulses back in 1942(Ceruzzi 2012) and, to this day, we describe information stored electronically as digital information. In contrast, analog information is defined as, not digital and it represents everything we have stored by any non-electronic means such as: hand written notes, books, newspapers and so on(Webster 2006). The world bank report on digital development provides an estimate of digital and analog information stored over the last decades(Group 2016). This report found out that digital information surpassed analog information around 2003, at that time, there were aboput 10M terabytes of digital information, which is roughly about 10M computer drives today. However, a more relevant finding from this report was that our footprint of digital information is growing at exponential rates: World’s capacity to store information. With the ambition to provide tools capable of searching all this new digital information, many companies attempted to provide such functionality with what we know today as search engines, used when searching the web. Given the vast amount of digital information, managing information at this scale was a challenging problem. Search engines were unble to store all the web page information required to support web searches in a single computer. This meant that they had to split information across many machines, which was accomlished by splitting this data and storing it as many files across many machines. This approach became known as the Google File System from a research paper published in 2003 by Google(Ghemawat, Gobioff, and Leung 2003). One year later, in 2004, Google published a new paper describing how to perform operations across the Google File System, this approach came to be known as MapReduce(Dean and Ghemawat 2008). As you would expect, there are two operations in MapReduce: Map and Reduce. The map operation provides an arbitrary way to transform each file into a new file, usually defined by arbitrary code that scans the file and outputs a different file. The reduce operation combines two files into a new one. These two operations are sufficient to process data at the scale of the data available in the web. For example, we can use MapReduce to count words in two different text files. The mapping operation splits each word in the original file and outputs a new word-counting file with a mapping of words and counts. The reduce operation can be defined to take two word-counting files and combine them by aggregating the totals for each word, this last file will contain a list of word counts across all the original files. Simple MapReduce Example. Counting words is often the most basic MapReduce example, but it can be also used for much more sophisticated and interesting applications. For instance, MapReduce can be used to rank web pages in Google’s PageRank algorithm, which assigns ranks to web pages based on the count of hyperlinks linking to a web page and the rank of the page linking to it. After these papers were released by Google, a team in Yahoo worked on implementing the Google File System and MapReduce as a single open source project. This project was released in 2006 as Hadoop with the Google File System implemented as the Hadoop File System, or HDFS for short. The Hadoop project made distributed file-based computing accessible to a wider range of users and organizations which enabled them to make use of MapReduce beyond web data processing. While Hadoop provided support to perform MapReduce operations over a distributed file system, it still required MapReduce operations to be written with code every time a data analysis was run. To improve over this tedious process, the Hive project released in 2008 by Facebook, brought Structured Query Language (SQL) support to Hadoop. This meant that data analysis could now be performed at large-scale without the need to write code for each MapReduce operation; instead, one could write generic data analysis statements in SQL that are much easier to understand and write. 1.2 Spark In 2009, Apache Spark starts as a research project at the UC Berkeley’s AMPLab to improve over MapReduce. Specifically, by providing a richer set of verbs beyond MapReduce that facilitate optimizing code running in multiple machines and, by loading data in-memory which made operations much fasters than Hadoop’s on-disk storage. One of the earliest results showed that running logistic regression, a data modeling technique that will be introduced under the modeling chapter, allowed Spark to run 10x faster than Hadoop by making use of in-memory datasets(Zaharia et al. 2010). Logistic regression performance in Hadoop and Spark. While Spark is well known for its in-memory performance, Spark was designed to be a general execution engine that works both in-memory and on-disk. For instance, Spark holds records in large-scale sorting, where data was not loaded in-memory; but rather, Spark made use of improvements in network serialization, network shuffling and efficient use of the CPU’s cache to dramatically improve performance. For comparison, one can sort 100TB of data in 72min and 2100 computers using Hadoop, but only 206 computers in 23min using Spark, it’s also the case that Spark holds the record in the cloud sorting benchmark, which makes Spark the most cost effective solution for large-scale sorting. Hadoop Record Spark Record Data Size 102.5 TB 100 TB Elapsed Time 72 mins 23 mins Nodes 2100 206 Cores 50400 6592 Disk 3150 GB/s 618 GB/s Network 10Gbps 10Gbps Sort rate 1.42 TB/min 4.27 TB/min Sort rate / node 0.67 GB/min 20.7 GB/min In 2010, Spark was released as an open source project and then donated to the Apache Software Foundation in 2013. Spark is licensed under the Apache 2.0, which allows you to freely use, modify, and distribute it. In 2015, Spark reaches more than 1000 contributors, making it one of the most active projects in the Apache Software Foundation. This gives an overview of how Spark came to be, which we can now use to formally introduce Apache Spark as follows: “Apache Spark is a fast and general engine for large-scale data processing.” — spark.apache.org To help us understand this definition of Apache Spark, we will break it down as follows: Data Processing: Data processing is the collection and manipulation of items of data to produce meaningful information(French 1996). General: Spark optimizes and executes parallel generic code, as in, there are no restrictions as to what type of code one can write in Spark. Large-Scale: One can interpret this as cluster-scale, as in, a set of connected computers working together to accomplish specific goals. Fast: Spark is much faster than its predecessor by making efficient use of memory, network and CPUs to speed data processing algorithms in computing cluster. Since Spark is general, you can use Spark to solve many problems, from calculating averages to approximating the value of Pi, predicting customer churn, aligning protein sequences or analyzing high energy physics at CERN. Describing Spark as large scale implies that a good use case for Spark is tackling problems that can be solved with multiple machines. For instance, when data does not fit in a single disk driver or does not fit into memory, Spark is a good candidate to consider. Since Spark is fast, it is worth considering for problems that may not be large-scale, but where using multiple processors could speed up computation. For instance, sorting large datasets or CPU intensive models could also bennefit from running in Spark. Therefore, Spark is good at tackling large-scale data processing problems, this usually known as big data (data sets that are more voluminous and complex that traditional ones), but also is good at tackling large-scale computation problems, known as big compute (tools and approaches using a large amount of CPU and memory resources in a coordinated way). Big data and big compute problems are usually easy to spot, if the data does not fit into a single machine, you might have a big data problem; if the data fits into a single machine but a process over the data takes days, weeks or even months to compute, you might have a big compute problem. However, there is also a third problem space where data nor compute are necessarily large-scale and yet, there are significant benefits from using Spark. For this third problem space, there are a few use cases this breaks to: Velocity: One can have a dataset of 10GB in size and a process that takes 30min to run over this data, this is by no means big-compute nor big-data; however, if a data scientist is researching ways to improve accuracy for their models, reducing the runtime down to 3min it’s a 10X improvement, this improvement can lead to significant advances and productivity gains by increasing the velocity at which one can analyze data. Variety: One can have an efficient process to collect data from many sources into a single location, usually a database, this process could be already running efficiently and close to realtime. Such processes are known at ETL (Extract-Transform-Load); data is extracted from multiple sources, transformed to the required format and loaded in a single data store. While this has worked for years, the tradeoff from this system is that adding a new data source is expensive, the system is centralized and tightly controlled. Since making changes to this type of systems could cause the entire process to come to a halt, adding new data sources usually takes long to be implemented. Instead, one can store all data its natural format and process it as needed using cluster computing, this architecture is currently known as a data lake. Some people refer to some of these benefits as the four ’V’s of big data: Velocity, Variety, Volume and Veracity (which asserts that data can vary greatly in quality which require analysis methods to improve accuracy across a variety of sources). Others have gone as far as expending this to five or even as the 10 Vs of Big Data. Mnemonics set aside, cluster computing is being used today in more innovative ways and and is not uncommon to see organizations experimenting with new workflows and a variety of tasks that were traditionally uncommon for cluster computing. Much of the hype attributed to big data falls into this space where, strictly speaking, one is not handling big data but there are still beneffits from using tools designed for big data and big compute. My hope is that this book will help you understand the opportunities and limitations of cluster computing, and specifically, the opportunities and limitations from using Apache Spark with R. 1.3 R R is a computing language with it’s inception dating back to Bell Laboratories. R was not created at Bell Labs, but it’s predecesor, the S computing language was. Rick Becker explained in useR 2016 that at that time in Bell Labs, computing was done by calling subroutines written in the Fortran language which, apparently, were not pleasant to deal with. The S computing language was designed as an interface language to solve particular problems without having to worry about other languages, Fortran at that time. The creator of S, John Chambers, describes how S was designed to provide an interface through the following diagram: Interface language diagram by John Chambers - Rick Becker useR 2016. R is a modern and free implementation of S, specifically: R is a programming language and free software environment for statistical computing and graphics. — The R Project for Statistical Computing While working with data, I believe there are two strong arguments for using R: The R Language was designed by statisticians for statisticians, meaning, this is one of the few successful languages designed for non-programmers; so learning R will probably feel more natural. Additionally, since the R language was designed to be an interface to other tools and languages, R allows you to focus more on modeling and less on peculiarities of computer science and engineering. The R Community provides a rich package archive provided by CRAN (The Comprehensive R Archive Network) which allows you to install ready-to-use packages to perform many tasks, most notably, high-quality statistic models with many only available in R. In addition, the R community is a welcoming and active group of talented individuals motivated to help you succeed. Many packages provided by the R community make R, by far, the place to do statistical computing. To mention some of the popular packages: dplyr to manipulate data, cluster to analyze clusters and ggplot2 to visualize data. We can quantify the growth of the R community by plotting daily downloads of R packages in CRAN. Daily downloads of CRAN packages. Aside from statistics, R is also used in many other fields, the following ones are particularily relevant to this book: Data Science: An exciting discipline that allows you to turn raw data into understanding, insight, and knowledge(Wickham and Grolemund 2016). In 1997, C.F. Jeff Wu advocated that statistics be renamed data science(Wu 1997); however, in 2001, William S. Cleveland introduced data science as an independent discipline extending the field of statistics to incorporate “advances in computing with data”(Cleveland 2001). Machine Learning: Field of artificial intelligence that uses statistical techniques to give computer systems the ability to “learn” from data. Arthur Samuel coined the term “machine learning” in 1959 while designing programs to play checkers using alpha-beta prunning(Samuel 1959). Machine learning is employed in computing tasks where designing explicit algorithms is difficult; for example, email filtering, detection of network intruders, and computer vision. Deep Learning: A field of machine learning where models are vaguely inspired in biological nervous systems. In 2006, Geoffrey E. Hinton overcomed the vanishing-gradient-problem in deep neural networks by pre-training one layer at a time(Hinton, Osindero, and Teh 2006). In 2012, convolutions and GPUs were introduced by Alex Krizhevsky making them the best image classification model over ImageNet(Krizhevsky, Sutskever, and Hinton 2012). 1.4 sparklyr Back in 2016, there was a need in the R community to support Spark through an R package that would provide an interface compatible with other R packages, easy to use and available in CRAN. To this end, development of sparklyr started in 2016 by RStudio under JJ Allaire, Kevin Ushey and Javier Luraschi, version 0.4 was released in summer during the useR! conference, this first version added support for dplyr, DBI, modeling with MLlib and an extensible API that enabled extensions like H2O’s rsparkling package. Since then, many new features and improvements have been made available through sparklyr 0.5, 0.6, 0.7, 0.8 and 0.9. Officially, sparklyr is an R interface for Apache Spark. — github.com/rstudio/sparklyr It’s available in CRAN and works like any other CRAN package, meaning that: it’s agnostic to Spark versions, it’s easy to install, it serves the R community, it embraces other packages and practices from the R community and so on. It’s hosted in GitHub under github.com/rstudio/sparklyr and licensed under Apache 2.0 which is allows you to clone, modify and contribute back to this project. While thinking of who and why should use sparklyr, the following roles come to mind: New Users: For new users, sparklyr provides the easiest way to get started with Spark. My hope is that the first chapters of this book will get you up running with ease and set you up for long term success. Data Scientists: For data scientists that already use and love R, sparklyr integrates with many other R practices and packages like dplyr, magrittr, broom, DBI, tibble and many others that will make you feel at home while working with Spark. For those new to R and Spark, the combination of high-level workflows available in sparklyr and low-level extensibility mechanisms make it a productive environment to match the needs and skills of every data scientist. Expert Users: For those users that are already immersed in Spark and can write code natively in Scala, consider making your libraries available as an sparklyr custom extension to the R community, a diverse and skilled community that can put your contributions to good use while moving open science forward. This book is titled “The R in Spark” as a way to describe and teach that area of overlap between Spark and R. The R package that represents this overlap is sparklyr; however, the overlap goes beyond a package. It’s an overlap of communities, expectations, future directions, packages and package extensions as well. Naming this book sparklyr or “Introduction to sparklyr” would have left behind a much more exciting opportunity, an opportunity to present this book as an intersection of the R and Spark communities. Both are solving very similar problems with a set of different skills and backgrounds; therefore, it is my hope that sparklyr can be a fertile ground for innovation, a welcoming place to newcomers, a productive place for experienced data scientists and an open community where cluster computing and modeling can come together. References "],
["starting.html", "Chapter 2 Getting Started 2.1 Prerequisites 2.2 Installing Spark 2.3 Connecting to Spark 2.4 Using Spark 2.5 Disconnecting 2.6 RStudio 2.7 Troubleshooting 2.8 Recap", " Chapter 2 Getting Started From R, getting started with Spark using a local cluster is as easy as running: library(sparklyr) spark_install() sc &lt;- spark_connect(master = &quot;local&quot;) To make sure we can all run the code above and understand it, this chapter will walk you through installing the prerequisites, installing Spark, connecting to a local Spark cluster and briefly explaining how to use Spark. However, if a Spark cluster and R environment have been made available to you, you do not need to install the prerequisites nor install Spark yourself. Instead, you should ask for the Spark master parameter and connect as follows, this parameter will be formally introduced under the clusters and connections chapters. sc &lt;- spark_connect(master = &quot;&lt;cluster-master&gt;&quot;) 2.1 Prerequisites As briefly mentioned in the Introduction chapter, R is a programming language that can run in many platforms and environments. Most people making use of a programming language also choose tools to make them more productive in it; for R, RStudio would be such tool. Strictly speaking, RStudio is an Integrated Development Environment or IDE for short, which also happens to support many platforms and environments. R and RStudio are the free software tools this book will make use of and therefore, I strongly recommend you get those installed if you haven’t done so already. Additionally, since Spark is build in the Scala programming language which is run by the Java Virtual Machine, you also need to install Java 8 in your system. It is likely that your system already has Java installed, but you should still check the version and update if needed as described in the Install Java section. 2.1.1 Install R From r-project.org, download and launch the installer for your platform, Windows, Macs or Linux available. The R Project for Statistical Computing. 2.1.2 Install Java From oracle.com/technetwork/java/javase/downloads/jdk8-downloads-2133151.html, download and launch the installer for your platform, Windows, Macs or Linux available. While installing the JRE (Java Runtime Environment) is sufficient for most operations, in order to build extensions you will need the JDK (Java Developer Kit); therefore, I rather recommend installing the JDK in the first place. Java Download. Starting with Spark 2.1, Java 8 is required; however, previous versions of Spark support Java 7. Regardless, we recommend installing Java 8 as described in this chapter 2.1.3 Install RStudio While installing RStudio is not strictly required to work with sparklyr in R, it will make you much more productive and therefore, I would recommend you take the time to install RStudio from rstudio.com/products/rstudio/download/, then download and launch the installer for your platform: Windows, Macs or Linux. RStudio Downloads. After launching RStudio, identify the Console panel since this is where most of the code will be executed in this book. For additional learning resources on R and RStudio consider visiting: rstudio.com/online-learning/. 2.1.4 Install sparklyr First of all, we would want to install sparkylr. As many other R packages, sparklyr is available in CRAN and can be easily installed as follows: install.packages(&quot;sparklyr&quot;) The CRAN release of sparklyr contains the most stable version and it’s the recommended version to use; however, for those that need or might want to try newer features being developed in sparklyr you can install directly from GitHub using the devtools package. First install the devtools package and then sparklyr as follows: install.packages(&quot;devtools&quot;) devtools::install_github(&quot;rstudio/sparklyr&quot;) 2.2 Installing Spark Start by loading sparklyr, library(sparklyr) This will makes all sparklyr functions available in R, which is really helpful; otherwise, we would have to run each sparklyr command prefixed with sparklyr::. As mentioned, Spark can be easily installed by running spark_install(); this will install the latest version of Spark locally in your computer, go ahead and run spark_install(). Notice that this command requires internet connectivity to download Spark. spark_install() All the versions of Spark that are available for installation can be displayed with spark_available_versions(): spark_available_versions() ## spark ## 1 1.6.3 ## 2 1.6.2 ## 3 1.6.1 ## 4 1.6.0 ## 5 2.0.0 ## 6 2.0.1 ## 7 2.0.2 ## 8 2.1.0 ## 9 2.1.1 ## 10 2.2.0 ## 11 2.2.1 ## 12 2.3.0 ## 13 2.3.1 ## 14 2.3.2 ## 15 2.4.0 A specific version can be installed using the Spark version and, optionally, by also specifying the Hadoop version. For instance, to install Spark 1.6.3, we would run spark_install(&quot;1.6.3&quot;). You can also check which versions are installed by running: spark_installed_versions() spark hadoop dir 7 2.3.1 2.7 /spark/spark-2.3.1-bin-hadoop2.7 Finally, in order to uninstall an specific version of Spark you can run spark_uninstall() by specifying the Spark and Hadoop versions, for instance: spark_uninstall(version = &quot;2.3.1&quot;, hadoop = &quot;2.7&quot;) 2.3 Connecting to Spark It’s important to mention that, so far, we’ve only installed a local Spark cluster. A local cluster is really helpful to get started, test code and troubleshoot with ease; further chapters will explain where to find, install and connect to real Spark clusters with many machines; but for the first few chapters, we will focus on using local clusters. Threfore, to connect to this local cluster we simply run: sc &lt;- spark_connect(master = &quot;local&quot;) The master parameter helps sparklyr find which is the “main” machine from the Spark cluster, this machine is often call the driver node. While working with real clusters using many machines, most machines will be worker machines and one will be the master. Since we only have a local cluster with only one machine, we will default to use &quot;local&quot; for now. 2.4 Using Spark Now that you are connected, we can run a few simple commands. For instance, let’s start by loading some data into Apache Spark. To accomplish this, lets first create a text file by running: write(&quot;Hello World!&quot;, &quot;hello.txt&quot;) We can now read this text file back from Spark by running: spark_read_text(sc, &quot;hello&quot;, &quot;hello.txt&quot;) ## # Source: spark&lt;hello&gt; [?? x 1] ## line ## * &lt;chr&gt; ## 1 Hello World! Congrats! You have successfully connected and loaded your first dataset into Spark. Let’s explain what’s going on in spark_read_text(). The first parameter, sc, gives the function a reference to the active Spark Connection that was earlier created with spark_connect(). The second parameter names this dataset in Spark. The third parameter specifies a path to the file to load into Spark. Now, spark_read_text() returns a reference to the dataset in Spark which R automatically prints. Whenever a Spark dataset is printed, sparklyr will collect some of the records and display them for you. In this particular case, that dataset contains just one row for the line: Hello World!. We will now use this simple example to present various useful tools in Spark we should get familiar with. 2.4.1 Web Interface Most of the Spark commands will get started from the R console; however, it is often the case that monitoring and analyzing execution is done through Sparks web interface. This interface is a web page provided by Spark which can be accessed from sparklyr by running: spark_web(sc) Apache Spark Web Interface. As mentioned, printing the “hello” dataset collected a few records to be displayed in the R console. You can see in the Spark web interface that a job was started to collect this information back from Spark. You can also select the storage tab to see the “hello” dataset cached in-memory in Spark: Apache Spark Web Interface - Storage Tab. The caching section in the tunning chapter will cover this in detail, but as a start, it’s worth noticing that this dataset is fully loaded into memory since the fraction cached is 100%, it is useful also to point out the size in memory column which tracks the total memory being used by this dataset. 2.4.2 Logs Another common tool to use in Spark that you should familiarize with are the Spark logs. A log is just a text file where Spark will append information relevant to the execution of tasks in the cluster. For local clusters, we can retrieve all the logs by running: spark_log(sc, n = 5) 18/10/09 19:41:46 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1499 bytes result sent to driver 18/10/09 19:41:46 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 32 ms on localhost (executor driver) (1/1) 18/10/09 19:41:46 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 18/10/09 19:41:46 INFO DAGScheduler: ResultStage 5 (collect at utils.scala:197) finished in 0.039 s 18/10/09 19:41:46 INFO DAGScheduler: Job 3 finished: collect at utils.scala:197, took 0.043086 s Or we can retrieve specific log entries containing, say sparklyr, by using the filter parameter as follows: spark_log(sc, filter = &quot;sparklyr&quot;, n = 5) ## 18/10/09 18:53:23 INFO SparkContext: Submitted application: sparklyr ## 18/10/09 18:53:23 INFO SparkContext: Added JAR file:/Library/Frameworks/R.framework/Versions/3.5/Resources/library/sparklyr/java/sparklyr-2.3-2.11.jar at spark://localhost:52930/jars/sparklyr-2.3-2.11.jar with timestamp 1539136403697 ## 18/10/09 18:53:27 INFO Executor: Fetching spark://localhost:52930/jars/sparklyr-2.3-2.11.jar with timestamp 1539136403697 ## 18/10/09 18:53:27 INFO Utils: Fetching spark://localhost:52930/jars/sparklyr-2.3-2.11.jar to /private/var/folders/ks/wm_bx4cn70s6h0r5vgqpsldm0000gn/T/spark-141b51a3-f277-4530-aa6a-69be176e0c0b/userFiles-3d94e32b-c65d-4081-a85e-d1e4716e0cef/fetchFileTemp1188493532217239876.tmp ## 18/10/09 18:53:27 INFO Executor: Adding file:/private/var/folders/ks/wm_bx4cn70s6h0r5vgqpsldm0000gn/T/spark-141b51a3-f277-4530-aa6a-69be176e0c0b/userFiles-3d94e32b-c65d-4081-a85e-d1e4716e0cef/sparklyr-2.3-2.11.jar to class loader 2.5 Disconnecting For local clusters and really, any cluster; once you are done processing data you should disconnect by running: spark_disconnect(sc) this will terminate the connection to the cluster but also terminate the cluster tasks as well. If multiple Spark connections are active, or if the conneciton instance sc is no longer available, you can also disconnect all your Spark connections by running spark_disconnect_all(). 2.6 RStudio If you are not familiar with RStudio, you will want to identify at least the following panes: Console: This is just a standalone R console you can use to execute all the code presented in this book. Packages: This pane allows you to install sparklyr with ease, check its version, navigate to the help contents, etc. Connections: This pane allows you to connecto to Spark, manage your active connection and view the available datasets. RStudio Overview. To start a new connections to Spark, you can use spark_connect() from the R console or, alternatevely, use the new connection action from the connections pane and then, select the Spark connection. Once a Spark connection is selected, you can customize the versions and connect to Spark which will simply generate the right spark_connect() command and execute this in the R console for you: RStudio New Spark Connection. Once connected to Spark, either by using the R console or through RStudio’s connections pane, RStudio will display your datasets available in the connections pane. This is a useful way to track your existing datasets and provides an easy way to explore each of them: RStudio Connections Pane. Additionally, an active connection provides the following custom actions: Spark UI: Opens the Spark web interface, a shortcut to spark_ui(sc). Log: Opens the Spark web logs, a shortcut to spark_log(sc). SQL: Opens a new SQL query, see DBI and SQL support in the data analysis chapter. Help: Opens the sparklyr reference documentation in a new web browser window. Disconnect: Disconnects from Spark, a shortcut to spark_disconnect(sc). The rest of this book will not cover any additional topics related to RStudio, whether you are using plain R from a terminal or the R console from RStudio, the code provided in this book executes in the same way across any R environment. 2.7 Troubleshooting While we’ve put significant effort to simplify the onboarding process, it is now a good time to give you additional resources that can help you troubleshoot particular issues while getting started and, in general, while working with Spark from R. We hope you will find the following resources helpful: Documentation: This should be your entry point to learn more about sparklyr, the documentation is kept up to date with examples, reference functions and many more relevant resources, spark.rstudio.com. RStudio Community: For general sparklyr questions, you can post then in the RStudio Community tagged as sparklyr, https://community.rstudio.com/tags/sparklyr. Stack Overflow: For general Spark questions, Stack Overflow is the good start, stackoverflow.com/questions/tagged/apache-spark. Github: If you believe something needs to get fixed, open a GitHub issue or send us a pull request, github.com/rstudio/sparklyr. Gitter: For urgent issues or to keep in touch you can chat with us in Gitter, gitter.im/rstudio/sparklyr. 2.8 Recap This chapter walked you through installing R, Java, RStudio and sparklyr as the main tools required to use Spark from R. We covered installing local Spark clusters using spark_install(), connecting using spark_connect() and learned how to launch the web interface and logs using spark_web(sc) and spark_log(sc) respectedly. It is my hope that this chapter will help anyone interested in learning cluster computing using Spark and R getting started, ready to experiment on your own and ready to tackle actual data analysis and modeling tasks without any major blockers. "],
["analysis.html", "Chapter 3 Analysis 3.1 Overview 3.2 dplyr 3.3 DBI", " Chapter 3 Analysis While this chatper has not been written, a few resources and basic examples were made available to help out until this chapter is written. 3.1 Overview 3.2 dplyr Using sparklyr, you can apply the same data analysis techniques described in Chapter 5 - Data transformation - R for Data Science by Garrett Grolemund and Hadley Wickham. Once you understand dplyr, you can make use of dplyr and sparklyr as follows: library(sparklyr) library(dplyr) # Connect to Spark sc &lt;- spark_connect(master = &quot;local&quot;) # Use dplyr&#39;s copy_to() to copy the iris dataset to Spark iris_tbl &lt;- copy_to(sc, iris, overwrite = TRUE) # The iris_tbl is a Spark data frame compatible with dplyr iris_tbl ## # Source: spark&lt;iris&gt; [?? x 5] ## Sepal_Length Sepal_Width Petal_Length Petal_Width Species ## * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; ## 1 5.1 3.5 1.4 0.2 setosa ## 2 4.9 3 1.4 0.2 setosa ## 3 4.7 3.2 1.3 0.2 setosa ## 4 4.6 3.1 1.5 0.2 setosa ## 5 5 3.6 1.4 0.2 setosa ## 6 5.4 3.9 1.7 0.4 setosa ## 7 4.6 3.4 1.4 0.3 setosa ## 8 5 3.4 1.5 0.2 setosa ## 9 4.4 2.9 1.4 0.2 setosa ## 10 4.9 3.1 1.5 0.1 setosa ## # ... with more rows # Transform iris_tbl with dplyr as usual iris_tbl %&gt;% group_by(Species) %&gt;% summarise_all(funs(mean)) ## # Source: spark&lt;?&gt; [?? x 5] ## Species Sepal_Length Sepal_Width Petal_Length Petal_Width ## * &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 versicolor 5.94 2.77 4.26 1.33 ## 2 virginica 6.59 2.97 5.55 2.03 ## 3 setosa 5.01 3.43 1.46 0.246 3.2.1 Pass-through Functions dates_tbl &lt;- copy_to(sc, data.frame(a = &quot;1/2/2001&quot;)) dates_tbl %&gt;% select(b = to_date(a)) %&gt;% as.sql() See Spark SQL built-in functions. 3.2.2 Resources To understand dplyr further, I would recommend taking a look at the following vignettes: Introduction to dplyr Two-table verbs Window functions Programming with dplyr 3.3 DBI The DBI provides an database interface for R, meaning, if you are familiar with SQL, you can make use of DBI to perform SQL queries in Spark using sparklyr. To learn more about DBI, I would recommend reading first A Common Database Interface (DBI). Once you are familiar with DBI, you can use this package with sparklyr as follows: library(DBI) dbGetQuery(sc, &quot;SELECT mean(Sepal_Length), mean(Sepal_Width), mean(Petal_Length), mean(Petal_Width) FROM iris GROUP BY Species&quot;) ## avg(Sepal_Length) avg(Sepal_Width) avg(Petal_Length) avg(Petal_Width) ## 1 5.936 2.770 4.260 1.326 ## 2 6.588 2.974 5.552 2.026 ## 3 5.006 3.428 1.462 0.246 More advanced DBI resources are available in the following vignettes: A Common Interface to Relational Databases from R and S – A Proposal Implementing a new backend DBI specification "],
["modeling.html", "Chapter 4 Modeling 4.1 Overview 4.2 Supervised 4.3 Unsupervised 4.4 Broom 4.5 Pipelines", " Chapter 4 Modeling While this chatper has not been written, a few resources and basic examples were made available to help out until this chapter is completed. 4.1 Overview MLlib is Apache Spark’s scalable machine learning library and is available through sparklyr, mostly, with functions prefixed with ml_. The following table describes some of the modeling algorithms supported: Algorithm Function Accelerated Failure Time Survival Regression ml_aft_survival_regression() Alternating Least Squares Factorization ml_als() Bisecting K-Means Clustering ml_bisecting_kmeans() Chi-square Hypothesis Testing ml_chisquare_test() Correlation Matrix ml_corr() Decision Trees ml_decision_tree () Frequent Pattern Mining ml_fpgrowth() Gaussian Mixture Clustering ml_gaussian_mixture() Generalized Linear Regression ml_generalized_linear_regression() Gradient-Boosted Trees ml_gradient_boosted_trees() Isotonic Regression ml_isotonic_regression() K-Means Clustering ml_kmeans() Latent Dirichlet Allocation ml_lda() Linear Regression ml_linear_regression() Linear Support Vector Machines ml_linear_svc() Logistic Regression ml_logistic_regression() Multilayer Perceptron ml_multilayer_perceptron() Naive-Bayes ml_naive_bayes() One vs Rest ml_one_vs_rest() Principal Components Analysis ml_pca() Random Forests ml_random_forest() Survival Regression ml_survival_regression() To complement those algorithms, you will often also want to consider using the following feature transformers: Transformer Function Binarizer ft_binarizer() Bucketizer ft_bucketizer() Chi-Squared Feature Selector ft_chisq_selector() Vocabulary from Document Collections ft_count_vectorizer() Discrete Cosine Transform ft_discrete_cosine_transform() Transformation using dplyr ft_dplyr_transformer() Hadamard Product ft_elementwise_product() Feature Hasher ft_feature_hasher() Term Frequencies using Hashing export(ft_hashing_tf) Inverse Document Frequency ft_idf() Imputation for Missing Values export(ft_imputer) Index to String ft_index_to_string() Feature Interaction Transform ft_interaction() Rescale to [-1, 1] Range ft_max_abs_scaler() Rescale to [min, max] Range ft_min_max_scaler() Locality Sensitive Hashing ft_minhash_lsh() Converts to n-grams ft_ngram() Normalize using the given P-Norm ft_normalizer() One-Hot Encoding ft_one_hot_encoder() Feature Expansion in Polynomial Space ft_polynomial_expansion() Maps to Binned Categorical Features ft_quantile_discretizer() SQL Transformation ft_sql_transformer() Standardizes Features using Corrected STD ft_standard_scaler() Filters out Stop Words ft_stop_words_remover() Map to Label Indices ft_string_indexer() Splits by White Spaces export(ft_tokenizer) Combine Vectors to Row Vector ft_vector_assembler() Indexing Categorical Feature ft_vector_indexer() Subarray of the Original Feature ft_vector_slicer() Transform Word into Code ft_word2vec() 4.2 Supervised Examples are reosurces are available in spark.rstudio.com/mlib. 4.3 Unsupervised 4.3.1 K-Means Clustering Here is an example to get you started with K-Means: library(sparklyr) # Connect to Spark in local mode sc &lt;- spark_connect(master = &quot;local&quot;) # Copy iris to Spark iris_tbl &lt;- sdf_copy_to(sc, iris, overwrite = TRUE) # Run K-Means for Species using only Petal_Width and Petal_Length as features iris_tbl %&gt;% ml_kmeans(centers = 3, Species ~ Petal_Width + Petal_Length) 4.3.2 Gaussian Mixture Clustering Alternatevely, we can also cluster using Gaussian Mixture Models (GMMs). predictions &lt;- copy_to(sc, fueleconomy::vehicles) %&gt;% ml_gaussian_mixture(~ hwy + cty, k = 3) %&gt;% ml_predict() %&gt;% collect() predictions %&gt;% ggplot(aes(hwy, cty)) + geom_point(aes(hwy, cty, col = factor(prediction)), size = 2, alpha = 0.4) + scale_color_discrete(name = &quot;&quot;, labels = paste(&quot;Cluster&quot;, 1:3)) + labs(x = &quot;Highway&quot;, y = &quot;City&quot;) + theme_light() FIGURE 4.1: Fuel economy data for 1984-2015 from the US EPA 4.4 Broom You can turn your sparklyr models into data frames using the broom package: model &lt;- cars_tbl %&gt;% ml_linear_regression(mpg ~ wt + cyl) # Turn a model object into a data frame broom::tidy(model) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 39.7 1.71 23.1 0 ## 2 wt -3.19 0.757 -4.22 0.000222 ## 3 cyl -1.51 0.415 -3.64 0.00106 # Construct a single row summary broom::glance(model) ## # A tibble: 1 x 5 ## explained.varia… mean.absolute.e… mean.squared.er… r.squared ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 29.2 1.92 5.97 0.830 ## # ... with 1 more variable: root.mean.squared.error &lt;dbl&gt; # Augments each observation in the dataset with the model broom::augment(model, cars_tbl) ## # A tibble: 32 x 14 ## `_c0` mpg cyl disp hp drat wt qsec vs am gear carb ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Mazd… 21 6 160 110 3.9 2.62 16.5 0 1 4 4 ## 2 Mazd… 21 6 160 110 3.9 2.88 17.0 0 1 4 4 ## 3 Dats… 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 ## 4 Horn… 21.4 6 258 110 3.08 3.22 19.4 1 0 3 1 ## 5 Horn… 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 ## 6 Vali… 18.1 6 225 105 2.76 3.46 20.2 1 0 3 1 ## 7 Dust… 14.3 8 360 245 3.21 3.57 15.8 0 0 3 4 ## 8 Merc… 24.4 4 147. 62 3.69 3.19 20 1 0 4 2 ## 9 Merc… 22.8 4 141. 95 3.92 3.15 22.9 1 0 4 2 ## 10 Merc… 19.2 6 168. 123 3.92 3.44 18.3 1 0 4 4 ## # ... with 22 more rows, and 2 more variables: fitted &lt;dbl&gt;, resid &lt;dbl&gt; 4.5 Pipelines Spark’s ML Pipelines provide a way to easily combine multiple transformations and algorithms into a single workflow, or pipeline. Take a look at spark.rstudio.com/guides/pipelines to learn about their purpose and functionality. "],
["clusters.html", "Chapter 5 Clusters 5.1 Overview 5.2 Managers 5.3 On-Premise 5.4 Cloud 5.5 Tools 5.6 Recap", " Chapter 5 Clusters Previous chapters focused on using Spark over a single computing instance, your personal computer. In this chapter we will introduce techniques to run Spark over multiple computing instances, also known as a computing cluster, to analyze data at scale. If you already have a Spark cluster in your organization, you could consider skipping to the next chapter, Connections, which will teach you how to connect to an existing cluster. If don’t have a cluster or are considering improvements to your existing infrastructure, this chapter will introduce some of the cluster trends, managers and providers available today. 5.1 Overview There are three major trends in cluster computing worth discussing: on-premise, cloud computing and kubernetes. Framing these trends over time will help us understand how they came to be, what they are and what their future might be: FIGURE 5.1: Google trends for on-premise (mainframe), cloud computing and kubernetes. For on-premise clusters, someone, either yourself or someone in your organiation purchased physical computers that are intended to be used for cluster computing. The computers in this cluster can made of off-the-shelf hardware, meaning that someone placed an order to purchase computers usually found in stores shelves or, high-performance hardware, meaning that a computing vendor provided highly customized computing hardware which also comes optimized for high-performance network connectivity, power consumption, etc. When purchasing hundreds or thousands of computing instances, it doesn’t make sense to keep them in the usual computing case that we are all familiar with, but rather, it makes sense to stack them as efficient as possible on top of each other to minimize room space. This group of efficiently stacked computing instances is known as a rack. Once a cluster grows to thousands of computers, you will also need to host hundreds of racks of computing devices, at this scale, you would also need significant physical space to hosts those racks. A building that provides racks of computing instances is usually known as a data-center. At the scale of a data center, optimizing the building that holds them, their heating system, power suply, network connectivity, etc. becomes also relevant to optimize. In 2011, Facebook announced the Open Compute Project inniciative which provides a set of data center blueprints free for anyone to use. There is nothing preventing us from building our own data centers and in fact, many organizations have followed this path. For instance, Amazon started as an online book store, over the years Amazon grew to sell much more than just books and, with it’s online store growth, their data centers also grew in size. In 2002, Amazon considered selling access to virtual servers, in their data centers to the public and, in 2004, Amazon Web Services launched as a way to let anyone rent a subset of their datacenters on-demand, meaning that one did not have to purchase, configure, maintain nor teardown it’s own clusters but could rather rent them from Amazon directly. The on-demand compute model is what we know today as Cloud Computing. It’s a concept that evolved from Amazon Web Services providing their data centers as a service. In the cloud, the cluster you use is not owned by you and is neither in your physical building, but rather, it’s a data center owned and managed by someone else. Today, there are many cloud providers in this space ranging from Amazon, Microsoft, Google, IBM and many others. Most cloud computing platforms provide a user interface either through a web applciation and command line to request and manage resources. While the bennefits of processing data in the cloud were obvious for many years, picking a cloud provider had the unintended side-effect of locking organizations with one particular provider, making it hard to switch between provideers or back to on-premise clusters. Kubernetes, announced by Google in 2014, is an open source system for managing containerized applications across multiple hosts. In practice, it provides common infrastructure otherwise proprietary to cloud providers making it much easier to deploy across multiple cloud providers and on-premise as well. However, being a much newer paradigm than on-premise or cloud computing, it is still in it’s adoption phase but, nevertheless, promising for cluster computing in general and, specifically, for Apache Spark. 5.2 Managers In order to run Spark within a computing cluster, one needs to run something capable of initializing Spark over each compute instance, this is known as a cluster manager. The available cluster managers in Spark are: Spark Standalone, YARN, Mesos and Kubernetes. 5.2.1 Standalone In Spark Standalone, Spark works on it’s own without additional software requirements since it provides it’s own cluster manager as part of the Spark installation. FIGURE 5.2: Spark Standalone Site. By completing the Getting Started chapter, you should have a local Spark installation available, which we can use to initialize a local stanalone Spark cluster. First, retrieve the SPARK_HOME directory by running sparklyr::spark_home_dir() from R and then, from a terminal or R, use start-master.sh and start-slave.sh as follows: # Retrieve the Spark installation directory spark_home &lt;- sparklyr::spark_home_dir() # Build path to start-master.sh start_master &lt;- file.path(spark_home, &quot;sbin&quot;, &quot;start-master.sh&quot;) # Execute start-master.sh to start the cluster manager master node system2(start_master) # Build path to start-slave start_slave &lt;- file.path(spark_home, &quot;sbin&quot;, &quot;start-slave.sh&quot;) # Execute start-slave.sh to start a worker and register in master node system2(start_slave, paste0(&quot;spark://&quot;, system2(&quot;hostname&quot;, stdout = TRUE), &quot;:7077&quot;)) The previous command initialized the master node and a worker node, the master node interface can be accessed under localhost:8080 and looks like the following: FIGURE 5.3: Spark Standalone Web Interface. Notice that there is one worker register in Spark standalone, you can follow the link to this worker node to see additional information: FIGURE 5.4: Spark Standalone Worker Web Interface. Once data analysis is complete, one can simply stop all the running nodes in this local cluster by running: stop_all &lt;- file.path(spark_home, &quot;sbin&quot;, &quot;stop-all.sh&quot;) system2(stop_all) A similar approach can be followed to configure a cluster by running each start-slave.sh command over each machine in the cluster. Further reading: Spark Standalone Mode 5.2.2 Yarn YARN for short, or Hadoop YARN, is the resource manager introduced in 2012 to the Hadoop project. As mentioned in in the Introduction chapter, Spark was built initially to speed up computation over Hadoop; then, when Hadoop 2 was launched, it introduced YARN as a component to manage resources in the cluster, to this date, using Hadoop YARN with Apache Spark is still very common. YARN applications can be submitted in two modes: yarn-client and yarn-cluster. In yarn-cluster mode the driver is running remotely, while in yarn-client mode, the driver is on the machine that started the job, sparklyr supports both modes. FIGURE 5.5: Hadoop YARN Site Further reading: Running Spark on YARN 5.2.3 Mesos Apache Mesos is an open-source project to manage computer clusters. Mesos began as a research project in the UC Berkeley RAD Lab by then PhD students Benjamin Hindman, Andy Konwinski, and Matei Zaharia, as well as professor Ion Stoica. Mesos uses Linux Cgroups to provide isolation for CPU, memory, I/O and file system. FIGURE 5.6: Mesos Landing Site Further reading: Running Spark on Mesos 5.2.4 Kubernetes Kubernetes is an open-source container-orchestration system for automating deployment, scaling and management of containerized applications that was originally designed by Google and now maintained by the Cloud Native Computing Foundation. FIGURE 5.7: Kubernetes Landing Site. Further reading: Running Spark on Kubernetes 5.3 On-Premise As mentioned in the overview section, on-premise clusters represent a set of computing instances procured, colocated and managed by staff members from your organization. These clusters can be highly customized and controlled; however, they can also inccur significant initial expenses and maintenance costs. One can use a cluster manager in on-premise clusters as described in the previous section; however, many organizations choose to partner with companies providing additional management software, services and resources to manage software in their cluster including, but not limited to, Apache Spark. Some of the on-premise cluster providers include: Cloudera, Hortonworks and MapR to mention a few which will be briefly introduced. 5.3.1 Cloudera Cloudera, Inc. is a United States-based software company that provides Apache Hadoop and Apache Spark-based software, support and services, and training to business customers. Cloudera’s hybrid open-source Apache Hadoop distribution, CDH (Cloudera Distribution Including Apache Hadoop), targets enterprise-class deployments of that technology. Cloudera says that more than 50% of its engineering output is donated upstream to the various Apache-licensed open source projects (Apache Hive, Apache Avro, Apache HBase, and so on) that combine to form the Apache Hadoop platform. Cloudera is also a sponsor of the Apache Software Foundation. FIGURE 5.8: Cloudera Landing Site. 5.3.2 Hortonworks Hortonworks is a big data software company based in Santa Clara, California. The company develops, supports, and provides expertise on an expansive set of entirely open source software designed to manage data and processing for everything from IOT, to advanced analytics and machine learning. Hortonworks believes it is a data management company bridging the cloud and the datacenter. FIGURE 5.9: Hortonworks Landing Site. 5.3.3 MapR MapR is a business software company headquartered in Santa Clara, California. MapR provides access to a variety of data sources from a single computer cluster, including big data workloads such as Apache Hadoop and Apache Spark, a distributed file system, a multi-model database management system, and event stream processing, combining analytics in real-time with operational applications. Its technology runs on both commodity hardware and public cloud computing services. FIGURE 5.10: MapR Landing Site. 5.4 Cloud For those readers that don’t have a cluster yet, it is likely that you will want to choose a cloud cluster, this section will briefly mention some of the major cloud infrastructure providers as a starting point to choose the right one for you. It is worth mentioning that in a cloud service model, the compute instances are charged by the hour and times the number of instances reserved for your cluster. Since the cluster size is flexible, it is a good practice to start with small clusters and scale compute resources as needed. Even if you know in advance that a cluster of significant size will be required, starting small provides an opportunity to troubleshoot issues at a lower cost since it’s unlikely that your data analysis will run at scale flawlessly on the first try. The major providers of cloud computing infrastructure are: Amazon, Google and Microsoft that this section will briefly introduce. 5.4.1 Amazon Amazon provides cloud services through Amazon Web Services; more specifically, they provide an on-demand Spark cluster through Amazon Elastic Map Reduce or EMR for short. FIGURE 5.11: Amazon EMR Landing Site. 5.4.2 Google Google provides their on-demand computing services through their Google Cloud, on-demand Spark cluster are provided by Google Dataproc. FIGURE 5.12: Google Dataprox Landing Site. 5.4.3 Microsoft Microsoft provides cloud services thorugh Microsft Azure and Spark clusters through Azure HDInsight. FIGURE 5.13: Azure HDInsight Landing Site. 5.5 Tools While using only R and Spark can be sufficient for some clusters, it is common to install complementary tools in your cluster to improve: monitoring, sql analysis, workflow coordination, etc. with applications like Ganglia, Hue and Oozie respectevly. This secton is not meant to cover all, but rather mention two that are relevant to R and sparklyr. 5.5.1 RStudio RStudio’s open source and professional products, like: RStudio Server, RStudio Server Pro, Shiny Server, Shiny Server Pro, or RStudio Connect; can be installed within the cluster to support many R workflows, while sparklyr does not require any additional tools, they provide significant productivity gains worth considering. FIGURE 5.14: RStudio Server 5.5.2 Jupyter Project Jupyter exists to develop open-source software, open-standards, and services for interactive computing across dozens of programming languages. FIGURE 5.15: Project Jupyter Their notebooks, provide support for various programming languages, including R. sparklyr can be used with Jupyter notebooks using the R Kernel. 5.5.3 Livy Apapche Livy is an incubation project in Apache providing support to use Spark clusters remotely through a web interface. It is ideal to connect directly into the Spark cluster; however, there are times where connecting directly to the cluster is not feasible. When facing those constraints, one can consider installing Livy in their cluster and secure it properly to enable remote use over web protocols. However, there is a significant performance overhead from using Livy in sparklyr for experimentation, meaning that, executing many client comamnds over Livy has a significant overhead; however, running a few commands to generate complex analysis is usually performant since the performance overhead of starting computation can be insignificant compared to the actual cluster computation. FIGURE 5.16: Apache Livy Landing Site. To help test Livy locally, sparklyr provides support to list, install, start and stop a local Livy instance by executing: # List versions of Livy available to install livy_available_versions() ## livy ## 1 0.2.0 ## 2 0.3.0 ## 3 0.4.0 ## 4 0.5.0 # Install default Livy version livy_install() # List installed Livy services livy_installed_versions() # Start the Livy service livy_service_start() # Stops the Livy service livy_service_stop() The default address for this local Livy service is http://localhost:8998 5.6 Recap This chapter explained the history and tradeoffs of on-premise, cloud computing and presented Kubernetes as a promising framework to provide flexibility across on-premise and cloud providers. It also introduced cluster managers (Spark Standalone, YARN, Mesos and Kubernetes) as the software needed to run Spark as a cluster application. This chapter briefly mentioned on-premise cluster providers like Cloudera, Hortonworks and MapR as well as the major cloud providers: Amazon, Google and Microsoft. While this chapter provided a solid foundation to understand current computing trends, cluster tools and providers useful to perform data science; it falls short to help those tasked with deliberately choosing a cluster manager, service provider or architecture. If you have this task assigned to you, use this chapter as a starting point to reach to many more resources to complete your understanding of the platform your organization needs. The next chapter, Connections, assumes a Spark cluster is already available to you and will focus on understanding how to connect to it from sparklyr. "],
["connections.html", "Chapter 6 Connections 6.1 Overview 6.2 Types 6.3 Troubleshooting 6.4 Recap", " Chapter 6 Connections The previous chapter, Clusters, presented the major cluster computing paradigms, cluster managers and cluster providers; this section explains the internal components of a Spark cluster and the how to perform connections to any cluster running Apache Spark. 6.1 Overview Before explaining how to connect to Spark clusters, it is worth discussing the components of a Spark cluster and how they interact, this is often known as the cluster architecture of Apache Spark. First, lets go over a couple definitions. As you know form previous chapters, a cluster is a collection of machines to perform analysis beyond a single computer. However, in distributed systems and clusters literature, we often reffer to each physical machine as a compute instance, compute node, or simply instance or node for short. It is helpful to remind this while reading through this chapter and making use of external resource. In a Spark cluster, there are three types of compute instances that are relevant to Spark: The driver node, the worker nodes and the cluster manager. A cluster manager is a service that allows Spark to be executed in the clsuter and was explained in the Cluster Managers section. The driver node is tasked with delegating work to the worker nodes, but also for aggregating their results and controlling computation flow. For the most part, aggregation happens in the worker nodes; however, even after the nodes aggregate data, it is often the case that the driver node would have to collect the worker’s results. Therefore, the driver node usually has at least, but often much more, compute resources (read RAM, CPU, Local Storage, etc.) than the worker node. The worker nodes execute compute tasks over partitioned data and communicate intermediate results to other workers or back to the driver node, worker nodes are also reffered as executors. Strictly speaking, the driver node and worker nodes are just names assigned to machines with particular roles, while the actual computation in the driver node is performed by the spark context. The Spark context is a Spark component tasked with scheduling tasks, managing data and so on. In the worker nodes, the actual computation is performed under a spark executor, which is also a Spark component tasked with executing subtasks against a data partition. FIGURE 6.1: Apache Spark Architecture If you already have an Spark cluster in their organization, you should ask your cluster administrator to provide connection information for this cluster and read carefully their usage policies and constraints. A cluster is usually shared among many users so you want to be respectful of others time and resources while using a shared cluster environment. Your system administrator will describe if it’s an on-premise vs cloud cluster, the cluster manager being used, supported connections and supported tools. You can use this information to jump directly to Local, Standalone, Yarn, Mesos, Livy or Kubernetes based on the information provided to you. 6.1.1 Edge Nodes Before connecting to Apache Spark, you will first have to connect to the cluster. Usually, by connecting to an edge node within the cluster. An edge node, is a machine that can accessed from outside the cluster but which is also part of the cluster. There are two methods to connect to this edge instance: Terminal: Using a computer terminal applicaiton, one can use a secure shell to establish a remote connection into the cluster, once you connect into the cluster, you can launch R and then use sparklyr. Web Browser: While using sparklyr from a terminal is possible, it is usually more producty to install a web server in an edge node that provides more tools and functionality to run R with sparklyr. Most likely, you will want to consider using RStudio Server rather than connecting from the terminal. FIGURE 6.2: Using a Spark Cluster from an Edge Node 6.1.2 Spark Home It is important to mention that, while connecting to a Spark cluster, you will need to find out the correct SPARK_HOME path which contains the installation of Spark in the given instance. The SPARK_HOME path must be set as an environment variable before connecting or explicitly specified in spark_connect() using the spark_home parameter. For system administrators, we recommend you set SPARK_HOME for all the users in your cluster; however, if this is not set in your cluster you can also specify SPARK_HOME while using spark_connect() as follows: sc &lt;- spark_connect(master = &quot;cluster-master&quot;, spark_home = &quot;local/path/to/spark&quot;) Where cluster-master is set to the correct cluster manager master for Spark Standalone, YARN, Mesos, etc. 6.2 Types 6.2.1 Local When connecting to Spark in local mode, Spark starts as a single application simulating a cluster with a single node, this is not a proper computing cluster but is ideal to perform work offline and troubleshoot issues. A local connection to Spark is represented in the following diagram: FIGURE 6.3: Local Connection Diagram Notice that in the local connections diagram, there is no cluster manager nor worker process since, in local mode, everything runs inside the driver application. It’s also worth pointing out that sparklyr starts the Spark Context through spark-submit, a script available in every Spark installation to enable users to submit custom application to Spark which sparklyr makes use of to submit itself to Spark. For the curious reader, the Contributing chapter explains the internal processes that take place in sparklyr to submit this application and connect properly from R. To perform this local connection, we can connect with the following familiar code used in previous chapters: # Connect to local Spark instance sc &lt;- spark_connect(master = &quot;local&quot;) By default, sparklyr, will connect using as many CPU cores are available in your compute instance; however, this can be customized by connecting using master=&quot;local[n]&quot;, where n is the desired number of cores to use. For example, we can connect using only 2 CPU cores as follows: # Connect to local Spark instance using 2 cores sc &lt;- spark_connect(master = &quot;local[2]&quot;) 6.2.2 Standalone Connecting to a Spark Standalone cluster requires the location of the cluster manager’s master instance, this location can be found in the cluster manager web interface as described in the clusters-standalone section, you can find this location by looking for a URL starting with spark://. A connection in standalone mode starts from sparklyr launching spark-submit to submit the sparklyr application and creating the Spark Context, which requests executors to Spark’s standalone cluster manager in the master location: FIGURE 6.4: Spark Standalone Connection Diagram In order to connect, use master=&quot;spark://hostname:port&quot; in spark_connect() as follows: sc &lt;- spark_connect(master = &quot;spark://hostname:port&quot;) 6.2.3 Yarn Hadoop YARN supports two connection modes: YARN Client and YARN Cluster. However, YARN Client mode is much more common that YARN Cluster since it’s more efficient and easier to set up. 6.2.3.1 Yarn Client When connecting in YARN Client mode, the driver instance runs R, sparklyr and the Spark Context which requests worker nodes from YARN to run Spark executors as follows: FIGURE 6.5: YARN Client Connection Diagram To connect, one can simply run with master = &quot;yarn&quot; as follows: sc &lt;- spark_connect(master = &quot;yarn-client&quot;) Once connected, you can use all techniques described in previous chapters using the sc connection; for instances, you can do data analysis or modeling. 6.2.3.2 Yarn Cluster The main difference between YARN Cluster mode and YARN Client mode is that in YARN Cluster mode, the driver node is not required to be the node where R and sparklyr get started; instead, the driver node remains the designated driver node which is usually a different node than the edge node where R is running. It can be helpful to consider using YARN Cluster when the edge node has too many concurrent users, is lacking computing resources or where tools (like RStudio) need to be managed independently of other clsuter resources. FIGURE 6.6: YARN Cluster Connection Diagram To connect in YARN Cluster mode, we can simple run: sc &lt;- spark_connect(master = &quot;yarn-cluster&quot;) This connection assumes that the node running spark_connect() is properly configured, meaning that, yarn-site.xml exists and the YARN_CONF_DIR environment variable is properly set. When using Hadoop as a file system, one would also need the HADOOP_CONF_DIR environment variable properly configured. This configuration is usually provided by your system administrator and is not something that you would have to manually configure. 6.2.4 Livy As opposed to other connection methods which require using an edge node in the cluster, Livy Livy provides a Web API that makes the Spark cluster accessible from outside the cluster and neither requires a local installation in the client. Once connected through the Web API, the Livy Service starts the Spark context by requesting reosurces from the cluster manager and distributing work as usual. FIGURE 6.7: Livy Connection Diagram Conencting through Livy requires the URL to the Livy service which should be similar to https://hostname:port/livy. Since remote connections are allowed, connections usually requires, at the very least, basic authentication: sc &lt;- spark_connect(master = &quot;https://hostname:port/livy&quot;, method = &quot;livy&quot;, config = livy_config( username=&quot;&lt;username&gt;&quot;, password=&quot;&lt;password&gt;&quot; )) Once connected through Livy, operations you can make use of an other sparklyr feature; however, Livy is not suitable for experimental data analysis, since executing commands have a significant delay; that said, while running long running computations, this overhead could be considered irrelevant. In general, it is preffered to avoid using Livy and work directly within an edge node in the cluster; if this is not feasible, using Livy could be a reasonable approach. 6.2.5 Mesos Similar to YARN, Mesos supports client mode and a cluster mode. However, sparklyr currently only supports client mode for Mesos. FIGURE 6.8: Mesos Connection Diagram Connecting requires the address to the Mesos master node, usually in the form of mesos://host:port or mesos://zk://host1:2181,host2:2181,host3:2181/mesos for Mesos using ZooKeeper. sc &lt;- spark_connect(master = &quot;mesos://host:port&quot;) 6.2.6 Kubernetes Kubernetes cluster do not support client modes similar to Mesos or YARN, instead, the connection model is similar to YARN Cluster, where the driver node is assigned by Kubernetes. FIGURE 6.9: Kubernetes Connection Diagram Kubernetes support is scheduled to be added to sparklyr with sparklyr/issues/1525, please follow progress for this feature directly in github. Once Kubernetes becomes supported in sparklyr, connecting to Kubernetes will work as follows: sc &lt;- spark_connect( master = &quot;k8s://https://&lt;apiserver-host&gt;:&lt;apiserver-port&gt;&quot; config = list( spark.executor.instances = 2, spark.kubernetes.container.image = &quot;spark-image&quot; ) ) If your computer is already configured to use a Kubernetes cluster, you can use the following commmand to find the apiserver-host and apiserver-port: system2(&quot;kubectl&quot;, &quot;cluster-info&quot;) 6.3 Troubleshooting 6.3.1 Logging One first step is to troubleshoot connections is to run in verbose to print directly to the console additional error messages: sc &lt;- spark_connect(master = &quot;local&quot;, log = &quot;console&quot;) ## Re-using existing Spark connection to local Verbose logging can also be enabled with the follwing option: options(sparklyr.verbose = TRUE) 6.3.2 Spark Submit If connections fail in sparklyr, first troubleshoot if this issue is specific to sparklyr or Spark in general. This can be accomplished by running an example spark-submit job and validating that no errors are thrown: # Find the spark directory using an environment variable Sys.getenv(&quot;SPARK_HOME&quot;) # Or by getting the local spark installation sparklyr::spark_home_dir() From the terminal run: cd path/to/spark/ bin/spark-submit 6.3.3 Multiple It is common to connect once, and only once, to Spark. However, you can also open multiple connections to Spark by connecting to different clusters or by specifying the app_name parameter, this can be helpful to compare Spark versions or validate you analysis before submitting to the cluster. The following example opens connections to Spark 1.6.3, 2.3.0 and Spark Standalone: # Connect to local Spark 1.6.3 sc_1_6_3 &lt;- spark_connect(master = &quot;local&quot;, version = &quot;1.6.3&quot;) # Connect to local Spark 2.3.0 sc_2_3_0 &lt;- spark_connect(master = &quot;local&quot;, version = &quot;2.3.0&quot;, appName = &quot;Spark23&quot;) # Connect to local Spark Standalone sc_standalone &lt;- spark_connect(master = &quot;spark://host:port&quot;) Finally, we can disconnect from each connection: spark_disconnect(sc_1_6_3) spark_disconnect(sc_2_3_0) spark_disconnect(sc_standalone) Alternatevely, you can disconnect from all connections at once: spark_disconnect_all() 6.3.4 Windows Connecting from Windows is, in most cases, as straightforward as connecting from Linux or OS X; however, there are a few common connection issues you might hit t Firewalls and atni-viruse software might block ports for your connection. The default port used by sparklyr is 8880, double check this port is not being blocked. Long path names can cause issues in, specially, older Windows systems like Windows 7. When using these systems, try connecting with Spark installed with all folders using 8 characters or less. 6.3.5 Submit Manually To troubleshoot Windows connections in detail, you can use a 2-step initialization that is often very helpful to diagnose connection issues. This 2-step initialization os performed by launching sparklyr through spark-submit followed by connecting with sparklyr from R. First, identify the Spark installation directory. Second, identify the path to the correct sparklyr*.jar, you can find this path by running; dir(system.file(&quot;java&quot;, package = &quot;sparklyr&quot;), pattern = &quot;sparklyr&quot;, full.names = T) ## [1] &quot;/Library/Frameworks/R.framework/Versions/3.5/Resources/library/sparklyr/java/sparklyr-1.5-2.10.jar&quot; ## [2] &quot;/Library/Frameworks/R.framework/Versions/3.5/Resources/library/sparklyr/java/sparklyr-1.6-2.10.jar&quot; ## [3] &quot;/Library/Frameworks/R.framework/Versions/3.5/Resources/library/sparklyr/java/sparklyr-2.0-2.11.jar&quot; ## [4] &quot;/Library/Frameworks/R.framework/Versions/3.5/Resources/library/sparklyr/java/sparklyr-2.1-2.11.jar&quot; ## [5] &quot;/Library/Frameworks/R.framework/Versions/3.5/Resources/library/sparklyr/java/sparklyr-2.2-2.11.jar&quot; ## [6] &quot;/Library/Frameworks/R.framework/Versions/3.5/Resources/library/sparklyr/java/sparklyr-2.3-2.11.jar&quot; Make sure you identify the correct version that matches your Spark cluster, for isntance sparklyr-2.1-2.11.jar for Spark 2.1. Third, from the terminal run: $SPARK_HOME/bin/spark-submit --class sparklyr.Shell $PATH_TO_SPARKLYR_JAR 8880 12345 18/06/11 12:13:53 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable 18/06/11 12:13:53 INFO sparklyr: Session (12345) is starting under 127.0.0.1 port 8880 18/06/11 12:13:53 INFO sparklyr: Session (12345) found port 8880 is available 18/06/11 12:13:53 INFO sparklyr: Gateway (12345) is waiting for sparklyr client to connect to port 8880 The parameter 8880 represents the default port to use in sparklyr while 12345 is the session number, this is a cryptographically secure number generated by sparklyr, but for troubleshooting purpuses can be as simple as 12345. Then, from R, connect as follows, notice that there is a 60 seconds timeout, so you’ll have to run the R command immediately after running the terminal command: library(sparklyr) sc &lt;- spark_connect(master = &quot;sparklyr://localhost:8880/12345&quot;, version = &quot;2.3&quot;) 6.4 Recap This chapter presented an overview of Spark’s architecture and detailed connections concepts and examples to connect in local mode, standalone, YARN, Mesos, Kubernetes and Livy. It also presented edge nodes and their role while connecting to Spark clusters. This information should give you enough information to effectevely connect to any cluster with Apache Spark enabled. To troubleshoot connection problems, it is recommended to search for the connection problem in StackOverflow, the sparklyr github issues and, if needed, open a new GitHub issue in sparklyr to assist further. In the next chapter, Data, you will learn how to read and write over multiple data sources and understand in-depth what a Spark dataframe is. "],
["data.html", "Chapter 7 Data 7.1 Overview 7.2 DataFrames 7.3 Formats 7.4 Data Types 7.5 External 7.6 Troubleshooting 7.7 Recap", " Chapter 7 Data While this chatper has not been written., a few resources are available to help explore these topics until this chapter gets written. 7.1 Overview 7.2 DataFrames TODO: Introduce Data Frames sparklyr provides access to Data Frame functionality, mostly for completeness and advanced use cases. As sparklyr keeps evolving, it is our goal to reduce the use of this functions and provide instead propert wrappers with practices known in the R community. For instnace, we will introduce a function to pivot data, which is quite useful but would rather preffer to support tidyr::spread() and tidyr::gather() implementations which are already well known in the R community. 7.2.1 Functions sdf_nrow() sdf_ncol() sdf_dim() sdf_len() sdf_pivot() sdf_schema() ... 7.2.2 Pivoting sensors &lt;- data.frame( sensor = c(&quot;sensor1&quot;, &quot;sensor1&quot;, &quot;sensor1&quot;, &quot;sensor2&quot;, &quot;sensor2&quot;, &quot;sensor2&quot;), time = c(1, 2, 3, 1, 2, 3), metric = c(1, 2, 3, 10, 20, 30) ) sensor time metric 1 sensor1 1 1 2 sensor1 2 2 3 sensor1 3 3 4 sensor2 1 10 5 sensor2 2 20 6 sensor2 3 30 sensors_tbl &lt;- sdf_copy_to(sc, sensors, overwrite = T) sensors_tbl %&gt;% sdf_pivot(time ~ sensor, fun.aggregate = c(&quot;avg&quot;, &quot;metric&quot;)) # Source: spark&lt;?&gt; [?? x 3] time sensor1 sensor2 * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 3 3 30 2 1 1 10 3 2 2 20 7.3 Formats spark_read_csv() spark_read_jdbc() spark_read_json() spark_read_libsvm() spark_read_orc() spark_read_parquet() spark_read_source() spark_read_table() spark_read_text() 7.4 Data Types 7.4.1 Dates Note: Some Spark date/time functions make timezone assumtions, for instance, the following code makes use of to_date() which assumes the timestamp will be given in the local time zone. This is not to discourage use of date/time functions, but to be aware of time zones to be handled with care. sdf_len(sc, 1) %&gt;% transmute( date = timestamp(1419126103) %&gt;% from_utc_timestamp(&#39;UTC&#39;) %&gt;% to_date() %&gt;% as.character() ) 7.5 External 7.5.1 Amazon S3 7.5.2 Azure Storage wasb files 7.5.3 Cassandra See https://blog.rstudio.com/2017/07/31/sparklyr-0-6/#external-data-sources. 7.5.4 Databases See https://blog.rstudio.com/2017/07/31/sparklyr-0-6/#external-data-sources. 7.5.4.1 Switching You can query multiple databases registered in Spark using the . syntax, as in: DBI::dbSendQuery(&quot;SELECT * FROM databasename.table&quot;) However, if you preffer to switch to a particular database and make it the default, you can run: tbl_change_db(sc, “db_name”) which an alias over DBI::dbGetQuery(sc, &quot;use db_name”). 7.5.4.2 Schemas in_schema(&quot;database&quot;, &quot;table&quot;) 7.5.5 HBase 7.5.6 Nested Data See nested data extension. 7.6 Troubleshooting 7.6.1 Troubleshoot CSVs writeLines(c(&quot;bad&quot;, 1, 2, 3, &quot;broken&quot;), &quot;tmp/bad.csv&quot;) There are a couple modes that can help troubleshoot parsing issues: - PERMISSIVE: NULLs are inserted for missing tokens. - DROPMALFORMED: Drops lines which are malformed. - FAILFAST: Aborts if encounters any malformed line. Which can be used as follows: spark_read_csv( sc, &quot;bad&quot;, &quot;tmp/bad.csv&quot;, columns = list(foo = &quot;integer&quot;), infer_schema = FALSE, options = list(mode = &quot;DROPMALFORMED&quot;)) # Source: table&lt;bad&gt; [?? x 1] # Database: spark_connection foo &lt;int&gt; 1 1 2 2 3 3 In Spark 2.X, there is also a secret column _corrupt_record that can be used to output those incorrect records: spark_read_csv( sc, &quot;decimals&quot;, &quot;tmp/bad.csv&quot;, columns = list(foo = &quot;integer&quot;, &quot;_corrupt_record&quot; = &quot;character&quot;), infer_schema = FALSE, options = list(mode = &quot;PERMISIVE&quot;) ) # Source: table&lt;decimals&gt; [?? x 2] # Database: spark_connection foo `_corrupt_record` &lt;int&gt; &lt;chr&gt; 1 1 NA 2 2 NA 3 3 NA 4 NA sdfsdfds 5 NA 2.16027303300001e+31 7.6.2 Column Names By default, sparklyr sanitizes column names by translating characters like . to _, this was required in Spark 1.6.X to avoid couple nuances in Spark. However, to disable this functionality, you can run the following code: options(sparklyr.sanitize.column.names = FALSE) dplyr::copy_to(sc, iris, overwrite = TRUE) # Source: table&lt;iris&gt; [?? x 5] # Database: spark_connection Sepal.Length Sepal.Width Petal.Length Petal.Width Species &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 5.1 3.5 1.4 0.2 setosa 2 4.9 3 1.4 0.2 setosa 3 4.7 3.2 1.3 0.2 setosa 4 4.6 3.1 1.5 0.2 setosa 5 5 3.6 1.4 0.2 setosa 6 5.4 3.9 1.7 0.4 setosa 7 4.6 3.4 1.4 0.3 setosa 8 5 3.4 1.5 0.2 setosa 9 4.4 2.9 1.4 0.2 setosa 10 4.9 3.1 1.5 0.1 setosa # ... with more rows 7.7 Recap In the next chapter, Tuning, you will learn in-detail how Spark works and use this knowledge to optimize it’s resource usage and performance. "],
["tuning.html", "Chapter 8 Tuning 8.1 Overview 8.2 Configuring 8.3 Partitioning 8.4 Caching 8.5 Shuffling 8.6 Serialization 8.7 Recap", " Chapter 8 Tuning Previous chapters focused on installing, using and connecting to Spark clusters, we’ve assumed so far that computation in a Spark cluster works efficiently. While this is true in many cases, it is often required to have some knowledge of how Spark works internally to perform tunning operations that will make computations run efficiently. Tunning is often required to run operations over datasets that make use of all resources in the Spark cluster. This chapter will explain how Spark works and provide details on how to tune its operations. 8.1 Overview Spark performs distributed computation by: configuring cluster resources and partitioning, executing, shuffling, caching and serializing data across machines. Configuring requests the cluster manager for resources: total machines, memory, etc. Partitioning splits the data among various machines. Partitions can be either implicit or explicit. Executing means running an arbitrary transformation over each partition. Shuffling redistributes data when data to the correct machine. Caching preserves data in-memory across different computation cycles. Serializing transforms data partitions or data collection to be sent over the network to other workers or back to the driver node. The following diagram shows an example on how a sorting job would conceptually work across a cluster of machines. First, Spark would configure the cluster to use three worker machines. In this example, the numbers 1-9 are partitioned across three storage instances. Since the data is already partitioned, each worker node loads this implicit partition; for instance, 4,9,1 is loaded in the first worker node. Afterwards, a custom transformation is applied to each partition in each worker node, this is denoted by f(x) in the diagram below and is defined as a stage in Spark terminalogy. In this example, f(x) executes a sorting operation within a partition. Since Spark is general, execution over a partition can be as simple or complex as needed. Once the execution completes, the result is shuffled to the right machine to finish the sorting operation across the entire dataset. Once the data is sorted across the cluster, the sorted results can be optionally cached in memory to avoid rerunning this computation multiple times. Finally, a small subset of the cached results is serialized, through the network connecting the cluster machines, back to the driver node to print a preview of this sorting example. FIGURE 8.1: Spark Sorting Overview Notice that while the diagram above describes a sorting operation, a similar approach applies to filtering or joining datasets and analyzing and modeling data at scale. Spark provides support to perform custom partitions, custom shuffling, etc; however, these lower level operations are not exposed in sparklyr, instead, sparklyr makes those operations available through higher level commands provided by the data analysis tools like dplyr or DBI, modeling and by using many community extensions. For advanced use cases, one can always use the Spark’s Scala API through an sparklyr custom extensions or run custom [distributed] R code. In order to effectevely tune Spark computations, there are two toolsthat are useful to understand: the graph visualization and the event timeline. Both tools are accessible through the Spark Web Interface and then selecting a particular job and a oarticular state under this job. 8.1.1 Graph Visualization This graph visualization is found under each stage by expanding “DAG Visualization”. DAG stands for Directed Acyclic Graph, since all computations in Spark move computaiton forward without repeating previous steps, this helps Spark optimize computations effectevely. What you will see in this visualization is a breakdown of the operations that Spark had to perform (or is performing if the stage is still active) to execute your computation. It’s hard to understand what they mean the first time you see them, but as you execute more Spark jobs, this graph will become more familiar and will help you identify unexpected steps to investigate further. The following graph represents the stage from ordering a dataset: FIGURE 8.2: Spark Graph Visualization 8.1.2 Event Timeline The event timeline is one of the best ways to optimize your Spark jobs is to use the Spark’s web interface, it’s also available for each Spark stage and gives you a great summary of how Spark is spending computation cycles. In general, you want to see a lot of CPU usage since the other tasks can be considered overhead. You also want to see one event lane per CPU allocated from the cluster to your job so ensure you are fully utilizing your Spark cluster. Lets the take a look at the event timeline for the ordering a data frame by a given column using three partitions: spark_connect(master = &quot;local&quot;) %&gt;% copy_to(iris, repartition = 3) %&gt;% arrange(Sepal_Width) FIGURE 8.3: Spark Event Timeline 8.2 Configuring When tuning a Spark application, consider defining a configuration specification to describe the resources your application needs to successfully run at scale. Some of the most obvious resources you would want to define are: Memory in Driver: The amount of memory available in the driver node, it is convenient to have significantly more memory available in the driver than the worker nodes. Number of Workers:. The number of workers required to be configured for this session. Memory per Worker: The amount of memory available to the worker node. In local mode, spark_connect(master = &quot;local&quot;); as mentioned in the local connections section, there are no workers; however, but we can set the driver settings through: # Initialize configuration with defaults config &lt;- spark_config() # Memory in Driver config[&quot;sparklyr.shell.driver-memory&quot;] &lt;- &quot;2g&quot; # Number of Workers config[&quot;sparklyr.connect.cores.local&quot;] &lt;- 2 # Connect to local cluster with custom configuration sc &lt;- spark_connect(master = &quot;local&quot;, config = config) When Spark runs in Hadoop Yarn, spark_connect(master = &quot;yarn&quot;): # Initialize configuration with defaults config &lt;- spark_config() # Memory in Driver config[&quot;sparklyr.shell.driver-memory&quot;] &lt;- &quot;2g&quot; # Total Workers config[&quot;sparklyr.shell.num-executors&quot;] &lt;- 3 # Cores per Worker config[&quot;sparklyr.shell.executor-cores&quot;] &lt;- 4 # Memory per Worker config[&quot;sparklyr.shell.executor-memory&quot;] &lt;- &quot;2g&quot; # Connect to Yarn with custom configuration sc &lt;- spark_connect(master = &quot;yarn&quot;, config = config) Notice that some of the settings are different between clusters, local and Yarn in the examples above. Therefore, it is common to research online which settings your cluster managers expects. There are a few types of configuration settings: Submit settings are set while sparklyr is being submitted to Spark. For instance, they can configure the driver node. Runtime settings configure Spark when the Spark session is created. For instance, to configure worker nodes settings. sparklyr settings configure sparklyr behaviour. For instance,sparklyr.verbose controls how much diagnostics data is printed. 8.2.1 Submit Settings Some settings must be specified when spark-submit (the terminal application that launches Spark) is run. For instance, since spark-submit launches driver node which runs as a Java instance, choosing how much memory is allocated needs to be specified as a parameter to spark-submit. You can list all the available spark-submit parameters by running: spark_home_dir() %&gt;% file.path(&quot;bin&quot;, &quot;spark-submit&quot;) %&gt;% system2() Usage: spark-submit [options] &lt;app jar | python file | R file&gt; [app arguments] Usage: spark-submit --kill [submission ID] --master [spark://...] Usage: spark-submit --status [submission ID] --master [spark://...] Usage: spark-submit run-example [options] example-class [example args] Options: --master MASTER_URL spark://host:port, mesos://host:port, yarn, k8s://https://host:port, or local (Default: local[*]). --deploy-mode DEPLOY_MODE Whether to launch the driver program locally (&quot;client&quot;) or on one of the worker machines inside the cluster (&quot;cluster&quot;) (Default: client). --class CLASS_NAME Your application&#39;s main class (for Java / Scala apps). --name NAME A name of your application. --jars JARS Comma-separated list of jars to include on the driver and executor classpaths. --packages Comma-separated list of maven coordinates of jars to include on the driver and executor classpaths. Will search the local maven repo, then maven central and any additional remote repositories given by --repositories. The format for the coordinates should be groupId:artifactId:version. --exclude-packages Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in --packages to avoid dependency conflicts. --repositories Comma-separated list of additional remote repositories to search for the maven coordinates given with --packages. --py-files PY_FILES Comma-separated list of .zip, .egg, or .py files to place on the PYTHONPATH for Python apps. --files FILES Comma-separated list of files to be placed in the working directory of each executor. File paths of these files in executors can be accessed via SparkFiles.get(fileName). --conf PROP=VALUE Arbitrary Spark configuration property. --properties-file FILE Path to a file from which to load extra properties. If not specified, this will look for conf/spark-defaults.conf. --driver-memory MEM Memory for driver (e.g. 1000M, 2G) (Default: 1024M). --driver-java-options Extra Java options to pass to the driver. --driver-library-path Extra library path entries to pass to the driver. --driver-class-path Extra class path entries to pass to the driver. Note that jars added with --jars are automatically included in the classpath. --executor-memory MEM Memory per executor (e.g. 1000M, 2G) (Default: 1G). --proxy-user NAME User to impersonate when submitting the application. This argument does not work with --principal / --keytab. --help, -h Show this help message and exit. --verbose, -v Print additional debug output. --version, Print the version of current Spark. Cluster deploy mode only: --driver-cores NUM Number of cores used by the driver, only in cluster mode (Default: 1). Spark standalone or Mesos with cluster deploy mode only: --supervise If given, restarts the driver on failure. --kill SUBMISSION_ID If given, kills the driver specified. --status SUBMISSION_ID If given, requests the status of the driver specified. Spark standalone and Mesos only: --total-executor-cores NUM Total cores for all executors. Spark standalone and YARN only: --executor-cores NUM Number of cores per executor. (Default: 1 in YARN mode, or all available cores on the worker in standalone mode) YARN-only: --queue QUEUE_NAME The YARN queue to submit to (Default: &quot;default&quot;). --num-executors NUM Number of executors to launch (Default: 2). If dynamic allocation is enabled, the initial number of executors will be at least NUM. --archives ARCHIVES Comma separated list of archives to be extracted into the working directory of each executor. --principal PRINCIPAL Principal to be used to login to KDC, while running on secure HDFS. --keytab KEYTAB The full path to the file that contains the keytab for the principal specified above. This keytab will be copied to the node running the Application Master via the Secure Distributed Cache, for renewing the login tickets and the delegation tokens periodically. Notice for instance the --driver-memory parameter, which we previously configured by setting: config &lt;- spark_config() config[&quot;sparklyr.shell.driver-memory&quot;] &lt;- &quot;2gb&quot; In general, any spark-submit setting is configured through sparklyr.shell.X where X is the name of the spark-submit parameter without the -- prefix. 8.2.2 Runtime Settings As mentioned, some Spark settings configure the session runtime. The runtime settings are a superset of the submit settings since is usually helpfull to retrieve the current configuration even if a setting can’t be changed. To list all the Spark settings available at runtime, we can run: spark_session_config(sc) ## Re-using existing Spark connection to local name value spark.master local[4] spark.sql.shuffle.partitions 4 spark.driver.port 55020 spark.submit.deployMode client spark.executor.id driver spark.jars /Library/…/sparklyr/java/sparklyr-2.3-2.11.jar spark.app.id local-1543775189099 spark.env.SPARK_LOCAL_IP 127.0.0.1 spark.sql.catalogImplementation hive spark.spark.port.maxRetries 128 spark.app.name sparklyr spark.home /Users/…/spark/spark-2.3.2-bin-hadoop2.7 spark.driver.host localhost See also, spark.apache.org/docs/latest/configuration.html. 8.2.3 sparklyr Settings Appart from Spark settings, there are a few settings particular to sparklyr listed below. sparklyr.connect.cores is useful to set the CPU cores to use in local mode; the remaining ones are not used as much while tuning, but they can prove helpful while troubleshooting other issues. spark_config_settings() name description sparklyr.apply.packages Configures default value for packages parameter in spark_apply(). sparklyr.apply.rlang Experimental feature. Turns on improved serialization for spark_apply(). sparklyr.apply.schema.infer Number of rows collected to infer schema when column types specified in spark_apply(). sparklyr.backend.interval Total seconds sparklyr will check on a backend operation. sparklyr.backend.timeout Total seconds before sparklyr will give up waiting for a backend operation to complete. sparklyr.connect.aftersubmit R function to call after spark-submit executes. sparklyr.connect.app.jar The path to the sparklyr jar used in spark_connect(). sparklyr.connect.cores.local Number of cores to use in spark_connect(master = “local”), defaults to parallel::detectCores(). sparklyr.connect.csv.embedded Regular expression to match against versions of Spark that require package extension to support CSVs. sparklyr.connect.csv.scala11 Use Scala 2.11 jars when using embedded CSV jars in Spark 1.6.X. sparklyr.connect.jars Additional JARs to include while submitting application to Spark. sparklyr.connect.master The cluster master as spark_connect() master parameter, notice that the ‘spark.master’ setting is usually preferred. sparklyr.connect.packages Spark packages to include when connecting to Spark. sparklyr.connect.ondisconnect R function to call after spark_disconnect(). sparklyr.connect.sparksubmit Command executed instead of spark-submit when connecting. sparklyr.connect.timeout Total seconds before giving up connecting to the sparklyr gateway while initializing. sparklyr.gateway.address The address of the driver machine. sparklyr.gateway.config.retries Number of retries to retrieve port and address from config, useful when using functions to query port or address in kubernetes. sparklyr.gateway.interval Total of seconds sparkyr wil check on a gateway connection. sparklyr.gateway.port The port the sparklyr gateway uses in the driver machine. sparklyr.gateway.remote Should the sparklyr gateway allow remote connections? This is required in yarn cluster, etc. sparklyr.gateway.routing Should the sparklyr gateway service route to other sessions? Consider disabling in kubernetes. sparklyr.gateway.service Should the sparklyr gateway be run as a service without shutting down when the last connection disconnects? sparklyr.gateway.timeout Total seconds before giving up connecting to the sparklyr gateway after initialization. sparklyr.gateway.wait Total seconds to wait before retrying to contact the sparklyr gateway. sparklyr.log.invoke Should every call to invoke() be printed in the console? Can be set to ‘callstack’ to log callstack. sparklyr.log.console Should driver logs be printed in the console? sparklyr.progress Should job progress be reported to RStudio? sparklyr.progress.interval Total of seconds to wait before attempting to retrieve job progress in Spark. sparklyr.sanitize.column.names Should partially unsupported column names be cleaned up? sparklyr.stream.collect.timeout Total seconds before stoping collecting a stream sample in sdf_collect_stream(). sparklyr.stream.validate.timeout Total seconds before stoping to check if stream has errors while being created. sparklyr.verbose Use verbose logging across all sparklyr operations? sparklyr.verbose.na Use verbose logging when dealing with NAs? sparklyr.verbose.sanitize Use verbose logging while sanitizing columns and other objects? sparklyr.worker.gateway.address The address of the worker machine, most likely localhost. sparklyr.worker.gateway.port The port the sparklyr gateway uses in the driver machine. sparklyr.yarn.cluster.accepted.timeout Total seconds before giving up waiting for cluster resources in yarn cluster mode. sparklyr.yarn.cluster.hostaddress.timeout Total seconds before giving up waiting for the cluster to assign a host address in yarn cluster mode. sparklyr.yarn.cluster.lookup.byname Should the current user name be used to filter yarn cluster jobs while searching for submitted one? sparklyr.yarn.cluster.lookup.prefix Application name prefix used to filter yarn cluster jobs while searching for submitted one. sparklyr.yarn.cluster.lookup.username The user name used to filter yarn cluster jobs while searching for submitted one. sparklyr.yarn.cluster.start.timeout Total seconds before giving up waiting for yarn cluster application to get registered. 8.3 Partitioning As mentioned in the introduction chapter, MapReduce and Spark were designed with the purpuse of performing computations against data stored across many machines, the subset of the data available for computation over each compute instance is known as a partition. By default, Spark will compute over each existing implicit partition since it’s more effective to run computaitons were the data is already located. However, there are cases where you will want to set an explicit partition to help Spark use more efficient use of your cluster resources. 8.3.1 Implicit There is always an implicit partition for each Spark computation. If your data is already spread across your cluster evenly, there is usually no need to tune this further. You can get the number of partitions a computation will require through sdf_num_partitions(): sdf_len(sc, 10) %&gt;% sdf_num_partitions() 8.3.2 Explicit There will be times when you have many more compute instances than data partitions, or much less compute instances than the number of partitions in your data. In both cases, it can help to repartition data to match your cluster resources. Various data functions, like spark_read_csv(), already support a repartition parameter to requrest Spark to repartition data appropriately. For instance, we can create a sequence of 10 numbers partitioned by 10 as follows: sdf_len(sc, 10, repartition = 10) %&gt;% sdf_num_partitions() For datasets that are already repartitioned, we can also use sdf_repartition: sdf_len(sc, 10, repartition = 10) %&gt;% sdf_repartition(4) %&gt;% sdf_num_partitions() However, lets look at this with a practical example. Suppose that we want to sort a large dataset that does not even fit in memory, for simplicity, we will generate this dataset by generating 1 billion rows and appending a column of random values. A first attempt to sort this in Spark would be to run: # Attempt to sort 20 GB dataset in disk with one billion entries sdf_len(sc, 10^9) %&gt;% mutate(x = rand()) %&gt;% arrange(x) %&gt;% spark_write_csv(&quot;billion.csv&quot;) However, since each partition needs to fit in memory in Spark, the code above will result in an OutOfMemory exception that shuts down Spark completely. Instead, we can explicitly partition the data into chunks that would fit in the default memory configureation by explicitly defining the total number of partitions to use with the repartition parameter set to 10,000 as follows: library(sparklyr) library(dplyr) sc &lt;- spark_connect(master = &quot;local&quot;) # Sort 20 GB dataset in disk with one billion entries sdf_len(sc, 10^9, repartition = 10^4) %&gt;% mutate(x = rand()) %&gt;% arrange(x) %&gt;% spark_write_csv(&quot;billion.csv&quot;) 8.4 Caching From the introduction chapter, we know that Spark was designed to be faster than it’s predecesors by using memory instead of disk to store data, this is formally known as an Spark RDD and stands for resilient distributed dataset. An RDD is resilient by duplicating copies of the same data across many machines, such that, if one machine fails other can complete the task. Resiliency is important in distributed systems since, while things will usually work in one machine, when running over thousands of machines the likelyhood of something failing is much higher; when a failure happens, it is prefferable be fault tolerant to avoid loosing the work of all the other machines. RDDs are fault tolerant by tracking data lineage information to rebuild lost data automatically on failure. In sparklyr, you can control when an RDD gets loaded or unloaded from memory using tbl_cache() and tbl_uncache(). Most sparklyr operations that retrieve a Spark data frame, cache the results in-memory, for instance, running spark_read_parquet() or sdf_copy_to() will provide a Spark dataframe that is already cached in-memory. As a Spark data frame, this object can be used in most sparklyr functions, including data analysis with dplyr or machine learning. library(sparklyr) sc &lt;- spark_connect(master = &quot;local&quot;) iris_tbl &lt;- sdf_copy_to(sc, iris, overwrite = TRUE) You can inspect which tables are cached by navigating to the Spark UI using spark_web(sc), opening the storage tab, and clicking on a given RDD: FIGURE 8.4: Cached RDD in Spark Web Interface. Data loaded in memory will be released when the R session terminates either explicitly or implicitly with a restart or disconnection; however, to free up resources, you can use tbl_uncache(): tbl_uncache(sc, &quot;iris&quot;) 8.4.1 Checkpointing Checkpointing is a slightly different type of caching, while it also persists data it will, additionally, break the graph computation lineage. So for instance, if a cached partition is lost, it can be computed from the computation graph which is not possible while checkpointing since the source of computation is lost. When performing expensive computation graphs, it can make sense to checkpoint to persist and break the computation lineage, this to help Spark reduce graph computation resources; otherwise, Spark might try to over-optimize a computation graph that is really not useful to optimize. You can checkpoint explicitly by saving to CSV, Parquet, etc. files. Or let Spark checkpoint this for you using sdf_checkpoint() in sparklyr as follows. Notice that checkpointing truncates the computation lineage graph which can speed up performance if the same intermediate result is used multiple times. 8.4.2 Memory Memory in Spark is categorized into: reserved, user, execution or storage: Reserved: Reserved memory is the memory required by Spark to function and therefore, is overhead that is required and should not be configured. This value defaults to 300MB. User: User memory is the memory used to execute custom code, sparklyr only makes use of this memory indirectly when executing dplyr expressions or modeling a dataset. Execution: Execution memory is used to execute code by Spark, mostly, to process the results from the partition and perform shuffling. Storage: Storage memory is used to cache RDDs, for instance, when using tbl_cache() in sparklyr. As part of tuning execution, you can consider tweaking the amount of memory allocated for user, execution and storage by creating a Spark connection with different values than the defaults provided in Spark: config &lt;- spark_config() # define memory available for storage and execution config$spark.memory.fraction &lt;- 0.75 # define memory available for storage config$spark.memory.storageFraction &lt;- 0.5 For instance, if you want to use Spark to store large amounts of data in-memory with the purpuse of filtering and retrieving subsets quickly, you can expect Spark to use little execution or user memory; therefore, to maximize storage memory, one can tune Spark as follows: config &lt;- spark_config() # define memory available for storage and execution config$spark.memory.fraction &lt;- 0.90 # define memory available for storage config$spark.memory.storageFraction &lt;- 0.90 However, notice that Spark will borrow execution memory from storage and viceversa if needed and if possible; therefore, in practice, there should be little need to tune the memory settings. 8.5 Shuffling Shuffling, is the operation that redistributes data across machines, it is usually an expensive operation and therefore, one we try to minimize. One can easily identify is significant time is being spent shuffling by looking at the event timeline. It is possible to reduce shuffling by reframing data analysis questions or hinting Spark appropriately. For instance, when joining dataframes that differ in size significantly, as in, one set being orders of magnitude smaller than the other one. You can consider using sdf_broadcast() to mark a dataframe as small enough for use in broadcast joins, meaning, it pushes one of the smaller dataframes to each of the worker nodes to reduce shuffling the bigger dataframe. One example for sdf_broadcast() follows: sdf_len(sc, 10000) %&gt;% sdf_broadcast() %&gt;% left_join(sdf_len(sc, 100)) 8.6 Serialization It is not that common to have to adjust serialization when tunning Spark; however, it is worth mentioning there are alternative serialization modules like the Kryo Serializer that can provide performance improvements over the default Java Serializer. The Kryo Serializer can be enabled in sparklyr through: config &lt;- spark_config() config$spark.serializer &lt;- &quot;org.apache.spark.serializer.KryoSerializer&quot; sc &lt;- spark_connect(master = &quot;local&quot;, config = config) 8.7 Recap This chapter provided a broad but also detailed overview to help you speed up and reduce resource consumption in Spark, it provided the foundations to understand bottlenecks and some common workarounds to known issues; however, fine-tunning Spark is a broad topic that would require many more chapters to cover extensively. Therefore, while troubleshooting Spark’s performance and scalability, searching the web and consulting online communities is often necessary to fine-tune your particular environment. ## NULL "],
["extensions.html", "Chapter 9 Extensions 9.1 RSparkling 9.2 GraphFrames 9.3 Mleap 9.4 Nested Data", " Chapter 9 Extensions While this chatper has not been written., a few resources are available to help explore these topics until this chapter gets written. 9.1 RSparkling rsparkling provies H2O support in Spark using sparklyr: library(rsparkling) library(sparklyr) library(h2o) cars_h2o &lt;- as_h2o_frame(sc, cars_tbl, strict_version_check = FALSE) h2o.glm(x = c(&quot;wt&quot;, &quot;cyl&quot;), y = &quot;mpg&quot;, training_frame = mtcars_h2o, lambda_search = TRUE) ## Loading required package: h2o ## ## ---------------------------------------------------------------------- ## ## Your next step is to start H2O: ## &gt; h2o.init() ## ## For H2O package documentation, ask for help: ## &gt; ??h2o ## ## After starting H2O, you can use the Web UI at http://localhost:54321 ## For more information visit http://docs.h2o.ai ## ## ---------------------------------------------------------------------- ## ## Attaching package: &#39;h2o&#39; ## The following objects are masked from &#39;package:stats&#39;: ## ## cor, sd, var ## The following objects are masked from &#39;package:base&#39;: ## ## &amp;&amp;, %*%, %in%, ||, apply, as.factor, as.numeric, colnames, ## colnames&lt;-, ifelse, is.character, is.factor, is.numeric, log, ## log10, log1p, log2, round, signif, trunc ## Model Details: ## ============== ## ## H2ORegressionModel: glm ## Model ID: GLM_model_R_1533086487173_1 ## GLM Model: summary ## family link regularization ## 1 gaussian identity Elastic Net (alpha = 0.5, lambda = 0.1013 ) ## lambda_search ## 1 nlambda = 100, lambda.max = 10.132, lambda.min = 0.1013, lambda.1se = -1.0 ## number_of_predictors_total number_of_active_predictors ## 1 2 2 ## number_of_iterations training_frame ## 1 100 frame_rdd_33_a539369727fb5223dbccfbc5b7894962 ## ## Coefficients: glm coefficients ## names coefficients standardized_coefficients ## 1 Intercept 38.941654 20.090625 ## 2 cyl -1.468783 -2.623132 ## 3 wt -3.034558 -2.969186 ## ## H2ORegressionMetrics: glm ## ** Reported on training data. ** ## ## MSE: 6.017684 ## RMSE: 2.453097 ## MAE: 1.940985 ## RMSLE: 0.1114801 ## Mean Residual Deviance : 6.017684 ## R^2 : 0.8289895 ## Null Deviance :1126.047 ## Null D.o.F. :31 ## Residual Deviance :192.5659 ## Residual D.o.F. :29 ## AIC :156.2425 See spark.rstudio.com/guides/h2o. 9.1.1 Trpoubleshooting Apache IVY is a popular dependency manager focusing on flexibility and simplicity, which happens to be used by Apache Spark while installing extensions. When connection fails while using rsparkling, consider clearing your IVY Cache by running: unlink(&quot;~/.ivy2/cache&quot;, recursive = TRUE) 9.2 GraphFrames GraphFrames provides graph algorithms: PageRank, ShortestPaths, etc. gf_graphframe(vertices_tbl, edges_tbl) %&gt;% gf_pagerank(reset_prob = 0.15, max_iter = 10L) GraphFrame Vertices: $ id &lt;dbl&gt; 12, 12, 59, 59, 1, 20, 20, 45, 45, 8, 8, 9, 9, 26, 26, 37, 37, 47, 47, 16, 16, 71, 71, ... $ pagerank &lt;dbl&gt; 0.0058199702, 0.0058199702, 0.0000000000, 0.0000000000, 0.1500000000, 0.0344953402, 0.0... Edges: $ src &lt;dbl&gt; 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 58, 58, 58, 58, 58, 58, 5... $ dst &lt;dbl&gt; 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 65, 65, 65, 65, 65, 65, 6... $ weight &lt;dbl&gt; 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0... FIGURE 9.1: Highschool ggraph dataset with pagerank highlighted. See also spark.rstudio.com/graphframes. 9.3 Mleap Mleap enables Spark pipelines in production. # Create pipeline pipeline_model &lt;- ml_pipeline(sc) %&gt;% ft_binarizer(&quot;hp&quot;, &quot;big_hp&quot;, threshold = 100) %&gt;% ft_vector_assembler(c(&quot;big_hp&quot;, &quot;wt&quot;, &quot;qsec&quot;), &quot;features&quot;) %&gt;% ml_gbt_regressor(label_col = &quot;mpg&quot;) %&gt;% ml_fit(cars_tbl) # Perform predictions predictions_tbl &lt;- ml_predict(pipeline_model, mtcars_tbl) # Export model with mleap ml_write_bundle(pipeline_model, predictions_tbl, &quot;mtcars_model.zip&quot;) Use model outside Spark and productions systems. For instance, in Java: import ml.combust.mleap.runtime.MleapContext; // Initialize BundleBuilder bundleBuilder = new BundleBuilder(); MleapContext context = (new ContextBuilder()).createMleapContext(); Bundle&lt;Transformer&gt; bundle = bundleBuilder.load(new File(request.get(&quot;mtcars_model.zip&quot;)), context); // Read into Mleap DataFrame DefaultLeapFrame inputLeapFrame = new DefaultLeapFrame(); // Perform Mleap transformation DefaultLeapFrame transformedLeapFrame = bundle.root().transform(inputLeapFrame).get(); See also spark.rstudio.com/guides/mleap. 9.4 Nested Data library(sparklyr.nested) "],
["distributed.html", "Chapter 10 Distributed R 10.1 Use Cases 10.2 Columns 10.3 Grouping 10.4 Packages 10.5 Context 10.6 Restrictions 10.7 Troubleshooting", " Chapter 10 Distributed R While this chatper has not been written., use spark.rstudio.com/guides/distributed-r to learn how to use R directly over each worker node. 10.1 Use Cases 10.1.1 Embarrassingly Parallel sdf_len(sc, total_executors, repartition = total_executors) %&gt;% spark_apply(~ data.frame(pi = 3.1416), columns = c(pi = &quot;character&quot;)) %&gt;% summarize(mean = mean(pi)) 10.2 Columns 10.2.1 Inference 10.2.2 Excplicit iris_tbl &lt;- spark_apply( I, columns = lapply(iris, class) ) 10.3 Grouping sdf_len(sc, 10, repartition = 1) %&gt;% transmute(groups = floor(id / 2)) %&gt;% spark_apply(~nrow(.x)) # Source: spark&lt;?&gt; [?? x 1] result * &lt;int&gt; 1 5 2 5 sdf_len(sc, 10, repartition = 1) %&gt;% transmute(groups = floor(id / 2)) %&gt;% sdf_repartition(partition_by = &quot;groups&quot;) %&gt;% spark_apply(~nrow(.x)) # Source: spark&lt;?&gt; [?? x 1] result * &lt;int&gt; 1 4 2 1 3 4 4 1 sdf_len(sc, 10, repartition = 1) %&gt;% transmute(groups = floor(id / 2)) %&gt;% sdf_repartition(partition_by = &quot;groups&quot;) %&gt;% spark_apply(~nrow(.x), group_by = &quot;groups&quot;) # Source: spark&lt;?&gt; [?? x 2] groups result * &lt;dbl&gt; &lt;int&gt; 1 1 2 2 2 2 3 5 1 4 3 2 5 4 2 6 0 1 Notice that spark_apply() does not repartition data automatically, so optimizing how data is repartitioned mus be considered using sdf_repartition(). 10.4 Packages 10.5 Context sdf_len(sc, 3, repartition = 3) %&gt;% spark_apply(function(data, context) context, context = data.frame(something = c(&quot;foo&quot;, &quot;bar&quot;))) # Source: spark&lt;?&gt; [?? x 1] a * &lt;dbl&gt; 1 1 2 2 3 3 4 1 5 2 6 3 sdf_len(sc, 3, repartition = 3) %&gt;% spark_apply( function(data, context) context$numbers * context$constant, context = list( numbers = c(2, 3, 5), constant = 10 ) ) # Source: spark&lt;?&gt; [?? x 1] result * &lt;dbl&gt; 1 20 2 30 3 50 4 20 5 30 6 50 7 20 8 30 9 50 10.6 Restrictions 10.7 Troubleshooting 10.7.1 Tips odbc_logs %&gt;% head() %&gt;% spark_apply(function(df) { tryCatch({ webreadr::read_s3(df[[1]]) &quot;&quot; }, error = function(e) { e$message }) }) 10.7.2 Logs 10.7.3 Debugging If a particular partition fails, you can detect the broken partition by computing a digest, and then retrieving that particular partition as follows: sdf_len(sc, 3) %&gt;% spark_apply(function(x) { worker_log(&quot;processing &quot;, digest::digest(x), &quot; partition&quot;) # your code }) This will add an entry similar to: 18/11/03 14:48:32 INFO sparklyr: RScript (2566) processing f35b1c321df0162e3f914adfb70b5416 partition When executing this in your cluster, you will have to look in the logs for the task that is not finishing, once you have that digest, you can cancel the job. Then you can use that digest to retrieve that specific data frame to R with something like: broken_partition &lt;- sdf_len(sc, 3) %&gt;% spark_apply(function(x) { if (identical(digest::digest(x), &quot;f35b1c321df0162e3f914adfb70b5416&quot;)) x else x[0,] }) %&gt;% collect() WHich you can then run in R to troubleshoot further. "],
["streaming.html", "Chapter 11 Streaming 11.1 Overview 11.2 Transformations 11.3 Shiny 11.4 Formats", " Chapter 11 Streaming 11.1 Overview One can understand a stream as an unbounded data frame, meaning, a data frame with finite columns but infinite rows. Streams are most relevant when processing real time data; for example, when analyzing a Twitter feed or stock prices. Both examples have well defined columns, like ‘tweet’ or ‘price’, but there are always new rows of data to be analyzed. Spark provided initial support for streams with Spark’s DStreams; however, a more versatile and efficient replacement is available through Spark structured streams. Structured streams provide scalable and fault-torerant data processing over streams of data. That means, one can use many machines to process multiple streaming sources, perform joins with other streams or static sources, and recover from failures with at-least-once guarantees (where each message is certain to be delivered, but may do so multiple times). In order to use structured streams in sparklyr, one needs to define the sources, transformations and a destination: The sources are defined using any of the stream_read_*() functions to read streams of data from various data sources. The transformations can be specified using dplyr, SQL, scoring pipelines or R code through spark_apply(). The destination is defined with the stream_write_*() functions, it often also referenced as a sink. Since the transformation step is optional, the simplest stream we can define is to continuously process files, which would effectively copy text files between source and destination. We can define this copy-stream in sparklyr as follows: library(sparklyr) sc &lt;- spark_connect(master = &quot;local&quot;) ## /Users/javierluraschi/spark/spark-2.3.2-bin-hadoop2.7/bin/spark-submit --class sparklyr.Shell &#39;/Library/Frameworks/R.framework/Versions/3.5/Resources/library/sparklyr/java/sparklyr-2.3-2.11.jar&#39; 8880 90477 stream &lt;- stream_read_text(sc, &quot;source/&quot;) %&gt;% stream_write_text(&quot;destination/&quot;) The streams starts running with stream_write_*(); once executed, the stream will monitor the source path and process data into the destination/ path as it arrives. We can use view_stream() to track the rows per second (rps) being processed in the source, destination and their latest values over time: stream_view(stream) FIGURE 11.1: Viewing a Spark Stream with sparklyr Notice that the rows-per-second in the destination stream are higher than the rows-per-second in the source stream; this is expected and desireable since Spark measures incoming rates from the source, but actual row processing times in the destination stream. For example, if 10 rows-per-second are written to the source/ path, the incoming rate is 10 RPS. However, if it takes Spark only 0.01 seconds to write all those 10 rows, the output rate is 100 RPS. Use stream_stop() to properly stop processing data from this stream: stream_stop(stream) In order to reproduce the above example, one needs to feed streaming data into the source/ path. This was accomplished by running stream_generate_test() to produce a file every second containing lines of text that follow overlapping binomial distributions. In practice, you would connect to existing sources without having to generate data artificially. See ?stream_generate_test for additional details and make sure the later package is installed. stream_generate_test(paste(&quot;Row&quot;, 1:1000), &quot;source/&quot;) For the subsequent examples, a stream with one hundred rows of text will be used: writeLines(paste(&quot;Row&quot;, 1:100), &quot;source/rows.txt&quot;) 11.2 Transformations Streams can be transformed using dplyr, SQL, pipelines or R code. We can use as many transformations as needed in the same way that Spark data frames can be transformed with sparklyr. The transformation source can be streams or data frames but the output is always a stream. If needed, one can always take a snapshot from the destination stream and save the output as a data frame, which is what sparklyr will do for you if a destination stream is not specified. Conceptually, this looks as follows: FIGURE 11.2: Streams Transformation Diagram 11.2.1 dplyr Using dplyr, we can process each row of the stream; for example, we can filter the stream to only the rows containing a number one: library(dplyr, warn.conflicts = FALSE) stream_read_text(sc, &quot;source/&quot;) %&gt;% filter(line %like% &quot;%1%&quot;) ## # Source: spark&lt;?&gt; [inf x 1] ## line ## * &lt;chr&gt; ## 1 Row 1 ## 2 Row 10 ## 3 Row 11 ## 4 Row 12 ## 5 Row 13 ## 6 Row 14 ## 7 Row 15 ## 8 Row 16 ## 9 Row 17 ## 10 Row 18 ## # ... with more rows Since the destination was not specified, sparklyr creates a temporary memory stream and previews the contents of a stream by capturing a few seconds of streaming data. We can also aggregate data with dplyr, stream_read_text(sc, &quot;source/&quot;) %&gt;% summarise(n = n()) ## # Source: spark&lt;?&gt; [inf x 1] ## n ## * &lt;dbl&gt; ## 1 100 and even join across many concurrent streams: left_join( stream_read_text(sc, &quot;source/&quot;) %&gt;% stream_watermark(), stream_read_text(sc, &quot;source/&quot;) %&gt;% stream_watermark() %&gt;% mutate(random = rand()), ) ## Joining, by = c(&quot;line&quot;, &quot;timestamp&quot;) ## # Source: spark&lt;?&gt; [inf x 3] ## line timestamp random ## * &lt;chr&gt; &lt;dttm&gt; &lt;dbl&gt; ## 1 Row 3 2018-12-02 18:27:01 0.183 ## 2 Row 6 2018-12-02 18:27:01 0.958 ## 3 Row 20 2018-12-02 18:27:01 0.184 ## 4 Row 32 2018-12-02 18:27:01 0.388 ## 5 Row 38 2018-12-02 18:27:01 0.685 ## 6 Row 39 2018-12-02 18:27:01 0.509 ## 7 Row 41 2018-12-02 18:27:01 0.959 ## 8 Row 43 2018-12-02 18:27:01 0.953 ## 9 Row 44 2018-12-02 18:27:01 0.630 ## 10 Row 45 2018-12-02 18:27:01 0.290 ## # ... with more rows However, some operations, require watermarks to define when to stop waiting for late data. You can specify watermarks in sparklyr using stream_watermak(), see also handling late data in Spark’s documentation. 11.2.2 Pipelines Spark pipelines can be used for scoring streams, but not to train over streaming data. The former is fully supported while the latter is a feature under active development by the Spark community. To use a pipeline for scoring a stream, first train a Spark pipeline over a static dataset. Once trained, save the pipeline, then reload and score over a stream as follows: fitted_pipeline &lt;- ml_load(sc, &quot;iris-fitted/&quot;) stream_read_csv(sc, &quot;iris-in&quot;) %&gt;% sdf_transform(fitted_pipeline) %&gt;% stream_write_csv(&quot;iris-out&quot;) 11.2.3 R Code Arbitrary R code can also be used to transform a stream with the use of spark_apply(). Following the same principles from executing R code over Spark data frames, for structured streams, spark_apply() runs R code over each executor in the cluster where data is available, this enables processing high-throughput streams and fullfill low-latency requirements. The following example splits a stream of Row # line entries and adds jitter using R code: stream_read_text(sc, &quot;source/&quot;) %&gt;% spark_apply(~ jitter(as.numeric(gsub(&quot;Row &quot;, &quot;&quot;, .x$text)))) ## # Source: spark&lt;?&gt; [inf x 1] ## # ... with 1 variable: result &lt;dbl&gt; 11.3 Shiny Streams can be used with Shiny by making use of the reactiveSpark() to retrieve the stream as a reactive data source. Internally, reactiveSpark() makes use of reactivePoll() to check the stream’s timestamp and collect the stream contents when needed. The following Shiny application makes use of reactiveSpark() to view a Spark stream summarized with dplyr: library(shiny) library(sparklyr) library(dplyr) sc &lt;- spark_connect(master = &quot;local&quot;) ui &lt;- fluidPage( sidebarLayout( mainPanel( tableOutput(&quot;table&quot;) ) ) ) server &lt;- function(input, output, session) { pollData &lt;- stream_read_text(sc, &quot;source/&quot;) %&gt;% summarise(n = n()) %&gt;% reactiveSpark(session = session) output$table &lt;- renderTable({ pollData() }) } shinyApp(ui = ui, server = server) ## NULL 11.4 Formats The following formats are available to read and write streaming data: Format Read Write CSV stream_read_csv stream_write_csv JDBC stream_read_jdbc stream_write_jdbc JSON stream_read_json stream_write_json Kafka stream_read_kafka stream_write_kafka ORC stream_read_orc stream_write_orc Parquet stream_read_parquet stream_write_parquet Text stream_read_text stream_write_text Memory stream_write_memory "],
["contributing.html", "Chapter 12 Contributing 12.1 Overview 12.2 R Extensions 12.3 Scala Extensions 12.4 Spark Extensions 12.5 R Packages 12.6 sparklyr 12.7 Recap", " Chapter 12 Contributing There are many ways to contribute, from helping community members to opening GitHub issues, to providing new functionality for yourself, colleagues or the R and Spark community; this last chapter will focus on writting and sharing code that extends Spark in many useful and probably also, awesome, ways. Specifically, in this chapter you will learn what an extension is, when to build one, what tools are available, how to build an extension and when to consider contributing to sparklyr itself. 12.1 Overview While working with R and therefore, while working with R and Spark, you will write R code. In fact, you have already written R code throught most of the previous chaters in this book. Writting code can be as simple as loading data from a text file to writting distributed R code. But for the sake of the argument, lets consider one of the first lines of code presented in this book: spark_read_text(sc, &quot;hello&quot;, &quot;hello.txt&quot;) When thinking of contributing back, the most important question you can ask about the code above, but really, about any piece of code you write is: Would this code be useful to someone else? For the code above, the answer is probably no, it’s just too generic and can be easiy found online; however, a more realistic example would be to tailor something the code above for something that you actually care about, perhaps: spark_read_text(sc, &quot;stuff-that-matters&quot;, &quot;/secret/path/which/was/hard/to/figure/out/&quot;) The code above is quite similar to the original one, but assuming that you work with colleages, the answer to: Would this code be useful to someone else? Is now completely different: Yes, most likely! Which is surprising since this means that not all useful code needs to be very advanced or complicated; however, for it to be useful to others, it does need to be packaged, presented and shared in a usable format. One first attempt would be to wrap this into a file useful.R and then write a function over it, as in: load_useful_data &lt;- function() { spark_read_text(sc, &quot;stuff-that-matters&quot;, &quot;/secret/path/which/was/hard/to/figure/out/&quot;) } Which is an improvement but it would require users to manually share this file over and over. Fortunately, this is a problem already solved in R quite well through R Packages. An R package contains R code packaged in a format installable using the install.packages() function. sparklyr is an R package, but there are many other packages available in R and you can also create your own packages. For those of you new to creating R packages, I would encourage reading Hadley Wickam’s book on packages: R Packages: Organize, Test, Document, and Share Your Code. Creating an R package allows you to easily share your functions with others by sharing the package file in your organization. Once a package is created, there are many ways to share this with colleagues or the world. For instance, for packages meant to be private, you can consider using Drat or products like RStudio Package Manager. R packages meant for public consumption are made available to the R community in CRAN, which stands for the Comprehensive R Archive Network. These repositories of R packages make packages allow users to install packages through install.packages(&quot;usefulness&quot;) without having to worry where to download the package from and allows other packages to reuse your package in their packages as well. While this was a very brief introduction to R packages in the context of Spark, it should be more or less obvious that you should be thinking of writting R packages while extending Spark from R. The rest of this chapter will present the tools and concepts require to extend functionality in sparklyr. There are three different ways in which sparklyr extensions can be written: R Extensions: These extensions make use of only R code and are the easiest one to get started with. Scala Extensions: These extensions make use of R code but also Scala code to get access to all the functionality available in Spark. Spark Extensions: These extensions make use of R code, Scala code and also Spark extensions on their own and while they could be seen as the most complex of all, they are also some of the most useful extensions we can write. Then we can wrap those extensions into an [R Package] or consider the functionality to be added back into sparklyr. 12.2 R Extensions R extensions make use of three functions in sparklyr: invoke_new(), invoke_static() and invoke(). For the most part, that’s all you need to extend Spark’s functionality in R. spark_context(sc) %&gt;% invoke(&quot;textFile&quot;, &quot;my-file.txt&quot;, 1L) %&gt;% invoke(&quot;count&quot;) 12.3 Scala Extensions 12.3.1 Prerequisites Changes in the scala sources require the Scala compilers to be installed. You can install the required compilers by running: library(sparklyr) download_scalac() Which will download the correct compilers from https://www.scala-lang.org/. 12.4 Spark Extensions 12.5 R Packages 12.5.1 RStudio Projects You can create an sparklyr extension with ease from RStudio. This feature requires RStudio 1.1 or newer and the sparklyr package to be installed. Then, from the File menu, New Project..., select R Packag using sparklyr: 12.5.2 Troubleshooting We can trace all the calls made to invoke(), invoke_new() and invoke_static() using the sparklyr.invoke.trace and sparklyr.invoke.trace.callstack options as follows: config &lt;- spark_config() config$sparklyr.invoke.trace &lt;- TRUE spark_connect(master = &quot;local&quot;, config = config) 12.6 sparklyr First of all, it’s worth mentioning that, sparklyr is just another R package; a package which contains R code packaged in a format installable using the install.packages() function. Now, since Spark was built in the Scala programming language, sparklyr also contains Scala code to provide the functionality required to interoperate with Spark efficiently. CRAN, which stands for the Comprehensive R Archive Network, provides a repository of packages that can be easily installable and which are carefully reviewed before they are made available. You can read more about the release process under releasing a package and Hadley Wickam’s book on packages: R Packages: Organize, Test, Document, and Share Your Code. CRAN users are always encouraged to install the latest version of a package, which means that sparklyr also needs to support multiple versions of Spark. Therefore, at a high level, sparklyr is composed of: R code and versioned Scala code: FIGURE 12.1: Sparklyr Package Architecture The sparklyr sources are built with R CMD build or from the RStudio’s build menu, this topic extensevely covered in the R Packages book. The Scala code is compiled into JAVA Archive files which will eventually be executed by Spark through the Java Virtual Machine. Compilation can be manually performed with the Scala compiler using compile_package_jars(), a sparklyr function that invokes the Scala compiler over a set of supported versions. When connecting to Spark using spark_connect(), sparklyr submits the correct version of the sparklyr JAR to Spark which then Spark executes. We will refer to this submitted application as the sparklyr backend and the R code as the sparklyr frontend. Fontend and backend are common software engineer concepts related to separating the user interface, the R console in this case, with the data layer, Spark data proessing in this context. For most connections, the backend is usually submitted by sparklyr to Spark using spark-submit, which is a command line tool well-known in Spark; however, for others connections, like Livy, the backend is submitted through Livy’s HTTP interface. Once the backend is submitted to Spark, the frontend communicates to the backend using socket connections, expect for Livy connections where this happens over HTTP: FIGURE 12.2: Sparklyr Connection Architecture 12.6.1 Compiling To compile sparklyr, make sure the prerequisites described in the Scala Extensions Prerequisites section are fullfilled. Then you can recompile all the jars by running configure.R in the root of the sparklyr sources. Once the jars are compiled, you can build the R package as described in the R Extensions section. 12.6.2 Serialization 12.6.3 Invocations 12.6.4 R Packages (dbi, dplyr, broom, etc) 12.6.5 Connections 12.6.6 Distributed R 12.7 Recap "],
["appendix.html", "Chapter 13 Appendix 13.1 Diagrams", " Chapter 13 Appendix 13.1 Diagrams 13.1.1 Worlds Store Capacity library(tidyverse) read_csv(&quot;data/01-worlds-capacity-to-store-information.csv&quot;, skip = 8) %&gt;% gather(key = storage, value = capacity, analog, digital) %&gt;% mutate(year = X1, terabytes = capacity / 1e+12) %&gt;% ggplot(aes(x = year, y = terabytes, group = storage)) + geom_line(aes(linetype = storage)) + geom_point(aes(shape = storage)) + scale_y_log10( breaks = scales::trans_breaks(&quot;log10&quot;, function(x) 10^x), labels = scales::trans_format(&quot;log10&quot;, scales::math_format(10^x)) ) + theme_light() + theme(legend.position = &quot;bottom&quot;) 13.1.2 Daily downloads of CRAN packages downloads_csv &lt;- &quot;data/01-intro-r-cran-downloads.csv&quot; if (!file.exists(downloads_csv)) { downloads &lt;- cranlogs::cran_downloads(from = &quot;2014-01-01&quot;, to = &quot;2018-01-01&quot;) readr::write_csv(downloads, downloads_csv) } cran_downloads &lt;- readr::read_csv(downloads_csv) ggplot(cran_downloads, aes(date, count)) + geom_point(colour=&quot;black&quot;, pch = 21, size = 1) + scale_x_date() + xlab(&quot;&quot;) + ylab(&quot;&quot;) + theme_light() 13.1.3 Google trends for mainframes, cloud computing and kubernetes library(r2d3) read.csv(&quot;data/05-cluster-trends.csv&quot;) %&gt;% mutate(month = as.Date(paste(month, &quot;-01&quot;, sep = &quot;&quot;))) %&gt;% r2d3(script=&quot;images/05-clusters-trends.js&quot;) "],
["references.html", "Chapter 14 References", " Chapter 14 References "]
]
