[
["starting.html", "Chapter 2 Getting Started 2.1 Prerequisites 2.2 Installing Spark 2.3 Connecting to Spark 2.4 Using Spark 2.5 Disconnecting 2.6 RStudio 2.7 Troubleshooting 2.8 Recap", " Chapter 2 Getting Started From R, getting started with Spark using sparklyr and a local cluster is as easy as running: spark_install() sc &lt;- spark_connect(master = &quot;local&quot;) To make sure we can all run the code above and understand it, this chapter will walk you through installing the prerequisites, installing Spark, connecting to a local Spark cluster and briefly explaining how to use Spark. However, if a Spark cluster and R environment have been made available to you, you do not need to install the prerequisites nor install Spark yourself. Instead, you should ask for the Spark master parameter and connect as follows, this parameter will be formally introduced under the clusters and connections chapters. sc &lt;- spark_connect(master = &quot;&lt;cluster-master&gt;&quot;) 2.1 Prerequisites As briefly mentioned in the introduction chapter, R is a programming language that can run in many platforms and environments. Most people making use of a programming language also choose tools to make them more productive in it; for R, RStudio would be such tool. Strictly speaking, RStudio is an Integrated Development Environment or IDE for short, which also happens to support many platforms and environments. R and RStudio are the free software tools this book will make use of and therefore, I strongly recommend you get those installed if you haven’t done so already. Additionally, since Spark is build in the Scala programming language which is run by the Java Virtual Machine, you also need to install Java in your system. It is likely that your system already has Java installed, but you should still check the version and update if needed as described in the Install Java section. 2.1.1 Install R From r-project.org, download and launch the R installer for your platform, Windows, Macs or Linux available. The R Project for Statistical Computing. 2.1.2 Install Java From java.com/download, download and launch the installer for your platform, Windows, Macs or Linux are also available. Java Download. Starting with Spark 2.1, Java 8 is required; however, previous versions of Spark support Java 7. Regardless, we recommend installing Java 8. 2.1.3 Install RStudio While installing RStudio is not strictly required to work with sparklyr in R, it will make you much more productive and therefore, I would recommend you take the time to install RStudio from rstudio.com/download, then download and launch the installer for your platform: Windows, Macs or Linux. RStudio Downloads. After launching RStudio, you can use RStudio’s console panel to execute the code provided in this chapter. 2.1.4 Install sparklyr As many other R packages, sparkylr is available to be installed from CRAN and can be easily installed as follows: install.packages(&quot;sparklyr&quot;) The CRAN release of sparklyr contains the most stable version and it’s the recommended version to use; however, to try out features being developed in sparklyr, you can install directly from GitHub using the devtools package. First, install the devtools package and then install sparklyr as follows: install.packages(&quot;devtools&quot;) devtools::install_github(&quot;rstudio/sparklyr&quot;) The sparklyr GitHub repository contains all the latest features,issues and project updates; it’s the place where sparklyr is actevely developed and a resource that is helpful while troubleshooting issues. The sparklyr GitHub code repository. 2.2 Installing Spark Start by loading sparklyr, library(sparklyr) This will makes all sparklyr functions available in R, which is really helpful; otherwise, we would have to run each sparklyr command prefixed with sparklyr::. As mentioned, Spark can be easily installed by running spark_install(); this will install the latest version of Spark locally in your computer, go ahead and run spark_install(). Notice that this command requires internet connectivity to download Spark. spark_install() All the versions of Spark that are available for installation can be displayed with spark_available_versions(): spark_available_versions() ## spark ## 1 1.6.3 ## 2 1.6.2 ## 3 1.6.1 ## 4 1.6.0 ## 5 2.0.0 ## 6 2.0.1 ## 7 2.0.2 ## 8 2.1.0 ## 9 2.1.1 ## 10 2.2.0 ## 11 2.2.1 ## 12 2.3.0 ## 13 2.3.1 ## 14 2.3.2 ## 15 2.4.0 A specific version can be installed using the Spark version and, optionally, by also specifying the Hadoop version. For instance, to install Spark 1.6.3, we would run: spark_install(version = &quot;1.6.3&quot;) You can also check which versions are installed by running: spark_installed_versions() spark hadoop dir 7 2.3.1 2.7 /spark/spark-2.3.1-bin-hadoop2.7 The path where Spark is installed is referenced as Spark’s home, which is defined in R code and system configuration settings with the SPARK_HOME identifier. When using a local Spark cluster installed with sparklyr, this path is already known and no additional configuration needs to take place. Finally, in order to uninstall an specific version of Spark you can run spark_uninstall() by specifying the Spark and Hadoop versions, for instance: spark_uninstall(version = &quot;2.3.1&quot;, hadoop = &quot;2.7&quot;) 2.3 Connecting to Spark It’s important to mention that, so far, we’ve only installed a local Spark cluster. A local cluster is really helpful to get started, test code and troubleshoot with ease; further chapters will explain where to find, install and connect to real Spark clusters with many machines; but for the first few chapters, we will focus on using local clusters. Threfore, to connect to this local cluster we simply run: sc &lt;- spark_connect(master = &quot;local&quot;) The master parameter helps sparklyr find which is the “main” machine from the Spark cluster, this machine is often call the driver node. While working with real clusters using many machines, most machines will be worker machines and one will be the master. Since we only have a local cluster with only one machine, we will default to use &quot;local&quot; for now. 2.4 Using Spark Now that you are connected, we can run a few simple commands. For instance, let’s start by loading some data into Apache Spark. To accomplish this, lets first create a text file by running: write(&quot;Hello World!&quot;, &quot;hello.txt&quot;) We can now read this text file back from Spark by running: spark_read_text(sc, &quot;hello&quot;, &quot;hello.txt&quot;) ## # Source: spark&lt;hello&gt; [?? x 1] ## line ## * &lt;chr&gt; ## 1 Hello World! Congrats! You have successfully connected and loaded your first dataset into Spark. Let’s explain what’s going on in spark_read_text(). The first parameter, sc, gives the function a reference to the active Spark Connection that was earlier created with spark_connect(). The second parameter names this dataset in Spark. The third parameter specifies a path to the file to load into Spark. Now, spark_read_text() returns a reference to the dataset in Spark which R automatically prints. Whenever a Spark dataset is printed, sparklyr will collect some of the records and display them for you. In this particular case, that dataset contains just one row for the line: Hello World!. We will now use this simple example to present various useful tools in Spark we should get familiar with. 2.4.1 Web Interface Most of the Spark commands will get started from the R console; however, it is often the case that monitoring and analyzing execution is done through Spark’s web interface. This interface is a web page provided by Spark which can be accessed from sparklyr by running: spark_web(sc) Apache Spark Web Interface. As mentioned, printing the “hello” dataset collected a few records to be displayed in the R console. You can see in the Spark web interface that a job was started to collect this information back from Spark. You can also select the storage tab to see the “hello” dataset cached in-memory in Spark: Apache Spark Web Interface - Storage Tab. The caching section in the tunning chapter will cover this in detail, but as a start, it’s worth noticing that this dataset is fully loaded into memory since the fraction cached is 100%, it is useful also to point out the size in memory column which tracks the total memory being used by this dataset. 2.4.2 Logs Another common tool to use in Spark that you should familiarize with are the Spark logs. A log is just a text file where Spark will append information relevant to the execution of tasks in the cluster. For local clusters, we can retrieve all the logs by running: spark_log(sc, n = 5) 18/10/09 19:41:46 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5). 1499 bytes result sent to driver 18/10/09 19:41:46 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 5) in 32 ms on localhost (executor driver) (1/1) 18/10/09 19:41:46 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 18/10/09 19:41:46 INFO DAGScheduler: ResultStage 5 (collect at utils.scala:197) finished in 0.039 s 18/10/09 19:41:46 INFO DAGScheduler: Job 3 finished: collect at utils.scala:197, took 0.043086 s Or we can retrieve specific log entries containing, say sparklyr, by using the filter parameter as follows: spark_log(sc, filter = &quot;sparklyr&quot;, n = 5) ## 18/10/09 18:53:23 INFO SparkContext: Submitted application: sparklyr ## 18/10/09 18:53:23 INFO SparkContext: Added JAR file:/Library/Frameworks/R.framework/Versions/3.5/Resources/library/sparklyr/java/sparklyr-2.3-2.11.jar at spark://localhost:52930/jars/sparklyr-2.3-2.11.jar with timestamp 1539136403697 ## 18/10/09 18:53:27 INFO Executor: Fetching spark://localhost:52930/jars/sparklyr-2.3-2.11.jar with timestamp 1539136403697 ## 18/10/09 18:53:27 INFO Utils: Fetching spark://localhost:52930/jars/sparklyr-2.3-2.11.jar to /private/var/folders/ks/wm_bx4cn70s6h0r5vgqpsldm0000gn/T/spark-141b51a3-f277-4530-aa6a-69be176e0c0b/userFiles-3d94e32b-c65d-4081-a85e-d1e4716e0cef/fetchFileTemp1188493532217239876.tmp ## 18/10/09 18:53:27 INFO Executor: Adding file:/private/var/folders/ks/wm_bx4cn70s6h0r5vgqpsldm0000gn/T/spark-141b51a3-f277-4530-aa6a-69be176e0c0b/userFiles-3d94e32b-c65d-4081-a85e-d1e4716e0cef/sparklyr-2.3-2.11.jar to class loader 2.5 Disconnecting For local clusters and really, any cluster; once you are done processing data you should disconnect by running: spark_disconnect(sc) This will terminate the connection to the cluster but also terminate the cluster tasks as well. If multiple Spark connections are active, or if the conneciton instance sc is no longer available, you can also disconnect all your Spark connections by running: spark_disconnect_all() Notice that exiting R, RStudio or restarting your R session will also cause the Spark connection to terminate, which in turn terminates the Spark cluster and cached data that is not explicitly persisted. 2.6 RStudio If you are not familiar with RStudio, you will want to identify at least the following panes: Console: This is just a standalone R console you can use to execute all the code presented in this book. Packages: This pane allows you to install sparklyr with ease, check its version, navigate to the help contents, etc. Connections: This pane allows you to connecto to Spark, manage your active connection and view the available datasets. RStudio Overview. To start a new connections to Spark, you can use spark_connect() from the R console or, alternatevely, use the new connection action from the connections pane and then, select the Spark connection. Once a Spark connection is selected, you can customize the versions and connect to Spark which will simply generate the right spark_connect() command and execute this in the R console for you: RStudio New Spark Connection. Once connected to Spark, either by using the R console or through RStudio’s connections pane, RStudio will display your datasets available in the connections pane. This is a useful way to track your existing datasets and provides an easy way to explore each of them: RStudio Connections Pane. Additionally, an active connection provides the following custom actions: Spark UI: Opens the Spark web interface, a shortcut to spark_ui(sc). Log: Opens the Spark web logs, a shortcut to spark_log(sc). SQL: Opens a new SQL query, see DBI and SQL support in the data analysis chapter. Help: Opens the sparklyr reference documentation in a new web browser window. Disconnect: Disconnects from Spark, a shortcut to spark_disconnect(sc). The rest of this book will not cover any additional topics related to RStudio, whether you are using plain R from a terminal or the R console from RStudio, the code provided in this book executes in the same way across any R environment. 2.7 Troubleshooting While we’ve put significant effort to simplify the onboarding process, it is now a good time to give you additional resources that can help you troubleshoot particular issues while getting started and, in general, while working with Spark from R. We hope you will find the following resources helpful: Documentation: This should be your entry point to learn more about sparklyr, the documentation is kept up to date with examples, reference functions and many more relevant resources, spark.rstudio.com. RStudio Community: For general sparklyr questions, you can post then in the RStudio Community tagged as sparklyr, community.rstudio.com/tags/sparklyr. Stack Overflow: For general Spark questions, Stack Overflow is the good start, stackoverflow.com/questions/tagged/apache-spark. Github: If you believe something needs to get fixed, open a GitHub issue or send us a pull request, github.com/rstudio/sparklyr. Gitter: For urgent issues or to keep in touch you can chat with us in Gitter, gitter.im/rstudio/sparklyr. 2.8 Recap This chapter walked you through installing R, Java, RStudio and sparklyr as the main tools required to use Spark from R. We covered installing local Spark clusters using spark_install(), connecting using spark_connect() and learned how to launch the web interface and logs using spark_web(sc) and spark_log(sc) respectedly. It is my hope that this chapter will help anyone interested in learning cluster computing using Spark and R getting started, ready to experiment on your own and ready to tackle actual data analysis and modeling tasks without any major blockers. "]
]
