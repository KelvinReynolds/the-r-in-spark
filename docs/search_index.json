[
["index.html", "Mastering Apache Spark with R Welcome", " Mastering Apache Spark with R Javier Luraschi, Kevin Kuo, Edgar Ruiz 2019-06-10 Welcome In this book you will learn how to use Apache Spark with R using the sparklyr R package. The book intends to take someone unfamiliar with Spark or R and help them become intermediate users by teaching a set of tools, skills and practices applicable to data science. This work is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 United States License. Note: While this book is being written, the content in this website will not be accessible, once published, this website will reopen. Contact javier@rstudio.com for early access. "],
["intro.html", "Chapter 1 Introduction 1.1 Information 1.2 Hadoop 1.3 Spark 1.4 R 1.5 sparklyr 1.6 Recap", " Chapter 1 Introduction With information growing at exponential rates, it’s no surprise that historians are referring to this period of history as the Information Age. The increasing speed at which data is being collected has created new opportunities and is certainly staged to create even more. This chapter presents the tools that have been used to solve large-scale data challenges and introduces Apache Spark as a leading tool that is democratizing our ability to process data at large-scale. We will then introduce the R computing language, which was specifically designed to simplify data analysis. It is then natural to ask what the outcome would be from combining the ease of use provided by R, with the compute power available through Apache Spark. This will lead us to introduce sparklyr, a project merging R and Spark into a powerful tool that is easily accessible to all. The next chapter, Getting Started, will present the prerequisites, tools and steps you will need to perform to get Spark and R working in your personal computer. You will learn how to install Spark, initialize Spark, get introduced to common operations and help you get your very first data processing and modeling task done. It is the goal of that chapter to help anyone grasp the concepts and tools required to start tackling large-scale data challenges which, until recently, were only accessible to just a few organizations. You will then move on into learning how to analyze large-scale data, followed by building models capable of predicting trends and discover information hidden in vasts amounts of information. At which point, you will have the tools required to perform data analysis and modeling at scale. Subsequent chapters will help you move away from your local computer into computing clusters required to solve many real world problems. The last chapters will present additional topics, like real time data processing and graph analysis, which you will need to truly master the art of analyzing data at any scale. The last chapter of this book will give you tools and inspiration to consider contributing back to this project and many others. We hope this is a journey you will enjoy, that will help you solve problems in your professional career to nudge the world into taking better decisions that can benefit us all. 1.1 Information As humans, we have been storing, retrieving, manipulating, and communicating information since the Sumerians in Mesopotamia developed writing in about 3000 BC. Based on the storage and processing technologies employed, it is possible to distinguish four distinct phases of development: pre-mechanical (3000 BC – 1450 AD), mechanical (1450–1840), electromechanical (1840–1940), and electronic (1940–present) (Laudon, Traver, and Laudon 1996). Mathematician George Stibitz used the word digital to describe fast electric pulses back in 1942 (Ceruzzi 2012) and, to this day, we describe information stored electronically as digital information. In contrast, analog information represents everything we have stored by any non-electronic means such as hand written notes, books, newspapers, and so on (Webster 2006). The world bank report on digital development provides an estimate of digital and analog information stored over the last decades (Group 2016). This report noted that digital information surpassed analog information around 2003. At that time, there were about 10 million terabytes of digital information, which is roughly about 10 million computer drives today. However, a more relevant finding from this report was that our footprint of digital information is growing at exponential rates. Figure 1.1 shows the findings of this report, notice that every other year, the worlds information has grown tenfold. FIGURE 1.1: World’s capacity to store information With the ambition to provide tools capable of searching all this new digital information, many companies attempted to provide such functionality with what we know today as search engines, used when searching the web. Given the vast amount of digital information, managing information at this scale was a challenging problem. Search engines were unable to store all the web page information required to support web searches in a single computer. This meant that they had to split information into several files and store them across many machines. This approach became known as the Google File System, a research paper published in 2003 by Google (Ghemawat, Gobioff, and Leung 2003). 1.2 Hadoop One year later, Google published a new paper describing how to perform operations across the Google File System, this approach came to be known as MapReduce (Dean and Ghemawat 2008). As you would expect, there are two operations in MapReduce: Map and Reduce. The map operation provides an arbitrary way to transform each file into a new file, while the reduce operation combines two files. Both operations require custom computer code, but the MapReduce framework takes care of automatically executing them across many computers at once. These two operations are sufficient to process all the data available in the web, while also providing enough flexibility to extract meaningful information from it. For example, as illustrated in Figure 1.2, we can use MapReduce to count words in two different text files stored in different machines. The mapping operation splits each word in the original file and outputs a new word-counting file with a mapping of words and counts. The reduce operation can be defined to take two word-counting files and combine them by aggregating the totals for each word, this last file will contain a list of word counts across all the original files. FIGURE 1.2: A simple MapReduce example Counting words is often the most basic MapReduce example, but it can be also used for much more sophisticated and interesting applications. For instance, MapReduce can be used to rank web pages in Google’s PageRank algorithm, which assigns ranks to web pages based on the count of hyperlinks linking to a web page and the rank of the page linking to it. After these papers were released by Google, a team in Yahoo worked on implementing the Google File System and MapReduce as a single open source project. This project was released in 2006 as Hadoop with the Google File System implemented as the Hadoop File System, or HDFS for short. The Hadoop project made distributed file-based computing accessible to a wider range of users and organizations which enabled them to make use of MapReduce beyond web data processing. While Hadoop provided support to perform MapReduce operations over a distributed file system, it still required MapReduce operations to be written with code every time a data analysis was run. To improve over this tedious process, the Hive project released in 2008 by Facebook, brought Structured Query Language (SQL) support to Hadoop. This meant that data analysis could now be performed at large-scale without the need to write code for each MapReduce operation; instead, one could write generic data analysis statements in SQL that are much easier to understand and write. 1.3 Spark In 2009, Apache Spark began as a research project at the UC Berkeley’s AMPLab to improve on MapReduce. Specifically, by providing a richer set of verbs beyond MapReduce that facilitate optimizing code running in multiple machines, and by loading data in-memory making operations much fasters than Hadoop’s on-disk storage. One of the earliest results showed that running logistic regression, a data modeling technique that will be introduced under the Modeling chapter, allowed Spark to run 10 times faster than Hadoop by making use of in-memory datasets (Zaharia et al. 2010), a chart similar to Figure 1.3 was presented in the original research publication. FIGURE 1.3: Logistic regression performance in Hadoop and Spark While Spark is well known for its in-memory performance, Spark was designed to be a general execution engine that works both in-memory and on-disk. For instance, Spark has set new records in large-scale sorting (“Sort Benchmark” 2019), where data was not loaded in-memory; but rather, Spark made use of improvements in network serialization, network shuffling and efficient use of the CPU’s cache to dramatically improve performance. For comparison, one can sort 100 terabytes of data in 72min and 2100 computers using Hadoop, but only 206 computers in 23 minutes using Spark (“Apache Spark Officially Sets a New Record in Large-Scale Sorting” 2014), it’s also the case that Spark holds the record in the cloud sorting benchmark (“Spark Wins Cloudsort Benchmark as the Most Efficient Engine” 2016), which makes Spark the most cost effective solution for large-scale sorting. Hadoop Record Spark Record Data Size 102.5 TB 100 TB Elapsed Time 72 mins 23 mins Nodes 2100 206 Cores 50400 6592 Disk 3150 GB/s 618 GB/s Network 10Gbps 10Gbps Sort rate 1.42 TB/min 4.27 TB/min Sort rate / node 0.67 GB/min 20.7 GB/min In 2010, Spark was released as an open source project and then donated to the Apache Software Foundation in 2013. Spark is licensed under the Apache 2.0, which allows you to freely use, modify, and distribute it. In 2015, Spark reaches more than 1000 contributors, making it one of the most active projects in the Apache Software Foundation. This gives an overview of how Spark came to be, which we can now use to formally introduce Apache Spark as follows: “Apache Spark is a fast and general engine for large-scale data processing.” — spark.apache.org To help us understand this definition of Apache Spark, we will break it down as follows: Data Processing Data processing is the collection and manipulation of items of data to produce meaningful information (French 1996). General Spark optimizes and executes parallel generic code, as in, there are no restrictions as to what type of code one can write in Spark. Large-Scale One can interpret this as cluster-scale, as in, a set of connected computers working together to accomplish specific goals. Fast Spark is much faster than its predecessor by making efficient use of memory, network and CPUs to speed data processing algorithms in computing clusters. Since Spark is general, you can use Spark to solve many problems, from calculating averages to approximating the value of Pi, predicting customer churn (“Churn Prediction with Apache Spark Machine Learning” 2017), aligning protein sequences (“Bioinformatics Applications on Apache Spark” 2018) or analyzing high energy physics at CERN (“Apache Spark and Cern Open Data Analysis, an Example” 2017). Describing Spark as large scale implies that a good use case for Spark is tackling problems that can be solved with multiple machines. For instance, when data does not fit in a single disk driver or does not fit into memory, Spark is a good candidate to consider. Since Spark is fast, it is worth considering for problems that may not be large-scale, but where using multiple processors could speed up computation. For instance, sorting large datasets or CPU intensive models could also bennefit from running in Spark. Therefore, Spark is good at tackling large-scale data processing problems, this usually known as big data (datasets that are more voluminous and complex that traditional ones (“Big Data Wikipedia” 2019)), but also is good at tackling large-scale computation problems, known as big compute ([tools and approaches using a large amount of CPU and memory resources in a coordinated way (“Big Compute Wikipedia” 2019)). Big data often requires big compute and big compute requires big data (“Big Compute Vs Big Data” 2013); but it’s not necessarily always the case. Big data and big compute problems are usually easy to spot – if the data does not fit into a single machine, you might have a big data problem; if the data fits into a single machine but a process over the data takes days, weeks or even months to compute, you might have a big compute problem. However, there is also a third problem space where neither data nor compute are necessarily large-scale and yet, there are significant benefits to using cluster computing frameworkds like Spark. For this third problem space, there are a few use cases this breaks to: Velocity Suppose you have a dataset of 10 gigabytes in size and a process that takes 30 minutes to run over this data – this is by no means big compute nor big data. However, if you happen to be researching ways to improve the accuracy of your models, reducing the runtime down to 3 minutes is a significant improvement, which can lead to significant advances and productivity gains by increasing the velocity at which you can analyze data. Alternatevely, you might need to process data faster, for stock trading for instance, while 3 minutes could seem fast enough; it can be way too slow for realtime data processing, where you might need to process data in a few seconds – or even down to a few milliseconds. Variety You could also have an efficient process to collect data from many sources into a single location, usually a database, this process could be already running efficiently and close to realtime. Such processes are known at ETL (Extract-Transform-Load); data is extracted from multiple sources, transformed to the required format and loaded in a single data store. While this has worked for years, the tradeoff from this approach is that adding a new data source is expensive. Since the system is centralized and tightly controlled, making changes could cause the entire process to halt; therefore, adding new data source usually takes too long to be implemented. Instead, one can store all data its natural format and process it as needed using cluster computing, this architecture is currently known as a data lake. In addition, storing data in its raw format allows you to process a variety of new file formats like images, audio and video; without having to figure out how to fit them into conventional structured storage systems. Veracity Asserts that data can vary greatly in quality which might require special analysis methods to improve its accuracy. For instance, suppose you have a table of cities with values like San Francisco, Seattle and Boston, what happens when data contains a misspelled entry like “Bston”? In a relational database, this invalid entry might get dropped; however, dropping values is not necessarily the best approach in all cases, you might want to correct this field by making use of geocodes, cross referencing data sources or attempting a best-effort match. Therefore, understanding the veracity of the original data source and what accuracy your particular analysis needs, can get yield a better outcome in many cases. If we include “Volume” as a synonym for big data, you get the mnemonics people refer as the four ’V’s of big data; others have gone as far as expanding this to five or even as the 10 Vs of Big Data. Mnemonics aside, cluster computing is being used today in more innovative ways and and is not uncommon to see organizations experimenting with new workflows and a variety of tasks that were traditionally uncommon for cluster computing. Much of the hype attributed to big data falls into this space where, strictly speaking, one is not handling big data but there are still benefits from using tools designed for big data and big compute. Our hope is that this book will help you understand the opportunities and limitations of cluster computing, and specifically, the opportunities and limitations from using Apache Spark with R. 1.4 R The R computing language has its origins in the S language, created at Bell Laboratories. R was not created at Bell Labs, but its predecesor, the S computing language was. Rick Becker explained in useR 2016 (“The History of R’s Predecessor, S, from Co-Creator Rick Becker” 2016) that at that time in Bell Labs, computing was done by calling subroutines written in the Fortran language which, apparently, were not pleasant to deal with. The S computing language was designed as an interface language to solve particular problems without having to worry about other languages, such as Fortran. The creator of S, John Chambers, describes in Figure 1.4 how S was designed to provide an interface that simplifies data processing, this was presented during useR! 2016 as the original diagram that inspired the creation of S. FIGURE 1.4: Interface language diagram by John Chambers - Rick Becker useR 2016 R is a modern and free implementation of S, specifically: R is a programming language and free software environment for statistical computing and graphics. — The R Project for Statistical Computing While working with data, I believe there are two strong arguments for using R: R Language was designed by statisticians for statisticians, meaning, this is one of the few successful languages designed for non-programmers; so learning R will probably feel more natural. Additionally, since the R language was designed to be an interface to other tools and languages, R allows you to focus more on modeling and less on peculiarities of computer science and engineering. R Community provides a rich package archive provided by CRAN (The Comprehensive R Archive Network) which allows you to install ready-to-use packages to perform many tasks; most notably, high-quality data manipulation, visualizations and statistic models, many of which are only available in R. In addition, the R community is a welcoming and active group of talented individuals motivated to help you succeed. Many packages provided by the R community make R, by far, the best option for statistical computing. Some of the most downloaded R packages include: dplyr to manipulate data, cluster to analyze clusters and ggplot2 to visualize data. Figure 1.5 quantifies the growth of the R community by plotting daily downloads of R packages in CRAN. FIGURE 1.5: Daily downloads of CRAN packages Aside from statistics, R is also used in many other fields. The following ones are particularily relevant to this book: Data Science Data science is based on knowledge and practices from statistics and computer science that turns raw data into understanding (Wickham and Grolemund 2016) by using data analysis and modeling techniques. Statistical methods provide a solid foundation to understand the world and perform predictions, while the automation provided by computing methods allows us to simplify statistical analysis and make it much more accessible. Some have advocated that statistics should be renamed data science (Wu 1997); however, data science goes beyond statistics by also incorporating advances in computing (Cleveland 2001). This book presents analysis and modeling techniques common in statistics, but applied to large datasets which requires incorporating advances in distributed computing. Machine Learning Machine learning uses practices from statistics and computer science; however, it is heavily focused on automation and prediction. For instance, the term “machine learning” was coined by Arthur Samuel while automating a computer program to play checkers (Samuel 1959). While we could perform data science on particular games, we rather need to automate the entire process. Therefore, this falls in the realm of machine learning, not data science. Machine learning makes it possible for many users to take advantage of statistical methods without being aware of the statistical methods that are being used. One of the first important applications of machine learning was to filter spam emails; in this case, it’s just not feasible to perform data analysis and modeling over each email account; therefore, machine learning automates the entire process of finding spam and filtering it out without having to involve users at all. This book will present the methods to transition data science workflows into fully-automated machine learning methods through, for instance, providing support to build and export Spark pipelines that can be easily reused in automated environments. Deep Learning Deep learning builds on knowledge of statistics, data science and machine learning to define models vaguely inspired on biological nervous systems. Deep learning models evolved from neural network models after the vanishing-gradient-problem was resolved by training one layer at a time (Hinton, Osindero, and Teh 2006) and have proven useful in image and speech recognition tasks. For instance, when using voice assistants like Siri, Alexa, Cortana or Google, the model performing the audio to text conversion is most likely to be based on deep learning models. While GPUs (Graphic Processing Units) have been successfully used to train deep learning models (Krizhevsky, Sutskever, and Hinton 2012); some datasets can not be processed in a single GPU. It is also the case that deep learning models require huge amounts of data, which needs to be preprocessed across many machines before they can be fed into a single GPU for training. This book won’t make any direct references to deep learning models; however, the methods presented in this book can be used to prepare data for deep learning and, in the years to come, using deep learning with large-scale computing will become a common practice. In fact, recent versions of Spark have already introduced execution models optimized for training deep learning in Spark. While working in any of the previous fields, you will be faced with increasingly large datasets or increasingly complex computations that are slow to execute or at times, even impossible to process in a single computer. However, it is important to understand that Spark does not need to be the answer to all our computations problems; instead, when faced with computing challenges in R, the following techniques can be as effective: Sampling A first approach to try is reduce the amount of data being handled, through sampling. However, data must be sampled properly by applying sound statistical principles. For instance, selecting the top results is not sufficient in sorted datasets; with simple random sampling, there might be underrepresented groups, which we could overcome with stratified sampling, which in turn adds complexity to properly select categories. It is out of the scope of this book to teach how to properly perform statistical sampling, but many online resources and literature is available on this subject. Profiling One can try to understand why a computation is slow and make the necessary improvements. A profiler, is a tool capable of inspecting code execution to help identify bottlenecks. In R, the R profiler, the profvis R package (“Profvis” 2018) and RStudio profiler feature (“RStudio Profiler” 2018), allow you to easily to retrieve and visualize a profile; however, it’s not always trivial to optimize. Scaling Up Speeding up computation is usually possible by buying faster or more capable hardware, say, increasing your machine memory, hard drive or procuring a machine with many more CPUs, this approach is known as “scaling up”. However, there are usually hard limits as to how much a single computer can scale up and even with significant CPUs, one needs to find frameworks that parallelize computation efficiently. Scaling Out Finally, we can consider spreading computation and storage across multiple machines; this approach provides the highest degree of scalability since one can potentially use an arbitrary number of machines to perform a computation, this approach is commonly known as “scaling out”. However, spreading computation effectively across many machines is a complex endeavour, specially without using specialized tools and frameworks like Apache Spark. This last point brings us closer to the purpose of this book, which is to bring the power of distributed computing systems provided by Apache Spark, to solve meaningful computation problems in Data Science and related fields, using R. 1.5 sparklyr When you think of the computation power that Spark provides and the ease of use of the R language, it is natural to want them to work together – seamlessly. This is also what the R community expected, an R package that would provide an interface to Spark that was, easy to use, compatible with other R packages and, available in CRAN; with this goal, we started developing sparklyr. The first version, sparklyr 0.4, was released during the useR! 2016 conference, this first version included support for dplyr, DBI, modeling with MLlib and an extensible API that enabled extensions like H2O’s rsparkling package. Since then, many new features and improvements have been made available through sparklyr 0.5, 0.6, 0.7, 0.8, 0.9 and 1.0. Officially, sparklyr is an R interface for Apache Spark. It’s available in CRAN and works like any other CRAN package, meaning that: it’s agnostic to Spark versions, it’s easy to install, it serves the R community, it embraces other packages and practices from the R community and so on. It’s hosted in GitHub under github.com/rstudio/sparklyr and licensed under Apache 2.0 which allows you to clone, modify and contribute back to this project. While thinking of who should use sparklyr, the following roles come to mind: New Users: For new users, it is our belief that sparklyr provides the easiest way to get started with Spark. Our hope is that the early chapters of this book will get you up running with ease and set you up for long term success. Data Scientists: For data scientists that already use and love R, sparklyr integrates with many other R practices and packages like dplyr, magrittr, broom, DBI, tibble, rlang and many others that will make you feel at home while working with Spark. For those new to R and Spark, the combination of high-level workflows available in sparklyr and low-level extensibility mechanisms make it a productive environment to match the needs and skills of every data scientist. Expert Users: For those users that are already immersed in Spark and can write code natively in Scala, consider making your Spark libraries available as an R package to the R community, a diverse and skilled community that can put your contributions to good use while moving open science forward. We wrote this book to describe and teach the exciting overlap between Apache Spark and R. sparklyr is the R package that materializes this overlap of communities, expectations, future directions, packages, and package extensions as well. We believe there is an opportunity to use this book to bridge the R and Spark communities, to present to the R community why Spark is exciting and to the Spark community what makes R great. Both communities are solving very similar problems with a set of different skills and backgrounds; therefore, it is our hope that sparklyr can be a fertile ground for innovation, a welcoming place to newcomers, a productive place for experienced data scientists and an open community where cluster computing and modeling can come together. 1.6 Recap This chapter presented Spark as a modern and powerful computing platform, R as an easy-to-use computing language with solid foundations in statistical methods and, sparklyr, as a project bridging both technologies and communities together. In a world where the total amount of information is growing exponentailly, learning how to analyze data at scale will help you tackle the problems and opportunities humanity is facing today. However, before we start analzing data, the Getting Started chapter will equip you with the tools you will need through the rest of this book. We recommend you follow each step carefully and take the time to install the recommended tools which, we hope will become familiar tools that you use and love. References "],
["starting.html", "Chapter 2 Getting Started 2.1 Prerequisites 2.2 Installing sparklyr 2.3 Installing Spark 2.4 Connecting to Spark 2.5 Using Spark 2.6 Disconnecting 2.7 Using RStudio 2.8 Resources 2.9 Recap", " Chapter 2 Getting Started From R, getting started with Spark using sparklyr and a local cluster is as easy as installing and loading the sparklyr package followed by running: spark_install() sc &lt;- spark_connect(master = &quot;local&quot;) To make sure we can all run the code above and understand it, this section will walk you through the prerequisites, installing sparklyr and Spark, connecting to a local Spark cluster and briefly explaining how to use Spark. However, if a Spark cluster and R environment have been made available to you, you do not need to install the prerequisites nor install Spark yourself. Instead, you should ask for the Spark master parameter and connect as follows; this parameter will be formally introduced under the Connections chapter. sc &lt;- spark_connect(master = &quot;&lt;cluster-master&gt;&quot;) 2.1 Prerequisites R can run in many platforms and environments; therfore, whether you use Windows, Mac or Linux, the first step is to install R from the r-project.org, detailed instructions are provided in the Installing R appendix. Most people use programming languages with tools to make them more productive; for R, RStudio would be such tool. Strictly speaking, RStudio is an Integrated Development Environment (or IDE), which also happens to support many platforms and environments. We strongly recommend you get RStudio installed if you haven’t done so already, see details under the Installing RStudio appendix. Additionally, since Spark is built in the Scala programming language which is run by the Java Virtual Machine, you also need to install Java 8 in your system. It is likely that your system already has Java installed, but you should still check the version and update or downgrade as described in the Installing Java appendix. 2.2 Installing sparklyr As many other R packages, sparkylr is available to be installed from CRAN and can be easily installed as follows: install.packages(&quot;sparklyr&quot;) The CRAN release of sparklyr contains the most stable version and it’s the recommended version to use; however, to try out features being developed in sparklyr, you can install directly from GitHub using the devtools package. First, install the devtools package and then install sparklyr as follows: install.packages(&quot;remotes&quot;) remotes::install_github(&quot;rstudio/sparklyr&quot;) The examples in this book assume you are using the latest version of sparklyr, you can verify your version is as new as the one we are using by running: packageVersion(&quot;sparklyr&quot;) [1] ‘1.0.0’ 2.3 Installing Spark Start by loading sparklyr, library(sparklyr) This will makes all sparklyr functions available in R, which is really helpful; otherwise, we would have to run each sparklyr command prefixed with sparklyr::. Spark can be easily installed by running spark_install(); this will install the latest version of Spark locally in your computer, go ahead and run spark_install(). Notice that this command requires internet connectivity to download Spark. spark_install() All the versions of Spark that are available for installation can be displayed by running: spark_available_versions() ## spark ## 1 1.6 ## 2 2.0 ## 3 2.1 ## 4 2.2 ## 5 2.3 ## 6 2.4 A specific version can be installed using the Spark version and, optionally, by also specifying the Hadoop version. For instance, to install Spark 1.6.3, we would run: spark_install(version = &quot;1.6&quot;) You can also check which versions are installed by running: spark_installed_versions() spark hadoop dir 7 2.3.1 2.7 /spark/spark-2.3.1-bin-hadoop2.7 The path where Spark is installed is referenced as Spark’s home, which is defined in R code and system configuration settings with the SPARK_HOME identifier. When using a local Spark cluster installed with sparklyr, this path is already known and no additional configuration needs to take place. Finally, in order to uninstall an specific version of Spark you can run spark_uninstall() by specifying the Spark and Hadoop versions, for instance: spark_uninstall(version = &quot;1.6.3&quot;, hadoop = &quot;2.6&quot;) Note: The default installation paths are ~/spark for OS X and Linux and, %LOCALAPPDATA%/spark for Windows. To customize the installation path you can run options(spark.install.dir = \"&lt;installation-path&gt;\") before spark_install() and spark_connect(). 2.4 Connecting to Spark It’s important to mention that, so far, we’ve only installed a local Spark cluster. A local cluster is really helpful to get started, test code and troubleshoot with ease. Further chapters will explain where to find, install and connect to real Spark clusters with many machines, but for the first few chapters, we will focus on using local clusters. To connect to this local cluster we simply run: sc &lt;- spark_connect(master = &quot;local&quot;) The master parameter identifies which is the “main” machine from the Spark cluster; this machine is often called the driver node. While working with real clusters using many machines, most machines will be worker machines and one will be the master. Since we only have a local cluster with only one machine, we will default to use \"local\" for now. If connection fails, the Connections chapter contains a troubleshooting section which can help you resolve your connection issue. 2.5 Using Spark Now that you are connected, we can run a few simple commands. For instance, let’s start by copying the mtcars dataset into Apache Spark using copy_to(). cars &lt;- copy_to(sc, mtcars) The data was copied into Spark but we can access it from R using the cars reference. To print it’s contents we can simply type cars. cars # Source: spark&lt;mtcars&gt; [?? x 11] mpg cyl disp hp drat wt qsec vs am gear carb &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 21 6 160 110 3.9 2.62 16.5 0 1 4 4 2 21 6 160 110 3.9 2.88 17.0 0 1 4 4 3 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 4 21.4 6 258 110 3.08 3.22 19.4 1 0 3 1 5 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 6 18.1 6 225 105 2.76 3.46 20.2 1 0 3 1 7 14.3 8 360 245 3.21 3.57 15.8 0 0 3 4 8 24.4 4 147. 62 3.69 3.19 20 1 0 4 2 9 22.8 4 141. 95 3.92 3.15 22.9 1 0 4 2 10 19.2 6 168. 123 3.92 3.44 18.3 1 0 4 4 # … with more rows Congrats! You have successfully connected and loaded your first dataset into Spark. Let’s explain what’s going on in copy_to(). The first parameter, sc, gives the function a reference to the active Spark Connection that was earlier created with spark_connect(). The second parameter specifies a dataset to load into Spark. Now, copy_to() returns a reference to the dataset in Spark which R automatically prints. Whenever a Spark dataset is printed, Spark will collect some of the records and display them for you. In this particular case, that dataset contains only a few rows describing automobile models and some of their specifications like Horse Power and expected Miles per Gallon. 2.5.1 Web Interface Most of the Spark commands are executed from the R console; however, monitoring and analyzing execution is done through Spark’s web interface, see Figure 2.1. This interface is a web application provided by Spark which can be accessed by running: spark_web(sc) FIGURE 2.1: Apache Spark Web Interface Printing the cars dataset collected a few records to be displayed in the R console. You can see in the Spark web interface that a job was started to collect this information back from Spark. You can also select the storage tab to see the “mtcars” dataset cached in-memory in Spark, Figure 2.2. FIGURE 2.2: Apache Spark Web Interface - Storage Tab Notice that this dataset is fully loaded into memory since the fraction cached is 100%, you can know exactly how much memory this dataset is using through the size in memory column. The executors tab, Figure 2.3, provides a view of your cluster resources. For local connections, you will find only one executor active with only 2GB of memory allocated to Spark and 384MB available for computation. The Tuning chapter you will learn how request more compute instances, resources and learn how memory is allocated. FIGURE 2.3: Apache Spark Web Interface - Executors Tab The last tab to explore is the environment tab, Figure 2.4, this tab lists all the settings for this Spark application which the tuning will also introduce them in detail. As you will learn, most settings don’t need to be configured explicitly, but in order to properly run at scale, you will have to become familiar with some of them, eventually. FIGURE 2.4: Apache Spark Web Interface - Environment Tab Next, you will make use of a small subset of the practices that the Analysis chapter will cover. 2.5.2 Analysis When using Spark from R to analyze data, you can use SQL (Structured Query Language) or dplyr (a grammar of data manipulation). SQL can be used through the DBI package; for instance, to count how many records are available in our cars dataset we can run: library(DBI) dbGetQuery(sc, &quot;SELECT count(*) FROM mtcars&quot;) count(1) 1 32 When using dplyr, you write less code and it’s often much easier to write than SQL; which is why we won’t make use SQL in this book; however, if you are profficient in SQL, this is a viable option to you. For instance, counting records in dplyr is more compact and easier to understand. library(dplyr) count(cars) # Source: spark&lt;?&gt; [?? x 1] n &lt;dbl&gt; 1 32 In general, we usually start by analysing data in Spark with dplyr, followed by sampling rows and selecting a subset of the available columns, the last step is to collect data from Spark to perform further data processing in R, like data visualization. Let’s perform a very simple data analysis example by selecting, sampling and plotting the cars dataset in Spark: select(cars, hp, mpg) %&gt;% sample_n(100) %&gt;% collect() %&gt;% plot() FIGURE 2.5: Horse Power vs Miles per Gallon The plot in Figure 2.5, shows that as we increase the horse power in a vehicle, their fuel efficiency measured in miles per gallon gets reduced. While this is insightful, it’s hard to predict numerically how increased horse power would affect fuel effiency, modeling can help us overcome this. 2.5.3 Modeling While data analysis can take you quite far when understanding data, building a mathematical model that describes and generalizes the dataset is quite powerful. In the Introduction chapter you learned that the fields of machine learning and data science make use of mathematical models to perform predictions and find additional insights. For instance, we can use a linear model to approximate the relationship between fuel efficiency and horse power: model &lt;- ml_linear_regression(cars, mpg ~ hp) This model can now be used to predict values that are not in the original datset. For instance, we can add entries for cars with horse power beyond 250 and also visualize the predicted values as shown in Figure 2.6. model %&gt;% ml_predict(copy_to(sc, data.frame(hp = 250 + 10 * 1:10))) %&gt;% transmute(hp = hp, mpg = prediction) %&gt;% full_join(select(cars, hp, mpg)) %&gt;% collect() %&gt;% plot() FIGURE 2.6: Horse power vs miles per gallon with predictions In addition, we can refine our insights using the broom package to retrive additional statistics from our model that can help us asses it’s quality. broom::glance(model) # A tibble: 1 x 5 explained.varia… mean.absolute.e… mean.squared.er… r.squared &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 21.2 2.91 14.0 0.602 # … with 1 more variable: root.mean.squared.error &lt;dbl&gt; While the previous example lacks many of the appropriate techniques you should use while modeling, it’s also a simple example to briefly introduce the modeling capabilities of Spark. All the Spark models, techniques and best practices will be properly introduced in the Modeling chapter. 2.5.4 Data For simplicity, we copied the mtcars dataset into Spark; however, data is usually not copied into Spark. Instead, data is read from existing data sources in a variety of formats like plain text, CSV, JSON, JDBC and many more which, the Data chapter will introduce in detail. For instance, we can export our cars dataset as a CSV file: spark_write_csv(cars, &quot;cars.csv&quot;) In practice, we would read an existing dataset from a distributed storage system like HDFS, but we can also read back from the local file system: cars &lt;- spark_read_csv(sc, &quot;cars.csv&quot;) 2.5.5 Extensions In the same way that R is known for it’s vibrant community of package authors, at a smaller scale, many extensions for Spark and R have been written and are available to you. The Extensions chapter will introduce many interesting ones to perform advanced modeling, graph analysis, preprocess datasets for deep learning, etc. For instance, the sparkly.nested extension is an R package that extends sparklyr to help you manage values that contain nested information. A common use case arises while dealing with JSON files which contain nested lists that require preprocessing before doing meaningful data analysis. To use this extension, we have to first install it as follows: install.packages(&quot;sparklyr.nested&quot;) Then we can use this extension to group all the horse power data points over the number of cylinders: sparklyr.nested::sdf_nest(cars, hp) %&gt;% group_by(cyl) %&gt;% summarize(data = collect_list(data)) # Source: spark&lt;?&gt; [?? x 2] cyl data &lt;int&gt; &lt;list&gt; 1 6 &lt;list [7]&gt; 2 4 &lt;list [11]&gt; 3 8 &lt;list [14]&gt; While nesting data makes it harder to read, it is a requirement while dealing with nested data formats like JSON using the spark_read_json() and spark_write_json() functions. 2.5.6 Distributed R For those few cases where a particular functionality is not available in Spark and no extension has been developed, you can consider distributing your own R code across the Spark cluster. This is a powerful tools but comes with additional complexity that you should only use as a last resort option. Suppose that we need to round all the values across all the columns in our dataset, one approach would be running custom R code making use of R’s round() function: cars %&gt;% spark_apply(~round(.x)) # Source: spark&lt;?&gt; [?? x 11] mpg cyl disp hp drat wt qsec vs am gear carb &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 21 6 160 110 4 3 16 0 1 4 4 2 21 6 160 110 4 3 17 0 1 4 4 3 23 4 108 93 4 2 19 1 1 4 1 4 21 6 258 110 3 3 19 1 0 3 1 5 19 8 360 175 3 3 17 0 0 3 2 6 18 6 225 105 3 3 20 1 0 3 1 7 14 8 360 245 3 4 16 0 0 3 4 8 24 4 147 62 4 3 20 1 0 4 2 9 23 4 141 95 4 3 23 1 0 4 2 10 19 6 168 123 4 3 18 1 0 4 4 # … with more rows If you are a profficient R user, it can be quite tempting to use spark_apply() for everything, but please, don’t! spark_apply() was designed for advanced use cases where Spark falls short; instead, you will learn how to do proper data analysis and modeling without having to distribute custom R code across your cluster. 2.5.7 Streaming While processing large static datasets is the most typical use case for Spark, processing dynamic datasets in realtime is also possible and for some applications, a requirement. You can think of a streaming dataset as a static data source with new data arriving continously, like stock market quotes. Streaming data is usually read from Kafka (an open-source stream-processing software platform) or from distributed storage that receives new data continuously. To try out streaming, lets first create an input/ folder with some data that we will use as the input for this stream: dir.create(&quot;input&quot;) write.csv(mtcars, &quot;input/cars_1.csv&quot;, row.names = F) Then we will define a stream that processes incoming data from the input/ folder, performs a custom transformation in R and, pushes the output into an output/ folder stream_read_csv(sc, &quot;input/&quot;) %&gt;% spark_apply(~sapply(.x, jitter)) %&gt;% stream_write_csv(&quot;output/&quot;) Stream: 720aac2a-d4aa-4e6c-828d-325d8b017fdb Status: Waiting for next trigger Active: TRUE As soon as the stream of realtime data starts, the input/ folder is processed and turned into a set of new files under the output/ folder containing the new transformed files. Since the input contained only one file, the output folder will also contain a single file resulting from applying the custom spark_apply() transformation. dir(&quot;output&quot;, pattern = &quot;.csv&quot;) [1] &quot;part-00000-eece04d8-7cfa-4231-b61e-f1aef8edeb97-c000.csv&quot; Up to this point, this resembles static data processing; however, we can keep adding files to the input/ location and Spark will parallelize and process data automatically. Let’s add one more file and validate that it’s automatically processed. # Write more data into the stream source write.csv(mtcars, &quot;input/cars_2.csv&quot;, row.names = F) # Wait for the input stream to be processed Sys.sleep(1) # Check the contents of the stream destination dir(&quot;output&quot;, pattern = &quot;.csv&quot;) [1] &quot;part-00000-2d8e5c07-a2eb-449d-a535-8a19c671477d-c000.csv&quot; [2] &quot;part-00000-eece04d8-7cfa-4231-b61e-f1aef8edeb97-c000.csv&quot; You can use dplyr, SQL, Spark models or distributed R to analyze streams in realtime, we will properly introduce you to all the interesting transformations you can perform to analyze realtime data during the Streaming chapter. 2.5.8 Logs Logging is definetely less interesting that realtime data processing; however, it’s a tool you should be familiar with. A log is just a text file where Spark will append information relevant to the execution of tasks in the cluster. For local clusters, we can retrieve all the recent log by running: spark_log(sc) 18/10/09 19:41:46 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5)... 18/10/09 19:41:46 INFO TaskSetManager: Finished task 0.0 in stage 5.0... 18/10/09 19:41:46 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose... 18/10/09 19:41:46 INFO DAGScheduler: ResultStage 5 (collect at utils... 18/10/09 19:41:46 INFO DAGScheduler: Job 3 finished: collect at utils... Or we can retrieve specific log entries containing, say sparklyr, by using the filter parameter as follows: spark_log(sc, filter = &quot;sparklyr&quot;) ## 18/10/09 18:53:23 INFO SparkContext: Submitted application: sparklyr ## 18/10/09 18:53:23 INFO SparkContext: Added JAR... ## 18/10/09 18:53:27 INFO Executor: Fetching spark://localhost:52930/... ## 18/10/09 18:53:27 INFO Utils: Fetching spark://localhost:52930/... ## 18/10/09 18:53:27 INFO Executor: Adding file:/private/var/folders/... Most of the time, you won’t need to worry about Spark logs, except in cases where you need to troubleshoot a failed computation; in those cases, logs are an invaluable resource to be aware of, now you know. 2.6 Disconnecting For local clusters (really, any cluster) once you are done processing data you should disconnect by running: spark_disconnect(sc) This will terminate the connection to the cluster as well as the cluster tasks . If multiple Spark connections are active, or if the conneciton instance sc is no longer available, you can also disconnect all your Spark connections by running: spark_disconnect_all() Notice that exiting R, RStudio or restarting your R session will also cause the Spark connection to terminate, which in turn terminates the Spark cluster and cached data that is not explicitly persisted. 2.7 Using RStudio Since it’s very common to use RStudio with R, sparklyr provides RStudio extensions to help simplify your workflows and increase your productivity while using Spark in RStudio. If you are not familiar with RStudio, take a quick look at the Using RStudio appendix section. Otherwise, there are a couple extensions worth highlighting. First, instead of starging a new connections using spark_connect() from RStudio’s R console, you can use the new connection action from the connections pane and then, select the Spark connection which will open the dialog shown in Figure 2.7. You can then customize the versions and connect to Spark which will simply generate the right spark_connect() command and execute this in the R console for you. FIGURE 2.7: RStudio New Spark Connection Second, once connected to Spark, either by using the R console or through RStudio’s connections pane, RStudio will display your datasets available in the connections pane, see Figure 2.8. This is a useful way to track your existing datasets and provides an easy way to explore each of them. FIGURE 2.8: RStudio Connections Pane Additionally, an active connection provides the following custom actions: Spark Opens the Spark web interface, a shortcut to spark_ui(sc). Log Opens the Spark web logs, a shortcut to spark_log(sc). SQL Opens a new SQL query, see DBI and SQL support in the data Analysis chapter. Help Opens the reference documentation in a new web browser window. Disconnect Disconnects from Spark, a shortcut to spark_disconnect(sc). The rest of this book will use plain R code, it is up to you to execute this code in the R console, RStudio, Jupyter Notebooks or any other tool that support executing R code since, the code provided in this book executes in any R environment. 2.8 Resources While we’ve put significant effort into simplifying the onboarding process, there are many additional resources that can help you troubleshoot particular issues while getting started and, in general, introduce you to the broader Spark and R communities to help you get specific answers, discuss topics and get connected with many users actevely using Spark with R. Documentation: This should be your first stop to learn more about Spark when using R. The documentation is kept up to date with examples, reference functions and many more relevant resources, spark.rstudio.com. Blog: To keep up to date with major sparklyr announcements, you can follow the RStudio blog, blog.rstudio.com/tags/sparklyr. Community: For general sparklyr questions, you can post then in the RStudio Community tagged as sparklyr, community.rstudio.com/tags/sparklyr. Stack Overflow: For general Spark questions, Stack Overflow is a great resource, stackoverflow.com/questions/tagged/apache-spark; there are also many topics specifically about sparklyr, stackoverflow.com/questions/tagged/sparklyr. Github: If you believe something needs to be fixed, open a GitHub issue or send us a pull request, github.com/rstudio/sparklyr. Gitter: For urgent issues, or to keep in touch, you can chat with us in Gitter, gitter.im/rstudio/sparklyr. 2.9 Recap In this chapter you learned about the prerequisites required to work with Spark, how to connect to Spark using spark_connect(), install a local cluster using spark_install(), load a simple dataset, launch the web interface and display logs using spark_web(sc) and spark_log(sc) respectively, disconnect from RStudio using spark_disconnect() and we closed this chapter presenting the RStudio extensions sparklyr provides. It is our hope that this chapter will help anyone interested in learning cluster computing using Spark and R getting started, ready to experiment on your own and ready to tackle actual data analysis and modeling problems which, the next two chapters will introduce you. The next chapter, Analysis, will present data analysis as the process to inspect, clean, and transform data with the goal of discovering useful information. Modeling can be considered part of data analysis; however, it deserves it’s own chapter to truly understand and take advantage of the modeling functionality available in Spark. "],
["analysis.html", "Chapter 3 Analysis 3.1 R as an Interface to Spark 3.2 Import / Access 3.3 Wrangle 3.4 Visualize 3.5 Model 3.6 Communicate 3.7 Recap", " Chapter 3 Analysis Previous chapters focused on introducing Spark and R. They also focused on helping you get started with the tools you need throughout this book. In this chapter, you will learn how to perform data analysis in Spark from R. In a data analysis project, the main goal is to understand what the data is trying to “tell us”. The hope is that the data provides an answer to a specific question. The question is typically poised by stakeholders of the analysis. Most data analysis projects follow a set of steps outlined in Figure 3.1. FIGURE 3.1: General steps of a data analysis As the diagram illustrates, the data is imported into our analysis stem, then wrangled by trying different data transformations, such as aggregations, and then visualized to help us perceive relationships and trends. In order to get deeper insight, one or multiple statistical models can be fitted against sample data. This will help in finding out if the patterns hold true when new data is applied to them. And lastly, the results are communicated to stakeholders. Commonly, in R all of the steps are performed in local memory. But that approach has to change when the analysis adds the use of Spark. The next section will introduce the main concept of how to best integrate R and Spark for data analysis. 3.1 R as an Interface to Spark For data analysis, the ideal approach is to let Spark do what its good at. Spark is a parallel computation engine that works at a large scale. Spark includes a SQL Engine and Machine Learning libraries. These can be used to perform most of the same operations R performs. Such operations include data selection, transformation, and modeling. Additionally, Spark include Graph analysis and Streaming libraries that extend the R user’s capabilities. Figure 3.2 lists the four main capabilities available to data analysts in Spark. FIGURE 3.2: Spark capabilities The data import, wrangling, and modeling can be performed inside Spark. Visualization can also partly be done by Spark, we will cover that later in this chapter. The idea is to use R to tell Spark what data operations to run, and then bring only the results into R. As illustrated in Figured 3.3, the ideal method pushes compute to the Spark cluster, and then collects results into R. FIGURE 3.3: Spark computes, R collects results The sparklyr package aids in using the “push compute, collect results” principle. Most of its functions are mainly wrappers on top of Spark API calls. This allows us to take advantage of Spark’s analysis components, instead of R’s. For example, an analyst needs to fit a Linear Regression model. The data is available in Spark. Instead of using the familiar lm() function, the analyst would use ml_linear_regression(). This R function will run the Scala code that uses the Spark’s API model. This specific example is illustrated in Figure 3.4. FIGURE 3.4: R functions call Scala functions For more common data manipulation tasks, sparklyr provides a back-end for dplyr. This means that already familiar dplyr verbs can be used in R, and then sparklyr and dplyr will translate those actions into Spark SQL statements. FIGURE 3.5: dplyr writes SQL An example of a practical implementation of this concept will be covered in the following sections. 3.1.1 Exercise In order to practice as you learn, the rest of this chapter’s code will use a single exercise that runs in the local Spark master. This way, the code can be replicated in your personal computer. Please, make sure to already have sparklyr and a local copy of Spark installed. The installation of Spark can be done by using the utility that comes with the package. For more information on how to do that please see the Local section in the Connections chapter. First, load the sparklyr and dplyr packages, and open a new local connection. library(sparklyr) sc &lt;- spark_connect(master = &quot;local&quot;) The environment is ready to be used. The next step is to add data that we can analyze. 3.2 Import / Access The step of importing data is to be approached differently when using Spark with R, as opposed to R alone. Usually, importing means that R will read files and save the information into memory. But when used with Spark, it is important to only focus in importing results into R. The data is either imported, or accessed by Spark. Notice how in Figure 3.6 the Data Source is connected to Spark, and not R. FIGURE 3.6: Import Data to Spark not R Most organization will use Spark inside a YARN managed cluster. For those cases the vast majority of the necessary data is already available in the cluster’s Hadoop File System (HDFS). It is made available to users via Hive tables, or by accessing the file system directly. Because the Spark session is initiated inside the same server hosts as the rest of the cluster, it is much easier to transfer large amounts of data between the HDFS and Spark memory. The decision of having Spark either access the data source, or to import data into memory is mostly a decisions based on speed and performance. Importing all of the data into the Spark session will incur an up-front cost of time. That is because we would have to wait for the data to be loaded before analyzing. Accessing the data incurs cost with every query sent to the cluster via Spark. That is because we depend on the cluster’s in-disk data, which has to be retrieved before it is analyzed. More will be covered in the Tuning chapter. The exercise’s Spark session does not have any data. So the next step is to prime the session with data, in this case mtcars. The copy_to() command from dplyr can be used for that. The mechanics of this operation is explained in the Getting Started chapter. library(dplyr) cars &lt;- copy_to(sc, mtcars, &quot;mtcars_remote&quot;) Note: In an enterprise setting, copy_to() should only be used to transfer small tables from R, such as a look up value table. Large data transfers should be performed by more formal data transfer or ETL tools. The data is now accessible to Spark and R. Transformations can now be applied to the data. The next section will cover how to wrangle data by running transformations inside Spark. 3.3 Wrangle Data wrangling uses transformations to understand the data. A data transformation can be interpreted as a change performed to the data. Malformed or missing values and columns with multiple attributes are common data issues. These issues prevent us from understanding the activity reflected in the data. For example, a “name” field contains the last and first name of a customer. There are two attributes (first &amp; last name) in a single column. In order to be usable, we need to transform the “name” field, by changing it into a “first_name” and “last_name” fields. After the data is cleaned, there is still the task to try and understand the basics about its content. Other transformations, such as aggregations, can help with this task. For example, the result of requesting the average balance of all customers will return a single row and column. The value will be the average of all customers. That information will give us context when we see individual, or grouped, customer balances. The main goal is to write the data transformations using R syntax as much as possible. This saves us from the cognitive cost of having to switch between multiple computer languages to accomplish a single task. In this case, it is better to take advantage of sparklyr’s dplyr back-end interface, instead of writing Spark SQL statements for data exploration, In the R environment, cars can be treated as if it is a local data frame, so dplyr verbs can be used, and in a piped fashion. cars %&gt;% group_by(am) %&gt;% summarise(mpg_mean = mean(mpg, na.rm = TRUE)) ## # Source: spark&lt;?&gt; [?? x 2] ## am mpg_mean ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 17.1 ## 2 1 24.4 Instead of importing the mtcars_remote data set from Spark, and then performing the aggregation, dplyr converts the verbs into SQL statements that are then sent to Spark. The show_query() command makes it possible to peer into the SQL statement that sparklyr and dplyr created and sent to Spark. cars %&gt;% group_by(am) %&gt;% summarise(mpg_mean = mean(mpg, na.rm = TRUE)) %&gt;% show_query() ## &lt;SQL&gt; ## SELECT `am`, AVG(`mpg`) AS `mpg_mean` ## FROM `mtcars_remote` ## GROUP BY `am` As it is evident, it will not be necessary to have to see the resulting query every time dplyr verbs are being used. The focus can remain on obtaining insights from the data, as opposed to figuring out how to express a given set of transformation in SQL. cars %&gt;% group_by(am) %&gt;% summarise( wt_mean = mean(wt, na.rm = TRUE), mpg_mean = mean(mpg, na.rm = TRUE) ) ## # Source: spark&lt;?&gt; [?? x 3] ## am wt_mean mpg_mean ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 3.77 17.1 ## 2 1 2.41 24.4 Most of the data transformation made available by dplyr to work with local data frames are also available to use with a Spark connection. This means that a general approach to learning dplyr can be taken in order to gain more proficiency with data exploration and preparation with Spark. The chapter on Data Transformation in the R for Data Science (Wickham and Grolemund 2016) book should be a great help with this. If proficiency with dplyr is not an issue for you, then please take some time to experiment with different dplyr functions against the cars table. 3.3.1 Correlations A very common exploration technique is to calculate and visualize correlations. We often calculate correlations to find out what kind of statistical relationship are between paired sets of variables. The Spark API provides an internal function that calculates correlations across the entire data set. The results are then returned to R as a data.frame object. ml_corr(cars) %&gt;% as_tibble() ## # A tibble: 11 x 11 ## mpg cyl disp hp drat wt qsec ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 1 -0.852 -0.848 -0.776 0.681 -0.868 0.419 ## 2 -0.852 1 0.902 0.832 -0.700 0.782 -0.591 ## 3 -0.848 0.902 1 0.791 -0.710 0.888 -0.434 ## 4 -0.776 0.832 0.791 1 -0.449 0.659 -0.708 ## 5 0.681 -0.700 -0.710 -0.449 1 -0.712 0.0912 ## 6 -0.868 0.782 0.888 0.659 -0.712 1 -0.175 ## 7 0.419 -0.591 -0.434 -0.708 0.0912 -0.175 1 ## 8 0.664 -0.811 -0.710 -0.723 0.440 -0.555 0.745 ## 9 0.600 -0.523 -0.591 -0.243 0.713 -0.692 -0.230 ## 10 0.480 -0.493 -0.556 -0.126 0.700 -0.583 -0.213 ## 11 -0.551 0.527 0.395 0.750 -0.0908 0.428 -0.656 ## # ... with 4 more variables: vs &lt;dbl&gt;, am &lt;dbl&gt;, ## # gear &lt;dbl&gt;, carb &lt;dbl&gt; The corrr R package specializes in correlations. It contains friendly functions to prepare and visualize the results. Included inside the package is a back-end for sparklyr table objects, so it will not return an error if a non local table is passed to it. In the background, the correlate() function runs ml_corr(), so there is no need to collect any data into R prior running the command. library(corrr) cars %&gt;% correlate(use = &quot;pairwise.complete.obs&quot;, method = &quot;pearson&quot;) ## # A tibble: 11 x 12 ## rowname mpg cyl disp hp drat wt ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 mpg NA -0.852 -0.848 -0.776 0.681 -0.868 ## 2 cyl -0.852 NA 0.902 0.832 -0.700 0.782 ## 3 disp -0.848 0.902 NA 0.791 -0.710 0.888 ## 4 hp -0.776 0.832 0.791 NA -0.449 0.659 ## 5 drat 0.681 -0.700 -0.710 -0.449 NA -0.712 ## 6 wt -0.868 0.782 0.888 0.659 -0.712 NA ## 7 qsec 0.419 -0.591 -0.434 -0.708 0.0912 -0.175 ## 8 vs 0.664 -0.811 -0.710 -0.723 0.440 -0.555 ## 9 am 0.600 -0.523 -0.591 -0.243 0.713 -0.692 ## 10 gear 0.480 -0.493 -0.556 -0.126 0.700 -0.583 ## 11 carb -0.551 0.527 0.395 0.750 -0.0908 0.428 ## # ... with 5 more variables: qsec &lt;dbl&gt;, vs &lt;dbl&gt;, ## # am &lt;dbl&gt;, gear &lt;dbl&gt;, carb &lt;dbl&gt; The correlate() function returns a local R object that corrr recognizes. This way, it is easy to perform more functions on top of the results. In this case, the shave() command turns all of the duplicated results into NA’s cars %&gt;% correlate(use = &quot;pairwise.complete.obs&quot;, method = &quot;pearson&quot;) %&gt;% shave() ## # A tibble: 11 x 12 ## rowname mpg cyl disp hp drat wt ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 mpg NA NA NA NA NA NA ## 2 cyl -0.852 NA NA NA NA NA ## 3 disp -0.848 0.902 NA NA NA NA ## 4 hp -0.776 0.832 0.791 NA NA NA ## 5 drat 0.681 -0.700 -0.710 -0.449 NA NA ## 6 wt -0.868 0.782 0.888 0.659 -0.712 NA ## 7 qsec 0.419 -0.591 -0.434 -0.708 0.0912 -0.175 ## 8 vs 0.664 -0.811 -0.710 -0.723 0.440 -0.555 ## 9 am 0.600 -0.523 -0.591 -0.243 0.713 -0.692 ## 10 gear 0.480 -0.493 -0.556 -0.126 0.700 -0.583 ## 11 carb -0.551 0.527 0.395 0.750 -0.0908 0.428 ## # ... with 5 more variables: qsec &lt;dbl&gt;, vs &lt;dbl&gt;, ## # am &lt;dbl&gt;, gear &lt;dbl&gt;, carb &lt;dbl&gt; Finally, the results can be easily visualized using rplot(). This function returns a ggplot object. Figure 3.7 is an example of what rplot() returns. cars %&gt;% correlate(use = &quot;pairwise.complete.obs&quot;, method = &quot;pearson&quot;) %&gt;% shave() %&gt;% rplot() FIGURE 3.7: Using rplot() to visualize correlations It is much easier to see which relationships are positive or negative. Positive relationships are in blue, and negative relationships are in red. The size of the circle indicates how significant their relationship is. The power of a visualizing data is in how much easier it makes it for us to understand results. The next section will expand on this step of the process. 3.4 Visualize Visualizations are a vital tool to help us find patterns in the data. It is easier for us to identify outliers in a data set of 1,000 observations when plotted in a graph, as opposed to reading them from a list. R is great at data visualizations. Its capabilities for creating plots is extended by the many R packages that focus on this analysis step. Unfortunately, the vast majority of R functions that create plots depend on the data already being in local memory within R, so they fail when using a remote table inside Spark. It is possible to create visualizations in R from data source from Spark. To understand how to do this, let’s first break down how computer programs build plots: It takes the raw data and performs some sort of transformation. The transformed data is then mapped to a set of coordinates. Finally, the mapped values are drawn in a plot. Figure 3.8 summarizes each of the steps. FIGURE 3.8: Stages of a plot For example, to create a bar plot in R, we simply call a function: library(ggplot2) ggplot(aes(as.factor(cyl), mpg), data = mtcars) + geom_col() FIGURE 3.9: Plotting inside R In this case, the mtcars raw data was automatically transformed into three discrete aggregated numbers, then each result was mapped into an x and y plane, and then the plot was drawn, see figure 3.10. As R users, all of the stages of building the plot are conveniently abstracted for us. FIGURE 3.10: R plotting function 3.4.1 Recommended approach In essence, the approach for visualizing is the same as in wrangling, push the computation to Spark, and then collect the results in R for plotting. As illustrated in figure 3.11, the heavy lifting of preparing the data, such as in aggregating the data by groups or bins, can be done inside Spark, and then collect the much smaller data set into R. Inside R, the plot becomes a more basic operation. For example, to plot a histogram, the bins are calculated in Spark, and then in R, use a simple column plot, as opposed to a histogram plot, because there is no need for R to re-calculate the bins. FIGURE 3.11: Plotting with Spark and R The next section will cover how to use plot simple, but useful, plots. 3.4.2 Simple Plots There are a couple of key steps when codifying the “push compute, collect results” approach. First, ensure that the transformation operations happen inside Spark. In the example below, group_by() and summarise() will run as SQL inside the Spark session. The second is to bring the results back into R after the data has been transformed. Make sure to transform and then collect, in that order, because if collect() is run first, then all R will try to ingest the entire data set from Spark. Depending on the size of the data, collecting all of the data will slow down or may even bring down your system. car_group &lt;- cars %&gt;% group_by(cyl) %&gt;% summarise(mpg = sum(mpg, na.rm = TRUE)) %&gt;% collect() car_group ## # A tibble: 3 x 2 ## cyl mpg ## &lt;dbl&gt; &lt;dbl&gt; ## 1 6 138. ## 2 4 293. ## 3 8 211. In this example, now that the data has been pre-aggregated and collected into R, only three records are passed to the plotting function. Figure 3.12 shows the resulting plot. ggplot(aes(as.factor(cyl), mpg), data = car_group) + geom_col() FIGURE 3.12: Plot from Spark Thanks to the consistency among the tidyverse packages, the entire operation can be written in a single piped code segment: cars %&gt;% group_by(cyl) %&gt;% summarise(mpg = sum(mpg, na.rm = TRUE)) %&gt;% collect() %&gt;% ggplot(aes(as.factor(cyl), mpg)) + geom_col() The resulting plot is in figure 3.13. FIGURE 3.13: Plot from Spark Using this approach, most visualizations can be easily produced. More complex plots require a bit more work, the next section will breakdown how to create a Histogram. 3.4.3 Histograms There are plots that are both useful, and commonly used. They provide a clear idea of the distribution of values in a single variable. Their calculations are not easily reproducible. For example, producing a histogram that runs over the entire large data set has not been typically feasible, because the data needs to be imported into R first. The following code breaks down the creation of bins 3 mpg wide into a combination of the most basic aggregation functions. It can run easily inside the mutate() command. It creates as many discrete bins size 3 mpg, and passes the minimum value as the “label” of that bin. mtcars %&gt;% mutate( fx = floor((mpg - min(mpg, na.rm = TRUE))/3), min_x = min(mpg, na.rm = TRUE), max_x = max(mpg, na.rm = TRUE), bins = (3 * ifelse(fx == (max_x - min_x)/3, fx - 1, fx)) + min_x ) %&gt;% select(mpg, bins) %&gt;% head() ## mpg bins ## 1 21.0 19.4 ## 2 21.0 19.4 ## 3 22.8 22.4 ## 4 21.4 19.4 ## 5 18.7 16.4 ## 6 18.1 16.4 The same R code can run inside Spark, dplyr will translate the R code into a SQL statement. The results can then be collected into R for visualizing. bins &lt;- cars %&gt;% mutate( fx = floor((mpg - min(mpg, na.rm = TRUE))/3), min_x = min(mpg, na.rm = TRUE), max_x = max(mpg, na.rm = TRUE), bins = (3 * ifelse(fx == (max_x - min_x)/3, fx - 1, fx)) + min_x ) %&gt;% count(bins)%&gt;% collect() bins ## # A tibble: 7 x 2 ## bins n ## &lt;dbl&gt; &lt;dbl&gt; ## 1 19.4 6 ## 2 22.4 3 ## 3 13.4 8 ## 4 10.4 3 ## 5 16.4 6 ## 6 28.4 4 ## 7 25.4 2 The bins and counts have been pre-calculated, so a simple column plot is used. The resulting plot is in figure 3.14. bins %&gt;% ggplot() + geom_col(aes(bins, n)) FIGURE 3.14: Plot from Spark There is an R package that can make plotting histograms and other plots easier. The next section will cover that. 3.4.3.1 Using dbplot The dbplot package provides helper function for plotting with remote data. The R code dbplot uses to transform the data is written so that it can be translated by sparklyr into SQL. It then uses the results to generate a ggplot2 plot. The dbplot_histogram() function will have Spark calculate the bins and the count per bin, and then outputs a ggplot object. It accepts a binwidth argument. The resulting plot is in figure 3.15. library(dbplot) cars %&gt;% dbplot_histogram(mpg, binwidth = 3) FIGURE 3.15: Histogram created by dbplot Because it is a ggplot object, it can be further refined by adding more steps to the plot object. In the example on figure 3.16, we added a title to the plot, after it was created by dbplot. cars %&gt;% dbplot_histogram(mpg, binwidth = 3) + labs(title = &quot;Histogram of Miles Per Galon&quot;) FIGURE 3.16: Refined Histogram The package also provides a way to obtain the raw results via the db_compute_bins() package. cars %&gt;% db_compute_bins(mpg, binwidth = 3) ## # A tibble: 7 x 2 ## mpg count ## &lt;dbl&gt; &lt;dbl&gt; ## 1 19.4 6 ## 2 22.4 3 ## 3 13.4 8 ## 4 10.4 3 ## 5 16.4 6 ## 6 28.4 4 ## 7 25.4 2 Histograms provide a great way to analyze a single variable. To analyze two variables, a scatter or raster plot is commonly used. The next section will discuss how to do that with dbplot. 3.4.4 Scatter vs Raster Plots Scatter plots are used to compare the relationship between two continuous variables. For example, a scatter plot will display the relationship between the weight of a car and its gas consumption. The plot will show that the higher the weight, the higher the gas consumption because the dots clump together into almost a line that goes from the top left towards the bottom right. See figure 3.17 for an example of the plot. ggplot(aes(mpg, wt), data = mtcars) + geom_point() FIGURE 3.17: Scatter plot example The problems that arise when trying to use this visualization with a large amount of data are: Performance problems Too many single dots have to be calculated and drawn. Perception problems It becomes hard to see the true amount of dots in a single area. No amount of “pushing the computation” to Spark will help with this problem because the data has to be plotted in individual dots. The best alternative is to find a plot type that represents the x/y relationship and concentration in a way that it is easy to perceive and to “physically” plot. The raster plot may be the best answer. It returns a grid of x/y positions and the results of a given aggregation usually represented by the color of the square. The dbplot package provides functionality that helps with this kind of plotting. The first one is dbplot_raster(). It is the way to quickly run a visualization of this kind. cars %&gt;% dbplot_raster(mpg, wt, resolution = 5) FIGURE 3.18: Using dbplot_raster() As shown in figure 3.18, the plot returns a grid no bigger than 5x5. This limits the number of records that need to be collected into R to 25. If the user prefers to either visualize, or simply obtain the result data, the db_compute_raster() and db_compute_raster2() functions return the data. The db_compute_raster2() includes the upper and lower bounds of each square. cars %&gt;% db_compute_raster2(mpg, wt, resolution = 5) ## # A tibble: 9 x 5 ## mpg wt `n()` mpg_2 wt_2 ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 19.8 2.30 5 24.5 3.08 ## 2 19.8 3.08 3 24.5 3.86 ## 3 15.1 3.08 10 19.8 3.86 ## 4 10.4 3.08 3 15.1 3.86 ## 5 15.1 3.86 1 19.8 4.64 ## 6 10.4 4.64 3 15.1 5.42 ## 7 29.2 1.51 4 33.9 2.30 ## 8 24.5 1.51 2 29.2 2.30 ## 9 15.1 2.30 1 19.8 3.08 3.5 Model An analysis project focuses on going through as many transformations and models to find the answer. The ideal data analysis framework enables the user to quickly complete each iteration. The transition from an analysis to a deployment project occurs after the model is selected, and findings are presented to the stakeholders. The Modeling chapter will dive deeper into how to prepare and run models. The focus of this section will be how to properly, and easily, transition from wrangling to modeling. There are some steps needed for Spark to run even the most simple models. The modeling functions in sparklyr already cover those steps in order to make them easier to use. Transitioning from data wrangling to model prototyping is as easy as piping the transformed code right into the a sparklyr modeling function. Consider the following example: cars %&gt;% mutate(cyl = paste0(&quot;cyl_&quot;, cyl)) %&gt;% ml_linear_regression(wt ~ .) %&gt;% summary() ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.36805 -0.18275 -0.01221 0.11616 0.56768 ## ## Coefficients: ## (Intercept) mpg cyl_cyl_8.0 cyl_cyl_4.0 disp ## -0.241536145 -0.046030695 0.085662905 0.193862495 0.006621045 ## hp drat qsec vs am ## -0.004654348 -0.126435269 0.189135280 0.017628206 0.037101380 ## gear carb ## -0.081473252 0.281287273 The cyl field is converted into a character variable, and then the ml_linear_regression() function is applied to the resulting data set. This is a very straight forward thing to do inside R, but it is not so in Spark. Firstly, Spark does not run models without a Spark PipelineModel. Secondly, it does not create Dummy Variables by default. The ml_linear_regression() already encases all of the needed Scala code to complete those steps and run the Pipeline model, which is then returned looking as a standard fitted model in R. At this point it is very easy to experiment with different formulas, as shows in the code below: cars %&gt;% ml_linear_regression(wt ~ mpg) %&gt;% summary() ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.6516 -0.3490 -0.1381 0.3190 1.3684 ## ## Coefficients: ## (Intercept) mpg ## 6.047255 -0.140862 ## ## R-Squared: 0.7528 ## Root Mean Squared Error: 0.4788 Additionally, it is also very easy to try out other kinds of models. The following code shows a Generalized Linear model using the ml_generalized_linear_regression() function: cars %&gt;% ml_generalized_linear_regression(am ~ .) %&gt;% summary() ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -0.46909 -0.16762 -0.00578 0.18601 0.35635 ## ## Coefficients: ## (Intercept) mpg cyl disp hp drat ## 1.6345144901 0.0264792061 -0.1207285158 -0.0005715857 0.0010378347 0.0952244996 ## wt qsec vs gear carb ## 0.0172804532 -0.1125888981 -0.2087047973 0.1952457664 -0.0180502642 ## ## (Dispersion paramter for gaussian family taken to be 0.0737941) ## ## Null deviance: 7.71875 on 31 degress of freedom ## Residual deviance: 1.54968 on 21 degrees of freedom ## AIC: 17.926 It is recommended that data used to fit a model be inside Spark memory. The reasons and explanation will be covered in the next section. 3.5.1 Cache model data The examples in this chapter are built using a very small data set. In real-life scenarios, large amounts of data are used for models. If the data needs to be transformed first, the volume of the data could exact a heavy toll on the Spark session. Before fitting the models, it is a good idea to save the results of all the transformations in a new table inside Spark memory. The compute() command can take the end of a dplyr piped command set and save the results to Spark memory. cached_cars &lt;- cars %&gt;% mutate(cyl = paste0(&quot;cyl_&quot;, cyl)) %&gt;% compute(&quot;cached_cars&quot;) cached_cars %&gt;% ml_linear_regression(mpg ~ .) %&gt;% summary() ## Deviance Residuals: ## Min 1Q Median 3Q Max ## -3.47339 -1.37936 -0.06554 1.05105 4.39057 ## ## Coefficients: ## (Intercept) cyl_cyl_8.0 cyl_cyl_4.0 disp hp drat ## 16.15953652 3.29774653 1.66030673 0.01391241 -0.04612835 0.02635025 ## wt qsec vs am gear carb ## -3.80624757 0.64695710 1.74738689 2.61726546 0.76402917 0.50935118 ## ## R-Squared: 0.8816 ## Root Mean Squared Error: 2.041 As more insights are gained from the data, more questions may be raised. That is why we expect to iterate through data wrangle, visualize, and model multiple times. Each iteration should provide incremental insights of what the data is “telling us”. There will be a point where we reach a satisfactory level of understanding. It is at this point that we will be ready to share the results with the stakeholders of the analysis. How to do this is the topic of the next section. 3.6 Communicate It is important to clearly communicate the analysis results to the stakeholders. It is as important as the analysis work itself. The stakeholders need to understand what you found out, and how you found it out. To communicate effectively we need to use artifacts, such as reports and presentation decks. These are common output formats that we can create in R, using R Markdown. R Markdown documents allow weave narrative text and code together. The amount of output formats provides a very compelling reason to learn and use R Markdown. The available output formats include: Hypertext Markup Language (HTML) Portable Document Format (PDF) Microsoft PowerPoint Microsoft Word Presentation slides Websites Books Most of these outputs are available in the core R packages of R Markdown: knitr and rmarkdown. R Markdown can be extended by other R packages. For example, this book was written using R Markdown thanks to an extension provided by the bookdown package. The best resource to delve deeper into R Markdown is the official book (Xie 2018). In R Markdown, one type of artifact could potentially be rendered in different formats. For example, the same report could be rendered in HTML, or as a PDF file by changing a setting within the report itself. Conversely, multiple types of artifacts could be rendered as the same output. For example, a presentation deck and a report could be rendered in HTML. 3.6.1 Reports Creating a new R Markdown report is easy. At the top, R Markdown expect a YAML header. The first and last line are three consecutive dashes (---). The content in between the dashes vary depending on the type of document. The only required one is the output value. R Markdown needs to know what kind of output it needs to render your report into. This YAML header is called Front Matter. Following the Front Matter are sections of code, called code chunks. These code chunks can be interlaced with the narratives. The following example shows how easy it is to create a fully reproducible report. The narrative, code and, most importantly, the output of the code is recorded inside the resulting HTML file. You can copy and paste this following code in a file. Save the file with a .Rmd extension, and choose whatever name you would like. --- title: &quot;mtcars analysis&quot; output: html_document --- ```{r, setup, include = FALSE} library(sparklyr) conf &lt;- spark_config() conf$`sparklyr.cores.local` &lt;- 4 conf$`sparklyr.shell.driver-memory` &lt;- &quot;1G&quot; conf$spark.memory.fraction &lt;- 0.9 ``` ## Setup Spark environment A local Spark environment is setup and the `mtcars` data set preloaded ```{r, eval = FALSE} sc &lt;- spark_connect(master = &quot;local&quot;, config = conf) cars &lt;- copy_to(sc, mtcars, &quot;mtcars_remote&quot;) cars ``` ## Visualize A histogram was run over the `mpg` data ```{r, eval = FALSE} library(dbplot) cars %&gt;% dbplot_histogram(mpg, binwidth = 3) ``` ## Model The selected model was a simple linear regression that uses the weight as the predictor of MPG ```{r, eval = FALSE} cars %&gt;% ml_linear_regression(wt ~ mpg) %&gt;% summary() ``` ```{r, include = FALSE} spark_disconnect(sc) ``` If using the RStudio IDE, click on the Knit button at the top of the document to run it. The HTML output should look like this: FIGURE 3.19: R Markdown HTML output This report can now be sent to stakeholders. The stakeholders will not need Spark or even R to be able to read the report. All of the needed output from your Spark session was captured in the HTML document. 3.6.2 Presentation decks It is common to distill insights of a long report into a presentation deck. This artifact is sometimes easier to digest than a long form because it segments the information into discrete slides. In R Markdown, switching from an HTML output, to a PowerPoint output is as easy as changing a single option. In the top Front Matter, change the output option to powerpoint_presentation. --- title: &quot;mtcars analysis&quot; output: powerpoint_presentation --- The result will be a PowerPoint presentation with all of the same information that was displayed in the HTML report. There will be a need to edit the PowerPoint template or the output of the code chunks. This minimal example show how easy it is to go from one format to another. Of course, it will take some more editing on the R user side to make sure the slides contain only the pertinent information. The main point is to highlight that it does not require to learn a different markup, or code conventions, to switch from one artifact to another. The xaringan package enhances R Markdown presentaions. It provides a self contained HTML5 presentation deck. This may be a better option when not everyone in the organization has PowerPoint. Converting the original R Markdown report to a xaringan deck is easy. Only make sure to include a tripple dash needs to indicate where a new slide begins, and update the output value of in the Front Matter to xaringan::moon_reader. --- title: &quot;mtcars analysis&quot; output: xaringan::moon_reader --- ```{r, setup, include = FALSE} library(sparklyr) conf &lt;- spark_config() conf$`sparklyr.cores.local` &lt;- 4 conf$`sparklyr.shell.driver-memory` &lt;- &quot;1G&quot; conf$spark.memory.fraction &lt;- 0.9 ``` ## Setup Spark environment A local Spark environment is setup and the `mtcars` dataset preloaded ```{r, eval = FALSE} sc &lt;- spark_connect(master = &quot;local&quot;, config = conf) cars &lt;- copy_to(sc, mtcars, &quot;mtcars_remote&quot;) cars ``` --- ## Visualize A histogram was run over the `mpg` data ```{r, fig.height = 6} library(dbplot) cars %&gt;% dbplot_histogram(mpg, binwidth = 3) ``` ```{r, include = FALSE} spark_disconnect(sc) ``` Here is what the first full slide should look like: FIGURE 3.20: R Markdown HTML output 3.7 Recap R and Spark are a very powerful combination. Being able to use a powerful computing platform, along with an incredibly robust ecosystem of packages makes up for an ideal analysis platform. Remember to push computation to Spark, and focus on collecting results in R. The result can then be used for further data manipulation or for plotting. The results can then be shared with stakeholders in a variety of outputs. A a learner of R, hopefully this chapter also encouraged you to learn more about the tidyverse as well as R Markdown. The next chapter will dive deeper into how to expand the models created during analysis into Machine Learning workflows that can be used in Production. References "],
["modeling.html", "Chapter 4 Modeling 4.1 Overview 4.2 The Data 4.3 Exploratory Data Analysis 4.4 Feature Engineering 4.5 Model Building 4.6 Working with Textual Data 4.7 Conclusion", " Chapter 4 Modeling In this chapter, we discover how Spark can be used to scale up machine learning workflows to big data. We build off of the ideas presented in the Analysis chapter and introduce the machine learning (ML) aspects. Spark MLlib is the component of Spark that allows one to write high level code to perform machine learning tasks on distributed data. Sparklyr provides an interface to the ML algorithms that should be familiar to R users. For example, you can run a logistic regression as follows: ml_logistic_regression(mtcars, am ~ .) ## Formula: am ~ . ## ## Coefficients: ## (Intercept) mpg cyl disp hp drat ## -0.68057477 1.73068529 -6.50306685 -0.11106774 0.01566047 33.02750111 ## wt qsec vs gear carb ## -20.68143251 -9.52647833 -6.81113196 29.16524289 3.33862282 As can be seen in List of ML functions, Spark provides a wide range of algorithms and feature transformers, and we will touch on a representative portion of the functionality. A complete treatment of predictive modeling concepts is outside the scope of this book, so we recommend complementing with “R for Data Science” (Wickham and Grolemund 2016) and “Feature Engineering and Selection: A Practical Approach for Predictive Models” (Kuhn, Max and Johnson, Kjell 2019), from which we adopted for examples and visualizations in this chapter. 4.1 Overview The tasks we focus on in this chapter involve machine learning, which is what Spark ML aims to enable, as opposed to statistical inference. This means that we are often more concerned about forecasting the future rather than inferring the process by which our data is generated.1 Machine learning can be categorized into supervised learning, or predictive modeling, and unsupervised learning. In supervised learning, we try to learn a function that will map from “X” to “Y”, from a dataset of “(x, y)” examples. In unsupervised learning, we just have “X” and not the “Y” labels, so instead we try to learn something about the structure of “X”. Some practical use cases for supervised learning include forecasting the weather tomorrow, determining whether a credit card transaction is fradulent, and coming up with a price for your car insurance policy. For unsupervised learning, examples include automated grouping of photos of individuals, segmenting customers based on their purchase history, and clustering of documents. The ML interface in sparklyr has been designed to minimize the cognitive effort for moving from a local, in-memory, native R workflow to the cluster, and back. While the Spark ecosystem is very rich, there is still a tremendous number of packages from CRAN, with some implementing functionality that you may require for a project. Also, you may want to leverage your skills and experience working in R to maintain productivity. What we learned in the Analysis section also applis here — it is important to keep track of where you are performing computations, and move between the cluster and your R session as appropriate. In the next section, we will introduce the dataset that will be used throughout the chapter. We then demonstrate a supervised learning workflow that includes exploratory data analysis, feature engineering, and model building. We then move on to a topic modeling example that looks at some unstructured text data. Keep in mind that our goal will be to show various techniques of executing data science tasks on large data, rather than conducting a rigorous and coherent analysis. 4.2 The Data The examples in this chapter will utilize the OkCupid dataset, available at.2 The dataset consists of user profile data from an online dating site, and contains a diverse set of features, including biographical characteristics such as gender and profession, and free text fields related to personal interests. There are about 60,000 profiles in the dataset, which fits comfortably into memory on a modern laptop and wouldn’t be considered “big data”, so you can easily follow along running Spark local mode. In a later chapter, we’ll discuss specific considerations for dealing with distributed datasets on clusters. To motivate the examples, we will consider the following problem: Predict whether someone is actively working, i.e. not retired, a student, or unemployed. Note: The examples in this chapter utilize small datasets so readers can easily follow along in local mode. In practice, if your dataset fits comfortably in memory on your local machine, you may be better off using an efficient non-distributed implementation of the ML algorithm. For example, you may want to use the ranger package instead of ml_random_forest_classifier(). Next up, we will look at the data. 4.3 Exploratory Data Analysis Exploratory data analysis (EDA), in the context of predictive modeling, is the exercise of looking at excerpts and summaries of the data. The specific goals of the EDA stage is informed by the business problem, but here are some common objectives: Checking for data quality — confirming meaning and prevalence of missing values and reconciling statistics against existing controls, Understand univariate relationships between variables, and Perform an initial assessment on what variables to include and what transformations need to be done on them. We’ll first read in the data, assuming profiles.csv is in the data/ folder. okc &lt;- spark_read_csv( sc, &quot;data/profiles.csv&quot;, escape = &quot;\\&quot;&quot;, options = list(multiline = TRUE) ) %&gt;% mutate( height = as.numeric(height), income = ifelse(income == &quot;-1&quot;, NA, as.numeric(income)) ) %&gt;% mutate_if(is.character, list(~ ifelse(is.na(.), &quot;missing&quot;, .))) We specify escape = \"\\\"\" and options = list(multiline = TRUE) here to accommodate for embedded quote characters and newlines in the essay fields. We also convert the height and income columns to numeric types, and recode missing values in the string columns. Note that it may very well take a few tries of specifying different parameters to get the initial data ingest correct, and sometimes you may have to revisit this step after you learn more about the data during modeling. We can now take a quick look at our data with glimpse(): glimpse(okc) ## Observations: ?? ## Variables: 31 ## Database: spark_connection ## $ age &lt;int&gt; 22, 35, 38, 23, 29, 29, 32, 31, 24, 37, 35… ## $ body_type &lt;chr&gt; &quot;a little extra&quot;, &quot;average&quot;, &quot;thin&quot;, &quot;thin… ## $ diet &lt;chr&gt; &quot;strictly anything&quot;, &quot;mostly other&quot;, &quot;anyt… ## $ drinks &lt;chr&gt; &quot;socially&quot;, &quot;often&quot;, &quot;socially&quot;, &quot;socially… ## $ drugs &lt;chr&gt; &quot;never&quot;, &quot;sometimes&quot;, &quot;missing&quot;, &quot;missing&quot;… ## $ education &lt;chr&gt; &quot;working on college/university&quot;, &quot;working … ## $ essay0 &lt;chr&gt; &quot;about me:&lt;br /&gt;\\n&lt;br /&gt;\\ni would love to … ## $ essay1 &lt;chr&gt; &quot;currently working as an international age… ## $ essay2 &lt;chr&gt; &quot;making people laugh.&lt;br /&gt;\\nranting about… ## $ essay3 &lt;chr&gt; &quot;the way i look. i am a six foot half asia… ## $ essay4 &lt;chr&gt; &quot;books:&lt;br /&gt;\\nabsurdistan, the republic, … ## $ essay5 &lt;chr&gt; &quot;food.&lt;br /&gt;\\nwater.&lt;br /&gt;\\ncell phone.&lt;br… ## $ essay6 &lt;chr&gt; &quot;duality and humorous things&quot;, &quot;missing&quot;, … ## $ essay7 &lt;chr&gt; &quot;trying to find someone to hang out with. … ## $ essay8 &lt;chr&gt; &quot;i am new to california and looking for so… ## $ essay9 &lt;chr&gt; &quot;you want to be swept off your feet!&lt;br /&gt;… ## $ ethnicity &lt;chr&gt; &quot;asian, white&quot;, &quot;white&quot;, &quot;missing&quot;, &quot;white… ## $ height &lt;dbl&gt; 75, 70, 68, 71, 66, 67, 65, 65, 67, 65, 70… ## $ income &lt;dbl&gt; NaN, 80000, NaN, 20000, NaN, NaN, NaN, NaN… ## $ job &lt;chr&gt; &quot;transportation&quot;, &quot;hospitality / travel&quot;, … ## $ last_online &lt;chr&gt; &quot;2012-06-28-20-30&quot;, &quot;2012-06-29-21-41&quot;, &quot;2… ## $ location &lt;chr&gt; &quot;south san francisco, california&quot;, &quot;oaklan… ## $ offspring &lt;chr&gt; &quot;doesn&amp;rsquo;t have kids, but might want t… ## $ orientation &lt;chr&gt; &quot;straight&quot;, &quot;straight&quot;, &quot;straight&quot;, &quot;strai… ## $ pets &lt;chr&gt; &quot;likes dogs and likes cats&quot;, &quot;likes dogs a… ## $ religion &lt;chr&gt; &quot;agnosticism and very serious about it&quot;, &quot;… ## $ sex &lt;chr&gt; &quot;m&quot;, &quot;m&quot;, &quot;m&quot;, &quot;m&quot;, &quot;m&quot;, &quot;m&quot;, &quot;f&quot;, &quot;f&quot;, &quot;f… ## $ sign &lt;chr&gt; &quot;gemini&quot;, &quot;cancer&quot;, &quot;pisces but it doesn&amp;r… ## $ smokes &lt;chr&gt; &quot;sometimes&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;, &quot;no&quot;,… ## $ speaks &lt;chr&gt; &quot;english&quot;, &quot;english (fluently), spanish (p… ## $ status &lt;chr&gt; &quot;single&quot;, &quot;single&quot;, &quot;available&quot;, &quot;single&quot;,… Now we will add our response variable as a column in the dataset and look at its distribution okc &lt;- okc %&gt;% mutate( not_working = ifelse(job %in% c(&quot;student&quot;, &quot;unemployed&quot;, &quot;retired&quot;), 1 , 0) ) okc %&gt;% group_by(not_working) %&gt;% tally() ## # Source: spark&lt;?&gt; [?? x 2] ## not_working n ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 54541 ## 2 1 5405 Before we proceed further, let us perform an initial split of our data into a training set and a testing set and put away the latter. In practice, this is a crucial step because we would like to have a holdout set that we set aside at the end of the modeling process to evaluate model performance. If we were to include the entire dataset during EDA, information from the testing set could “leak” into the visualizations and summary statistics, and bias our model building process even though the data is not used directly in a learning algorithm. This would undermine the credibility of our performance metrics. Splitting the data can be done easily by using the sdf_partition() function: data_splits &lt;- sdf_random_split(okc, training = 0.8, testing = 0.2, seed = 42) okc_train &lt;- data_splits$training okc_test &lt;- data_splits$testing We can quickly look at the distribution of our response variable: okc_train %&gt;% group_by(not_working) %&gt;% tally() %&gt;% mutate(frac = n / sum(n, na.rm = TRUE)) ## # Source: spark&lt;?&gt; [?? x 3] ## not_working n frac ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 43785 0.910 ## 2 1 4317 0.0897 Using the sdf_describe() function, we can obtain numerical summaries of specific columns: sdf_describe(okc_train, cols = c(&quot;age&quot;, &quot;income&quot;)) ## # Source: spark&lt;?&gt; [?? x 3] ## summary age income ## &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; ## 1 count 48102 9193 ## 2 mean 32.336534863415245 104968.99815076689 ## 3 stddev 9.43908920033797 202235.2291773537 ## 4 min 18 20000.0 ## 5 max 110 1000000.0 Like we saw in the Analysis chapter, we can also utilize the dbplot package to plot distributions of these variables: okc_train %&gt;% dbplot_histogram(age) FIGURE 4.1: Distribution of age A common EDA exercise is to look at the relationship between the response and the individual predictors. For example, we can explore the religion variable: prop_data &lt;- okc_train %&gt;% mutate(religion = regexp_extract(religion, &quot;^\\\\\\\\w+&quot;, 0)) %&gt;% group_by(religion, not_working) %&gt;% tally() %&gt;% group_by(religion) %&gt;% summarize( count = sum(n, na.rm = TRUE), prop = sum(not_working * n, na.rm = TRUE) / sum(n, na.rm = TRUE) ) %&gt;% mutate(se = sqrt(prop * (1 - prop) / count)) %&gt;% collect() prop_data ## # A tibble: 10 x 4 ## religion count prop se ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 judaism 2520 0.0794 0.00539 ## 2 atheism 5624 0.118 0.00436 ## 3 christianity 4671 0.120 0.00480 ## 4 hinduism 358 0.101 0.0159 ## 5 islam 115 0.191 0.0367 ## 6 agnosticism 7078 0.0958 0.00346 ## 7 other 6240 0.0841 0.00346 ## 8 missing 16152 0.0719 0.002 ## 9 buddhism 1575 0.0851 0.007 ## 10 catholicism 3769 0.0886 0.00458 Note that prop_data is a small data frame that has been collected into memory in our R session, we can take advantage of ggplot2 to create an informative visualization. FIGURE 4.2: Proportion of individuals not working by religion Next, we take a look at the relationship between a couple of predictors: alcohol use and drug use. We would expect there to be some correlation between them. You can compute a contingency table via sdf_crosstab(): contingency_tbl &lt;- okc_train %&gt;% sdf_crosstab(&quot;drinks&quot;, &quot;drugs&quot;) %&gt;% collect() contingency_tbl ## # A tibble: 7 x 5 ## drinks_drugs missing never often sometimes ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 very often 54 144 44 137 ## 2 socially 8221 21066 126 4106 ## 3 not at all 146 2371 15 109 ## 4 desperately 72 89 23 74 ## 5 often 1049 1718 69 1271 ## 6 missing 1121 1226 10 59 ## 7 rarely 613 3689 35 445 We can visualize this contingency table using a mosaic plot. FIGURE 4.3: Mosaic plot of drug and alcohol use To further explore the relationship between this two variables, we can perform correspondence analysis using the FactoMineR package and visualize the results. FIGURE 4.4: Correspondence analysis principal coordinates for drugs and alcohol use Correspondence analysis transforms the factors into variables called principal coordinates, which correspond to the axes in the plot and represent how much information in the contingency table they contain. We can, for example, interpret the proximity of “drinking often” and “using drugs very often” as indicating association. This concludes our discussion on EDA, and we will now proceed to feature engineering. 4.4 Feature Engineering The feature engineering exercise comprises transforming the data to increase the performance of the model. This can include things like centering and scaling numerical values and performing string manipulation to extract meaningful variables. It also often includes variable selection — the process of selecting which predictors are used in the model. Let us start with scaling the age variable by removing the mean and scaling to unit variance. Some algorithms, especially neural networks, train faster if we normalize our inputs. We begin by calculating the mean and standard deviation of the variable: scale_values &lt;- okc_train %&gt;% summarize( mean_age = mean(age, na.rm = TRUE), sd_age = sd(age) ) %&gt;% collect() scale_values ## # A tibble: 1 x 2 ## mean_age sd_age ## &lt;dbl&gt; &lt;dbl&gt; ## 1 32.3 9.44 We can then use these to transform the dataset: okc_train &lt;- okc_train %&gt;% mutate(scaled_age = (age - !!scale_values$mean_age) / !!scale_values$sd_age) okc_train %&gt;% dbplot_histogram(scaled_age) FIGURE 4.5: Distribution of scaled age Since some of the profile feature are multiple-select, we need to process them before we can build meaningful models. If we take a look at the ethnicity column, for example, we see that there are many different combinations: okc_train %&gt;% group_by(ethnicity) %&gt;% tally() ## # Source: spark&lt;?&gt; [?? x 2] ## ethnicity n ## &lt;chr&gt; &lt;dbl&gt; ## 1 hispanic / latin, white 1051 ## 2 black, pacific islander, hispanic / latin 2 ## 3 asian, black, pacific islander 5 ## 4 black, native american, white 91 ## 5 middle eastern, white, other 34 ## 6 asian, other 78 ## 7 asian, black, white 12 ## 8 asian, hispanic / latin, white, other 7 ## 9 middle eastern, pacific islander 1 ## 10 indian, hispanic / latin 5 ## # … with more rows For our model, we create indicator variables for each race, as follows: ethnicities &lt;- c(&quot;asian&quot;, &quot;middle eastern&quot;, &quot;black&quot;, &quot;native american&quot;, &quot;indian&quot;, &quot;pacific islander&quot;, &quot;hispanic / latin&quot;, &quot;white&quot;, &quot;other&quot;) ethnicity_vars &lt;- ethnicities %&gt;% map(~ expr(ifelse(like(ethnicity, !!.x), 1, 0))) %&gt;% set_names(paste0(&quot;ethnicity_&quot;, gsub(&quot;\\\\s|/&quot;, &quot;&quot;, ethnicities))) okc_train &lt;- mutate(okc_train, !!!ethnicity_vars) okc_train %&gt;% select(starts_with(&quot;ethnicity_&quot;)) %&gt;% glimpse() ## Observations: ?? ## Variables: 9 ## Database: spark_connection ## $ ethnicity_asian &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ ethnicity_middleeastern &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ ethnicity_black &lt;dbl&gt; 0, 1, 0, 0, 0, 0, 0, 0, 0, 0… ## $ ethnicity_nativeamerican &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ ethnicity_indian &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ ethnicity_pacificislander &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ ethnicity_hispaniclatin &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… ## $ ethnicity_white &lt;dbl&gt; 1, 0, 1, 0, 1, 1, 1, 0, 1, 0… ## $ ethnicity_other &lt;dbl&gt; 0, 0, 0, 0, 0, 0, 0, 0, 0, 0… For the free text fields, a straightforward way to extract features is counting the total number of characters. okc_train &lt;- okc_train %&gt;% mutate( essay_length = char_length(paste(!!!syms(paste0(&quot;essay&quot;, 0:9)))) ) okc_train %&gt;% dbplot_histogram(essay_length, bins = 50) FIGURE 4.6: Distribution of essay length Now that we have a few more features to work with, we can begin running some ML algorithms! 4.5 Model Building Once we have a good grasp on our dataset, we can start building some models. Before we do so, however, we need to come up with a plan to tune and validate our candidate models. Since we are dealing with a binary classification problem, the metrics one can use include accuracy, precision, sensitivity, and area under the receiver operating characteristic (ROC) curve (AUC), among others. The metric you optimize depends on your specific business problem, but for this exercise, we will focus on the AUC. It is important that we don’t peek at the testing holdout set until the very end. For tuning and validation, we will perform 10-fold cross validation. The scheme works as follows: We first divide our dataset into 10 approximately equal sized subsets. We take the 2nd to 10th sets together as the training set for an algorithm, and validate the resulting model on the 1st set. Next, we reserve the 2nd set as the validation set, and train the algorithm on the 1st and 3rd to 10th sets. In total, we train ten models and average the performance. If time and resources allow, you can also perform this procedure multiple times with different random partitions of the data. In our case, we will demonstrate how to perform the cross validation once. Using the sdf_partition() function, we can create a list of subsets from our okc_train table: vfolds &lt;- sdf_partition( okc_train, weights = set_names(rep(0.1, 10), paste0(&quot;fold&quot;, 1:10)), seed = 42 ) We then create our first training/validation split as follows: train_set &lt;- do.call(rbind, vfolds[2:10]) validation_set &lt;- vfolds[[1]] One item we need to carefully treat here is the scaling of variables. We need to make sure we do not leak any information from the validation set to the training set, so we calculate the mean and standard deviation on the training set only, and apply the same transformation to both sets. Here is how we would handle this for the age variable: make_scale_age &lt;- function(training) { scale_values &lt;- training %&gt;% summarize( mean_age = mean(age, na.rm = TRUE), sd_age = sd(age) ) %&gt;% collect() function(data) { data %&gt;% mutate(scaled_age = (age - !!scale_values$mean_age) / !!scale_values$sd_age) } } scale_age &lt;- make_scale_age(train_set) train_set &lt;- scale_age(train_set) validation_set &lt;- scale_age(validation_set) Logistic regression is often a reasonable starting point for binary classification problems, so let us give it a try. Suppose also that our domain knowledge provides us with an initial set of predictors. We can then fit a model by using the formula interface: lr &lt;- ml_logistic_regression( train_set, not_working ~ scaled_age + sex + drinks + drugs + essay_length ) lr ## Formula: not_working ~ scaled_age + sex + drinks + drugs + essay_length ## ## Coefficients: ## (Intercept) scaled_age sex_m drinks_socially ## -2.823517e+00 -1.309498e+00 -1.918137e-01 2.235833e-01 ## drinks_rarely drinks_often drinks_not at all drinks_missing ## 6.732361e-01 7.572970e-02 8.214072e-01 -4.456326e-01 ## drinks_very often drugs_never drugs_missing drugs_sometimes ## 8.032052e-02 -1.712702e-01 -3.995422e-01 -7.483491e-02 ## essay_length ## 3.664964e-05 To obtain a summary of performance metrics on the validation set, we can use the ml_evaluate() function. validation_summary &lt;- ml_evaluate(lr, validation_set) You can print validation_summary to see the available metrics validation_summary ## BinaryLogisticRegressionSummaryImpl ## Access the following via `$` or `ml_summary()`. ## - features_col() ## - label_col() ## - predictions() ## - probability_col() ## - area_under_roc() ## - f_measure_by_threshold() ## - pr() ## - precision_by_threshold() ## - recall_by_threshold() ## - roc() ## - prediction_col() ## - accuracy() ## - f_measure_by_label() ## - false_positive_rate_by_label() ## - labels() ## - precision_by_label() ## - recall_by_label() ## - true_positive_rate_by_label() ## - weighted_f_measure() ## - weighted_false_positive_rate() ## - weighted_precision() ## - weighted_recall() ## - weighted_true_positive_rate() We can plot the ROC curve by collecting the output of validation_summary$roc() and using ggplot2: FIGURE 4.7: ROC curve for the logistic regression model Recall that the ROC curve plots the true positive rate (sensitivity) against the false positive rate (1 - specificity) for varying values of the classification threshold. In practice, the business problem helps to determine where on the curve one sets the threshold for classification. The AUC is a summary measure for determining the quality of a model, and we can compute it by calling the area_under_roc() function. validation_summary$area_under_roc() ## [1] 0.7872754 Note: Spark only provides evaluation methods for generalized linear models (including linear models and logistic regression.) For other algorithms, you can use the evaluator functions (e.g. ml_binary_classification_evaluator() on the prediction data frame) or compute your own metrics. Now, we can easily repeat the logic we have above and apply it to each train/validation split: cv_results &lt;- map_df(1:10, function(v) { train_set &lt;- do.call(rbind, vfolds[setdiff(1:10, v)]) validation_set &lt;- vfolds[[v]] scale_age &lt;- make_scale_age(train_set) train_set &lt;- scale_age(train_set) validation_set &lt;- scale_age(validation_set) model &lt;- ml_logistic_regression( train_set, not_working ~ scaled_age + sex + drinks + drugs + essay_length ) s &lt;- ml_evaluate(model, validation_set) roc_df &lt;- s$roc() %&gt;% collect() auc &lt;- s$area_under_roc() tibble( Resample = paste0(&quot;Fold&quot;, str_pad(v, width = 2, pad = &quot;0&quot;)), roc_df = list(roc_df), auc = auc ) }) This gives us 10 ROC curves: FIGURE 4.8: Cross-validated ROC curves for the logistic regression model and we can obtain the average AUC metric: mean(cv_results$auc) ## [1] 0.7715102 4.5.1 Logistic Regression as a Generalized Linear Regression In Spark ML, you can also fit a logistic regression via the generalized linear regression interface by specifying family = \"binomial\". Because the result is a regression model, the ml_predict() method does not give class probabilities. However, this interface may be of interest to those who are looking for generalized linear model (GLM) diagnostics, including confidence intervals for coefficient estimates. glr &lt;- ml_generalized_linear_regression( train_set, not_working ~ scaled_age + sex + drinks + drugs, family = &quot;binomial&quot; ) tidy_glr &lt;- tidy(glr) We can extract the coefficient estimates into a tidy data frame, which we can then process further, for example, to create a coefficient plot. FIGURE 4.9: Coefficient estimates with 95% confidence intervals Note: Both ml_logistic_regression() and ml_linear_regression() support elastic net regularization (Zou and Hastie 2005) through the reg_param and elastic_net_param parameters. reg_param corresponds to \\(\\lambda\\) whereas elastic_net_param correspond to \\(\\alpha\\). ml_generalized_linear_regression() supports only reg_param. 4.5.2 More Machine Learning Algorithms Beyond linear models, Spark ML supports many of the standard ML algorithms with the mechanism demonstrated above, you can try different algorithms and hyperparameters for your problem. You can find a list of supported ML related functions in the Appendix. The interfaces to access these functionalities are largely identical, so it is easy to experiment with them. For example, to fit a neural network model we can run: nn &lt;- ml_multilayer_perceptron_classifier( train_set, not_working ~ scaled_age + sex + drinks + drugs + essay_length, layers = c(12, 64, 64, 2) ) This gives us a feedforward neural network model with two hidden layers of 64 nodes each. Note that you have to specify the correct values for the input and output layers in the layers argument. We can obtain predictions on a validation set using ml_predict() predictions &lt;- ml_predict(nn, validation_set) then compute the AUC via ml_binary_classification_evaluator() ml_binary_classification_evaluator(predictions) ## [1] 0.7812709 Up until now, we have not look into the unstructured text in the essay fields apart from doing simple character counts. In the next section, we will explore the textual data in more depth. 4.6 Working with Textual Data Along with speech, images, and videos, textual data is one of the components of the “big data” explosion. Prior to modern text mining techniques and the computational resources to support them, companies had little use for freeform text fields. Today, text is considered a rich source of insights, and can be found anywhere from physician’s notes to customer complaints. In this section, we show some basic text analysis capabilities of sparklyr. If you would like more background on text mining techniques, we recommend checking out “Text Mining with R: A Tidy Approach” (???). In this section, we show how to perform a basic topic modeling task on the essay data in the OKCupid dataset. Our plan is to concatenate the essay fields (of which there are 10) of each profile, and regard each profile as a document, then attempt to discover topics using Latent Dirichlet Allocation (LDA). 4.6.1 Data Prep As always, before analyzing a dataset (or a subset of one), we want to take a quick look to orient ourselves. essay_cols &lt;- paste0(&quot;essay&quot;, 0:9) essays &lt;- okc %&gt;% select(!!essay_cols) essays %&gt;% glimpse() ## Observations: ?? ## Variables: 10 ## Database: spark_connection ## $ essay0 &lt;chr&gt; &quot;about me:&lt;br /&gt;\\n&lt;br /&gt;\\ni would love to think that… ## $ essay1 &lt;chr&gt; &quot;currently working as an international agent for a f… ## $ essay2 &lt;chr&gt; &quot;making people laugh.&lt;br /&gt;\\nranting about a good sa… ## $ essay3 &lt;chr&gt; &quot;the way i look. i am a six foot half asian, half ca… ## $ essay4 &lt;chr&gt; &quot;books:&lt;br /&gt;\\nabsurdistan, the republic, of mice an… ## $ essay5 &lt;chr&gt; &quot;food.&lt;br /&gt;\\nwater.&lt;br /&gt;\\ncell phone.&lt;br /&gt;\\nshelt… ## $ essay6 &lt;chr&gt; &quot;duality and humorous things&quot;, &quot;missing&quot;, &quot;missing&quot;,… ## $ essay7 &lt;chr&gt; &quot;trying to find someone to hang out with. i am down … ## $ essay8 &lt;chr&gt; &quot;i am new to california and looking for someone to w… ## $ essay9 &lt;chr&gt; &quot;you want to be swept off your feet!&lt;br /&gt;\\nyou are … Just from this output, we see that The text contain HTML tags, The text contains the newline \\n character, and There are missing values in the data. As you analyze your own text data, you will quickly come across and become familiar with the peculiarities of the specific dataset. Preoprocessing text data, like with tabular numerical data, is an iterative process, and after a few tries we have the following transformations: essays &lt;- essays %&gt;% # Replace `missing` with empty string. mutate_all(list(~ ifelse(. == &quot;missing&quot;, &quot;&quot;, .))) %&gt;% # Concatenate the columns. mutate(essay = paste(!!!syms(essay_cols))) %&gt;% # Replace URLs with the &quot;URL&quot; string mutate(words = regexp_replace(essay, !!re2, &quot;URL&quot;)) %&gt;% # Remove miscellaneous characters and HTML tags mutate(words = regexp_replace(words, &quot;\\\\n|&amp;nbsp;|&lt;[^&gt;]*&gt;|[^A-Za-z|&#39;]&quot;, &quot; &quot;)) Note here we are using regex_replace(), which is a Spark SQL function. 4.6.2 Topic Modeling LDA is a type of topic model for identifying abstract “topics” in a set of documents. It is an unsupervised algorithm in that we do not provide any labels, or topics, for the input documents. LDA posits that each document is a mixture of topics, and each topic is a mixture of words. During training, it attempts to estimate both of these simultaneously. A typical use case for topic models involves categorizing many documents, where the large number of documents renders manually approaches infeasible. The application domains range from GitHub issues to legal documents. Once we have a reasonably clean dataset following the workflow in the previous section, we can fit an LDA model with ml_lda(): stop_words &lt;- ml_default_stop_words(sc) %&gt;% c( &quot;like&quot;, &quot;love&quot;, &quot;good&quot;, &quot;music&quot;, &quot;friends&quot;, &quot;people&quot;, &quot;life&quot;, &quot;time&quot;, &quot;things&quot;, &quot;food&quot;, &quot;really&quot;, &quot;also&quot;, &quot;movies&quot; ) lda_model &lt;- ml_lda(essays, ~ words, k = 6, max_iter = 1, min_token_length = 4, stop_words = stop_words, min_df = 5) We are also including a stop_words vector consisting of commonly used English words and common words in our dataset, which tells the algorithm to ignore them. After the model is fit, we can use the tidy() function to extract the associated betas, which are the per-topic-per-word probabilities, from the model. betas &lt;- tidy(lda_model) betas ## # A tibble: 256,992 x 3 ## topic term beta ## &lt;int&gt; &lt;chr&gt; &lt;dbl&gt; ## 1 0 know 303. ## 2 0 work 250. ## 3 0 want 367. ## 4 0 books 211. ## 5 0 family 213. ## 6 0 think 291. ## 7 0 going 160. ## 8 0 anything 292. ## 9 0 enjoy 145. ## 10 0 much 272. ## # … with 256,982 more rows We can then visualize this output by looking at word probabilities by topic. In Fig. N we show the results at 1 iteration and 100 iterations. FIGURE 4.10: The terms that are most common within each topic, 1 iteration FIGURE 4.11: The terms that are most common within each topic, 100 iterations We can see that, at 100 iterations, we can see “topics” starting to emerge! 4.7 Conclusion In this chapter, we cover the basics of building preditive models with sparklyr. You should now have the knowledge to begin applying machine learning to your own large datasets. In the next chapter on pipelines and deployment, we will look at ways to make your workflow more flexible for the advanced use cases. References "],
["pipelines.html", "Chapter 5 Pipelines 5.1 Estimators and Transformers 5.2 Pipelines and Pipeline Models 5.3 Applying Pipelines to OKCupid Data 5.4 Operating Modes of Pipelines Functions 5.5 Model Persistence and Interoperability 5.6 Model Deployment 5.7 Conclusion", " Chapter 5 Pipelines In this chapter, we dive into ML Pipelines, which are the engine that powers the machine learning functionality we saw in the Modeling chapter. When you invoke an ML function via the formula interface, for example ml_logistic_regression(mtcars, am ~ .), sparklyr actually constructs a pipeline for you under the hood. The Pipelines API is a lower level interface that enables advanced data processing and modeling workflows. In addition, it also facilitates the deployment of ML models. We wil begin with a few definitions and move on to specific examples. 5.1 Estimators and Transformers The building blocks of pipelines are objects called transformers and estimators, which are collectively referred to as pipeline stages. A transformer can be used to apply transformations to a data frame and return another data frame; the resulting data frame often comprises the original data frame with new columns appended to it. An estimator, on the other hand, can be used to create a transformer giving some training data. Consider the following example to illustrate this relationship: a “center and scale” estimator can learn the mean and standard deviation of some data and store the statistics in a resulting transformer object; this transformer can then be used to normalize the data it was trained on and also any new, yet unseen, data. Here is an example of how to define an estimator: scaler &lt;- ft_standard_scaler( sc, input_col = &quot;features&quot;, output_col = &quot;features_scaled&quot;, with_mean = TRUE ) scaler ## StandardScaler (Estimator) ## &lt;standard_scaler_7f6d46f452a1&gt; ## (Parameters -- Column Names) ## input_col: features ## output_col: features_scaled ## (Parameters) ## with_mean: TRUE ## with_std: TRUE We can now create some data (for which we know the mean and standard deviation) then fit our scaling model to it using the ml_fit() function. df &lt;- copy_to(sc, data.frame(value = rnorm(100000))) %&gt;% ft_vector_assembler(input_cols = &quot;value&quot;, output_col = &quot;features&quot;) scaler_model &lt;- scaler %&gt;% ml_fit(df) scaler_model ## StandardScalerModel (Transformer) ## &lt;standard_scaler_7f6d46f452a1&gt; ## (Parameters -- Column Names) ## input_col: features ## output_col: features_scaled ## (Transformer Info) ## mean: num 0.00421 ## std: num 0.999 Note: In Spark ML, many algorithms and feature transformers require that the input be a vector column. The function ft_vector_assembler() performs this task. The function can also be used to initialize a transformer to be used in a pipeline. We see that the mean and standard deviation are very close to \\(0\\) and \\(1\\), respectively, which is what we expect. We can then use the transformer to transform a data frame, using the ml_transform() function: scaler_model %&gt;% ml_transform(df) %&gt;% glimpse() ## Observations: ?? ## Variables: 3 ## Database: spark_connection ## $ value &lt;dbl&gt; 0.75373300, -0.84207731, 0.59365113, -… ## $ features &lt;list&gt; [0.753733, -0.8420773, 0.5936511, -0.… ## $ features_scaled &lt;list&gt; [0.7502211, -0.8470762, 0.58999, -0.4… 5.2 Pipelines and Pipeline Models A pipeline is simply a sequence of transformers and estimators, and a pipeline model is a pipeline that has been trained on data so all of its components have been converted to transformers. Note that Spark ML internals dictate that pipelines are always estimators, even if they comprise only of transformers. There are a couple ways to construct a pipeline in sparklyr, both of which uses the ml_pipeline() function. We can initialize an empty pipeline with ml_pipeline(sc) and append stages to it: ml_pipeline(sc) %&gt;% ft_standard_scaler( input_col = &quot;features&quot;, output_col = &quot;features_scaled&quot;, with_mean = TRUE ) ## Pipeline (Estimator) with 1 stage ## &lt;pipeline_7f6d6a6a38ee&gt; ## Stages ## |--1 StandardScaler (Estimator) ## | &lt;standard_scaler_7f6d63bfc7d6&gt; ## | (Parameters -- Column Names) ## | input_col: features ## | output_col: features_scaled ## | (Parameters) ## | with_mean: TRUE ## | with_std: TRUE Alternatively, we can pass stages directly to ml_pipeline(): pipeline &lt;- ml_pipeline(scaler) We fit a pipeline as we would fit an estimator: pipeline_model &lt;- pipeline %&gt;% ml_fit(df) pipeline_model ## PipelineModel (Transformer) with 1 stage ## &lt;pipeline_7f6d64df6e45&gt; ## Stages ## |--1 StandardScalerModel (Transformer) ## | &lt;standard_scaler_7f6d46f452a1&gt; ## | (Parameters -- Column Names) ## | input_col: features ## | output_col: features_scaled ## | (Transformer Info) ## | mean: num 0.00421 ## | std: num 0.999 pipeline 5.3 Applying Pipelines to OKCupid Data Now that we have an understanding of the rudimentary concepts for ML Pipelines, let us apply them to the predictive modeling problem from the previous chapter, where we are trying to predict whether people are currently employed by looking at their profiles. Our starting point is the okc_train data frame with the relevant columns. okc_train &lt;- okc_train %&gt;% select(not_working, age, sex, drinks, drugs, essay1:essay9) We first exhibit the pipeline, which includes feature engineering and modeling steps, then walk through it. pipeline &lt;- ml_pipeline(sc) %&gt;% ft_string_indexer(input_col = &quot;sex&quot;, output_col = &quot;sex_indexed&quot;) %&gt;% ft_string_indexer(input_col = &quot;drinks&quot;, output_col = &quot;drinks_indexed&quot;) %&gt;% ft_string_indexer(input_col = &quot;drugs&quot;, output_col = &quot;drugs_indexed&quot;) %&gt;% ft_one_hot_encoder_estimator( input_cols = c(&quot;sex_indexed&quot;, &quot;drinks_indexed&quot;, &quot;drugs_indexed&quot;), output_cols = c(&quot;sex_encoded&quot;, &quot;drinks_encoded&quot;, &quot;drugs_encoded&quot;) ) %&gt;% ft_vector_assembler( input_cols = c(&quot;age&quot;, &quot;sex_encoded&quot;, &quot;drinks_encoded&quot;, &quot;drugs_encoded&quot;, &quot;essay_length&quot;), output_col = &quot;features&quot; ) %&gt;% ft_standard_scaler(input_col = &quot;features&quot;, output_col = &quot;features_scaled&quot;, with_mean = TRUE) %&gt;% ml_logistic_regression(features_col = &quot;features_scaled&quot;, label_col = &quot;not_working&quot;) The first three stages index the sex, drinks, and drugs columns, which are character, into numeric indicies via ft_string_indexer(). This is necessary for the ft_one_hot_encoder_estimator() that comes next which requires numeric column inputs. Once all of our predictor variables are of numeric type (recall that age is numeric already), we can create our features vector using ft_vector_assembler() which concatenates all of its inputs together into one column of vectors. We can then use ft_standard_scaler() to normalize all elements of the features column (including the one-hot encoded 0/1 values of the categorical variables), and finally apply a logistic regression via ml_logistic_regression(). During prototyping, you may want to execute these transformations eagerly on a small subset of the data, by passing the data frame to the ft_ and ml_ functions, and inspecting the transformed data frame. For example, you can do the following: okc_train %&gt;% ft_string_indexer(&quot;sex&quot;, &quot;sex_indexed&quot;) %&gt;% select(sex_indexed) ## # Source: spark&lt;?&gt; [?? x 1] ## sex_indexed ## &lt;dbl&gt; ## 1 0 ## 2 0 ## 3 1 ## 4 0 ## 5 1 ## 6 0 ## 7 0 ## 8 1 ## 9 1 ## 10 0 ## # … with more rows Once you have found the right transformations for your dataset, you can then replace the data frame input with ml_pipeline(sc), and the result will be a pipeline that you can apply to any data frame with the appropriate schema. 5.3.1 Hyperparameter Tuning Going back to the pipeline we have created above, we can use ml_cross_validator() to perform the cross validation workflow we demonstrated in the previous chapter and easily test different hyperparameter combinations. In this example, we test whether centering the variables improve predictions together with various regularization values for the logistic regression. We define the cross validator as follows: cv &lt;- ml_cross_validator( sc, estimator = pipeline, estimator_param_maps = list( standard_scaler = list(with_mean = c(TRUE, FALSE)), logistic_regression = list( elastic_net_param = c(0.25, 0.75), reg_param = c(1e-2, 1e-3) ) ), evaluator = ml_binary_classification_evaluator(sc, label_col = &quot;not_working&quot;), num_folds = 10 ) The estimator argument is simply the estimator we want to tune, and in this case it is the pipeline that we defined. We provide the hyperparameter values we are interested in via the estimator_param_maps parameter, which takes a nested named list. The names at the first level correspond to UIDs of the stages we want to tune (if a partial UID is provided sparklyr will attempt to match it to a pipeline stage) and the names at the second level correspond to parameters of each stage. In the snippet above, we are specifying that we want to test Standard scaler: the values TRUE and FALSE for with_mean, which denotes whether predictor values are centered Logistic regression: The values 0.25 and 0.75 for \\(\\alpha\\), and the values 1e-2 and 1e-3 for \\(\\lambda\\) We expect this to give rise to \\(2 \\times 2 \\times 2 = 8\\) hyperparameter combinations, which we can confirm by printing the cv object: cv ## CrossValidator (Estimator) ## &lt;cross_validator_d5676ac6f5&gt; ## (Parameters -- Tuning) ## estimator: Pipeline ## &lt;pipeline_d563b0cba31&gt; ## evaluator: BinaryClassificationEvaluator ## &lt;binary_classification_evaluator_d561d90b53d&gt; ## with metric areaUnderROC ## num_folds: 10 ## [Tuned over 8 hyperparameter sets] As with any other estimator, we can fit the cross validator using ml_fit() cv_model &lt;- cv %&gt;% ml_fit(okc_train) and inspect the results: ml_validation_metrics(cv_model) %&gt;% arrange(-areaUnderROC) ## areaUnderROC elastic_net_param_1 reg_param_1 with_mean_2 ## 1 0.7722700 0.75 0.001 TRUE ## 2 0.7718431 0.75 0.010 FALSE ## 3 0.7718350 0.75 0.010 TRUE ## 4 0.7717677 0.25 0.001 TRUE ## 5 0.7716070 0.25 0.010 TRUE ## 6 0.7715972 0.25 0.010 FALSE ## 7 0.7713816 0.75 0.001 FALSE ## 8 0.7703913 0.25 0.001 FALSE 5.4 Operating Modes of Pipelines Functions By now, you have likely noticed that the pipeline stage functions, such as ft_string_indexer() and ml_logistic_regression() behave differently depending on the first argument passed to them3. The full pattern is as follows: First argument Returns Example Spark connection Estimator or transformer object ft_string_indexer(sc) Pipeline Pipeline ml_pipeline(sc) %&gt;% ft_string_indexer() Data frame, without formula Data frame ft_string_indexer(iris, \"Species\", \"indexed\") Data frame, with formula sparklyr ML model object ml_logistic_regression(iris, Species ~ .) If a Spark connection is provided, the function returns a transformer or estimator object, which can be utilized directly using ml_fit() or ml_transform() or be included in a pipeline. If a pipeline is provided, the function returns a pipeline object with the stage appended to it. If a data frame is provided to a feature transformer function (those with prefix ft_), or an ML algorithm without also providing a formula, the function instantiates the pipeline stage object, fit it to the data if necessary (if the stage is an estimator), then transforms the data frame returning a data frame. If a data frame and a formula are provided to an ML algorithm that supports the formula interface, sparklyr builds a pipeline model under the hood and returns an ML model object which contains additional metadata information. The formula interface approach is what we studied in the Modeling section, and is what we recommend new users to Spark start with, since its syntax is similar to existing R modeling packages and abstracts away some Spark ML peculiarities. However, to take advantage of the full power of Spark ML and leverage pipelines for workflow organization and interoperability, it is worthwhile to learn the ML Pipelines API. 5.5 Model Persistence and Interoperability One of the most powerful aspects of pipelines is that they can be serialized to disk and are fully interoperable with the other Spark APIs, such as Python and Scala. To save a pipeline model, call ml_save() and provide a path. model_dir &lt;- file.path(&quot;spark_model&quot;) ml_save(cv_model$best_model, model_dir, overwrite = TRUE) ## Model successfully saved. Let us take a look at the directory we just wrote to. list.dirs(model_dir,full.names = FALSE) %&gt;% head(10) ## [1] &quot;&quot; ## [2] &quot;metadata&quot; ## [3] &quot;stages&quot; ## [4] &quot;stages/0_string_indexer_5b42c72817b&quot; ## [5] &quot;stages/0_string_indexer_5b42c72817b/data&quot; ## [6] &quot;stages/0_string_indexer_5b42c72817b/metadata&quot; ## [7] &quot;stages/1_string_indexer_5b423192b89f&quot; ## [8] &quot;stages/1_string_indexer_5b423192b89f/data&quot; ## [9] &quot;stages/1_string_indexer_5b423192b89f/metadata&quot; ## [10] &quot;stages/2_string_indexer_5b421796e826&quot; We can dive into a couple of the files to see what type of data was saved. spark_read_json(sc, file.path( model_dir, &quot;stages/1_string_indexer_5b423192b89f/metadata&quot; )) %&gt;% glimpse() ## Observations: ?? ## Variables: 6 ## Database: spark_connection ## $ class &lt;chr&gt; &quot;org.apache.spark.ml.feature.StringIndexerModel&quot; ## $ defaultParamMap &lt;list&gt; [[&quot;error&quot;, &quot;string_indexer_5b423192b89f__output&quot;, &quot;frequencyDesc&quot;]] ## $ paramMap &lt;list&gt; [[&quot;error&quot;, &quot;drinks&quot;, &quot;drinks_indexed&quot;, &quot;frequencyDesc&quot;]] ## $ sparkVersion &lt;chr&gt; &quot;2.4.0&quot; ## $ timestamp &lt;dbl&gt; 1.559467e+12 ## $ uid &lt;chr&gt; &quot;string_indexer_5b423192b89f&quot; spark_read_parquet(sc, file.path( model_dir, &quot;stages/6_logistic_regression_5b423b539d0f/data&quot; )) ## # Source: spark&lt;data&gt; [?? x 5] ## numClasses numFeatures interceptVector coefficientMatr… isMultinomial ## &lt;int&gt; &lt;int&gt; &lt;list&gt; &lt;list&gt; &lt;lgl&gt; ## 1 2 12 &lt;dbl [1]&gt; &lt;-1.27950828662… FALSE We see that quite a bit of information has been exported, from the SQL statement in the dplyr transformer to the fitted coefficient estimates of the logistic regression. We can then (in a new Spark session) reconstruct the model by using ml_load(): model_2 &lt;- ml_load(sc, model_dir) Let us see if we can retrieve the logistic regression stage from this pipeline model: model_2 %&gt;% ml_stage(&quot;logistic_regression&quot;) ## LogisticRegressionModel (Transformer) ## &lt;logistic_regression_5b423b539d0f&gt; ## (Parameters -- Column Names) ## features_col: features_scaled ## label_col: not_working ## prediction_col: prediction ## probability_col: probability ## raw_prediction_col: rawPrediction ## (Transformer Info) ## coefficient_matrix: num [1, 1:12] -1.2795 -0.0915 0 0.126 -0.0324 ... ## coefficients: num [1:12] -1.2795 -0.0915 0 0.126 -0.0324 ... ## intercept: num -2.79 ## intercept_vector: num -2.79 ## num_classes: int 2 ## num_features: int 12 ## threshold: num 0.5 ## thresholds: num [1:2] 0.5 0.5 Note that the exported JSON and parquet files are agnostic of the API that exported them. This means that in a multilingual machine learning engineering team, you can pick up a data preprocessing pipeline from a data engineer working in Python, build a prediction model on top of it, then hand off the final pipeline off to a deployment engineering working in Scala. Note: When ml_save() is called for sparklyr ML models (created using the formula interface), the associated pipeline model is saved, but any sparklyr specific metadata, such as index labels, are not. In other words, saving a sparklyr ml_model object then loading it will yield a pipeline model object, as if you created it via the ML Pipelines API. What we gain from this tradeoff of loss information is interoperability with other languages. 5.6 Model Deployment What we have just demonstrated bears emphasizing: by collaborating within the framework of ML pipelines, we reduce friction among different personas in a data science team. In particular, we can cut down on the time from modeling to deployment. In many cases, a data science project does not end with just a slide deck with insights and recommendations. Instead, the business problem at hand may require scoring new data points on a schedule or on-demand in real time. For example, a bank might want to evaluate its mortgage portfolio risk nightly, or to provide instant decisions on credit card applications. This process of taking a model and turning it into a service that others can consume is usually referred to as deployment or productionization. Historically, there was a large gap between the analyst who built the model and the engineer who deployed it: the former might work in R and develop extensive documentation on the scoring mechanism, so the latter can re-implement the model in C++ or Java. This practice, which may easily take months in some organizations, is less prevalent today, but is almost always unnecessary in Spark ML workflows. The nightly portfolio risk and credit application scoring examples we mention above represent two modes of ML deployment known as batch and real-time. Loosely, batch processing implies processing many records at the same time and that execution time is not important as long it is reasonable (often on the scale of minutes to hours.) On the other hand, real-time processing implies scoring one or a few records at a time but the latency is crucial (on the scale of &lt;1 second.) We will now see how we can take our OKCupid pipeline model to “production.” 5.6.1 Batch Scoring With ML Pipelines For both cases, we will expose our model as web services, in the form of an API over the Hypertext Transfer Protocol (HTTP). This is the primary medium over which software communicates. By providing an API, other services or end users can utilize our model without any knowledge of R or Spark. The plumber4 R package enables us to do this very easily by annotating our prediction function. In the batch scoring use case, we simply initiate a Spark connection and load the saved model. Save the following script as plumber/spark-plumber.R: library(sparklyr) sc &lt;- spark_connect(master = &quot;local&quot;, version = &quot;2.3.0&quot;) spark_model &lt;- ml_load(sc, &quot;spark_model&quot;) #* @post /predict score_spark &lt;- function(age, sex, drinks, drugs, essay_length) { new_data &lt;- data.frame( age = age, sex = sex, drinks = drinks, drugs = drugs, essay_length = essay_length, stringsAsFactors = FALSE ) new_data_tbl &lt;- copy_to(sc, new_data, overwrite = TRUE) ml_transform(spark_model, pred_data_tbl) %&gt;% dplyr::pull(prediction) } We can then initialize the service by executing the following: p &lt;- plumber::plumb(&quot;plumber/spark-plumber.R&quot;) p$run(port = 8000) This should start the web service locally and emit a message similar to ## Starting server to listen on port 8000 ## Running the swagger UI at http://127.0.0.1:8000/__swagger__/ In a separate R session, we can try to query the service with new data to be scored: httr::POST( &quot;http://127.0.0.1:8000/predict&quot;, body = &#39;{&quot;age&quot;: [42], &quot;sex&quot;: &quot;m&quot;, &quot;drinks&quot;: &quot;not at all&quot;, &quot;drugs&quot;: &quot;never&quot;, &quot;essay_length&quot;: [99]}&#39; ) %&gt;% httr::content() ## [[1]] ## [1] 0 If we were to time this operation (e.g. with system.time()), we see that the latency is on the order of hundreds of milliseconds, which may be appropriate for batch applications but insufficient for real-time. The main bottleneck is the serialization of the R data frame to a Spark data frame and back. Also, it also requires an active Spark session which is a heavy runtime requirement. 5.6.2 Real-Time Scoring with MLeap For real-time production, we want to keep dependencies as light as possible so we can target more platforms for deployment. We now show how we can use the mleap5 package, which provides an interface to the MLeap6 library, to serialize and serve Spark ML models. At run time, the only prerequisites for the environment are the Java Virtual Machine (JVM) and the MLeap runtime library. This avoids both the Spark binaries and expensive overhead in converting data to and from Spark data frames. Since mleap is a sparklyr extension, it must be loaded when spark_connect() is called. We can start a new R session and establish a new Spark connection7, and load the pipeline model that we previously saved. library(sparklyr) library(mleap) sc &lt;- spark_connect(master = &quot;local&quot;, version = &quot;2.3.0&quot;) spark_model &lt;- ml_load(sc, &quot;spark_model&quot;) The way we save a model to MLeap bundle format is very similar to saving a model using the Spark ML Pipelines API; the only additional argument is sample_input, which is a Spark data frame with schema that we expect new data to be scored to have. sample_input &lt;- data.frame( sex = &quot;m&quot;, drinks = &quot;not at all&quot;, drugs = &quot;never&quot;, essay_length = 99, age = 25, stringsAsFactors = FALSE ) sample_input_tbl &lt;- copy_to(sc, sample_input) ml_write_bundle(spark_model, sample_input = sample_input_tbl, path = &quot;mleap_model.zip&quot;) The artifact we just created, mleap_model.zip, can now be deployed in any device that runs Java and has the open source MLeap runtime dependencies, without needing Spark. To test this model, we can create a new plumber API to expose it. The script plumber/mleap-plumber.R is very similar to the previous example: library(mleap) # install_maven() # install_mleap() mleap_model &lt;- mleap_load_bundle(&quot;mleap_model.zip&quot;) #* @post /predict score_spark &lt;- function(age, sex, drinks, drugs, essay_length) { new_data &lt;- data.frame( age = as.double(age), sex = sex, drinks = drinks, drugs = drugs, essay_length = as.double(essay_length), stringsAsFactors = FALSE ) mleap_transform(mleap_model, new_data)$prediction } And the way we launch the service is exactly the same: p &lt;- plumber::plumb(&quot;plumber/mleap-plumber.R&quot;) p$run(port = 8000) Again, in a separate session, we can run the exact same code we did previously to test this new service: httr::POST( &quot;http://127.0.0.1:8000/predict&quot;, body = &#39;{&quot;age&quot;: [42], &quot;sex&quot;: &quot;m&quot;, &quot;drinks&quot;: &quot;not at all&quot;, &quot;drugs&quot;: &quot;never&quot;, &quot;essay_length&quot;: [99]}&#39; ) %&gt;% httr::content() ## [[1]] ## [1] 0 If we were to time this operation, we will see that the service now returns predictions in tens of milliseconds! 5.7 Conclusion In this chapter, we discuss the Spark ML Pipelines API which is the engine behind the modeling functions covered in the previous chapter. We show how to tidy up our sparklyr predictive modeling workflow by organize data processing and machine learning routines into pipelines. Pipelines also facilitate collaboration among members of a multilingual data science and engineering team by sharing a language agnostic serialization format. Model deployment is examined, and paths to productionization for both batch and real-time settings are demonstrated. https://www.rplumber.io/↩ https://www.rplumber.io/↩ https://github.com/rstudio/mleap↩ https://github.com/combust/mleap↩ Note that, as of the writing of this book, MLeap does not yet support Spark 2.4, so we use Spark 2.3 instead.↩ "],
["clusters.html", "Chapter 6 Clusters 6.1 Overview 6.2 On-Premise 6.3 Cloud 6.4 Kubernetes 6.5 Tools 6.6 Recap", " Chapter 6 Clusters Previous chapters focused on using Spark over a single computing instance, your personal computer. In this chapter we will introduce techniques to run Spark over multiple computing instances, also known as a computing cluster. This chapter and subsequent ones will introduce and make use of concepts applicable to computing clusters; however, it’s not required to use a computing cluster to follow along, you can still use your personal computer. It’s worth mentioning that while previous chapters focused on single computing instances, all the data analysis and modeling techniques we presented, can also be used in a computing cluster without changing any code. For those of you who already have a Spark cluster in your organization, you could consider skipping to the next chapter, Connections, which will teach you how to connect to an existing cluster. Otherwise, if you don’t have a cluster or are considering improvements to your existing infrastructure, this chapter will introduce the cluster trends, managers, and providers available today. 6.1 Overview There are three major trends in cluster computing worth discussing: On-Premise, Cloud computing, and Kubernetes. Framing these trends over time will help us understand how they came to be, what they are, and what their future might be. To illustrate this, Figure 6.1 plots these trends over time using data from Google trends. FIGURE 6.1: Google trends for on-premise (mainframe), cloud computing and Kubernetes For on-premise clusters, yourself or someone in your organization purchased physical computers that were intended to be used for cluster computing. The computers in this cluster are made of off-the-shelf hardware, meaning that someone placed an order to purchase computers usually found in stores shelves or, high-performance hardware, meaning that a computing vendor provided highly customized computing hardware which also comes optimized for high-performance network connectivity, power consumption, etc. When purchasing hundreds or thousands of computing instances, it doesn’t make sense to keep them in the usual computing case that we are all familiar with, instead, it makes sense to stack them as efficiently as possible on top of each other to minimize the space the use. This group of efficiently stacked computing instances is known as a rack. Once a cluster grows to thousands of computers, you will also need to host hundreds of racks of computing devices, at this scale, you would also need significant physical space to hosts those racks. A building that provides racks of computing instances is usually known as a data center. At the scale of a data center, you would also need to find ways to make the building more efficient, specially the cooling system, power supplies, network connectivity, and so on. Since this is time consuming, a few organization have come together to open source their infrastructure under the Open Compute Project initiative, which provides a set of data center blueprints free for anyone to use. There is nothing preventing you from building our own data center and in fact, many organizations have followed this path. For instance, Amazon started as an online book store, over the years Amazon grew to sell much more than just books and, with its online store growth, their data centers also grew in size. In 2002, Amazon considered renting servers in their data centers to the public, two year laters, Amazon Web Services launched as a way to let anyone rent servers in their data centers on-demand, meaning that, one did not have to purchase, configure, maintain nor teardown it’s own clusters but could rather rent them from Amazon directly. This on-demand compute model is what we know today as Cloud Computing. In the cloud, the cluster you use is not owned by you and it’s neither in your physical building, but rather, it’s a data center owned and managed by someone else. Today, there are many cloud providers in this space ranging from Amazon, Databricks, IBM, Google, Microsoft and many others. Most cloud computing platforms provide a user interface either through a web application and command line to request and manage resources. While the benefits of processing data in the cloud were obvious for many years, picking a cloud provider had the unintended side-effect of locking organizations with one particular provider, making it hard to switch between providers or back to on-premise clusters. Kubernetes, announced by Google in 2014, is an open source system for managing containerized applications across multiple hosts. In practice, it makes it easier to deploy across multiple cloud providers and on-premise as well. In summary, we have seen a transition from on-premise, to cloud computing and more recently Kubernetes which have been also described as the private cloud, the public cloud and the hybrid cloud respectevely. This chapter will walk you through each cluster computing trend in the context of Spark and R. 6.2 On-Premise As mentioned in the overview section, on-premise clusters represent a set of computing instances procured and managed by staff members from your organization. These clusters can be highly customized and controlled; however, they can also incur higher initial expenses and maintenance costs. When using On-Premise Spark clusters, there are two concepts you should consider: Cluster Manager: In a similar way as to how an Operating Systems (like Windows os OS X) allows you to run multiple applications in the same computer; a cluster manager allows multiple applications to be run in the same cluster. You will have to choose one yourself when working with On-Premise clusters. Spark Distribution: While you can install Spark from the Apache Spark site, many companies partner with companies that can provide support and enhancements to Apache Spark which we often reffer as, Spark distributions. 6.2.1 Managers In order to run Spark within a computing cluster, you will need to run software capable of initializing Spark over each pyshical machine and register all the available computing nodes, this software is known as a cluster manager. The available cluster managers in Spark are: Spark Standalone, YARN, Mesos and Kubernetes. Note: In distributed systems and clusters literature, we often refer to each physical machine as a compute instance, compute node, instance or node. 6.2.1.1 Standalone In Spark Standalone, Spark uses itself as its own cluster manager, which allows you to use Spark without installing additional software in your cluster. This can be useful if you are planning to use your cluster to only run Spark applications; if this cluster is not dedicated to Spark, a generic cluster manager like YARN, Mesos or Kubernetes would be more suitable. The Spark Standalone documentation is available under spark.apache.org (“Spark Standalone Mode” 2019) and contains detailed information on configuring, launching, monitoring and enabling high-availability, see Figure 6.2. FIGURE 6.2: Spark Standalone Site However, since Spark Standalone is contained within a Spark installation; then, by completing the Getting Started chapter, you have now a Spark installation available that you can use to initialize a local Spark Standalone cluster in your own machine. In practice, you would want to start the worker nodes in different machines but, for simplicity, we will present the code to start a standalone cluster in a single machine. First, retrieve the SPARK_HOME directory by running spark_home_dir() then, run start-master.sh and start-slave.sh as follows: # Retrieve the Spark installation directory spark_home &lt;- spark_home_dir() # Build path to start-master.sh start_master &lt;- file.path(spark_home, &quot;sbin&quot;, &quot;start-master.sh&quot;) # Execute start-master.sh to start the cluster manager master node system2(start_master) # Build path to start-slave start_slave &lt;- file.path(spark_home, &quot;sbin&quot;, &quot;start-slave.sh&quot;) # Execute start-slave.sh to start a worker and register in master node system2(start_slave, paste0(&quot;spark://&quot;, system2(&quot;hostname&quot;, stdout = TRUE), &quot;:7077&quot;)) The previous command initialized the master node and a worker node, the master node interface can be accessed under localhost:8080 as captured in Figure 6.3: FIGURE 6.3: Spark Standalone Web Interface. Notice that there is one worker register in Spark standalone, you can follow the link to this worker node to see, Figure 6.4, details for this particular worker like available memory and cores. FIGURE 6.4: Spark Standalone Worker Web Interface Once you are done performing computations in this cluster, you can simply stop all the running nodes in this local cluster by running: # Build path to stop-all stop_all &lt;- file.path(spark_home, &quot;sbin&quot;, &quot;stop-all.sh&quot;) # Execute stop-all.sh to stop the workers and master nodes system2(stop_all) A similar approach can be followed to configure a cluster by running each start-slave.sh command over each machine in the cluster. Note: When running on a Mac, if you hit: ssh: connect to host localhost port 22: Connection refused, you will need to manually turn off the workers using system2(\"jps\") to list the running Java process and then, system2(\"kill\", c(\"-9\", \"&lt;process id&gt;\")) to stop the specific workers. 6.2.1.2 Yarn YARN for short, or Hadoop YARN, is the resource manager of the Hadoop project. It was originally developed in the Hadoop project but, refactored into it’s own project in Hadoop 2. As we mentioned in in the introduction chapter, Spark was built to speed up computation over Hadoop and therefore, it’s very common to find Spark intalled on Hadoop clusters. One advantage of YARN, is that it is likely to be already installed in many existing clusters that support Hadoop; which means that you can easily use Spark with many existing Hadoop clusters without requesting any major changes to the existing cluster infrastructure. It is also very common to find Spark deployed in YARN clusters since many started out as Hadoop clusters that were eventually upgraded to also support Spark. YARN applications can be submitted in two modes: yarn-client and yarn-cluster. In yarn-cluster mode the driver is running remotely (potentially), while in yarn-client mode, the driver is running locally, both modes are supported and are explained further in the connections chapter. YARN provides a resource management user interface useful to access logs, monitor available resources, terminate applications, etc. Once connecting to Spark from R, you will be able to manage the running application in YARN, this is shown in Figure 6.5. FIGURE 6.5: YARN’s Resource Manager running sparklyr application Since YARN is the cluster manager from the Hadoop project, YARN’s documentation can be found under the hadoop.apache.org (“Apache Hadoop Yarn” 2019), you can also reference the “Running Spark on YARN” guide from spark.apache.org (???). 6.2.1.3 Mesos Apache Mesos is an open-source project to manage computer clusters. Mesos began as a research project in the UC Berkeley RAD Lab and makes use of Linux Cgroups to provide isolation for CPU, memory, I/O and file system access. Mesos, like YARN, supports executing many cluster frameworks, including Spark. However, one advantage particular to Mesos is that, it allows cluster framework like Spark to implement custom task schedulers. An scheduler is the component that coordinates in a cluster which applications get execution time and which resources are assign to them. Spark uses a coarse-grained scheduler (“Spark on Mesos” 2018) which schedules resources for the duration of the application; however, other frameworks might use Mesos’ fine-grained scheduler, which can increase the overall efficiency in the cluster by scheduling tasks in shorter intervals allowing them to share resources between them. Mesos provides a web interface to manage your running applications, resources, and so on. After connecting to Spark from R, your application will be registered like any other application running in Mesos, Figure 6.6 shows a successful connection to Spark from R. FIGURE 6.6: Mesos web interface running Spark from R Mesos is an Apache project with its documentation available under mesos.apache.org. The “Running Spark on Mesos” guide from spark.apache.org is also a great resource if you choose to use Mesos as your cluster manager. 6.2.2 Distributions One can use a cluster manager in on-premise clusters as described in the previous section; however, many organizations choose to partner with companies providing additional management software, services and resources to help manage applications in their cluster; including, but not limited to, Apache Spark. Some of the on-premise cluster providers include: Cloudera, Hortonworks and MapR to mention a few which we will be briefly introduce next. Cloudera, Inc. is a United States-based software company that provides Apache Hadoop and Apache Spark-based software, support and services, and training to business customers. Cloudera’s hybrid open-source Apache Hadoop distribution, CDH (Cloudera Distribution Including Apache Hadoop), targets enterprise-class deployments of that technology. Cloudera donates more than 50% of its engineering output to the various Apache-licensed open source projects (Apache Hive, Apache Avro, Apache HBase, and so on) that combine to form the Apache Hadoop platform. Cloudera is also a sponsor of the Apache Software Foundation (“Cloudera Wikipedia” 2018). Cloudera clusters make use of parcels, which are binary distributions containing program files and metadata (“Cloudera Documentation” 2018), Spark happens to be installed as a parcel in Cloudera. It’s beyond the scope of this book to present how to configure Cloudera clusters, resources and documentation can be found under cloudera.com, and “Introducing sparklyr, an R Interface for Apache Spark” (“Cloudera Engineering” 2016) under Cloudera’s Engineering Blog. Cloudera provides the Cloudera Manager web interface to manage resource, services, parcels, diagnostics, etc. Figure 6.7 shows a Spark parcel running in Cloduera Manager which you can later use to connect from R. FIGURE 6.7: Cloudera Manager running Spark parcel sparklyr is certified with Cloudera (“Cloudera Partners” 2017); meaning that, Cloudera’s support is aware of sparklyr and can be effective helping organizations that are using Spark and R, the following table summarizes the versions currently certified. Cloudera Version Product Version Components Kerberos CDH5.9 sparklyr 0.5 HDFS, Spark Yes CDH5.9 sparklyr 0.6 HDFS, Spark Yes CDH5.9 sparklyr 0.7 HDFS, Spark Yes Hortonworks is a big data software company based in Santa Clara, California. The company develops, supports, and provides expertise on an expansive set of entirely open source software designed to manage data and processing for everything from IOT, to advanced analytics and machine learning. Hortonworks believes it is a data management company bridging the cloud and the datacenter (“Hortonworks Wikipedia” 2018). Hortonworks partnered with Microsoft (“Hortonworks Microsoft” 2018) to improve support in Microsoft Windows for Hadoop and Spark, this used to be a differentiation point; however, comparing Hortonworks and Cloudera is less relevant today since both companies are merging in 2019 (“Hortonworks Cloudera” 2018). While the companies are merging, support for the Cloudera and Hortonworks Spark distributions are still available. Additional resources to configure Spark under Hortonworks are available under hortonworks.com. MapR is a business software company headquartered in Santa Clara, California. MapR provides access to a variety of data sources from a single computer cluster, including big data workloads such as Apache Hadoop and Apache Spark, a distributed file system, a multi-model database management system, and event stream processing, combining analytics in real-time with operational applications. Its technology runs on both commodity hardware and public cloud computing services (“MapR Wikipedia” 2018). 6.3 Cloud If you don’t have an on-prem cluster nor spare machines to reuse, starting with a cloud cluster can be quite convenient since it will allow you to access a proper cluster in a matter of minutes. This section will briefly mention some of the major cloud infrastructure providers and give you resources to help you get started if you choose to use a cloud provider. In cloud services, the compute instances are billed for as long the Spark cluster runs; you start getting billed when the cluster launches and stops when the cluster stops. This cost needs to be multiplied by the number of instances reserved for your cluster. SO for instance, if a cloud provider chargets $1.00USD per compute instance per hour and you start a three node cluster that you use for one hour and 10 minutes; it is likely that you’ll get billed $1.00 * 2 hours * 3 nodes = $6.00. Some cloud providers charge per minute but, at least, you can rely on all of them charging per compute hour. Please be aware that, while compute costs can be quite low for small clusters, accidentally leaving a cluster running can cause significant billing expenses. Therefore, is is worth taking the extra time to check twice that your cluster is terminated when you no longer need it. It’s also a good practice to monitor costs daily while using clusters to make sure your expectations match the daily bill. From past experience, you should also plan to request compute resources in advance while dealing with large-scale projects; various cloud providers will not allow you to start a cluster with hundreds of machines before requesting them explicitly through a support request. While this can be cumbersome, it’s also a way to help you controll costs in your organization. Since the cluster size is flexible, it is a good practice to start with small clusters and scale compute resources as needed. Even if you know in advance that a cluster of significant size will be required, starting small provides an opportunity to troubleshoot issues at a lower cost since it’s unlikely that your data analysis will run at scale flawlessly on the first try. As a rule oh thumb, grow the instances exponentially; if you need to run a computation over an eight node cluster, start with one node and an eighth of the entire dataset, then two nodes with a fourth, then four nodes with a half the dataset and then, finally, eight nodes and the entire dataset. As you become more experienced, you’ll develop a good sense of how to troubleshoot issues, the size of the required cluster and you’ll be able to skip intermediate steps, but for starters, this is a good practice to follow. One can also use a cloud provider to acquire bare computing resources and then, install the on-premise distributions presented in the previous section yourself; for instance, you can run the Cloudera distribution on Amazon Elastic Compute Cloud (EC2). This model would avoid procuring colocated hardware, but still allow you to closely manage and customize your cluster. This book presents an overview of only the fully-managed Spark services available by cloud providers; however, you can usually find with ease instructions online on how to install on-premise distributions in the cloud. Some of the major providers of cloud computing infrastructure are: Amazon, Databricks, Google, IBM and Microsoft that this section will briefly introduce. 6.3.1 Amazon Amazon provides cloud services through Amazon Web Services(Amazon AWS); more specifically, provides an on-demand Spark cluster through Amazon Elastic MapReduce or EMR for short, Detailed instructions on using R with Amazon EMR was published under Amazon’s Big Data Blog: “Running sparklyr on Amazon EMR” (“AWS Blog” 2016), this post introduced the launch of sparklyr and instructions to configure EMR clusters with sparklyr. For instance, it suggests you can use the Amazon Command Line Interface to launch a cluster with three nodes as follows: aws emr create-cluster --applications Name=Hadoop Name=Spark Name=Hive \\ --release-label emr-5.8.0 --service-role EMR_DefaultRole --instance-groups \\ InstanceGroupType=MASTER,InstanceCount=1,InstanceType=m3.2xlarge \\ InstanceGroupType=CORE,InstanceCount=2,InstanceType=m3.2xlarge \\ --bootstrap-action Path=s3://aws-bigdata-blog/artifacts/aws-blog-emr-\\ rstudio-sparklyr/rstudio_sparklyr_emr5.sh,Args=[&quot;--user-pw&quot;, &quot;&lt;password&gt;&quot;, \\ &quot;--rstudio&quot;, &quot;--arrow&quot;] --ec2-attributes InstanceProfile=EMR_EC2_DefaultRole You can then see the cluster launching, and eventually running under the AWS portal, see Figure 6.8. FIGURE 6.8: Launching an Amazon EMR Cluster You can then navigate to the Master Public DNS and find RStudio under port 8787, for instance: ec2-12-34-567-890.us-west-1.compute.amazonaws.com:8787, and then login with user hadoop and password &lt;password&gt;. It is also possible to launch the EMR cluster using the web interface, the same introductory post contains additional details and walkthroughs specifically designed for EMR. Please remember to turn off your cluster to avoid unnecessary charges and use appropriate security restrictions when starting EMR clusters for sensitive data analysis. Regarding cost, the most up to date information can be found under aws.amazon.com/emr/pricing. As of this writing, these are some of the instance types available in the us-west-1 region, it is meant to provide a glimpse of the resources and costs associated with cloud processing. Notice that the “EMR price is in addition to the Amazon EC2 price (the price for the underlying servers)”. Instance CPUs Memory Storage EC2 Cost EMR Cost c1.medium 2 1.7GB 350GB $0.148 USD/hr $0.030 USD/hr m3.2xlarge 8 30GB 160GB $0.616 USD/hr $0.140 USD/hr i2.8xlarge 32 244GB 6400GB $7.502 USD/hr $0.270 USD/hr Note: We are only presentring a subset of the available compute instances for Amazon and subsequent cloud providers during 2019; however, please note that hardware (CPU speed, hard drive speed, etc.) varies between vendors and locations; therefore, you can’t use these hardware tables as an accurate price comparison, an accurate comparison would require running your particular workloads and considering other aspects beyond compute instance cost. 6.3.2 Databricks Databricks is a company founded by the creators of Apache Spark, that aims to help clients with cloud-based big data processing using Spark. Databricks grew out of the AMPLab project at University of California, Berkeley (“Databricks Wikipedia” 2018). Databricks provides enterprise-level cluster computing plans, while also providing a free/community tear to explore functionality and get familiar with their environment. Once a cluster is launched, R and sparklyr can be used from Databricks notebooks following the steps from the Getting Started chapter or, by installing RStudio on Databricks (“Databricks Rstudio” 2018). Figure 6.9 shows a Databricks notebook using Spark through sparkylr. FIGURE 6.9: Databricks community notebook running sparklyr Additional resources are available under the Databricks Engineering Blog post: “Using sparklyr in Databricks” (“Databricks Blog” 2017) and the “Databricks Documentation for sparklyr” (“Databricks Documentation” 2018). The latest pricing information can be found under databricks.com/product/pricing, as of this writing, available plans Plan Basic Data Engineering Data Analytics AWS Standard $0.07 USD/DBU $0.20 USD/DBU $0.40 USD/DBU Azure Standard $0.20 USD/DBU $0.40 USD/DBU Azure Premium $0.35 USD/DBU $0.55 USD/DBU Notice that pricing is based on cost of DBU/hr. From Databricks, “A Databricks Unit (DBU) is a unit of Apache Spark processing capability per hour. For a varied set of instances, DBUs are a more transparent way to view usage instead of the node-hour” (“Databricks Units” 2018). 6.3.3 Google Google provides Gooble Cloud Dataproc as a cloud-based managed Spark and Hadoop service offered on Google Cloud Platform. Dataproc utilizes many Google Cloud Platform technologies such as Google Compute Engine and Google Cloud Storage to offer fully managed clusters running popular data processing frameworks such as Apache Hadoop and Apache Spark (“Dataproc Wikipedia” 2018). A cluster can be easily created from the Google Cloud console or the Google Cloud command line interface as illustrated in Figure 6.10. FIGURE 6.10: Launching a Dataproc cluster Once created, ports can be forwarded to allow you to access this cluster from your machine; for instance, by launching Chrome to make use of this proxy and securely connect to the Dataproc cluster. Configuring this connection looks as follows: gcloud compute ssh sparklyr-m --project=&lt;project&gt; --zone=&lt;region&gt; -- -D 1080 \\ -N &quot;&lt;path to chrome&gt;&quot; --proxy-server=&quot;socks5://localhost:1080&quot; \\ --user-data-dir=&quot;/tmp/sparklyr-m&quot; http://sparklyr-m:8088 There are various tutorials available under cloud.google.com/dataproc/docs/tutorials, including, a comprehensive tutorial to configure RStudio and sparklyr (“Dataproc Sparklyr” 2018). The latest pricing information can be found under cloud.google.com/dataproc/pricing. Notice that the cost is split between Compute Engine and a Dataproc Premium. Instance CPUs Memory Compute Engine Dataproc Premium n1-standard-1 1 3.75GB $0.0475 USD/hr $0.010 USD/hr n1-standard-8 8 30GB $0.3800 USD/hr $0.080 USD/hr n1-standard-64 64 244GB $3.0400 USD/hr $0.640 USD/hr 6.3.4 IBM IBM cloud computing is a set of cloud computing services for business offered by the information technology company IBM. IBM cloud includes infrastructure as a service (IaaS), software as a service (SaaS) and platform as a service (PaaS) offered through public, private and hybrid cloud delivery models, in addition to the components that make up those clouds (“IBM Cloud Wikipedia” 2018). From within IBM Cloud, open Watson Studio and create a Data Science project, add a Spark cluster under the project settings and launch RStudio from the Launch IDE menu. Please note that, as of this writting, the provided version of sparklyr was not the latest version available in CRAN, since sparklyr was modified to run under the IBM Cloud. In any case, please follow IBMs documentation as an authoritative reference to run R and Spark on the IBM Cloud and particularily, on how to upgrade sparklyr appropiately. Figure 6.11 captures IBM’s Cloud portal launching a Spark cluster. FIGURE 6.11: IBM Watson Studio launching Spark with R support The most up to date pricing information is available under ibm.com/cloud/pricing. In the following table, compute cost was normalized using 31 days from the per-month costs. Instance CPUs Memory Storage Cost C1.1x1x25 1 1GB 25GB $0.033 USD/hr C1.4x4x25 4 4GB 25GB $0.133 USD/hr C1.32x32x25 32 25GB 25GB $0.962 USD/hr 6.3.5 Microsoft Microsoft Azure is a cloud computing service created by Microsoft for building, testing, deploying, and managing applications and services through a global network of Microsoft-managed data centers. It provides software as a service (SaaS), platform as a service (PaaS) and infrastructure as a service (IaaS) and supports many different programming languages, tools and frameworks, including both Microsoft-specific and third-party software and systems (“Azure Wikipedia” 2018). From the Azure portal, the Azure HDInsight service provides support for on-demand Spark clusters. An HDInsight cluster with support for Spark and RStudio can be easily created by selecting the ML Services cluster type. Please note that the provided version of sparklyr might not be the latest version available in CRAN since the default package repo seems to be initialized using an MRAN (Microsoft R Application Network) snapshot, not directly from CRAN. Figure 6.12 shows the Azure portal launching an Spark cluster with support for R. FIGURE 6.12: Creating an Azure HDInsight Spark Cluster Up to date pricing for HDInsight is available under azure.microsoft.com/en-us/pricing/details/hdinsight. Instance CPUs Memory Total Cost D1 v2 1 3.5 GB $0.074/hour D4 v2 8 28 GB $0.59/hour G5 64 448 GB $9.298/hour 6.3.6 Qubole Qubole is a company founded in 2013 with a mission to close the data accessibility gap. Qubole delivers a Self-Service Platform for Big Data Analytics built on Amazon, Microsoft, Google and Oracle Clouds. You can use R with Spark clusters though Qubole notebooks or following Qubole’s documentation to use RStudio with Qubole clusters. Figure 6.13 shows a Qubole notebook using Spark through sparkylr. FIGURE 6.13: Qubole notebook running sparklyr The latest pricing information can be found under qubole.com/qubole-pricing/, as of this writing the following plans are described in their pricing page: Test Drive Full-Featured Trial Enterprise Edition $0 USD $0 USD $0.14 USD/QCU Notice that pricing is based on cost of QCU/hr which stands for “Qubole Compute Unit per hour” and the “Enterprise Edition” requires an annual contract as of this writting. 6.4 Kubernetes Kubernetes is an open-source container-orchestration system for automating deployment, scaling and management of containerized applications that was originally designed by Google and now maintained by the Cloud Native Computing Foundation. Kubernetes was originally based on Docker while, like Mesos, it’s also based on Linux Cgroups. Kubernetes can execute many cluster applications and frameworks that can be highly customized by using container images with specific resources and libraries. This allows a single Kubernetes cluster to be used for many different purposes beyond data analysis, which in turn helps organizations manage their compute resources with ease. One trade off from using custom images is that they add additional configuration overhead but make kubernetes clusters extremely flexible. Nevertheless, this flexibility has proven to be instrumental to administrate with ease cluster resources in many organizations and, as shown in the overview section, it’s becoming a very popular cluster framework. Kubernetes is supported across all major cloud providers. They all provide extensive documentation as to how to launch, manage and teard down Kubernetes clusters; Figure 6.14 shows Google Gloud’s console while creating a Kubernetes cluster. Spark can be deployed over any Kubernetes cluster and R used to connect, analyze, model and so on. FIGURE 6.14: Creating a Kubernetes cluster for Spark and R using Google Cloud You can learn more about kubernetes.io, and the “Running Spark on Kubernetes” guide from spark.apache.org. Strictly speaking, Kubernetes is a cluster technology not an specific cluster architecture. However, Kubernetes represents a larger trend often refered as a hybrid cloud. A hybrid cloud is a computing environment that makes use of on-premises and public cloud services with orchestration between the various platforms. It’s still early to precesily categorize the leading technologies that will form a hybrid approach to cluster computing; while Kubernetes is the leading one, many more are likely to form to complement or even replace existing technologies. 6.5 Tools While using only R and Spark can be sufficient for some clusters, it is common to install complementary tools in your cluster to improve: monitoring, sql analysis, workflow coordination, etc. with applications like Ganglia, Hue and Oozie respectively. This section is not meant to cover all, but rather mention the ones that are commonly to use. 6.5.1 RStudio From reading the Introduction chapter, you are aware that RStudio is a well known, free, desktop development environment for R; therefore, it is likely that you are following the examples in this book using RStudio Desktop; however, you might not be aware that RStudio can also be run as a web service inside an Spark cluster, this version of RStudio is known as RStudio Server. You can see RStudio Server running in Figure 6.15. In the same way that the Spark UI runs in the cluster, RStudio Server can be installed inside the cluster, then you can connect to RStudio Server and use RStudio in exactly the same way you use RStudio Desktop but with the ability to run code against the Spark cluster. As you can see on the following image, RStudio Server is running on a web browser inside a Spark cluster; it looks and feels just like RStudio Desktop, but adds support to run commands efficiently by being located within the cluster. FIGURE 6.15: RStudio Server Pro running inside Apache Spark For those familiar with R, Shiny is a very popular tool for building interactive web applications from R; which it is also recommended you install directly in your Spark cluster. RStudio Server and Shiny Server are a free and open source; however, RStudio also provides professional producs, like: RStudio Server, RStudio Server Pro (“RStudio Server Pro” 2019), Shiny Server Pro (“Shiny Server Pro” 2019) and RStudio Connect (“RStudio Connect” 2019) which can be installed within the cluster to support additional R workflows, while sparklyr does not require any additional tools, they provide significant productivity gains worth considering. You can learn more about them at rstudio.com/products/. 6.5.2 Jupyter Project Jupyter exists to develop open-source software, open-standards, and services for interactive computing across dozens of programming languages. A Jupyter notebook, provide support for various programming languages, including R. sparklyr can be used with Jupyter notebooks using the R Kernel. Figure 6.16 shows sparklyr running inside a local Jupyter notebook. FIGURE 6.16: Jupyter notebook running sparklyr 6.5.3 Livy Apache Livy is an incubation project in Apache providing support to use Spark clusters remotely through a web interface. It is ideal to connect directly into the Spark cluster; however, there are times where connecting directly to the cluster is not feasible. When facing those constraints, one can consider installing Livy in their cluster and secure it properly to enable remote use over web protocols. However, there is a significant performance overhead from using Livy in sparklyr. To help test Livy locally, sparklyr provides support to list, install, start and stop a local Livy instance by executing: ## livy ## 1 0.2.0 ## 2 0.3.0 ## 3 0.4.0 ## 4 0.5.0 Which lists the versions that you can install, we recommend installing the latest version and verifying the installed version as follows # Install default Livy version livy_install() # List installed Livy services livy_installed_versions() # Start the Livy service livy_service_start() You can then navigate to this local Livy session under http://localhost:8998, the Livy Connections section will detail how to connect to this local instance and also proper clusters with Livy enabled, once connected, you can navigate to the Livy web application as captured by Figure 6.17. FIGURE 6.17: Apache Livy running as a local service Make sure you also stop the Livy service when working with local Livy instances, for proper Livy services running in a cluster, you won’t have to. # Stops the Livy service livy_service_stop() 6.6 Recap This chapter explained the history and tradeoffs of on-premise, cloud computing and presented Kubernetes as a promising framework to provide flexibility across on-premise and cloud providers. It also introduced cluster managers (Spark Standalone, YARN, Mesos and Kubernetes) as the software needed to run Spark as a cluster application. This chapter briefly mentioned on-premise cluster providers like Cloudera, Hortonworks and MapR as well as the major cloud providers: Amazon, Google and Microsoft. While this chapter provided a solid foundation to understand current cluster computing trends, tools and providers useful to perform data science at scale; it did not provide a comprehensive framework to decide which cluster technologies to choose. Instead, use this chapter as an overview and a starting point to reach out to additional resources to help you find the cluster stack that best fits your organization needs. The next chapter, connections, will focus on understanding how to connect to existing clusters; therefore, it assumes a Spark cluster like the ones presented in this chapter, is already available to you. References "],
["connections.html", "Chapter 7 Connections 7.1 Overview 7.2 Local 7.3 Standalone 7.4 Yarn 7.5 Livy 7.6 Mesos 7.7 Kubernetes 7.8 Cloud 7.9 Batches 7.10 Tools 7.11 Multiple 7.12 Troubleshooting 7.13 Recap", " Chapter 7 Connections The previous chapter, Clusters, presented the major cluster computing trends, cluster managers, distributions and cloud service providers to help you choose the Spark cluster that best suits your needs. In contrast, this chapter presents the internal components of a Spark cluster and how to connect to Spark clusters running on any cluster manager, distribution or services presented in the previous chapter. In addition, this chapter provides various troubleshooting connection techniques. While we hope you won’t need to use these, this chapter will prepare you to use them as effective techniques to resolve connectivity issues. While this chapter might feel a bit dry – connecting and troubleshooting connections is definetely not the most exciting part of large-scale data analysis – it will introduce the components of a Spark cluster and how they interact, often known as the architecture of Apache Spark. This chapter, in addition to the Data and Tuning chapters, will provide a detailed view of how Spark works, which will help you move towards becoming an intermediate Spark user that can go beyond analysis into the exciting world of distributed computing, using Apache Spark. 7.1 Overview The overall connection architecture for a Spark cluster is composed of three type of compute instances: the driver node, the worker nodes and the cluster manager. A cluster manager is a service that allows Spark to be executed in the cluster, this was detailed in the previous chapter under the cluster managers section. The worker nodes (also referred to as executors) execute compute tasks over partitioned data and communicate intermediate results to other workers or back to the driver node. The driver node is tasked with delegating work to the worker nodes, but also for aggregating their results and controlling computation flow. For the most part, aggregation happens in the worker nodes; however, even after the nodes aggregate data, it is often the case that the driver node would have to collect the worker’s results. Therefore, the driver node usually has at least, but often much more, compute resources (memory, CPUs, local storage, etc.) than the worker node. Strictly speaking, the driver node and worker nodes are just names assigned to machines with particular roles, while the actual computation in the driver node is performed by the spark context. The Spark context is the main entry point for Spark functionality (“Azure Wikipedia” 2019) since it’s tasked with scheduling tasks, managing storage, tracking execution status, access configuration settings, canceling jobs and so on. In the worker nodes, the actual computation is performed under a spark executor, which is a Spark component tasked with executing subtasks against specific data partition. We can illustrate this concepts in Figure 7.1, where the driver node orchestrates worker’s work through the cluster manager. FIGURE 7.1: Apache Spark Architecture If you already have a Spark cluster in your organization, you should request the connection information to this cluster from your cluster administrator, read their usage policies carefully and follow their advice. Since a cluster may be shared among many users, you want to make sure you only request the compute resources you need – we will cover how to request resources in the Tuning chapter. Your system administrator will describe if it’s an on-premise vs cloud cluster, the cluster manager being used, supported connections and supported tools. You can use this information to jump directly to Local, Standalone, YARN, Mesos, Livy or Kubernetes based on the information provided to you. Note: Once connected is performed with spark_connect(), you can use all techniques described in previous chapters using the sc connection; for instance, you can do data analysis or modeling with the same code previous chapters presented. 7.1.1 Edge Nodes Computing clusters are configured to enable high-bandwidth and fast network connectivity between nodes. To optimize network connectivity, the nodes in the cluster are configured to trust each other and to disable security features. This improves performance but requires the cluster to be secured by closing all external network communication, making the entire cluster secure as a whole. Except for a few cluster machines that are carefully configured to accept connections from outside the cluster; conceptually, these machines are located in the “edge” of the cluster and are known as edge nodes. Therefore, before connecting to Apache Spark, it is likely you will first have to connect to an edge node in your cluster. There are two methods to connect: Terminal Using a computer terminal application, one can use a secure shell to establish a remote connection into the cluster, once you connect into the cluster, you can launch R and then use sparklyr. However, a terminal can be cumbersome for some tasks, like exploratory data analysis, so it’s often only used while configuring the cluster or troubleshooting issues. Web Browser While using sparklyr from a terminal is possible, it is usually more productive to install a web server in an edge node that provides access to run R with sparklyr from a web browser. Most likely, you will want to consider using RStudio or Jupyter rather than connecting from the terminal. Figure 7.2 explains these concepts visually. The left block is usually your web browser, the right block is the edge node, client and edge node communicate over HTTP when using a web browser or SSH when using the terminal. FIGURE 7.2: Connecting to Sparks Edge Node 7.1.2 Spark Home After you connect to an edge node, the next step is to find out where Spark is installed, this location is known as the SPARK_HOME. In most cases, your cluster administrator will have already set the SPARK_HOME environment variable to the correct installation path. If not, you will need to find out the correct SPARK_HOME path. The SPARK_HOME path must be specified as an environment variable or explicitly when running spark_connect() using the spark_home parameter. If your cluster provider or cluster administrator already provided SPARK_HOME for you, the following code should return a path instead of an empty string. Sys.getenv(&quot;SPARK_HOME&quot;) If the code above returns an empty string, this would mean the SPARK_HOME environment vatiable is not set in your cluster, so you will have to specify SPARK_HOME while using spark_connect() as follows: sc &lt;- spark_connect(master = &quot;&lt;cluster-master&gt;&quot;, spark_home = &quot;local/path/to/spark&quot;) Where &lt;cluster-master&gt; is set to the correct cluster manager master for Spark Standalone, YARN, Mesos, Kubernetes or Livy. 7.2 Local When connecting to Spark in local mode, Spark starts a single process which runs most of the cluster components like the Spark context and a single executor. This is ideal to learn Spark, work offline, troubleshoot issues or to test code before you run it over a large compute cluster. A local connection to Spark is represented in Figure 7.3. FIGURE 7.3: Local Connection Diagram Notice that in the local connections diagram, there is no cluster manager nor worker process since, in local mode, everything runs inside the driver application. It’s also worth noting that sparklyr starts the Spark Context through spark-submit, a script available in every Spark installation to enable users to submit custom application to Spark which, sparklyr makes use of to submit itself to Spark. For the curious reader, the Contributing chapter explains the internal processes that takes place in sparklyr to submit this application and connect properly from R. To perform this local connection, we can connect with the following familiar code used in previous chapters: # Connect to local Spark instance sc &lt;- spark_connect(master = &quot;local&quot;) 7.3 Standalone Connecting to a Spark Standalone cluster requires the location of the cluster manager’s master instance, which can be found in the cluster manager web interface as described in the Standalone Clusters section. You can find this location by looking for a URL starting with spark://. A connection in standalone mode starts from sparklyr, which launches spark-submit, which then submits the sparklyr application, and creates the Spark Context, which requests executors from the Spark Standalone instance running under the given master address. Visually, this is described in Figure 7.4, which is quite similar to the overall connection architecture from Figure 7.1 but, with additional details that are particular to standalone clusters and sparklyr. FIGURE 7.4: Spark Standalone Connection Diagram In order to connect, use master = \"spark://hostname:port\" in spark_connect() as follows: sc &lt;- spark_connect(master = &quot;spark://hostname:port&quot;) 7.4 Yarn Hadoop YARN is the cluster manager from the Hadoop project, it’s the most common cluster manager which you are likely to find in clusters that started out as Hadoop clusters; with Cloudera, Hortonworks and MapR distributions as when using Amazon EMR. YARN supports two connection modes: YARN Client and YARN Cluster. However, YARN Client mode is much more common that YARN Cluster since it’s more efficient and easier to set up. 7.4.1 Yarn Client When connecting in YARN Client mode, the driver instance runs R, sparklyr and the Spark Context which requests worker nodes from YARN to run Spark executors as shown in Figure 7.5. FIGURE 7.5: YARN Client Connection Diagram To connect, one can simply run with master = \"yarn\" as follows: sc &lt;- spark_connect(master = &quot;yarn&quot;) Behind the scenes, when running YARN in client mode, the cluster manager will do what you would expect a cluster manager would do; it will allocate resources from the cluster and assign them to your Spark application, which the Spark Context will manage for you. The important piece to notice in Figure ?? is that, the Spark Context resides in the same machine where you run R code, this is different when running YARN in cluster mode. 7.4.2 Yarn Cluster The main difference between YARN in cluster mode and running YARN in client mode is that, in cluster mode, the driver node is not required to be the node where R and sparklyr were launched; instead, the driver node remains the designated driver node which is usually a different node than the edge node where R is running. It can be helpful to consider using cluster mode when the edge node has too many concurrent users, when is lacking computing resources, or where tools (like RStudio or Jupyter) need to be managed independently of other cluster resources. Figure 7.6 shows how the different components become decoupled when running in cluster mode. Notice there is still a line connecting the client with the cluster manager since, first of all, resources still need to be allocated from the cluster manager; however, once allocated, the client communicates directly with the driver node which will then communicate with the worker nodes. From this diagram, you might think that cluster mode looks much more complicated than the client mode diagram – this would be a correct assesment; therefore, it’s best to avoid cluster mode when possible due to additional configuration overhead that is best to avoid, if possible. FIGURE 7.6: YARN Cluster Connection Diagram To connect in YARN Cluster mode, we can simple run: sc &lt;- spark_connect(master = &quot;yarn-cluster&quot;) Cluster mode assumes that the node running spark_connect() is properly configured, meaning that, yarn-site.xml exists and the YARN_CONF_DIR environment variable is properly set. When using Hadoop as a file system, you will also need the HADOOP_CONF_DIR environment variable properly configured. In addition, you would need to have proper network connectivity between the client and the driver node, not just with sufficient bandwidth but also making sure both machines are reachable and no intermediate. This configuration is usually provided by your system administrator and is not something that you would have to manually configure. 7.5 Livy As opposed to other connection methods which require using an edge node in the cluster, Livy provides a Web API that makes the Spark cluster accessible from outside the cluster and does not require a Spark installation in the client. Once connected through the Web API, the Livy Service starts the Spark context by requesting resources from the cluster manager and distributing work as usual. Figure 7.7 illustrates a Livy connection, notice that the client connects remotely to the driver through a web API. FIGURE 7.7: Livy Connection Diagram Connecting through Livy requires the URL to the Livy service which should be similar to https://hostname:port/livy. Since remote connections are allowed, connections usually requires, at the very least, basic authentication: sc &lt;- spark_connect( master = &quot;https://hostname:port/livy&quot;, method = &quot;livy&quot;, config = livy_config( spark_version = &quot;2.4.0&quot;, username = &quot;&lt;username&gt;&quot;, password = &quot;&lt;password&gt;&quot; )) To try out Livy in your local machine, you can install and run a Livy service as described under the Livy Clusters section and then, connect as follows: sc &lt;- spark_connect( master = &quot;http://localhost:8998&quot;, method = &quot;livy&quot;, version = &quot;2.4.0&quot;) Once connected through Livy, you can make use of any sparklyr feature; however, Livy is not suitable for exploratory data analysis, since executing commands has a significant performance cost. That said, while running long running computations, this overhead could be considered irrelevant. In general, it is preferred to avoid using Livy and work directly within an edge node in the cluster; when this is not feasible, using Livy could be a reasonable approach. Note: Specifying the Spark version through the spark_version parameter is optional; however, when the version is specified, performance is significantly improved by deploying precompiled Java binaries compatible with the given version. Therefore, it is a best practice to specify the Spark version when connecting to Spark using Livy. 7.6 Mesos Similar to YARN, Mesos supports client mode and a cluster mode; however – sparklyr currently only supports client mode under Mesos. Therefore, the diagram from Figure 7.8, is equivalent to YARN Client’s diagram with only the cluster manager changed from YARN to Mesos. FIGURE 7.8: Mesos Connection Diagram Connecting requires the address to the Mesos master node, usually in the form of mesos://host:port or mesos://zk://host1:2181,host2:2181,host3:2181/mesos for Mesos using ZooKeeper. sc &lt;- spark_connect(master = &quot;mesos://host:port&quot;) The MESOS_NATIVE_JAVA_LIBRARY environment variable needs to be set by your system administrator, or manually set when running mesos on your local machine. For instance, in OS X, you can install and initialize Mesos from a terminal, followed by manually setting the mesos library and connecting with spark_connect(): brew install mesos /usr/local/Cellar/mesos/1.6.1/sbin/mesos-master --registry=in_memory --ip=127.0.0.1 MESOS_WORK_DIR=. /usr/local/Cellar/mesos/1.6.1/sbin/mesos-slave --master=127.0.0.1:5050 Sys.setenv(MESOS_NATIVE_JAVA_LIBRARY = &quot;/usr/local/Cellar/mesos/1.6.1/lib/libmesos.dylib&quot;) sc &lt;- spark_connect(master = &quot;mesos://localhost:5050&quot;, spark_home = spark_home_dir()) 7.7 Kubernetes Kubernetes cluster do not support client modes like Mesos or YARN; instead, the connection model is similar to YARN Cluster, where the driver node is assigned by Kubernetes. This is illustrated in Figure 7.9. FIGURE 7.9: Kubernetes Connection Diagram Kubernetes support is scheduled to be added to sparklyr with sparklyr/issues/1525, please follow progress for this feature directly in github. Once Kubernetes becomes supported in sparklyr, connecting to Kubernetes will work as follows: sc &lt;- spark_connect( master = &quot;k8s://https://&lt;apiserver-host&gt;:&lt;apiserver-port&gt;&quot;, config = list( spark.executor.instances = 2, spark.kubernetes.container.image = &quot;spark-image&quot; ) ) If your computer is already configured to use a Kubernetes cluster, you can use the following command to find the apiserver-host and apiserver-port: system2(&quot;kubectl&quot;, &quot;cluster-info&quot;) 7.8 Cloud When working with cloud providers, there are a few connection differences. For instance, connecting from Databricks requires the following connection method: sc &lt;- spark_connect(method = &quot;databricks&quot;) Since Amazon EMR makes use of YARN, you can connect using master = \"yarn\": sc &lt;- spark_connect(master = &quot;yarn&quot;) Connections to Spark when using IBM’s Watson Studio requires you to retrieve a configuration object through a load_spark_kernels() function IBM provides: kernels &lt;- load_spark_kernels() sc &lt;- spark_connect(config = kernels[2]) Under Microsoft Azure HDInsights and when using ML Services (R Server), creating an sparklyr connection gets initialized through: library(RevoScaleR) cc &lt;- rxSparkConnect(reset = TRUE, interop = &quot;sparklyr&quot;) sc &lt;- rxGetSparklyrConnection(cc) Connecting from Qubole requires using the qubole connection method: sc &lt;- spark_connect(method = &quot;qubole&quot;) Please reference your cloud provider documentation and their support channels if assistance is needed. 7.9 Batches Most of the time, sparklyr used interactively; as in, you explicitly connect with spark_connect() and then execute commands to analyze and model large-scale data. However, you can also automate processes by scheduling Spark jobs that use sparklyr. Spark does not provide tools to schedule data processing tasks; so instead, you would use other workflow management tools. This can be useful useful to transform data, prepare a model and score data overnight or to make use of Spark by other systems. As an example, you can create a file named batch.R with contents: library(sparklyr) sc &lt;- spark_connect(master = &quot;local&quot;) sdf_len(sc, 10) %&gt;% spark_write_csv(&quot;batch.csv&quot;) spark_disconnect(sc) You can then submit this application to Spark in batch mode using spark_submit(), the master parameter should be set to the appropriately. spark_submit(master = &quot;local&quot;, &quot;batch.R&quot;) You can also invoke spark-submit from the shell directly through: /spark-home-path/spark-submit --files batch.R --class sparklyr.Shell &#39;/spark-jars-path/sparklyr-2.3-2.11.jar&#39; 8880 12345 --batch batch.R The last parameters represent the port number 8880 and the session number, 12345, which can be set to any unique numeric identifier. You can use the following R code to get the correct paths: # Retrieve spark-home-path spark_home_dir() # Retrive spark-jars-path system.file(&quot;java&quot;, package = &quot;sparklyr&quot;) You can customize your script by passing additional command line arguments to spark-submit and then reading them back in R using commandArgs() 7.10 Tools When connecting to a Spark Cluster using tools like Jupyter and RStudio, you can run the same connection parameters presented in this chapater. However, since many cloud providers make use of a web proxy to secure Spark’s web interface, in order to use spark_web() or the RStudio connections pane extension, you will need to properly configure the sparklyr.web.spark setting which you would then pass to spark_config() through the config parameter. For instance, when using Amazon EMR, you can configure sparklyr.web.spark and sparklyr.web.yarn by dinamically retrieving the YARN application and building the EMR proxy URL: domain &lt;- &quot;http://ec2-12-345-678-9.us-west-2.compute.amazonaws.com&quot; config &lt;- spark_config() config$sparklyr.web.spark &lt;- ~paste0( domain, &quot;:20888/proxy/&quot;, invoke(spark_context(sc), &quot;applicationId&quot;)) config$sparklyr.web.yarn &lt;- paste0(domain, &quot;:8088&quot;) sc &lt;- spark_connect(master = &quot;yarn&quot;, config = config) 7.11 Multiple It is common to connect once, and only once, to Spark. However, you can also open multiple connections to Spark by connecting to different clusters or by specifying the app_name parameter. This can be helpful to compare Spark versions or validate your analysis before submitting to the cluster. The following example opens connections to Spark 1.6.3, 2.3.0 and Spark Standalone: # Connect to local Spark 1.6.3 sc_1_6_3 &lt;- spark_connect(master = &quot;local&quot;, version = &quot;1.6.3&quot;) # Connect to local Spark 2.3.0 sc_2_3_0 &lt;- spark_connect(master = &quot;local&quot;, version = &quot;2.3.0&quot;, appName = &quot;Spark23&quot;) # Connect to local Spark Standalone sc_standalone &lt;- spark_connect(master = &quot;spark://host:port&quot;) Finally, we can disconnect from each connection: spark_disconnect(sc_1_6_3) spark_disconnect(sc_2_3_0) spark_disconnect(sc_standalone) Alternatively, you can disconnect from all connections at once: spark_disconnect_all() 7.12 Troubleshooting Last but not least, we will introduce the following troubleshooting techniques: Logging, Spark Submit and Windows. When in doubt of where to start, start with the Windows section when using Windows systems, followed by Logging and closing with Spark Submit. This techniques are useful when running spark_connect() fails with an error message. 7.12.1 Logging The first technique to troubleshoot connections is to print Spark logs directly to the console to help you spot additional error messages: sc &lt;- spark_connect(master = &quot;local&quot;, log = &quot;console&quot;) In addition, you can enable verbose logging by setting the sparklyr.verbose option when connecting: sc &lt;- spark_connect(master = &quot;local&quot;, log = &quot;console&quot;, config = list(sparklyr.verbose = TRUE)) 7.12.2 Spark Submit You can diagnose if a connection issue is specific to R or Spark in general. This can be accomplished by running an example job through spark-submit and validating that no errors are thrown: # Find the spark directory using an environment variable spark_home &lt;- Sys.getenv(&quot;SPARK_HOME&quot;) # Or by getting the local spark installation spark_home &lt;- sparklyr::spark_home_dir() Then execute the sample compute Pi example by replacing \"local\" with the correct master parameter you are troubleshooting: # Launching a sample application to compute Pi system2( file.path(spark_home, &quot;bin&quot;, &quot;spark-submit&quot;), c( &quot;--master&quot;, &quot;local&quot;, &quot;--class&quot;, &quot;org.apache.spark.examples.SparkPi&quot;, file.path(spark_home, &quot;examples&quot;, &quot;jars&quot;, &quot;spark-examples_2.11-2.4.0.jar&quot;), 100) ) ... Pi is roughly 3.1415503141550314 ... If the above message is not displayed, you will have to investigate why your Spark cluster is not properly configured, which is beyond the scope of this book. When using a cloud provider or a Spark distribution, you can contact their support team to help you troubleshoot this further; otherwise, StackOverflow is a good place to start. If you do see the message above, this means your Spark cluster is properly configured but somehow, R is not being able to use Spark, so you will have to troubleshoot in-detail as we will explain next. 7.12.2.1 Detailed To troubleshoot the connection process in detail, you can manually replicate the two-step connection process, which is often very helpful to diagnose connection issues. Connecting to Spark is performed in two steps; first, spark-submit is triggered from R which submits the application to Spark, second, R connects to the running Spark application. First, identify the Spark installation directory and the path to the correct sparklyr*.jar by running: dir(system.file(&quot;java&quot;, package = &quot;sparklyr&quot;), pattern = &quot;sparklyr&quot;, full.names = T) Make sure you identify the correct version that matches your Spark cluster, for instance sparklyr-2.1-2.11.jar for Spark 2.1. Then, from the terminal, run: $SPARK_HOME/bin/spark-submit --class sparklyr.Shell $PATH_TO_SPARKLYR_JAR 8880 42 18/06/11 12:13:53 INFO sparklyr: Session (42) found port 8880 is available 18/06/11 12:13:53 INFO sparklyr: Gateway (42) is waiting for sparklyr client to connect to port 8880 The parameter 8880 represents the default port to use in sparklyr while 42 the session number, this is a cryptographically secure number generated bysparklyr, but for troubleshooting purposes can be as simple as42`. If this first connection step fails, it means that the cluster can’t accept the application. This usually means that there are not enough resources, there are permission restrictions, etc. The second step is to connect from R as follows – notice that there is a 60 seconds timeout, so you’ll have to run the R command after running the terminal command. If needed, this timeout can be configured as described in the Tuning chapter. library(sparklyr) sc &lt;- spark_connect(master = &quot;sparklyr://localhost:8880/42&quot;, version = &quot;2.3&quot;) If this second connection step fails, it usually means that there is a connectivity problem between R and the driver node. You can try using a different connection port, for instance. 7.12.3 Windows Connecting from Windows is, in most cases, as straightforward as connecting from Linux and OS X. However, there are a few common connection issues you should be aware of: Firewalls and antivirus software might block ports for your connection. The default port used by sparklyr is 8880; double check this port is not being blocked. Long path names can cause issues, specially in older Windows systems like Windows 7. When using these systems, try connecting with Spark installed with all folders using at most eight characters and no spaces in their names. 7.13 Recap This chapter presented an overview of Spark’s architecture, connection concepts and examples to connect in local mode, standalone, YARN, Mesos, Kubernetes and Livy. It also presented edge nodes and their role while connecting to Spark clusters. This should have provided you with enough information to successfully connect to any Apache Spark cluster. To troubleshoot connection problems beyond the techniques described in this chpater, it is recommended to search for the connection problem in StackOverflow, the sparklyr GitHub issues and, if needed, open a new GitHub issue in sparklyr to assist further. In the next chapter, Data, we will cover how to read and write over multiple data sources, help you understand how Spark makes use of Spark DataFrames, and describe how to import and export data from your Spark clusters. References "],
["data.html", "Chapter 8 Data 8.1 Source types and file systems 8.2 Reading data 8.3 Writing Data 8.4 Date &amp; time 8.5 Specific types and protocols 8.6 Recap", " Chapter 8 Data The goal of this chapter is to help you learn how to access, read and write data using Spark. It will provide the necessary background to help you work with a variety of data. This chapter will cover how to access data in different source types and file systems. It will show you the pattern of how to extend Spark’s capabilities to work with data no accessible “out-of-the-box”. Additionally, this chapter will introduce several recommendations. The recommendations will focused on improving performance and efficiency for writing or reading data. 8.1 Source types and file systems It may be challenging accessing data for the first time. The likely reasons are problems with a new source type, or file system. “Out-of-the-box”, Spark is able to interact with several source types and file system. Source types include: Comma separated values (CSV), Apache Parquet, and JDBC. File system protocols include: local file system (Linux, Windows, Mac), and Hadoop file System (HDFS). There is a way for Spark to interact with other source types and file systems. The next sub section will cover how to do that. 8.1.1 Default packages Spark is a very flexible computing platform. It can add functionality by using extension programs, called packages. Accessing a new source type or file system can be done by using the appropriate package. Packages need to be loaded into Spark at connection time. To load the package, Spark needs its location. The location could be inside the cluster, in a file share or the Internet. In sparklyr, the package location is passed to spark_connect(). All packages should be listed in the defaultPackages entry of the connection configuration. Here is an example that loads the package needed to access Amazon S3 buckets: conf &lt;- spark_config() conf$sparklyr.defaultPackages &lt;- &quot;org.apache.hadoop:hadoop-aws:2.7.7&quot; sc &lt;- spark_connect(master = &quot;local&quot;, config = conf) 8.1.2 Source types Spark can read and write several source types. In sparklyr, the source types are aligned to R functions: Format Read Write Comma separated values (CSV) spark_read_csv() spark_write_csv() JavaScript Object Notation (JSON) spark_read_json() spark_write_json() Library for Support Vector Machines (LIBSVM) spark_read_libsvm() spark_write_libsvm() Java Database Connectivity (JDBC) spark_read_jdbc() spark_write_jdbc() Optimized Row Columnar (ORC) spark_read_orc() spark_write_orc() Apache Parquet spark_read_parquet() spark_write_parquet() Text spark_read_text() spark_write_text() 8.1.2.1 New Source Type It is possible to access data source types not listed above. Loading the appropriate default package for Spark is the first of two steps The second step is to actually read or write the data. The spark_read_source() and spark_write_source() functions do that. They are generic functions that can use the libraries imported by a default package. The following example code shows how to use the datastax:spark-cassandra-connector package to read from Cassandra. The key is to use the org.apache.spark.sql.cassandra library as the source argument. It provides the mapping Spark can use to make sense of the data source. con &lt;- spark_config() conf$sparklyr.defaultPackages &lt;- &quot;datastax:spark-cassandra-connector:2.0.0-RC1-s_2.11&quot; sc &lt;- spark_connect(master = &quot;local&quot;, config = conf) spark_read_source( sc, name = &quot;emp&quot;, source = &quot;org.apache.spark.sql.cassandra&quot;, options = list(keyspace = &quot;dev&quot;, table = &quot;emp&quot;) ) 8.1.3 File systems Spark will default to the file system that it is currently running on. In a YARN managed cluster, the default file system will be HDFS. An example path of “/home/user/file.csv” will be read from cluster’s HDFS folders, and not the Linux folders. The Operating System’s file system will be accessed for other deployments, such as Stand Alone, and sparklyr’s local. The file system protocol can be changed when reading or writing. It is done via the path argument of the sparklyr function. For example, a full path of “file://home/user/file.csv” will force the use of the local Operating System’s file system. There are other file system protocols. An example is Amazon’s S3 service. Spark is does not know how to read the S3 protocol. Accessing the “s3a” protocol involves adding a package to the defaultPackages configuration variable passed at connection time. conf &lt;- spark_config() conf$sparklyr.defaultPackages &lt;- &quot;org.apache.hadoop:hadoop-aws:2.7.7&quot; sc &lt;- spark_connect(master = &quot;local&quot;, config = conf) my_file &lt;- spark_read_csv(sc, &quot;my-file&quot;, path = &quot;s3a://my-bucket/my-file.csv&quot;) Currently, only “file://” and “hdfs://” file protocols are supported when used in their respective environments. Accessing a different file protocol requires loading a default package. In some cases, the vendor providing the Spark environment could already be loading the package for you. 8.2 Reading data This section will introduce several techniques that improve the speed and efficiency of reading data. If new to Spark and sparklyr, it is highly recommended to review this section before starting work with large data sets. 8.2.1 Folders as a table Loading multiple files into a single data object is a common scenario. In R, we typically use a loop or functional programming to accomplish this. lapply(c(&quot;data-folder/file1.csv&quot;, &quot;data-folder/file2.csv&quot;), read.csv) In Spark, there is the notion of a folder as a table. Instead of enumerating each file, simply pass the path the containing folder’s path. Spark assumes that every file in that folder is part of the same table. This implies that the target folder should only be used for data purposes. spark_read_csv(sc, &quot;my_data&quot;, path = &quot;data-folder&quot;) The folder as a table notion is also found in other open source technologies. Under the hood, Hive tables work the same way. When querying a Hive table, the mapping is done over multiple files inside the same folder. The folder’s name usually match the name of the table visible to the user. 8.2.2 File layout When reading data, Spark is able to determine the data source’s column names and types. This comes at a cost. To determine the type Spark has to do an initial pass on the data, and then assign a type. For large data, this may add a significant amount of time to the data ingestion process. This can become costly even for medium size data loads. For files that are read over and over again, the additional read time accumulates over time. Spark allows the user to provide a column layout. If provided, Spark will bypass the step that it uses to determine the file’s layout. In sparklyr, the column argument is used to take advantage of this functionality. The infer_schema argument also needs to be set to FALSE. This arguments is the switch that indicates if the column argument should be used. For example, a file called test.csv is going to be loaded to Spark. This is its layout: &quot;x&quot;,&quot;y&quot; &quot;a&quot;,1 &quot;b&quot;,2 &quot;c&quot;,3 &quot;d&quot;,4 &quot;e&quot;,5 The column spec is started with a vector containing the column types. The vector’s values are named to match the field names. col_spec_1 &lt;- c(&quot;character&quot;, &quot;numeric&quot;) names(col_spec_1) &lt;- c(&quot;x&quot;, &quot;y&quot;) col_spec_1 ## x y ## &quot;character&quot; &quot;numeric&quot; The accepted variable types are: integer character logical double numeric factor Date POSIXct In spark_read_csv(), col_spec_1 is passed to the columns argument, and infer_schema is set to FALSE. sc &lt;- spark_connect(master = &quot;local&quot;) test_1 &lt;- spark_read_csv(sc, &quot;test1&quot;,&quot;test.csv&quot;, columns = col_spec_1, infer_schema = FALSE) test_1 ## # Source: spark&lt;test1&gt; [?? x 2] ## x y ## &lt;chr&gt; &lt;dbl&gt; ## 1 a 1 ## 2 b 2 ## 3 c 3 ## 4 d 4 ## 5 e 5 In the example we tried to match the names and types of the original file. The ability to pass a column spec provides additional flexibility. The following example shows how to set the field type to something different. The new field type needs a compatible type. For example, a character field could not be set tp numeric. The example also changes the names of the fields. col_spec_2 &lt;- c(&quot;character&quot;, &quot;character&quot;) names(col_spec_2) &lt;- c(&quot;my_letter&quot;, &quot;my_number&quot;) test_2 &lt;- spark_read_csv(sc, &quot;test2&quot;,&quot;test.csv&quot;, columns = col_spec_2, infer_schema = FALSE) test_2 # Source: spark&lt;test2&gt; [?? x 2] my_letter my_number &lt;chr&gt; &lt;chr&gt; 1 a 1 2 b 2 3 c 3 4 d 4 5 e 5 The ability to change the field type can be very useful. Malformed entries can cause error during reading. This is common in non-character fields. The practical approach is to import the field as a character field, and then use dplyr to coerce the field’s conversion. 8.2.3 Spark memory Spark copies the data into its distributed memory. This makes analyses and other processes very fast. There are cases where loading all of the data may not be practical, or necessary. For those cases, Spark can then just “map” the files without copying data into memory. The mapping creates a sort of “virtual” table in Spark memory. The implication is that when a query runs against that table, Spark has to read the data from the files at that time. Any consecutive read after that will do the same. In effect, Spark becomes a pass-through for the data. The advantage of this method is that there is almost no up-front time cost to “reading” the file. The mapping process is comparatively fast. In sparklyr, that is controlled by the memory argument of its read functions. Setting it to FALSE prevents the data copy. It defaults to TRUE. mapped_test &lt;- spark_read_csv(sc, &quot;test&quot;,&quot;test.csv&quot;, memory = FALSE) There are good use cases for this method. One of them is when not all columns of a table are needed. For example, take a very large file that contain many columns. This is not first time we interact with this data. We know what columns are needed for the analysis. The files can be read using memory = FALSE, and then select the needed columns with dplyr. The resulting dplyr variable can then be cached into memory, using the compute() function. This will make Spark query the file(s), pull the selected fields, and copy only that data into memory. The result is a in-memory table that took comparatively less time to ingest. mapped_test %&gt;% select(y) %&gt;% compute(&quot;test&quot;) 8.2.4 Column Names By default, sparklyr sanitizes column names. It translates characters such as . to _. This was required in Spark 1.6.X. To disable this functionality, you can run the following code: options(sparklyr.sanitize.column.names = FALSE) dplyr::copy_to(sc, iris, overwrite = TRUE) # Source: table&lt;iris&gt; [?? x 5] # Database: spark_connection Sepal.Length Sepal.Width Petal.Length Petal.Width Species &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 5.1 3.5 1.4 0.2 setosa 2 4.9 3 1.4 0.2 setosa 3 4.7 3.2 1.3 0.2 setosa 4 4.6 3.1 1.5 0.2 setosa 5 5 3.6 1.4 0.2 setosa 6 5.4 3.9 1.7 0.4 setosa 7 4.6 3.4 1.4 0.3 setosa 8 5 3.4 1.5 0.2 setosa 9 4.4 2.9 1.4 0.2 setosa 10 4.9 3.1 1.5 0.1 setosa # ... with more rows 8.3 Writing Data Some projects require that new data generated in Spark to be written back to a remote source. For example, the data could be new predicted values returned by a Spark model. The job processes the mass generation of predictions, and the predictions need to be stored. This section will cover recommendations when working on such projects. 8.3.1 Spark, not R, as pass-through Avoid collecting data in R to then upload it to the target (see Figure 8.1) That seems to be the first approach attempted by new users. It may look like a faster alternative. Performance wise, it is not faster. Additionally, this approach will not scale properly. The data will eventually grow to the point where R cannot handle being the middle point. FIGURE 8.1: Avoid using R as a pass through All efforts should be made to have Spark connect to the target location. This way, reading, processing and writing all happens within the same Spark session. FIGURE 8.2: Spark as a pass through 8.3.2 Practical approach Consider the following use scenario: A Spark job just processed predictions for a large data set. The data size of only the predictions are also considerable. Choosing a method to write results will depend on infrastructure. For example, Spark and the target location share the same infrastructure. For example, Spark and the target Hive table are in the same cluster. Copying the results is not a problem. The data transfer is between RAM and disk of the same cluster. A contrasting example, Spark and the target location are not in the same infrastructure. There are two options, choosing one will depend on the size of the data, and network speed: Spark connects to the remote target location, and copy the new data If this is done within the same Data Center, or cloud provider, the data transfer could be fast enough to have Spark write the data directly. Spark writes the results locally, and transfer the results via a third-party application For example, Spark could write the results into CSV files, and then have a separate job copy the files over via FTP. In the target location, use a separate process to transfer the data into the target location. Spark, R, and any other technology are tools. It is best to recognize that one tool cannot, and should not be expected to, do everything. 8.4 Date &amp; time Some Spark date/time functions make timezone assumptions. For instance, the following code makes use of to_date(). It assumes that the timestamp will be given in the local time zone. This is not to discourage use of date/time functions. Please be aware of time zones to be handled with care. sdf_len(sc, 1) %&gt;% transmute( date = timestamp(1419126103) %&gt;% from_utc_timestamp(&#39;UTC&#39;) %&gt;% to_date() %&gt;% as.character() ) 8.5 Specific types and protocols This section will cover techniques to help you interface with some of the most popular data types and protocols. 8.5.1 Amazon S3 Amazon Simple Storage Service, or S3, has become a common location to store file. Spark is able to directly access S3. This functionality can be used inside sparklyr. There are three key items to have, or use, when working with data from an S3 bucket: AWS Credentials - They are required by the S3 service, even for publicly accessible buckets. Hadoop-to-AWS package - It is loaded at connection time. A bucket location - The recommended file system to use is “s3a”. There are multiple ways to set the credentials to use to access the bucket. Please refer to the official documentation for more information. It is found in the Apache Spark official site (“Spark Integration with Cloud Infrastructures” 2019). The easiest way is to set the credentials using Environment variables. Choose a secure way to load the values into variables in R, and then load them into the appropriate Environment variable name. In case show below, AWS_ACCESS_KEY_ID and AWS_SECRET_ACCESS_KEY. Sys.setenv(AWS_ACCESS_KEY_ID = my_key_id) Sys.setenv(AWS_SECRET_ACCESS_KEY = my_secret_key) Spark requires an integration package in order to access Amazon S3 buckets. Interestingly, the package is not a Spark package, it is a Hadoop package. This means that the selected version will be a Hadoop version. After some experiments, it seems that with Spark versions 2, only up to Hadoop 2.7.7 will work. That may change when Spark enters version 3. If using a YARN managed cluster, the package may be different. That would depend on the Hadoop vendor. The official site for Apache based project is called Maven (“Maven Repository: Home Page” 2019). Please visit that site to find alternative package versions if the recommended one does not work. The recommended search term to use would be: “hadoop-aws”. conf &lt;- spark_config() conf$sparklyr.defaultPackages &lt;- &quot;org.apache.hadoop:hadoop-aws:2.7.7&quot; sc &lt;- spark_connect(master = &quot;local&quot;, config = conf) For the file system prefix use “s3a”. There are other options, such as “s3” and “s3n”. As per the Hadoop documents, the “s3a” file system should be the default selection. my_file &lt;- spark_read_csv(sc, &quot;my-file&quot;, path = &quot;s3a://my-bucket/my-file.csv&quot;) 8.5.2 SQL The sparklyr package provides a DBI compliant interface (???). This means that DBI functions can be used to interact with data via Spark. This includes non-SQL sources that are accessible via or cached in Spark. library(sparklyr) library(dplyr) sc &lt;- spark_connect(master = &quot;local&quot;) cars &lt;- copy_to(sc, mtcars, &quot;remote_mtcars&quot;) DBI::dbGetQuery(sc, &quot;SELECT * FROM remote_mtcars LIMIT 5&quot;) ## mpg cyl disp hp drat wt qsec vs am gear carb ## 1 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 ## 2 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 ## 3 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 ## 4 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 ## 5 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 8.5.3 Hive In YARN managed clusters, Spark provides a deeper integration with Apache Hive. Hive tables are easily accessible after opening a Spark connection. sc &lt;- spark_connect(master = &quot;yarn-client&quot;, spark_home = &quot;/usr/lib/spark/&quot;, version = &quot;2.1.0&quot;, config = conf) Accessing a Hive table’s data can be done with a simple reference. Using DBI, a table can be referenced within a SQL statement. DBI::dbSendQuery(sc, &quot;SELECT * FROM table limit 10&quot;) Another way to reference a table is with dplyr. The tbl() function, creates a pointer to the table. library(dplyr) t &lt;- tbl(sc, &quot;table&quot;) It is important to reiterate that no data is imported into R, the tbl() function creates only a reference. The expectation is that there will be more dplyr verbs following the tbl() command. t %&gt;% group_by(field1) %&gt;% summarise(totals = sum(field2)) 8.5.3.0.1 Database selection Hive table references assume a default database source. Often, the table needed table is in a different database within the Metastore. To access it using SQL, prefix the database name to the table. Separate them using a period. DBI::dbSendQuery(sc, &quot;SELECT * FROM databasename.table&quot;) In dplyr, the in_schema() function can be used. The function is used inside the tbl() call. tbl(sc, in_schema(&quot;databasename&quot;, &quot;table&quot;)) In sparklyr, the tbl_change_db() function sets the current session’s default database. Any subsequent call via DBI or dplyr will use the selected name as the default database. tbl_change_db(sc, &quot;databasename&quot;) 8.5.4 Comma Delimited Values (CSV) The CSV format may be the most common file type in use today. Spark offers a couple of techniques to help you troubleshoot issues when reading these kinds of files. Spark offers the following modes for addressing parsing issues: PERMISSIVE: NULLs are inserted for missing tokens. DROPMALFORMED: Drops lines which are malformed. FAILFAST: Aborts if encounters any malformed line. These can be used in sparklyr by passing them inside the options argument. The following example creates a file with a broken entry. It then shows how it can be read into Spark. library(sparklyr) sc &lt;- spark_connect(master = &quot;local&quot;) ## Creates bad test file writeLines(c(&quot;bad&quot;, 1, 2, 3, &quot;broken&quot;), &quot;bad.csv&quot;) spark_read_csv( sc, &quot;bad3&quot;, &quot;bad.csv&quot;, columns = list(foo = &quot;integer&quot;), infer_schema = FALSE, options = list(mode = &quot;DROPMALFORMED&quot;)) ## # Source: spark&lt;bad3&gt; [?? x 1] ## foo ## &lt;int&gt; ## 1 1 ## 2 2 ## 3 3 Spark 2 provides an issue tracking column. The column is hidden by default. To enable it, add _corrupt_record to the columns list. This can be combines with the use of the PERMISIVE mode. All rows will be imported, invalid entries will receive an NA, and the issue tracked in the _corrupt_record column. spark_read_csv( sc, &quot;bad2&quot;, &quot;bad.csv&quot;, columns = list(foo = &quot;integer&quot;, &quot;_corrupt_record&quot; = &quot;character&quot;), infer_schema = FALSE, options = list(mode = &quot;PERMISIVE&quot;) ) ## # Source: spark&lt;bad2&gt; [?? x 2] ## foo `_corrupt_record` ## &lt;int&gt; &lt;chr&gt; ## 1 1 NA ## 2 2 NA ## 3 3 NA ## 4 NA broken 8.6 Recap In the next chapter, Tuning, you will learn in-detail how Spark works and use this knowledge to optimize it’s resource usage and performance. References "],
["tuning.html", "Chapter 9 Tuning 9.1 Overview 9.2 Configuring 9.3 Partitioning 9.4 Caching 9.5 Shuffling 9.6 Serialization 9.7 Configuration Files 9.8 Recap", " Chapter 9 Tuning In previous chapters we’ve assumed that computation within a Spark cluster works efficiently. While this is true in some cases, it is often necessary to have some knowledge of the operations Spark runs internally to fine tune configuration settings that will make computations run efficiently. This chapter will explain how Spark computes data over large datasets and provide details on how to fine-tune its operations. For instance, you will learn how to request more compute nodes and increase the amount of memory which, if you remember from the Getting Started chapter, defaults to only 2GB in local instances. You will learn how Spark unifies computation through partioning, shuffling and caching. As mentioned a few chapters back, this is the last chapter describing the internals of Spark; once you complete this chapter, we believe that you will have the intermediate Spark skills necessary to be productive at using Spark. In subsequent chapters, Extensions, Distributed R and Streaming, you will learn exciting techniques to deal with specific modeling, scaling and computation problems. However, we must first understand how spark performs internal computations, what pieces we can control, and why. 9.1 Overview Spark performs distributed computation by configuring, partitioning, executing, shuffling, caching and serializing data, tasks and resources across multiple machines: Configuring requests the cluster manager for resources: total machines, memory, etc. Partitioning splits the data among various machines. Partitions can be either implicit or explicit. Executing means running an arbitrary transformation over each partition. Shuffling redistributes data to the correct machine. Caching preserves data in-memory across different computation cycles. Serializing transforms data to be sent over the network to other workers or back to the driver node. To illustrate each concept, we will create three partitions with unordered integers and then sort them with arrange(): data &lt;- copy_to(sc, data.frame(id = c(4, 9, 1, 8, 2, 3, 5, 7, 6)), repartition = 3) data %&gt;% arrange(id) %&gt;% collect() The diagram in Figure 9.1 shows how this sorting job would conceptually work across a cluster of machines. First, Spark would configure the cluster to use three worker machines. In this example, the numbers 1-9 are partitioned across three storage instances. Since the data is already partitioned, each worker node loads this implicit partition; for instance, 4,9,1 is loaded in the first worker node. Afterwards, a task is distributed to each worker to apply a transformation to each data partition in each worker node, this task is denoted by f(x). In this example, f(x) executes a sorting operation within a partition. Since Spark is general, execution over a partition can be as simple or complex as needed. The result is then shuffled to the correct machine to finish the sorting operation across the entire dataset, this completes a stage. A stage is a set of operations that Spark can execute without shuffling data between machines. Once the data is sorted across the cluster, the sorted results can be optionally cached in memory to avoid rerunning this computation multiple times. Finally, a small subset of the results is serialized, through the network connecting the cluster machines, back to the driver node to print a preview of this sorting example. FIGURE 9.1: Sorting Distributed Data with Apache Spark Notice that while Figure 9.1 describes a sorting operation, a similar approach applies to filtering or joining datasets and analyzing and modeling data at scale. Spark provides support to perform custom partitions, custom shuffling, etc; however, most of these lower level operations are not exposed in sparklyr; instead, sparklyr makes those operations available through higher level commands provided by data analysis tools like dplyr or DBI, modeling, and by using many extensions. For those few cases where you might need to implement lowe-level operations, you can always use the Spark’s Scala API through an sparklyr extensions or run custom distributed R code. In order to effectively tune Spark, we will start by getting familiar with Spark’s computation graph and Spark’s event timeline. Both are accessible through Spark’s web interface. 9.1.1 Graph Spark describes all computation steps using a Directed Acyclic Graph (DAG), which means that all computations in Spark move computation forward without repeating previous steps, this helps Spark optimize computations effectively. The best way to understand Spark’s computation graph for a given operation, sorting for our example, is to open the last completed query under the SQL tab in Spark’s web interface. Figure 9.2 show the resulting graph for this sorting operation, this graph contains the following operations: WholeStageCodegen: This block describes that the operations it contains were used to generate computer code that was efficiently translated to byte code. There is usually a small cost associated with translating operations into byte code, but this is a small cost to pay since then the operations can be execute much faster from Spark. In general, you can ignore this blog and focus on the blocks that it contains. InMemoryTableScan: This means that the original dataset data was stored in-memory and traversed row-by-row once. Exchange: Partitions were exchanged, read shuffled, across executors in your cluster. Sort: Once the records arrived at the right executor, they were sorted in this final stage. FIGURE 9.2: Spark Graph for a Sorting Query From the query details, you can then open the last Spark job to arrive to the job details page, you can then expand “DAG Visualization” to visualize a graph similar to Figure 9.3. This graph shows a few additional details and the stages in this job. Notice that there are no arrows pointing back to previous steps, since Spark makes use of acyclic graphs. FIGURE 9.3: Spark Graph for a Sorting Job Next, we will dive into a Spark stage and explore its event timeline. 9.1.2 Timeline The event timeline is a great summary of how Spark is spending computation cycles over each stage. Ideally, you want to see this timeline consisting of mostly CPU usage since other tasks can be considered overhead. You also want to see Spark using all the CPUs across all the cluster nodes available to you. Select the first stage in the current job and expand the “Event Timeline”, you should see a timeline similar to Figure 9.4. Notice that we explicitly requested three partitions, which is represented by three lanes in this visualization. FIGURE 9.4: Spark Event Timeline Since our machine is equiped with four CPUs, we can parallelize this computation even further by explicitly repartition data using sdf_repartition(): data %&gt;% sdf_repartition(4) %&gt;% arrange(id) %&gt;% collect() FIGURE 9.5: Spark Event Timeline with Additional Partitions Figure 9.5 now shows four execution lanes with most time spent under “Executor Computing Time”, which shows us that this particular operation is making better use of our compute resources. When working with clusters, requesting more compute nodes from your cluster should shorten computation time. In contrast, for timelines that show significant time spent shuffling, requesting more compute nodes might not shorten time and might actualy makes everything slower. There is no concrete set of rules to follow to optimize a stage; however, as you gain experience understanding this timeline over multiple operations – you will develop insights as to how to properly optimize Spark operations. 9.2 Configuring When tuning a Spark application, the most common resources to configure are memory and cores, specifically: Memory in Driver: The amount of memory required in the driver node. Memory per Worker: The amount of memory required in the worker nodes. Cores per Worker: The number of CPUs to required in the worker nodes. Number of Workers: The number of workers required for this session. Note: It is recommended to request significantly more memory for the driver than the memory available over each worker node. In most cases, you will want to request one core per worker. In local mode there are no workers, but we can still configure memory and cores to use through: # Initialize configuration with defaults config &lt;- spark_config() # Memory config[&quot;sparklyr.shell.driver-memory&quot;] &lt;- &quot;2g&quot; # Cores config[&quot;sparklyr.connect.cores.local&quot;] &lt;- 2 # Connect to local cluster with custom configuration sc &lt;- spark_connect(master = &quot;local&quot;, config = config) When using the Spark Standalone and the Mesos cluster managers, all the available memory and cores are assigned by default; therefore, there are no additional configuration changes required, unless, you want to restrict resources to allow multiple users to share this cluster. In this case you can use total-executor-cores to restrict the total executors requested. The “Spark Standalone” (“Spark Standalone Mode” 2018) and “Spark on Mesos” (“Running Spark on Mesos” 2018) guides provided additional information when sharing clusters. When running under YARN Client, you would configure memory and cores as follows: # Memory in Driver config[&quot;sparklyr.shell.driver-memory&quot;] &lt;- &quot;2g&quot; # Memory per Worker config[&quot;sparklyr.shell.executor-memory&quot;] &lt;- &quot;2g&quot; # Cores per Worker config[&quot;sparklyr.shell.executor-cores&quot;] &lt;- 1 # Number of Workers config[&quot;sparklyr.shell.num-executors&quot;] &lt;- 3 When using YARN in Cluster mode, sparklyr.shell.driver-cores can be used to configure total cores requested in the driver node. The “Spark on YARN” (“Running Spark on Yarn” 2018) guide provides additional configuration settings worth familiarizing yourself with. There are a few types of configuration settings: Connect settings are set as parameters to spark_connect(), they are common settings used while connecting. Submit settings are set while sparklyr is being submitted to Spark through spark-submit, some dependent on the cluster manager being used. Runtime settings configure Spark when the Spark session is created, these settings are independent to the cluster manager and specific to Spark. sparklyr settings configure sparklyr behaviour, these settings are independent to the cluster manager and particular to R. The following subsections present extensive lists of all the available settings. It is not required to fully understand them all while tuning Spark, but skimming through them could prove useful in the future while troubleshooting issues. You can also consider skipping the following settings subsections and use them instead as reference material as needed. 9.2.1 Connect Settings The following parameters can be used with spark_connect(), they configure high-level settings that define the connection method, Spark’s intallation path and the version of Spark to use. name value master Spark cluster url to connect to. Use “local” to connect to a local instance of Spark installed via spark_install(). spark_home The path to a Spark installation. Defaults to the path provided by the SPARK_HOME environment variable. If SPARK_HOME is defined, it will be always be used unless the version parameter is specified to force the use of a locally installed version. method The method used to connect to Spark. Default connection method is “shell” to connect using spark-submit, use “livy” to perform remote connections using HTTP, or “databricks” when using a Databricks clusters. app_name The application name to be used while running in the Spark cluster. version The version of Spark to use. Only applicable to “local” Spark connections. config Custom configuration for the generated Spark connection. See spark_config for details. You can configure additional settings by specifying a list in the config parameter, we will now learn what those settings can be. 9.2.2 Submit Settings Some settings must be specified when spark-submit (the terminal application that launches Spark) is run. For instance, since spark-submit launches driver node which runs as a Java instance, choosing how much memory is allocated needs to be specified as a parameter to spark-submit. You can list all the available spark-submit parameters by running: spark_home_dir() %&gt;% file.path(&quot;bin&quot;, &quot;spark-submit&quot;) %&gt;% system2() For readability, we’ve provided the output of this command in table format, replacing the spark-submit parameter with the appropriate spark_config() setting and removing the parameters that are not applicable or already presented in this chapter: name value sparklyr.shell.jars Specified as ‘jars’ parameter in ‘spark_connect()’. sparklyr.shell.packages Comma-separated list of maven coordinates of jars to include on the driver and executor classpaths. Will search the local maven repo, then maven central and any additional remote repositories given by ‘sparklyr.shell.repositories’. The format for the coordinates should be groupId:artifactId:version. sparklyr.shell.exclude-packages Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in ‘sparklyr.shell.packages’ to avoid dependency conflicts. sparklyr.shell.repositories Comma-separated list of additional remote repositories to search for the maven coordinates given with ‘sparklyr.shell.packages’ sparklyr.shell.files Comma-separated list of files to be placed in the working directory of each executor. File paths of these files in executors can be accessed via SparkFiles.get(fileName). sparklyr.shell.conf Arbitrary Spark configuration property set as PROP=VALUE. sparklyr.shell.properties-file Path to a file from which to load extra properties. If not specified, this will look for conf/spark-defaults.conf. sparklyr.shell.driver-java-options Extra Java options to pass to the driver. sparklyr.shell.driver-library-path Extra library path entries to pass to the driver. sparklyr.shell.driver-class-path Extra class path entries to pass to the driver. Note that jars added with ‘sparklyr.shell.jars’ are automatically included in the classpath. sparklyr.shell.proxy-user User to impersonate when submitting the application. This argument does not work with ‘sparklyr.shell.principal’ / ‘sparklyr.shell.keytab’. sparklyr.shell.verbose Print additional debug output. The remaining settings are specific to YARN: name value sparklyr.shell.queue The YARN queue to submit to (Default: “default”). sparklyr.shell.archives Comma separated list of archives to be extracted into the working directory of each executor. sparklyr.shell.principal Principal to be used to login to KDC, while running on secure HDFS. sparklyr.shell.keytab The full path to the file that contains the keytab for the principal specified above. This keytab will be copied to the node running the Application Master via the Secure Distributed Cache, for renewing the login tickets and the delegation tokens periodically. In general, any spark-submit setting is configured through sparklyr.shell.X, where X is the name of the spark-submit parameter without the -- prefix. 9.2.3 Runtime Settings As mentioned, some Spark settings configure the session runtime. The runtime settings are a superset of the submit settings since is usually helpful to retrieve the current configuration even if a setting can’t be changed. To list the Spark settings set in your current Spark session, you can run: spark_session_config(sc) name value spark.master local[4] spark.sql.shuffle.partitions 4 spark.driver.port 62314 spark.submit.deployMode client spark.executor.id driver spark.jars /Library/…/sparklyr/java/sparklyr-2.3-2.11.jar spark.app.id local-1545518234395 spark.env.SPARK_LOCAL_IP 127.0.0.1 spark.sql.catalogImplementation hive spark.spark.port.maxRetries 128 spark.app.name sparklyr spark.home /Users/…/spark/spark-2.3.2-bin-hadoop2.7 spark.driver.host localhost However, there are many more configuration settings available in Spark as described in the “Spark Configuration” (“Spark Configuration” 2018) guide. It is not in the scope of this book to describe them all so, if possible, take some time to identify the ones that might be of interest to your paritcular use cases. 9.2.4 sparklyr Settings Apart from Spark settings, there are a few settings particular to sparklyr listed below. You usually don’t use these settings while tuning SPark; instead, they are helpful while troubleshooting Spark from R. For instance, you can use sparklyr.log.console = TRUE to output the Spark logs into the R console, this is ideal while troubleshooting but too noisy otherwise. spark_config_settings() name description sparklyr.apply.packages Configures default value for packages parameter in spark_apply(). sparklyr.apply.rlang Experimental feature. Turns on improved serialization for spark_apply(). sparklyr.apply.schema.infer Number of rows collected to infer schema when column types specified in spark_apply(). sparklyr.arrow Use Apache Arrow to serialize data? sparklyr.backend.interval Total seconds sparklyr will check on a backend operation. sparklyr.backend.timeout Total seconds before sparklyr will give up waiting for a backend operation to complete. sparklyr.collect.batch Total rows to collect when using batch collection, defaults to 100,000. sparklyr.cancellable Cancel spark jobs when the R session is interrupted? sparklyr.connect.aftersubmit R function to call after spark-submit executes. sparklyr.connect.app.jar The path to the sparklyr jar used in spark_connect(). sparklyr.connect.cores.local Number of cores to use in spark_connect(master = “local”), defaults to parallel::detectCores(). sparklyr.connect.csv.embedded Regular expression to match against versions of Spark that require package extension to support CSVs. sparklyr.connect.csv.scala11 Use Scala 2.11 jars when using embedded CSV jars in Spark 1.6.X. sparklyr.connect.jars Additional JARs to include while submitting application to Spark. sparklyr.connect.master The cluster master as spark_connect() master parameter, notice that the ‘spark.master’ setting is usually preferred. sparklyr.connect.packages Spark packages to include when connecting to Spark. sparklyr.connect.ondisconnect R function to call after spark_disconnect(). sparklyr.connect.sparksubmit Command executed instead of spark-submit when connecting. sparklyr.connect.timeout Total seconds before giving up connecting to the sparklyr gateway while initializing. sparklyr.dplyr.period.splits Should ‘dplyr’ split column names into database and table? sparklyr.extensions.catalog Catalog PATH where extension JARs are located. Defaults to ‘TRUE’, ‘FALSE’ to disable. sparklyr.gateway.address The address of the driver machine. sparklyr.gateway.config.retries Number of retries to retrieve port and address from config, useful when using functions to query port or address in kubernetes. sparklyr.gateway.interval Total of seconds sparkyr will check on a gateway connection. sparklyr.gateway.port The port the sparklyr gateway uses in the driver machine. sparklyr.gateway.remote Should the sparklyr gateway allow remote connections? This is required in yarn cluster, etc. sparklyr.gateway.routing Should the sparklyr gateway service route to other sessions? Consider disabling in kubernetes. sparklyr.gateway.service Should the sparklyr gateway be run as a service without shutting down when the last connection disconnects? sparklyr.gateway.timeout Total seconds before giving up connecting to the sparklyr gateway after initialization. sparklyr.gateway.wait Total seconds to wait before retrying to contact the sparklyr gateway. sparklyr.livy.auth Authentication method for Livy connections. sparklyr.livy.headers Additional HTTP headers for Livy connections. sparklyr.livy.sources Should sparklyr sources be sourced when connecting? If false, manually register sparklyr jars. sparklyr.log.invoke Should every call to invoke() be printed in the console? Can be set to ‘callstack’ to log call stack. sparklyr.log.console Should driver logs be printed in the console? sparklyr.progress Should job progress be reported to RStudio? sparklyr.progress.interval Total of seconds to wait before attempting to retrieve job progress in Spark. sparklyr.sanitize.column.names Should partially unsupported column names be cleaned up? sparklyr.stream.collect.timeout Total seconds before stopping collecting a stream sample in sdf_collect_stream(). sparklyr.stream.validate.timeout Total seconds before stopping to check if stream has errors while being created. sparklyr.verbose Use verbose logging across all sparklyr operations? sparklyr.verbose.na Use verbose logging when dealing with NAs? sparklyr.verbose.sanitize Use verbose logging while sanitizing columns and other objects? sparklyr.web.spark The URL to Spark’s web interface. sparklyr.web.yarn The URL to YARN’s web interface. sparklyr.worker.gateway.address The address of the worker machine, most likely localhost. sparklyr.worker.gateway.port The port the sparklyr gateway uses in the driver machine. sparklyr.yarn.cluster.accepted.timeout Total seconds before giving up waiting for cluster resources in yarn cluster mode. sparklyr.yarn.cluster.hostaddress.timeout Total seconds before giving up waiting for the cluster to assign a host address in yarn cluster mode. sparklyr.yarn.cluster.lookup.byname Should the current user name be used to filter yarn cluster jobs while searching for submitted one? sparklyr.yarn.cluster.lookup.prefix Application name prefix used to filter yarn cluster jobs while searching for submitted one. sparklyr.yarn.cluster.lookup.username The user name used to filter yarn cluster jobs while searching for submitted one. sparklyr.yarn.cluster.start.timeout Total seconds before giving up waiting for yarn cluster application to get registered. 9.3 Partitioning As mentioned in the introduction chapter, MapReduce and Spark were designed with the purpose of performing computations against data stored across many machines. The subset of the data available for computation over each compute instance is known as a partition. By default, Spark will compute over each existing implicit partition since it’s more effective to run computations were the data is already located. However, there are cases where you will want to set an explicit partition to help Spark use more efficient use of your cluster resources. 9.3.1 Implicit You learned in the Data chapter that Spark can read data stored in many formats and different storage systems; however, since shuffling data is an expensive operations, Spark executes tasks reusing the partitions in the storage system. Therefore, these partitions are implicit to Spark since they are already well defined and expensive to rearrange. There is always an implicit partition for every computation in Spark defined by the distributed storage system, by creating a dataset or by copying datasets into Spark. You can explore the number of partitions a computation will require through sdf_num_partitions(): sdf_len(sc, 10) %&gt;% sdf_num_partitions() [1] 2 While in most cases the default partitions works just fine, there are cases where we you will need to be explicit on the partitions you choose. 9.3.2 Explicit There will be times when you have many more compute instances than data partitions, or much less compute instances than the number of partitions in your data. In both cases, it can help to repartition data to match your cluster resources. Various data functions, like spark_read_csv(), already support a repartition parameter to request Spark to repartition data appropriately. For instance, we can create a sequence of 10 numbers partitioned by 10 as follows: sdf_len(sc, 10, repartition = 10) %&gt;% sdf_num_partitions() [1] 10 For datasets that are already partitioned, we can also use sdf_repartition: sdf_len(sc, 10, repartition = 10) %&gt;% sdf_repartition(4) %&gt;% sdf_num_partitions() [1] 4 The number of partitions usually significantly changes the speed and resources being used; for instance, the following example calculates the mean over 10M rows with different partition sizes. library(microbenchmark) library(ggplot2) microbenchmark( &quot;1 Partition(s)&quot; = sdf_len(sc, 10^7, repartition = 1) %&gt;% summarise(mean(id)) %&gt;% collect(), &quot;2 Partition(s)&quot; = sdf_len(sc, 10^7, repartition = 2) %&gt;% summarise(mean(id)) %&gt;% collect(), times = 10 ) %&gt;% autoplot() + theme_light() FIGURE 9.6: Computation speed when using explicit Spark partitions The results show that sorting data with two partitions is almost twice as fast; this is the case since two CPUs can be used to execute this operation. However, it is not necessarily the case that higher-partitions produce faster computation; instead, partitioning data is particular to your computing cluster and the data analysis operations being performed. 9.4 Caching Recall from the introduction that Spark was designed to be faster than its predecessors by using memory instead of disk to store data. This is formally known as a Spark RDD and stands for resilient distributed dataset. An RDD distributes copies of the same data across many machines, such that, if one machine fails other can complete the task – hence the resilient name. Resiliency is important in distributed systems since, while things will usually work in one machine, when running over thousands of machines the likelihood of something failing is much higher. When a failure happens, it is preferable to be fault tolerant to avoid losing the work of all the other machines. RDDs accomplish this by tracking data lineage information to rebuild lost data automatically on failure. In sparklyr, you can control when an RDD gets loaded or unloaded from memory using tbl_cache() and tbl_uncache(). Most sparklyr operations that retrieve a Spark DataFrame, cache the results in-memory. For instance, running spark_read_parquet() or copy_to() will provide a Spark DataFrame that is already cached in-memory. As a Spark DataFrame, this object can be used in most sparklyr functions, including data analysis with dplyr or machine learning. library(sparklyr) sc &lt;- spark_connect(master = &quot;local&quot;) iris_tbl &lt;- copy_to(sc, iris, overwrite = TRUE) You can inspect which tables are cached by navigating to the Spark UI using spark_web(sc), opening the storage tab, and clicking on a given RDD: FIGURE 9.7: Cached RDD in Spark Web Interface Data loaded in memory will be released when the R session terminates either explicitly or implicitly with a restart or disconnection; however, to free up resources, you can use tbl_uncache(): tbl_uncache(sc, &quot;iris&quot;) 9.4.1 Checkpointing Checkpointing is a slightly different type of caching; while it also persists data it will, additionally, break the graph computation lineage. For example, if a cached partition is lost, it can be computed from the computation graph which is not possible while checkpointing since the source of computation is lost. When performing expensive computation graphs, it can make sense to checkpoint to persist and break the computation lineage in order to help Spark reduce graph computation resources; otherwise, Spark might try to over-optimize a computation graph that is really not useful to optimize. You can checkpoint explicitly by saving to CSV, Parquet, etc. files. Or let Spark checkpoint this for you using sdf_checkpoint() in sparklyr as follows. Notice that checkpointing truncates the computation lineage graph, which can speed up performance if the same intermediate result is used multiple times. 9.4.2 Memory Memory in Spark is categorized into reserved, user, execution or storage: Reserved: Reserved memory is the memory required by Spark to function and therefore, is overhead that is required and should not be configured. This value defaults to 300MB. User: User memory is the memory used to execute custom code. sparklyr only makes use of this memory indirectly when executing dplyr expressions or modeling a dataset. Execution: Execution memory is used to execute code by Spark, mostly, to process the results from the partition and perform shuffling. Storage: Storage memory is used to cache RDDs, for instance, when using tbl_cache() in sparklyr. As part of tuning execution, you can consider tweaking the amount of memory allocated for user, execution and storage by creating a Spark connection with different values than the defaults provided in Spark: config &lt;- spark_config() # define memory available for storage and execution config$spark.memory.fraction &lt;- 0.75 # define memory available for storage config$spark.memory.storageFraction &lt;- 0.5 For instance, if you want to use Spark to store large amounts of data in-memory with the purpose of filtering and retrieving subsets quickly, you can expect Spark to use little execution or user memory. Therefore, to maximize storage memory, one can tune Spark as follows: config &lt;- spark_config() # define memory available for storage and execution config$spark.memory.fraction &lt;- 0.90 # define memory available for storage config$spark.memory.storageFraction &lt;- 0.90 However, notice that Spark will borrow execution memory from storage and viceversa if needed and if possible; therefore, in practice, there should be little need to tune the memory settings. 9.5 Shuffling Shuffling, is the operation that redistributes data across machines; it is usually an expensive operation and therefore, one we try to minimize. One can easily identify if significant time is being spent shuffling by looking at the event timeline. It is possible to reduce shuffling by reframing data analysis questions or hinting Spark appropriately. This would be relevant, for instance, when joining data frames that differ in size significantly, as in, one set being orders of magnitude smaller than the other one. You can consider using sdf_broadcast() to mark a data frame as small enough for use in broadcast joins, meaning, it pushes one of the smaller data frames to each of the worker nodes to reduce shuffling the bigger dataframe. One example for sdf_broadcast() follows: sdf_len(sc, 10000) %&gt;% sdf_broadcast() %&gt;% left_join(sdf_len(sc, 100)) 9.6 Serialization Serialization is the process of translating data and tasks into a format that can be transmitted between machines and reconstructed on the receiving end. It is not that common to have to adjust serialization when tuning Spark; however, it is worth mentioning there are alternative serialization modules like the Kryo Serializer that can provide performance improvements over the default Java Serializer. The Kryo Serializer can be enabled in sparklyr through: config &lt;- spark_config() config$spark.serializer &lt;- &quot;org.apache.spark.serializer.KryoSerializer&quot; sc &lt;- spark_connect(master = &quot;local&quot;, config = config) 9.7 Configuration Files Configuring the spark_config() settings before connecting is the most common approach while tuning Spark. However, once the desired connection is known, you should consider switching to use a configuration file since it will remove the clutter in your connection code and also allow you to share the configuration settings across projects and coworkers. For instance, instead of connecting to Spark through: config &lt;- spark_config() config[&quot;sparklyr.shell.driver-memory&quot;] &lt;- &quot;2G&quot; sc &lt;- spark_connect(master = &quot;local&quot;, config = config) You can instead define a config.yml with the desired settings. This file should be located in the current working directory or in parent directories. For example, you can create the following config.yml file to modify the default driver memory: default: sparklyr.shell.driver-memory: 2G Then, connecting with the same configuration settings becomes much cleaner by using instead: sc &lt;- spark_connect(master = &quot;local&quot;) You can also specify an alternate config file name or location by setting the file parameter in spark_config(). One additional benefit from using configuration files, is that a system administrator can change the default configuration file without modifying the original one and additional functionality provided by the config package, see github.com/rstudio/config. 9.8 Recap This chapter provided a broad overview of Spark internals and a detailed configuration settings to help you speed up computation and enable high computation loads. It provided the foundations to understand bottlenecks and guidance on common configuration considerations; however, fine-tuning Spark is a broad topic that would require many more chapters to cover extensively. Therefore, while troubleshooting Spark’s performance and scalability, searching the web and consulting online communities it is often necessary to fine-tune your particular environment. The next chapter, Extensions, introduces the ecosystem of Spark extensions available in R. Most extensions are highly-specialized, but they will prove to be extremly useful in specific cases and readers with particular needs. For instance, they can process nested data, perform graph analysis or use different modeling libraries like rsparkling from H20. Not only that, but the next few chapters introduce many advanced data analysis and modeling topics that are required to master large-scale computing in R. References "],
["extensions.html", "Chapter 10 Extensions 10.1 Overview 10.2 H2O 10.3 Graphs 10.4 XGBoost 10.5 Deep Learning 10.6 Genomics 10.7 Spatial 10.8 Troubleshooting 10.9 Recap", " Chapter 10 Extensions In the previous chapter, Tuning, you learned how Spark processes data at large-scale by allowing users to configure the cluster resources, partition data implicitly or explicitly, execute commands across distributed compute nodes, shuffle data across them when needed, cache data to improve performance and serialize data efficiently over the network. You also learned how to configure the different Spark settings used while connecting, submitting a job, running and application and particular settings applicable only to R and R extensions that we will present in this chapter. The Analysis, Modeling and Data chapters provided a foundation to read and understand most datasets. However, the functionality that was presented was scoped to Spark’s built-in features and tabular datasets. This chapter will go beyond tabular data and explore how to analyze and model networks of interconnected objects through graph processing, read genomics datasets, prepare data for deep learning, analyze geographic datasets and use advanced modeling libraries like H2O and XGBoost over large-scale datasets. The combination of all the content presented in all the previous chapters should take care of most of your large-scale computing needs. However, for those few use cases where functionality is still lacking, the following chapters will teach you provide the tools to extend Spark yourself; either, through custom R transformation, custom Scala code or through recent new execution mode in Spark that enable analyzing realtime datasets. Although, before reinventing the wheel, we will present all the extensions available when using Spark with R. 10.1 Overview In the Introduction chapter we presented the R community as a vibrant group of individuals collaborating with each other in many ways, one of them, by moving open science forward by creating R packages that can be installed from CRAN. In a similar way, but in a much smaller scale, the R community has contributed extensions that increase the functionality initially supported in Spark and R. Spark itself also provides support for creating Spark extensions and, in-fact, many R extensions make use of Spark extensions. Extensions are constantly being created so this section will be outdated at any given point in time, in addition, we might not be even aware of many Spark and R extensions; however, at the very least we can track the extensions that are available in CRAN by looking at the “reverse imports” for sparklyr in CRAN (“CRAN - Package Sparklyr” 2019). Extensions and R packages published in CRAN tend to be the most stable since when a package is published in CRAN, it will go through a review process which increases the overall quality of a contribution. While we wish we could present all the extensions, we will present next the extensions that should be the most interesting to most readers. You can find additional ones under the github.com/r-spark organization or by searching repos in GitHub with the sparklyr tag. rsparkling The rsparkling extensions allows you to use H2O and Spark from R. This extension is what we would consider advanced modeling in Spark. While Spark’s built-in modeling library, Spark MLlib, is quite useful in many cases; H2O’s modeling capabilities can compute additional statistical metrics and can proivide performance and scalability improvements over Spark MLlib. We, ourselves, have not performed detailed comparisons nor benchamarks between MLlib and H2O; so this is something you will have to research on your own to create a complete picture of when to use H2O’s capabilities. graphframes The graphframes extensions adds support to process graphs in Spark. A graph is a structure that describes a set of objects in which some pairs of the objects are in some sense related. As you learned in the Introduction chapter, ranking web pages was an early motivation to develop precursos to Spark powered by MapReduce; web pages happen to form a graph if you consider a link between pages as the relationship between each pair of pages. Computing operations likes PageRank over graphs can be quite useful in web search and social networks to mention a few applications. sparktf The sparktf extension provides support to write TensorFlow records in Spark. TensorFlow is one of the leading deep learning frameworks and it is often used with large amounts of numerical data represented as TensorFlow records, a file format optimized for TensorFlow. Spark it is often used to process unstructured and large-scale datasets into smaller numerical datasets that can easily fit into a GPU. You can use this extension to save datasets in the TensorFLow record file format. xgboost The xgboost extension brings the well-known XGBoost modeling library to the world of large-scale computing. XGBoost is a scalable, portable and distributed library for gradient boosting. It became well known in the machine learning competition circles after its use in the winning solution of the Higgs Machine Learning Challenge (“Higgs Boson Machine Learning Challenge” 2019) and has remain popular in other Kaggle competitions since then. variantspark The variantspark extension provides an interface to use Variant Spark, a scalable toolkit for genome-wide association studies (GWAS). It currently provides functionality to build random forest models, estimating variable importance and reading Variant Call Format (VCF) files. While there are other random forest implementations in Spark, most of them are not optimized to deal with GWAS datasets, which usually come with thousands of samples and millions of variables. geospark The geospark extensions enables us to load and query large-scale geographic datasets. Usually datasets containing latitude and longitude points or complex areas defined in the Well-known Text (WKT) format, a text markup language for representing vector geometry objects on a map. Before you learn how and when to use each extension, we should first briefly explain how extensions can be used with R and Spark. First, an Spark extension is just and R package that happens to be aware of Spark. As any other R package, you will first have to install the R package. Once installed, it is important to know that you will need to reconnect to Spark before the extension can be used. So, in general, the pattern you should follow goes as follows: library(sparkextension) library(sparklyr) sc &lt;- spark_connect(master = &quot;&lt;master&gt;&quot;) Notice that sparklyr is loaded after the extensions to allow the extension to register properly. If you had to install and load a new extension you would simply have to disconnect first using spark_disconnect(sc) and repeat the steps above with the new extension. It’s not hard to install and use Spark extensions from R; however, each extension can be a world on it’s own so most of the time you will have to spend time understand what the extension is, when to use it and how to use it properly. The first extension you will learn about is the rsparkling extension which enables you to use H2O in Spark with R. 10.2 H2O H2O is open-source software for large-scale modeling created by H2O.ai, which allows you to fit thousands of potential models as part of discovering patterns in data. You can consider using H2O to complement or replace Spark’s default modeling algorithms. It is common to Spark’s default modeling algorithms and transition to H2O when Spark’s algorithms fall short or when advanced functionality (like additional modeling metrics or automatic model selection) are desired We can’t do justice to H2O’s great modeling capabilities in a single paragraph, explaining H2O properly will require a book in itself. Instead, we would like to recommend reading the “Practical machine learning with H2O” (Cook 2016) book to explore in-depth H2O’s modeling algorithms and features. In the meantime, you can use this section as a brief guide to get started using H2O in Spark with R. In order to use H2O with Spark, it is important to know that there are four compoinents involved: H2O, Sparkling Water, [rsparkling][rsparkling](https://github.com/h2oai/sparkling-water/tree/master/r) and Spark. Sparkling Water allows users to combine the fast, scalable machine learning algorithms of H2O with the capabilities of Spark. You can think of Sparkling Water as a component bridging Spark with H2O and rsparkling as the R front-end for Sparkling Water, this is illustrated in Figure 10.1. FIGURE 10.1: H2O components with Spark and R First, install rsparkling and h2o: install.packages(&quot;rsparkling&quot;) install.packages(&quot;h2o&quot;) Is is then important to notice that you need to use compatible versions of Spark, Sparkling Water and H2O. So let’s start by checking the version of H2O by running, packageVersion(&quot;h2o&quot;) ## [1] &#39;3.22.1.3&#39; Then we can explore the Spark and Sparkling Water versions that are compatible with h2o_release_table(). rsparkling::h2o_release_table() %&gt;% dplyr::filter(H2O_Version == !!as.character(packageVersion(&quot;h2o&quot;))) # A tibble: 4 x 5 Spark_Version Sparkling_Water_V… H2O_Version H2O_Release_Name H2O_Release_Patch… &lt;dbl&gt; &lt;fct&gt; &lt;fct&gt; &lt;fct&gt; &lt;int&gt; 1 2.4 2.4.5 3.22.1.3 rel-xu 3 2 2.3 2.3.23 3.22.1.3 rel-xu 3 3 2.2 2.2.34 3.22.1.3 rel-xu 3 4 2.1 2.1.48 3.22.1.3 rel-xu 3 We can then connect with the supported Spark versions as follows, you will have to adjust the master parameter for your particular cluster. library(rsparkling) library(sparklyr) library(h2o) sc &lt;- spark_connect(master = &quot;local&quot;, version = &quot;2.3&quot;) cars &lt;- copy_to(sc, mtcars) H2O provides a web interface which can help you monitor training and access much of H2O’s functionality. The web interface can be accessed through h2o_flow(sc), it is reffered to as H2O Flow and is shown in Figure 10.2. FIGURE 10.2: H2O Flow Interface using Spark with R When using H2O, you will have to convert your Spark DataFrame into and H2O DataFrame through as_h2o_frame: cars_h2o &lt;- as_h2o_frame(sc, cars) cars_h2o mpg cyl disp hp drat wt qsec vs am gear carb 1 21.0 6 160 110 3.90 2.620 16.46 0 1 4 4 2 21.0 6 160 110 3.90 2.875 17.02 0 1 4 4 3 22.8 4 108 93 3.85 2.320 18.61 1 1 4 1 4 21.4 6 258 110 3.08 3.215 19.44 1 0 3 1 5 18.7 8 360 175 3.15 3.440 17.02 0 0 3 2 6 18.1 6 225 105 2.76 3.460 20.22 1 0 3 1 [32 rows x 11 columns] Then you can use many of the modeling functions available in the h2o package with ease. For instance, we can fit a generalized linear model with ease: model &lt;- h2o.glm(x = c(&quot;wt&quot;, &quot;cyl&quot;), y = &quot;mpg&quot;, training_frame = cars_h2o, lambda_search = TRUE) H2O provides additional metrics not necessarily available in Spark’s modeling algorithms, the model that we just fit Residual Deviance is provided in the model while this would not be a standard metric when using Spark MLlib. model ... MSE: 6.017684 RMSE: 2.453097 MAE: 1.940985 RMSLE: 0.1114801 Mean Residual Deviance : 6.017684 R^2 : 0.8289895 Null Deviance :1126.047 Null D.o.F. :31 Residual Deviance :192.5659 Residual D.o.F. :29 AIC :156.2425 Then you can run prediction over the generalized linear model model, a similar approach would work for many other models available in H2O: predictions &lt;- as_h2o_frame(sc, copy_to(sc, data.frame(wt = 2, cyl = 6))) h2o.predict(model, predictions) predict 1 24.05984 [1 row x 1 column] H2O can also be used to perform automatic training and tuning of many models; meaning that, H2O can choose which model to use for you using h2o.automl: automl &lt;- h2o.automl(x = c(&quot;wt&quot;, &quot;cyl&quot;), y = &quot;mpg&quot;, training_frame = cars_h2o, max_models = 20, seed = 1) For this particular dataset, H2O finds out that XGBoost is a better fit than GLM. Specifically, the H2O explored using XGBoost, Deep Learning, GLM and a Stacked Ensemble. automl model_id mean_residual_deviance rmse mse mae rmsle 1 XGBoost_1_... 6.627278 2.574350 6.627278 2.066412 0.1329469 2 DeepLearning_... 6.945850 2.635498 6.945850 2.209201 0.1258008 3 XGBoost_grid_1_... 7.025614 2.650587 7.025614 2.192791 0.1339280 4 XGBoost_grid_1_... 7.266691 2.695680 7.266691 2.167930 0.1331849 5 GLM_grid_... 7.416367 2.723301 7.416367 2.184664 0.1269808 6 StackedEnsemble... 7.596133 2.756108 7.596133 2.029900 0.1340302 Many additional examples are available under spark.rstudio.com/guides/h2o, you can also request help from github.com/h2oai/sparkling-water/tree/master/r, the official GitHub repository for the rsparkling package. The next extension, graphframes, will allow you to process large-scale relational datasets; however, before you start using it, make make sure to disconnect with spark_disconnect(sc) since using a different extensions requires you to reconnect to Spark. 10.3 Graphs The first paper in the history of graph theory was written by Leonhard Euler on the Seven Bridges of Königsberg in 1736. The problem was to devise a walk through the city that would cross each of those bridges once and only, the original diagram is shown in Figure 10.3. FIGURE 10.3: Seven Bridges of Königsberg from the Euler Archive Today, a graph is defined as an ordered pair \\(G=(V,E)\\), with \\(V\\) a set of vertices (nodes or points) and \\(E \\subseteq \\{\\{x, y\\} | (x, y) ∈ \\mathrm{V}^2 \\land x \\ne y\\}\\) a set of edges (links or lines) which are either an unordered pair for undirected graphs or an ordered pair for directed graphs. The former describing links where the direction does not matter and the latter linked where it does. As a simple example, we can use the highschool dataset from the ggraph package which tracks friendship among high school boys. In this dataset, the vertices are the students and the edges describe pairs of students who happen to be friends in a particular year. library(ggraph) library(igraph) highschool # A tibble: 506 x 3 from to year &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 1 14 1957 2 1 15 1957 3 1 21 1957 4 1 54 1957 5 1 55 1957 6 2 21 1957 7 2 22 1957 8 3 9 1957 9 3 15 1957 10 4 5 1957 # … with 496 more rows While the highschool dataset can be easily processed in R, even medium size graph datasets can be hard to process without distributing this work across a cluster of machines which Spark is suited for. Spark supports processing graphs through the graphframes extension which in turn uses the GraphX Spark component. GraphX is Apache Spark’s API for graphs and graph-parallel computation and allows you to seamlessly work with both graphs and collection, it’s comparable in performance to the fastest specialized graph processing systems and provides a growing library of graph algorithms. A graph in Spark is also represented as a dataframe of edges and vertices; however, the format is slightly different since we will need to construct a dataframe for vertices. Lets first install the GraphFrames extension, install.packages(graphframes) Followed by connecting, copying the highschool dataset and transforming the graph to the format that this extension expects, we will scope this dataset to the friendships of year 1957. The vertices_tbl table is expected to have a single id column: # Source: spark&lt;?&gt; [?? x 1] id &lt;chr&gt; 1 1 2 34 3 37 4 43 5 44 6 45 7 56 8 57 9 65 10 71 # … with more rows While the edges_tbl is expected to have a src and dst columns: # Source: spark&lt;?&gt; [?? x 2] src dst &lt;chr&gt; &lt;chr&gt; 1 1 14 2 1 15 3 1 21 4 1 54 5 1 55 6 2 21 7 2 22 8 3 9 9 3 15 10 4 5 # … with more rows You can now create a GraphFrame, graph &lt;- gf_graphframe(vertices_tbl, edges_tbl) We can now use this graph to start analyzing this dataset. For instance, by finding out how many friends on average every one has, this is reffered as the degree or valency of a vertex: gf_degrees(graph) %&gt;% summarise(friends = mean(degree)) # Source: spark&lt;?&gt; [?? x 1] friends &lt;dbl&gt; 1 6.94 We can then find what the shortest path to some specific vertex (person for this dataset). Since the data is annonimized, we can just pick the person identified as \\(33\\) and find how many degrees of separation exist between them: gf_shortest_paths(graph, 33) %&gt;% filter(size(distances) &gt; 0) %&gt;% mutate(distance = explode(map_values(distances))) %&gt;% select(id, distance) # Source: spark&lt;?&gt; [?? x 2] id distance &lt;chr&gt; &lt;int&gt; 1 19 5 2 5 4 3 27 6 4 4 4 5 11 6 6 23 4 7 36 1 8 26 2 9 33 0 10 18 5 # … with more rows Finally, we can compute PageRank over this graph, which is named after Google’s foudner Larry Page: gf_graphframe(vertices_tbl, edges_tbl) %&gt;% gf_pagerank(reset_prob = 0.15, max_iter = 10L) GraphFrame Vertices: Database: spark_connection $ id &lt;dbl&gt; 12, 12, 14, 14, 27, 27, 55, 55, 64, 64, 41, 41, 47, 47, 6… $ pagerank &lt;dbl&gt; 0.3573460, 0.3573460, 0.3893665, 0.3893665, 0.2362396, 0.… Edges: Database: spark_connection $ src &lt;dbl&gt; 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 7, 12, 12, 12,… $ dst &lt;dbl&gt; 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17, 17,… $ weight &lt;dbl&gt; 0.25000000, 0.25000000, 0.25000000, 0.25000000, 0.25000000,… To give you some insights into this dataset, Figure 10.4 plots this chart using the ggraph and highlights the highest PageRank scores fot this dataset, FIGURE 10.4: Highschool ggraph dataset with highest pagerank highlighted There are many more graph algorithms provided in graphframes, to mention some: bread depth search, connected components, label propagation for detecting communities, strongly connected components and triangle count. For questions on this extension reffer to the official GitHub repo, github.com/rstudio/graphframes. We will now present a popular gradient boosting framework. 10.4 XGBoost A decision tree is a flowchart-like structure in which each internal node represents a test on an attribute, each branch represents the outcome of the test, and each leaf node represents a class label. For example, the diagram in Figure 10.5 shows an a decision tree that could help classify if an employee is likely to leave given a set of factors, like job satisfaction and overtime. When a decision tree is used to predict continuous variables instead of discrete outcomes, say, how likely someone is to leave a company, decision trees are reffered to as regression trees. FIGURE 10.5: A Decision tree to predict job attrition based on known factors While a decision tree representation is quite easy to understand and to interpret, finding out the decisions in the treee requires mathematical techniquest like gradient descent to find a local minimum. Gradient descent takes steps proportional to the negative of the gradient of the function at the current point. The gradient is represented by \\(\\nabla\\), the learning rate by \\(\\gamma\\) and one simply starts from a given state \\(a_n\\) and compute the next iteration \\(a_{n+1}\\) by simply following the direction of the gradient: \\[ a_{n+1} = a_n - \\gamma \\nabla F(a_n) \\] XGBoost is an open-source software library which provides a gradient boosting framework. It aims to provide a scalable, portable and distributed gradient boosting for training gradient-boosted decision trees (GBDT) and gradient-boosted regression trees (GBDT). Gradient-boosted means xgboost uses gradient descent and boostrin, which is a technique that chooses each predictor sequentially. sparkxgb is an extension that you can use to train XGBoost models in Spark. To use this extension, first install it from CRAN: install.packages(&quot;sparkxgb&quot;) Then you would need to import the sparkxgb extension followed by your usual Spark connection code, adjusting master as needed: library(sparkxgb) library(sparklyr) library(dplyr) sc &lt;- spark_connect(master = &quot;local&quot;) For this example, we will use the attrition dataset from the rsample package which you would need to install with install.packages(\"rsample\"). This dataset is a fictional dataset created by IBM data scientists to uncover the factors that lead to employee attrition. attrition &lt;- copy_to(sc, rsample::attrition) attrition # Source: spark&lt;?&gt; [?? x 31] Age Attrition BusinessTravel DailyRate Department DistanceFromHome &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;int&gt; 1 41 Yes Travel_Rarely 1102 Sales 1 2 49 No Travel_Freque… 279 Research_… 8 3 37 Yes Travel_Rarely 1373 Research_… 2 4 33 No Travel_Freque… 1392 Research_… 3 5 27 No Travel_Rarely 591 Research_… 2 6 32 No Travel_Freque… 1005 Research_… 2 7 59 No Travel_Rarely 1324 Research_… 3 8 30 No Travel_Rarely 1358 Research_… 24 9 38 No Travel_Freque… 216 Research_… 23 10 36 No Travel_Rarely 1299 Research_… 27 # … with more rows, and 25 more variables: Education &lt;chr&gt;, # EducationField &lt;chr&gt;, EnvironmentSatisfaction &lt;chr&gt;, Gender &lt;chr&gt;, # HourlyRate &lt;int&gt;, JobInvolvement &lt;chr&gt;, JobLevel &lt;int&gt;, JobRole &lt;chr&gt;, # JobSatisfaction &lt;chr&gt;, MaritalStatus &lt;chr&gt;, MonthlyIncome &lt;int&gt;, # MonthlyRate &lt;int&gt;, NumCompaniesWorked &lt;int&gt;, OverTime &lt;chr&gt;, # PercentSalaryHike &lt;int&gt;, PerformanceRating &lt;chr&gt;, # RelationshipSatisfaction &lt;chr&gt;, StockOptionLevel &lt;int&gt;, # TotalWorkingYears &lt;int&gt;, TrainingTimesLastYear &lt;int&gt;, # WorkLifeBalance &lt;chr&gt;, YearsAtCompany &lt;int&gt;, YearsInCurrentRole &lt;int&gt;, # YearsSinceLastPromotion &lt;int&gt;, YearsWithCurrManager &lt;int&gt; To build an XGBoost model in Spark use xgboost_classifier(), we will compute attrition against all other features by using the Attrition ~ . formula and specify two for the number of classes since the attrition attribute tracks only whether an employee leaves or not. Then you can use ml_predict() to predict over large-scale datasets: xgb_model &lt;- xgboost_classifier(attrition, Attrition ~ ., num_class = 2, num_round = 50, max_depth = 4) xgb_model %&gt;% ml_predict(attrition) %&gt;% select(Attrition, predicted_label, starts_with(&quot;probability_&quot;)) %&gt;% glimpse() Observations: ?? Variables: 4 Database: spark_connection $ Attrition &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, … $ predicted_label &lt;chr&gt; &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Yes&quot;, &quot;Y… $ probability_No &lt;dbl&gt; 0.753938094, 0.024780750, 0.915146366, 0.143568754, 0.07… $ probability_Yes &lt;dbl&gt; 0.24606191, 0.97521925, 0.08485363, 0.85643125, 0.927375… XGBoost became well known in the competition circles after its use in the winning solution of the Higgs Machine Learning Challenge which uses the ATLAS experiment to identify the Higgs boson. Since then, it has become a popular model and used for a large number of Kaggle competitions. However, decision trees could prove limiting specially in datasets with non tabular data like images, audio and text which you can tackle with deep learning models. 10.5 Deep Learning A perceptron is a mathematical model introduced by Rosenblatt (Rosenblatt 1958) whom developed it as a theory for a hypothetical nervous system. The perceptron tries to mimic a neuron by mapping stimulae to numeric inputs that are weighted into an threshold function that activates only when enough stimulae is present, mathematically: \\[ f(x) = \\begin{cases} 1 &amp; \\sum_{i=1}^m w_i x_i + b &gt; 0\\\\ 0 &amp; \\text{otherwise} \\end{cases} \\] Minsky found out that a single perceptron can only classify datasets that are linearly separable; however, he also presented in his perceptrons book (Minsky and Papert 2017) that layering perceptrons would bring additional classification capabilities, the original diagram showcasing a multi-layered perceptron is presented in Figure 10.6. FIGURE 10.6: Layered perceptrons as illustrated in the perceptrons book Using Spark we can create a multi-layer perceptron classifier with ml_multilayer_perceptron_classifier() to classify and predict over large datasets. This technique was introduced by Hinton (Ackley, Hinton, and Sejnowski 1985) and like XGBoost, it also makes use of gradient descent. nn_model &lt;- ml_multilayer_perceptron_classifier( attrition, Attrition ~ Age + DailyRate + DistanceFromHome + MonthlyIncome, layers = c(4, 3, 2)) nn_model %&gt;% ml_predict(attrition) %&gt;% select(Attrition, predicted_label, starts_with(&quot;probability_&quot;)) %&gt;% glimpse() Observations: ?? Variables: 4 Database: spark_connection $ Attrition &lt;chr&gt; &quot;Yes&quot;, &quot;No&quot;, &quot;Yes&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;… $ predicted_label &lt;chr&gt; &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, &quot;No&quot;, … $ probability_No &lt;dbl&gt; 0.8439275, 0.8439275, 0.8439275, 0.8439275, 0.8439275,… $ probability_Yes &lt;dbl&gt; 0.1560725, 0.1560725, 0.1560725, 0.1560725, 0.1560725,… Notice that the columns must be numeric so you will have to manually convert them with feature transformin techniques presented in the Modeling chapter. It is natural to try to add more layers to classify more complex datasets; however, adding too many layers will cause the gradient to vanish and other techniques will have to use these deep layered networds also known as, deep learning models. Deep learning models solve the vanishing gradient problem by making use of special activation functions, dropout, data augmentation and graphic processing units (GPUs). You can use Spark to retrieve and preprocess large datasets into numerical-only datasets that can fit in a GPU for deep learning training. Tensorflow is one of the most popular deep learning frameworks and supports a binary format known as TensorFlow Records. You can write TensorFlow records using the sparktf in Spark which you can prepare to process in GPU instances with libraries like Keras or TensorFlow. sparktf is available on CRAN and can be installed as follows: install.packages(&quot;sparktf&quot;) You can then preprocess large datasets in Spark and write it as TensorFlow records using spark_write_tf(): library(sparktf) library(sparklyr) sc &lt;- spark_connect(master = &quot;local&quot;) copy_to(sc, iris) %&gt;% ft_string_indexer_model( &quot;Species&quot;, &quot;label&quot;, labels = c(&quot;setosa&quot;, &quot;versicolor&quot;, &quot;virginica&quot;) ) %&gt;% spark_write_tfrecord(path = &quot;tfrecord&quot;) Once trained, you can use the tfdatasets package to load the dataset followed by training with keras or tensorflow. To learn more about training deep learning models with Keras we recommend reading “Deep Learning with R” (Chollet and Allaire 2018). tfdatasets::tfrecord_dataset(&quot;tfrecord/part-r-00000&quot;) &lt;DatasetV1Adapter shapes: (), types: tf.string&gt; Training deep learning models in a single local node with one or more GPUs is often enough for most applciations; however, recent state-of-the-art deep learning models train using distributed computing frameworks like Apache Spark. Distributed computing frameworks are used to achieve higher petaflops each day the systems spends training these models. OpenAI analyzed trends in AI and Compute (“AI and Compute” 2019) which Figure 10.7 adapted to show which systems were trained in local instances or distributed computing systems. It should be obvious from the figure that there is a trend in recent years to use distributed computing frameworks. FIGURE 10.7: Training using distributed systems based on OpenAI analysis Training large-scale deep learning models is possible in Spark and TensorFlow through frameworks like Horovod. Today, it’s possible to use Horovod with Spark from R using the reticualte package since Horovod requires Python and Open MPI which goes beyond the scope of this book. Instead, we will introduce a different Spark extension in the domain of genomics. 10.6 Genomics The human genome consists of two copies of about three billion base pairs of DNA within the 23 chromosome pairs, Figure 10.8 shows the organization of the genome into chromosomes. DNA strands are composed of nucleotides, each composed of one of four nitrogen-containing nucleobases: cytosine (C), guanine (G), adenine (A) or thymine (T) (“HUman Genome” 2019). Since the DNA of all humans is nearly identical, we only need to store the differences from the reference genome in the form of a Variant Call Format (VCF) file. FIGURE 10.8: The idealized human diploid karyotype showing the organization of the genome into chromosomes VariantSpark is a framework based on scala and spark to analyze genome datasets. It is being developed by CSIRO Bioinformatics team in Australia. VariantSpark was tested on datasets with 3000 samples each one containing 80 million features in either unsupervised clustering approaches and supervised applications, like classification and regression. VariantSpark can read VCF files and run analyses while using familiar Spark DataFrames. To get started, install variantspark from CRAN and connect to Spark: library(variantspark) library(sparklyr) sc &lt;- spark_connect(master = &quot;local&quot;) vsc &lt;- vs_connect(sc) We can start by loading a VCF file, vsc_data &lt;- system.file(&quot;extdata/&quot;, package = &quot;variantspark&quot;) hipster_vcf &lt;- vs_read_vcf(vsc, file.path(vsc_data, &quot;hipster.vcf.bz2&quot;)) hipster_labels &lt;- vs_read_csv(vsc, file.path(vsc_data, &quot;hipster_labels.txt&quot;)) labels &lt;- vs_read_labels(vsc, file.path(vsc_data, &quot;hipster_labels.txt&quot;)) VariantSpark uses Random Forest to assign an Importance score to each tested variant reflecting its association to the interest phenotype. A variant with higher Importance score implies it is more strongly associated with the phenotype of interest. You can compute the Importance and transform it into a Spark table as follows, importance_tbl &lt;- vs_importance_analysis(vsc, hipster_vcf, labels, n_trees = 100) %&gt;% importance_tbl() importance_tbl # Source: spark&lt;?&gt; [?? x 2] variable importance &lt;chr&gt; &lt;dbl&gt; 1 2_109511398 0 2 2_109511454 0 3 2_109511463 0.00000164 4 2_109511467 0.00000309 5 2_109511478 0 6 2_109511497 0 7 2_109511525 0 8 2_109511527 0 9 2_109511532 0 10 2_109511579 0 # … with more rows You can then use dplyr and ggplot2 to transform the output and visualize it, importance_df &lt;- importance_tbl %&gt;% arrange(-importance) %&gt;% head(20) %&gt;% collect() ggplot(importance_df) + aes(x = variable, y = importance) + geom_bar(stat = &#39;identity&#39;) + scale_x_discrete(limits = importance_df[order(importance_df$importance), 1]$variable) + coord_flip() FIGURE 10.9: Genomic importance analysis using variantspark This concludes a brief introduction to genomic analysis in Spark using the variantspark extensions. Next, we will move away from micrososcopic genes, to macroscopic datasets that contain geographic locations across the world. 10.7 Spatial geospark enables distributed geospatial computing using a grammar compatible with dplyr and sf package which provides a set of tools for working with geospatial vectors. You can install geospark from GitHub as follows: install.packages(&quot;remotes&quot;) remotes::install_github(&quot;r-spark/geospark&quot;) Then we will initialize the geospark extension and connect to Spark: library(geospark) library(sparklyr) sc &lt;- spark_connect(master = &quot;local&quot;) register_gis(sc) Next we will load a spatial dataset containing polygons and points. polygons &lt;- system.file(&quot;examples/polygons.txt&quot;, package=&quot;geospark&quot;) %&gt;% read.table(sep=&quot;|&quot;, col.names = c(&quot;area&quot;, &quot;geom&quot;)) points &lt;- system.file(&quot;examples/points.txt&quot;, package=&quot;geospark&quot;) %&gt;% read.table(sep = &quot;|&quot;, col.names = c(&quot;city&quot;, &quot;state&quot;, &quot;geom&quot;)) polygons_wkt &lt;- copy_to(sc, polygons) points_wkt &lt;- copy_to(sc, points) There are various spatial operations defined in geospark, which Figure 10.10 describes. These operations allow you to control how geospatial data should be queried based on overlap, intersection, disjoint sets, etc. FIGURE 10.10: Spatial operations available in geospark. For isntance, we can use these operations to find the polygons that contain a given set of points using st_contains(), library(dplyr) polygons_wkt &lt;- mutate(polygons_wkt, y = st_geomfromwkt(geom)) points_wkt &lt;- mutate(points_wkt, x = st_geomfromwkt(geom)) sc_res &lt;- inner_join(polygons_wkt, points_wkt, join = sql(&quot;st_contains(y,x)&quot;)) %&gt;% group_by(area, state) %&gt;% summarise(cnt = n()) %&gt;% head() # Source: spark&lt;?&gt; [?? x 3] # Groups: area area state cnt &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; 1 texas area TX 10 2 dakota area SD 1 3 dakota area ND 10 4 california area CA 10 5 new york area NY 9 You can also plot these datasets by collecting a subset of the entire dataset or aggregating the geometries in spark before collecting them, one package you should look into is the sf package. We will now start closing this chapter by presenting a couple troubleshooting techniques applicable to all extensions. 10.8 Troubleshooting When using a new extension for the first time, we recommend increasing the connection timeout since Spark will usually have to download extension dependencies and changing logging to verbose to help troubleshoot when the download process does not complete: config &lt;- spark_config() config$sparklyr.connect.timeout &lt;- 3 * 60 config$sparklyr.log.console = TRUE sc &lt;- spark_connect(master = &quot;local&quot;, config = config) In addition, you should know that Apache IVY is a popular dependency manager focusing on flexibility, simplicity, and is used by Apache Spark while installing extensions. When the connection fails while using an extension, consider clearing your IVY Cache by running: unlink(&quot;~/.ivy2/cache&quot;, recursive = TRUE) In addition, you can also consider opening GitHub issues from the following extensions repos to get help from the extension authors: rsparkling: github.com/h2oai/sparkling-water. sparkxgb: github.com/rstudio/sparkxgb. sparktf: github.com/rstudio/sparktf. variantspark: github.com/r-spark/variantspark. geospark: github.com/r-spark/geospark. 10.9 Recap This chapter provideed a brief oveview on using some of the Spark extensions available in R, which happens to be as easy as installing a package. You then learned how to use the rsparkling extension which provides access to H2O in Spark to which provides additional modeling functionality like enhanced metrics and ability to automatically select models. We then jumped to graphframes, an extension to help you process relational datasets which are formaly referred as graphs; you learned how to compute simple connection metrics or run complex algorithms like pagerank. THe XGBoost and Deep Learning sections provided alternate modeling techniques that use gradient descent, the former over decision trees and the latter over deep multi-layered perceptrons where Spark can be used to preprocess datasets into records that can then be later consument by TensorFlow and Keras using the sparktf extension. The last two sections untroduced extensions to process genomic and spatial datasets through the variantspark and geospark extensions. These extensions and many more, provide a comprehensive library of advanced functionality that in combination with analysis and modeling techniques presented, should cover most tasks required to run in computing clusters. However, when functionality is lacking, you can consideer writting your own extension as we will present in the Contributing or you can apply custom transformations over each partition using R code as we will described in the next chapter, Ditributed R. References "],
["distributed.html", "Chapter 11 Distributed R 11.1 Overview 11.2 Use Cases 11.3 Partitions 11.4 Grouping 11.5 Columns 11.6 Context 11.7 Functions 11.8 Packages 11.9 Cluster Requirements 11.10 Troubleshooting 11.11 Recap", " Chapter 11 Distributed R In previous chapters you learned how to perform data analysis and modeling in local Spark instances and proper Spark clusters and the previous Extensions chapter described how to make use of additional functionality provided by the Spark and R communities at large. In most cases, the combination of Spark functionality and extensions is more than enough to perform almost any computation. However, for those cases where functionality is lacking in Spark and their extensions, you should consider distributing R computations to worker nodes yourself. %% while leveraging any existing R package. You can run arbitrary R code in each worker node to run any computation – you can run simulations, crawl content from the web, transform data and so on. In addition, you can also make use of any package available in CRAN and private packages available in your organization, this reduces the amount of code you need to write to help you keep productive. If you are already a familiar R user, you might be tempted to use this approach for all Spark operations; however, this is not the recommended use of spark_apply(). Previous chapters provided more efficient techiniques and tools to solve well known problems – in contrast, spark_apply() introduces additional cognitive overhead, additional troubleshooting steps, performance trade-offs and, in general, additional complexity that should be avoided. Not to say that spark_apply() should never used; but instead, spark_apply() is reserved to support use cases where previous tools and techniques fall short. 11.1 Overview The Introduction chapter introduced MapReduce as a technique capable of processing large-scale datasets; it also described how Apache Spark provided a superset of operations to perform MapReduce computations with ease and more efficiently. The Tuning chapter presented insights into how Spark works by applying custom transformation over each partition of the distributed datasets. For instance, if we were to multiply by ten each element of a distributed numeric dataset, Spark would apply a mapping operation over each partition through multiple workers, conceptually, this is illustrated in Figure 11.1. FIGURE 11.1: Map Operation when Multiplying by Ten This chapter presents how to define a custom f(x) mapping operation using spark_apply(); for the previous example, spark_apply() provides support to define 10 * x as follows: sdf_len(sc, 3) %&gt;% spark_apply(~ 10 * .x) # Source: spark&lt;?&gt; [?? x 1] id * &lt;dbl&gt; 1 10 2 20 3 30 Notice that ~ 10 * .x is plain R code executed across all worker nodes; the ~ operator is defined in the rlang package and provides a compact definition of a function equivalent to function(.x) 10 * .x – this compact form is also known as an anonymous function or lambda expression. The f(x) function must take an R data frame as input and must also produce an R data frame as output, conceptually illustrated in Figure 11.2. FIGURE 11.2: Expected Function Signature in spark_apply() Mappings We can refer back to the orginal MapReduce example from the Introduction chapter, where the map operation was defined to split sentences into words and then, the total unique words were counted as the reduce operation. In R, we could make use of the unnest_tokens() function in the tidytext R package, combining the functionality of tidytext with spark_apply() would allow us to tokenize those sentences into a table of words as follows: sentences &lt;- copy_to(sc, data_frame(text = c(&quot;I like apples&quot;, &quot;I like bananas&quot;))) sentences %&gt;% spark_apply(~tidytext::unnest_tokens(.x, word, text)) # Source: spark&lt;?&gt; [?? x 1] word * &lt;chr&gt; 1 i 2 like 3 apples 4 i 5 like 6 bananas Finally, we can reduce this dataset using dplyr to compute this original MapReduce word-count example using dplyr as follows: sentences %&gt;% spark_apply(~tidytext::unnest_tokens(., word, text)) %&gt;% group_by(word) %&gt;% summarise(count = count()) # Source: spark&lt;?&gt; [?? x 2] word count * &lt;chr&gt; &lt;dbl&gt; 1 i 2 2 apples 1 3 like 2 4 bananas 1 The rest of this chapter will explain in detail use cases, features, caveats, considerations and troubleshooting techniques required when defining custom mappings through spark_apply() Note: The previous sentence tokenizer example can be more efficiently implemented using concepts from previous chapters, specifically through sentences %&gt;% ft_tokenizer(\"text\", \"words\") %&gt;% transmute(word = explode(words)). 11.2 Use Cases In the overview section we presented an example to help you understand how spark_apply() works; this section will cover a few practical use case for spark_apply(): Import You can consider using R to import data from external data sources and formats. For example, when a file format is not natevely supported in Spark or its extensions, you can consider using R code to implement a distributed custom parser using R packages. Model It is natural to use the rich modeling capabilities already available in R with Spark. In most cases, R models can’t be used across large data; however, we will present two particular use cases where R models can be useful at scale. For instance, when data fits into a single machine you can make use of grid search to optimize their parameters in parallel. In cases where the data can be partitioned to create several models over subsets of the data you can use partitioned modeling in R to compute models across partitions. Transform You can make use of R’s rich data transformation capabilities to complement Spark. We will present a use case of evaluating data by external systems and use R to interoperate with them by calling them through a web API. Compute When you need to perform large-scale computation in R, or big-compute as described in the Introduction chapter, Spark is ideal to distribute this computation. We will present distributed rendering as a particular use case for large-scale computing in R. We will now explore each use case in detail and provide a working example to help you understand how you use spark_apply() effectevely. 11.2.1 Custom Parsers While Spark and its various extensions provide support for many file formats (CSVs, JSON, Parquet, AVRO, etc) there are many more file formats that you might need to use at scale. You can parse these additional formats using spark_apply() and many of the existing R packages. In this section we will understand how to parse log files, but similar approaches can be followed to parse other file formats. It is common to use Spark to analize log files; for instance, logs that track download data from Amazon S3. To parse logs, the webreadr package can simplify this process by providing support to load logs stored as: Amazon S3, Squid and the Common log format. An Amazon S3 log looks as follows: #Version: 1.0 #Fields: date time x-edge-location sc-bytes c-ip cs-method cs(Host) cs-uri-stem sc-status cs(Referer) cs(User-Agent) cs-uri-query cs(Cookie) x-edge-result-type x-edge-request-id x-host-header cs-protocol cs-bytes time-taken 2014-05-23 01:13:11 FRA2 182 192.0.2.10 GET d111111abcdef8.cloudfront.net /view/my/file.html 200 www.displaymyfiles.com Mozilla/4.0%20 (compatible;%20MSIE%205.0b1;%20Mac_PowerPC) - zip=98101 RefreshHit MRVMF7KydIvxMWfJIglgwHQwZsbG2IhRJ07sn9AkKUFSHS9EXAMPLE== d111111abcdef8.cloudfront.net http - 0.001 Which can be parsed easily with read_aws() as follows: aws_log &lt;- system.file(&quot;extdata/log.aws&quot;, package = &quot;webreadr&quot;) webreadr::read_aws(aws_log) # A tibble: 2 x 18 date edge_location bytes_sent ip_address http_method host path &lt;dttm&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 2014-05-23 01:13:11 FRA2 182 192.0.2.10 GET d111… /vie… 2 2014-05-23 01:13:12 LAX1 2390282 192.0.2.2… GET d111… /sou… # ... with 11 more variables: status_code &lt;int&gt;, referer &lt;chr&gt;, user_agent &lt;chr&gt;, # query &lt;chr&gt;, cookie &lt;chr&gt;, result_type &lt;chr&gt;, request_id &lt;chr&gt;, # host_header &lt;chr&gt;, protocol &lt;chr&gt;, bytes_received &lt;chr&gt;, time_elapsed &lt;dbl&gt; To scale this operation, we can make use of read_aws() using spark_apply(): spark_read_text(sc, &quot;logs&quot;, aws_log, overwrite = TRUE, whole = TRUE) %&gt;% spark_apply(~webreadr::read_aws(.x$contents)) # Source: spark&lt;?&gt; [?? x 18] date edge_location bytes_sent ip_address http_method host path * &lt;dttm&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 2014-05-23 01:13:11 FRA2 182 192.0.2.10 GET d111… /vie… 2 2014-05-23 01:13:12 LAX1 2390282 192.0.2.2… GET d111… /sou… # ... with 11 more variables: status_code &lt;int&gt;, referer &lt;chr&gt;, user_agent &lt;chr&gt;, # query &lt;chr&gt;, cookie &lt;chr&gt;, result_type &lt;chr&gt;, request_id &lt;chr&gt;, # host_header &lt;chr&gt;, protocol &lt;chr&gt;, bytes_received &lt;chr&gt;, time_elapsed &lt;dbl&gt; The code between plain R and spark_apply() is similar; howerver, when using spark_apply() logs are parsed in parallel across all the worker nodes available in your cluster. This concludes the custom parsers section, there are many other file formats you can parse at scale from R following a similar approach. We will now present partitioned modeling as another use case focused on modeling across several datasets in parallel. 11.2.2 Partitioned Modeling There are many modeling packages available in R that can also be run at scale by partitioning the data into manegable groups that do fit in the resources of a single machine. For instance, suppose that you have a 1TB dataset for sales data across multiple cities and you are tasked with creating sales predictions over each city. For this case, you can consider partitioning the original dataset per city, say into 10GB of data per city, which could be managed by a single compute instance. For this kind of partitionable dataset, you can also consider using spark_apply() by training each model over each city. As a simple example of partitoned modeling, we can run a linear regression using the iris dataset partiotioned by Species: iris &lt;- copy_to(sc, datasets::iris) iris %&gt;% spark_apply(nrow, group_by = &quot;Species&quot;) # Source: spark&lt;?&gt; [?? x 2] Species result &lt;chr&gt; &lt;int&gt; 1 versicolor 50 2 virginica 50 3 setosa 50 Then you can run a linear regression over each species using spark_apply(): iris %&gt;% spark_apply( function(e) summary(lm(Petal_Length ~ Petal_Width, e))$r.squared, names = &quot;r.squared&quot;, group_by = &quot;Species&quot;) # Source: spark&lt;?&gt; [?? x 2] Species r.squared &lt;chr&gt; &lt;dbl&gt; 1 versicolor 0.619 2 virginica 0.104 3 setosa 0.110 As you can see from the r.squared results and intuitevely in Figure 11.3, the linear model for versicolor better fits to the regression line. FIGURE 11.3: Modeling over Species This concludes our brief overview on how to perform modeling over several different, partitionable, datasets. A similar technique can be applied to perform modeling over the same dataset using different modeling parameters which we will cover next. 11.2.3 Grid Search Many R packages provide models that require defining multiple parameters to configure and optimize their particular models. When the value of these parameters is unknown, we can distribute this list of unknown parameters across a cluster of machines to find the optimal parameter combination. If the list contains more than one parameter to optimize, it is common to test against all the combinations between paramter A and parameter B, creating a grid of parameters. The process of searching for the best parameter over this parameter grid is commonly known as grid search. For example, we can define a grid of parameters to optimize decision tree models as follows: grid &lt;- list(minsplit = c(2, 5, 10), maxdepth = c(1, 3, 8)) %&gt;% purrr:::cross_df() %&gt;% copy_to(sc, ., repartition = 9) grid # Source: spark&lt;?&gt; [?? x 2] minsplit maxdepth &lt;dbl&gt; &lt;dbl&gt; 1 2 1 2 5 1 3 10 1 4 2 3 5 5 3 6 10 3 7 2 8 8 5 8 9 10 8 The grid dataset was copied with repartition = 9 to ensure that each partition is contained in one machine since the grid has also nine rows. Now, assuming that the original dataset fits in every machine, we can distribute this dataset to many machines and perform parameter search to find the model that best fits this data. spark_apply( grid, function(grid, cars) { model &lt;- rpart::rpart( am ~ hp + mpg, data = cars, control = rpart::rpart.control(minsplit = grid$minsplit, maxdepth = grid$maxdepth) ) dplyr::mutate( grid, accuracy = mean(round(predict(model, dplyr::select(cars, -am))) == cars$am) ) }, context = mtcars) # Source: spark&lt;?&gt; [?? x 3] minsplit maxdepth accuracy &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 2 1 0.812 2 5 1 0.812 3 10 1 0.812 4 2 3 0.938 5 5 3 0.938 6 10 3 0.812 7 2 8 1 8 5 8 0.938 9 10 8 0.812 For this particular model, minsplit = 2 and maxdepth = 8 produces the most accurate results. You can now use this specific parameter combination to properly a train model. 11.2.4 Web APIs A Web API is a program that can do something useful through an web interface that other programs can reuse. For instance, services like Twitter provide Web APIs that allow you to automate reading tweets from a program written in R and other programming languages. You can make use of Web APIs using spark_apply() by sending programatic requests to external services using R code. For example, Google provides a Web API to label images using deep learning techniques; you can make use of this API from R, but for larger datasets, you would need to access their APIs from Spark. You can use Spark to prepare data to be consumed by a webAPI, then use spark_apply() to perform this call and process all the incoming results back in Spark. The following example makes use of the googleAuthR package to authenticate to Google Cloud, the RoogleVision package to perform labeling over the Google Vision API, and spark_apply() to interoperate between Spark and Google’s deep learning service. If you want to run the following example you will need to disconnect first from Spark and download your cloudml.json from the Google developer portal. sc &lt;- spark_connect( master = &quot;local&quot;, config = list(sparklyr.shell.files = &quot;cloudml.json&quot;)) images &lt;- copy_to(sc, data.frame( image = &quot;http://pbs.twimg.com/media/DwzcM88XgAINkg-.jpg&quot; )) spark_apply(images, function(df) { googleAuthR::gar_auth_service( scope = &quot;https://www.googleapis.com/auth/cloud-platform&quot;, json_file = &quot;cloudml.json&quot;) RoogleVision::getGoogleVisionResponse( df$image, download = FALSE) }) # Source: spark&lt;?&gt; [?? x 4] mid description score topicality &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 /m/04rky Mammal 0.973 0.973 2 /m/0bt9lr Dog 0.958 0.958 3 /m/01z5f Canidae 0.956 0.956 4 /m/0kpmf Dog breed 0.909 0.909 5 /m/05mqq3 Snout 0.891 0.891 In order to successfully run a large distirbuted computation over a Web API, the Web API would have to be able to scale to support the load from all the Spark executors. One can trust that major service providers are likely to support all the request incoming from your cluster. However, when calling internal Web APIs, make sure the API can handle the load. Also, when using third-party services, consider the cost of calling their API across all the executors in your cluster to avoid potentially expensive and unexpected charges. Next you will learn an use case for large-compute where R is use to perform distributed rendering. 11.2.5 Intensive Computations R can be used in combination with Spark to perform large-scale computing. The use case we will explore in this section is rendering computationally-expensive images using the rayrender package, which uses ray tracing – a photorealistic technique commonly used in movie production. Lets use this package to render a simple scene that includes a few spheres. library(rayrender) scene &lt;- generate_ground(material = lambertian()) %&gt;% add_object(sphere(material = metal(color=&quot;orange&quot;), z = -2)) %&gt;% add_object(sphere(material = metal(color=&quot;orange&quot;), z = +2)) %&gt;% add_object(sphere(material = metal(color=&quot;orange&quot;), x = -2)) render_scene(scene, width = 1920, height = 1080, lookfrom = c(10, 5, 0), parallel = TRUE) FIGURE 11.4: Ray tracing in Spark using R and rayrender The previous example takes several minutes to render the single frame from Figure 11.4, rendering a few seconds at 30 frames-per-second would take several hours in a single machine. However, we can reduce this time using multiple machines by parallelizing computation across them. For instance, using ten machines with the same amount of CPUs would cut rendering time ten fold: system2(&quot;hadoop&quot;, args = c(&quot;fs&quot;, &quot;-mkdir&quot;, &quot;/rendering&quot;)) sdf_len(sc, 628, repartition = 628) %&gt;% spark_apply(function(idx, scene) { render &lt;- sprintf(&quot;%04d.png&quot;, idx$id) rayrender::render_scene(scene, width = 1920, height = 1080, lookfrom = c(12 * sin(idx$id/100), 5, 12 * cos(idx$id/100)), filename = render) system2(&quot;hadoop&quot;, args = c(&quot;fs&quot;, &quot;-put&quot;, render, &quot;/user/hadoop/rendering/&quot;)) }, context = scene, columns = list()) %&gt;% collect() After all the images are rendered, the last step is to collect them from HDFS and use tools like ffmpeg to convert individual images into an animation: hadoop fs -get rendering/ ffmpeg -s 1920x1080 -i rendering/%d.png -vcodec libx264 -crf 25 -pix_fmt yuv420p rendering.mp4 Note: This example assumes HDFS is used as the storage technology for Spark and being run under a hadoop user, you will need to adjust this for your particular storage or user. We’ve covered some of the common use cases for spark_apply(), but you are certainly welcome to find other use cases for your particular needs. The following sections will present technical concepts you will need to understand to create additional use cases and to use spark_apply() effectevely. 11.3 Partitions Most Spark operations, say to analyze data with dplyr or model with MLlib, do not require understanding how Spark partitions data, it works automatically – this is not the case for distributed R computations. Instead, you will have to learn and understand how exactly Spark is partitioning your data and provide transformations that are compatible with them. This is required since spark_apply() receives each partition and allows you to perform any transformation, not the entire dataset. You can refresh concepts like partitioning and transformations using the diagrams and examples from the Tuning chapter. To help us understand how partitions are represented in spark_apply(), consider the following code. Should we expect the output to be the total number of rows? sdf_len(sc, 10) %&gt;% spark_apply(~nrow(.x)) # Source: spark&lt;?&gt; [?? x 1] result * &lt;int&gt; 1 5 2 5 As you can see from the results, the general the answer is no; Spark assumes data will be distributed across multiple machines so you will often find it already partitioned, even for small datasets. So we should not expect spark_apply() to operate over a single partition, let’s find out how many partitions sdf_len(sc, 10) contains: sdf_len(sc, 10) %&gt;% sdf_num_partitions() [1] 2 This explains why counting rows through nrow() under spark_apply() retrieves two rows since there are two partitions, not one. spark_apply() is retrieving the count of rows over each partition, each partition contains five rows; not ten rows total as you might have expected. For this particular example, we could further aggregate these partitions by repartitioning and then adding up – this would resemple a simple MapReduce operation using spark_apply(): sdf_len(sc, 10) %&gt;% spark_apply(~nrow(.x)) %&gt;% sdf_repartition(1) %&gt;% spark_apply(~sum(.x)) # Source: spark&lt;?&gt; [?? x 1] result * &lt;int&gt; 1 10 It was the intent of this section is to make you aware of partitions while using spark_apply(); the next section presents group_by as a way to control partitions. 11.4 Grouping When using spark_apply(), we can request explicit partitions from Spark. For instance, if we had to process numbers less than four in one partition and the remaining ones in a second partition, we could create these groups explicitly and then request spark_apply() to use them: sdf_len(sc, 10) %&gt;% transmute(groups = id &lt; 4) %&gt;% spark_apply(~nrow(.x), group_by = &quot;groups&quot;) # Source: spark&lt;?&gt; [?? x 2] groups result * &lt;lgl&gt; &lt;int&gt; 1 TRUE 3 2 FALSE 7 Notice that spark_apply() is still processing two partitions, but in this case, we expect these partitions since we explicitly requested them in spark_apply(); therefore, you can safely interpret the results as “there are three integers less than four”. Note: You can only group data by partitions that fit in a single machine, if one of the groups is too large, an exception will be thrown. To perform operations over groups that exceed the resources of a single node, you can consider partitioning to smaller units or use dplyr::do which is currently optimized for large partitions. The takeaway from this section is to always consider partitions when dealing with spark_apply(). Next, we will zoom inside spark_apply() to understand how columns are interpreted. 11.5 Columns By default, spark_apply(), will inspect the data frame being produced to find out column names and types automatically, for example: sdf_len(sc, 1) %&gt;% spark_apply(~ data.frame(numbers = 1, names = &quot;abc&quot;)) # Source: spark&lt;?&gt; [?? x 2] numbers names * &lt;dbl&gt; &lt;chr&gt; 1 1 abc However, this is inneficient since spark_apply() needs to run twice. First to find columns by computing spark_apply() against a subset of all the data, and then to compute the actual desired values. To improve performance, the columns can be specified explicitly through the columns parameters. The columns expects a named list of types expected in the resulting data frame. We can then rewrite the previous example to run only once by specifying the correct type for the numbers column: sdf_len(sc, 1) %&gt;% spark_apply( ~ data.frame(numbers = 1, names = &quot;abc&quot;), columns = list(numbers = &quot;double&quot;, names = &quot;character&quot;)) # Source: spark&lt;?&gt; [?? x 2] numbers names * &lt;dbl&gt; &lt;chr&gt; 1 1 abc In this section and the previous one, we presented how rows and columns interact with spark_apply(). The following section will allow us to make use of contextual information that is sometimes required when processing distributed datasets. 11.6 Context In order to process partitions using spark_apply(), you might need to include auxility data that is small-enough to fit in each node. This was the case in the Grid Search use case, where the dataset was passed to all partitions and remained unpartitioned itself. We can modify the initial f(x) = 10 * x example in this chapter to allow us to customize the multiplier – it was originally set to 10 but we can make it configurable by specifying it as the context parameter. sdf_len(sc, 4) %&gt;% spark_apply( function(data, context) context * data, context = 100 ) # Source: spark&lt;?&gt; [?? x 1] id &lt;dbl&gt; 1 100 2 200 3 300 4 400 Figure 11.5 illustrates this example conceptually. Notice that the data partitions are still variable; however, the contextual parameter is distributed to all the nodes. FIGURE 11.5: Map Operation when Multiplying with Context The grid search example used this parameter to pass a data frame to each worker node; however, since the context parameter is serialized as an R object, it can contain anything. For instance, if you need to pass multiple values – or even multiple datasets – you can pass a list with values. The following example defines a f(x) = m * x + b function and runs m = 10 and b = 2: sdf_len(sc, 4) %&gt;% spark_apply( ~.y$m * .x + .y$b, context = list(b = 2, m = 10) ) # Source: spark&lt;?&gt; [?? x 1] id &lt;dbl&gt; 1 12 2 22 3 32 4 42 Notice that we’ve renamed context to .y to shorten the variable name, this works since spark_apply() assumes context is the second parameter in functions and expressions. The context parameter will proof to be extremely useful; for instance, the next section will present how to properly construct functions, and context will be used in advanced use cases to construct functions dependent on other functions. 11.7 Functions Previous sections presented spark_apply() as an operation to perform custom transformations using a function or expression, in programming literature functions with a context are also reffered as a closure. Expressions are useful to define short transformations, like ~ 10 * .x. For an expression, .x contains a partition and .y the context, when available. However, it can be hard to define an expression for complex code that spans multiple lines, for those cases, functions are more appropriate. Functions enable complex and multi-line transformations, they are defined as function(data, context) {} and allow you to provide arbitrary code within {}. We’ve used them in previous sections when using Google Cloud to transform images into image captions. The function passed to spark_apply() is serialized using serialize(), which is described as “a simple low-level interface for serializing to connections.”. One of the current limitations of serialize() is that it wont serialize objects being referenced outside of it’s environment. For instance, the following function will error out since the closures references external_value: external_value &lt;- 1 spark_apply(iris, function(e) e + external_value) As a workarounds to this limitation, you can add the functions your clousure needs into the context followed by assigning the functions into the global environment: func_a &lt;- function() 40 func_b &lt;- function() func_a() + 1 func_c &lt;- function() func_b() + 1 sdf_len(sc, 1) %&gt;% spark_apply(function(df, context) { for (name in names(context)) assign(name, context[[name]], envir = .GlobalEnv) func_c() }, context = list( func_a = func_a, func_b = func_b, func_c = func_c )) # Source: spark&lt;?&gt; [?? x 1] result &lt;dbl&gt; 1 42 When this is not feasible, you can also create your own R package with the functionality you need and then use your package in spark_apply(). Up to this point, you’ve learned all the functionality available in spark_apply() using plain R code; however, we have not presented how to use packages when distributing computations – R packages are essential when creating useful transformations. 11.8 Packages With spark_apply() you can use any R package inside Spark. For instance, you can use the broom package to create a tidy data frame from linear regression output. spark_apply( iris, function(e) broom::tidy(lm(Petal_Length ~ Petal_Width, e)), names = c(&quot;term&quot;, &quot;estimate&quot;, &quot;std.error&quot;, &quot;statistic&quot;, &quot;p.value&quot;), group_by = &quot;Species&quot;) # Source: spark&lt;?&gt; [?? x 6] Species term estimate std.error statistic p.value &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 versicolor (Intercept) 1.78 0.284 6.28 9.48e- 8 2 versicolor Petal_Width 1.87 0.212 8.83 1.27e-11 3 virginica (Intercept) 4.24 0.561 7.56 1.04e- 9 4 virginica Petal_Width 0.647 0.275 2.36 2.25e- 2 5 setosa (Intercept) 1.33 0.0600 22.1 7.68e-27 6 setosa Petal_Width 0.546 0.224 2.44 1.86e- 2 The first time you call spark_apply() all of the contents in your local .libPaths(), which contains all R packages, will be copied into each Spark worker node. Packages will only be copied once and will persist as long as the connection remains open. It’s not uncommon for R libraries to be several gigabytes in size, so be prepared for a one-time tax while the R packages are copied over to your Spark cluster. You can disable package distribution by setting packages = FALSE. Note: Since packages are copied only once for the duration of the spark_connect() connection, installing additional packages is not supported while the connection is active. Therefore, if a new package needs to be installed, spark_disconnect() the connection, modify packages and reconnect. In addition, R packages are not copied in local mode because the packages already exist on the local system. While this was a short section, using packages with distributed R code opens up and entire new universe of interesting use cases. Some where covered in the use cases section, but you can think of many more by looking at the rich ecosystem of R packages available today. This section completes all the functionality you need to distribute R code with R packages, next we will cover some of the requirements your cluster needs to make use of spark_apply(). 11.9 Cluster Requirements All the functionality presented in previous chapters, does not require any special configuration of your Spark cluster – as long as you have a properly configured Spark cluster, you can use R with it. This is not the case for the functionality presented in this chapter, your cluster administrator, your cloud provider or yourself will have to configure your cluster by: Installing R in every node, to execute R code across your cluster. Optionally, installing Apache Arrow in every node when using Spark 2.3 or later, Arrow provides performance improvements that bring distributed R code closer to native Scala code. Lets take a look at each requirement to make sure you properly consider the tradeoffs or benefits that they provide. 11.9.1 Installing R Starting with the first requirement, the R Runtime is expected to be pre-installed in every node in the cluster, this is a requirement specific to spark_apply(). Failure to install R in every node will trigger a Cannot run program, no such file or directory error while attempting to use spark_apply(). Contact your cluster administrator to consider making the R runtime available throughout the entire cluster. If R is already installed, you can specify the installation path to use using the spark.r.command configuration setting, as in: config &lt;- spark_config() config[&quot;spark.r.command&quot;] &lt;- &quot;&lt;path-to-r-version&gt;&quot; sc &lt;- spark_connect(master = &quot;local&quot;, config = config) sdf_len(sc, 10) %&gt;% spark_apply(function(e) e) A Homogeneous Cluster is required since the driver node distributes, and potentially compiles, packages to the workers. For instance, the driver and workers must have the same processor architecture, system libraries, etc. This is usually the case for most clusters, but might not be the case for yours. Different cluster managers, Spark distributions and cloud providers, support different solutions to install additional software, like R, across every node in the cluster; those instructions should be followed when installing R over each worker node. To mention a few, Spark Standalone Requires connecting to each machine and installing R; there are tools like pssh that allow you to run a single installation command against multiple machines. Cloudera Provides an R parcel, see “How to Distribute your R code with sparklyr and Cloudera Data Science Workbench” (???), which enables R over each worker node. Amazon EMR R is pre-installed when starting an EMR cluster as mentioned in the Amazon EMR section. Microsoft HDInsight R is pre-installed and no additional steps are needed. Livy Livy connections do not support distributing packages since the client machine where the libraries are precompiled might not have the same processor architecture, nor operating systems than the cluster machines. Strictly speaking, this completes the last requirement for your cluster. However, we strongly recommend you use Apache Arrow with spark_apply() to support large-scale computation with minimal overhead. 11.9.2 Apache Arrow Before we introduce Apache Arrow, we need to present how data is stored and transfered between Spark and R. R was designed from its inception to perform fast numeric computations, to accomplish this, figuring out the best way to store data is very important. Some computing systems store data internally by row; however, most interesting numerical operations usually require processing data by column. For example, calculating the mean of a column requires processing each column on its own, not the entire row. Spark stores data by default by row, since it’s easier to partition; in contrast, R stores data by column. Therefore, something needs to transform both represantations when data is transfered between Spark and R, see Figure 11.6. FIGURE 11.6: Data Transformation between Spark and R This transformation from rows to columns needs to happen for each partition. In addition, data also needs to be transformed from Scala’s internal representation to R’s internal representation. These transformations wastes a lot of CPU cycles, Apache Arrow reduces these transformations. Apache Arrow is a cross-language development platform for in-memory data. In Spark, it speeds up transfering data between Scala and R by defining a common data format compatible with many programming languages – instead of having to transform between Scala’s internal representation and R’s, the same structure can be used for both languages. In addition, transforming data from row based storage to columnar storage, is performed in parallel in Spark, which can be further optimized by using the columnar storage formats presented in the Data chapter. The improved transformation are available in Figure 11.6. FIGURE 11.7: Data Transformation between Spark and R using Arrow Arrow is not required but it is strongly advised while working spark_apply(). It has been available since Spark 2.3.0; however, it requires system administrators to install the Apache Arrow runtime in every node, see arrow.apache.org/install. In addition, to use Apache Arrow with sparklyr you also need to install the arrow package: devtools::install_github(&quot;apache/arrow&quot;, subdir = &quot;r&quot;, ref = &quot;apache-arrow-0.12.0&quot;) Before we use arrow, we will take a measurement to validate system.time( sdf_len(sc, 10^4) %&gt;% spark_apply(nrow) %&gt;% collect() ) user system elapsed 0.240 0.020 7.957 In our particular system, processing 10K rows takes about 8 seconds. To enable Arrow, simply include the library and use spark_apply() as usual. Lets measure how long it takes spark_apply() to process 1M rows: library(arrow) system.time( sdf_len(sc, 10^6) %&gt;% spark_apply(nrow) %&gt;% collect() ) user system elapsed 0.317 0.021 3.922 In our system, Arrow can process 100X more data in half the time, just 4 seconds. Most functionality in arrow simply works on the background improving performance and data serialization; however, there is one setting you should be aware of. The spark.sql.execution.arrow.maxRecordsPerBatch configuiration settings specifies the default size of each arrow data transfer. It’s shared with other Spark components and defaults to 10,000 rows. library(arrow) sdf_len(sc, 2 * 10^4) %&gt;% spark_apply(nrow) # Source: spark&lt;?&gt; [?? x 1] result &lt;int&gt; 1 10000 2 10000 You might need to adjust this number based on how much data your system can handle, making it smaller for large dataset or bigger for operations that require records to be processed together. We can change this setting to 5K rows and verify the partitions change appropriately: spark_disconnect(sc) config &lt;- spark_config() config$spark.sql.execution.arrow.maxRecordsPerBatch &lt;- 5 * 10^3 sc &lt;- spark_connect(master = &quot;local&quot;, config = config) sdf_len(sc, 2 * 10^4) %&gt;% spark_apply(nrow) # Source: spark&lt;?&gt; [?? x 1] result &lt;int&gt; 1 5000 2 5000 3 5000 4 5000 So far we’ve presented use cases, main operations and cluster requirements. The next and last section will teach you troubleshooting techniques useful when distributing R code. 11.10 Troubleshooting A custom transformation can fail for many reasons, to learn how to troubleshoot errors, lets simulate a problem by triggering an errors ourselves: sdf_len(sc, 1) %&gt;% spark_apply(~stop(&quot;force an error&quot;)) Error in force(code) : sparklyr worker rscript failure, check worker logs for details Log: /var/folders/ks/wm_bx4cn70s6h0r5vgqpsldm0000gn/T//Rtmpob83LD/file2aac1a6188_spark.log ---- Output Log ---- 19/03/11 14:12:24 INFO sparklyr: Worker (8057) completed wait using lock for RScript Notice that the error message points out to inspect the logs. When running in local mode, you can simply run: spark_log(sc, filter = &quot;terminated unexpectedly&quot;) 19/03/11 14:12:24 ERROR sparklyr: RScript (8057) terminated unexpectedly: force an error Which points out to the artifial stop(\"force an error\") error we introduced ourselves. However, if you are not working in local mode, you will have to retrieve the worker logs from your cluster manager. Since this can be cumbersome, one alternative is to rerun spark_apply() but return the error message yourself: sdf_len(sc, 1) %&gt;% spark_apply(~tryCatch( stop(&quot;force an error&quot;), error = function(e) e$message )) # Source: spark&lt;?&gt; [?? x 1] result &lt;chr&gt; 1 force an error There are a few other more advanced troubleshooting techniquest applicable to spark_apply(), the following sections present these techniques in-order; meaning, you should try to troubleshoot using worker logs first, followed by identifying partitioning errors and finally, attempting to debug a worker node. 11.10.1 Worker Logs Whenever spark_apply() is executed, information regarding execution is written over each worker node. You can use this log to write custom messages o help you diagnose and fine-tune your code. For instance, suppose that you don’t know what the first column name of df is, we can write a custom log message executed from the worker nodes using spark_apply_log() as follows: sdf_len(sc, 1) %&gt;% spark_apply(function(df) { spark_apply_log(&quot;the first column in the data frame is named &quot;, names(df)[[1]]) df }) # Source: spark&lt;?&gt; [?? x 1] id * &lt;int&gt; 1 1 When running locally, we can filter the log entries for the worker as follows: spark_log(sc, filter = &quot;sparklyr: RScript&quot;) 18/12/18 11:33:47 INFO sparklyr: RScript (3513) the first column in the dataframe is named id 18/12/18 11:33:47 INFO sparklyr: RScript (3513) computed closure 18/12/18 11:33:47 INFO sparklyr: RScript (3513) updating 1 rows 18/12/18 11:33:47 INFO sparklyr: RScript (3513) updated 1 rows 18/12/18 11:33:47 INFO sparklyr: RScript (3513) finished apply 18/12/18 11:33:47 INFO sparklyr: RScript (3513) finished Notice that the logs print our custom log entry showing that id is the name of the first column in the given data frame. This functionality is useful when troubleshooting errors; for instance, if we force an error using the stop() function: sdf_len(sc, 1) %&gt;% spark_apply(function(df) { stop(&quot;force an error&quot;) }) You will get an error similar to, Error in force(code) : sparklyr worker rscript failure, check worker logs for details As suggested in the error, we can look in the worker logs for the specific errors as follows: spark_log(sc) This will show an entry containing the error and the callstack: 18/12/18 11:26:47 INFO sparklyr: RScript (1860) computing closure 18/12/18 11:26:47 ERROR sparklyr: RScript (1860) terminated unexpectedly: force an error 18/12/18 11:26:47 ERROR sparklyr: RScript (1860) collected callstack: 11: stop(&quot;force and error&quot;) 10: (function (df) { stop(&quot;force and error&quot;) })(structure(list(id = 1L), class = &quot;data.frame&quot;, row.names = c(NA, -1L))) Notice that, spark_log(sc) only retrieves the worker logs when using local clusters, when running in proper clusters with multiple machines, you will have to use the tools and user interface provided by the cluster manager to find these log entries. 11.10.2 Resolving Timeouts When running with several hundred executors, it becomes more likely that some tasks will hang indefinetely. You might be in this situation if most of the tasks in your job complete successfully, but a handful of them are still running and do not fail nor succeed. Suppose that we need to calculate the size of many web pages, we could use spark_apply() with something similar to: sdf_len(sc, 3, repartition = 3) %&gt;% spark_apply(~ download.file(&quot;https://google.com&quot;, &quot;index.html&quot;) + file.size(&quot;index.html&quot;)) Some web pages might not exist or take too long to download. In which case, most tasks will succeed, but a few will hang. To prevent a few tasks from blocking all computations, you can use the spark.speculation Spark setting. When this setting is enabled, once 75% of all tasks succeed, Spark will look for tasks taking longer than the median task execution time and retry this tasks. You can use the spark.speculation.multiplier setting to configure the time multiplier used to consider a task slow. Therefore, for the previous example, you can consider configuring Spark to retry tasks that take two times longer than the median as follows: config &lt;- spark_config() config[&quot;spark.speculation&quot;] &lt;- TRUE config[&quot;spark.speculation.multiplier&quot;] &lt;- 4 11.10.3 Inspecting Partition If a particular partition fails, you can detect the broken partition by computing a digest, and then retrieving that particular partition: sdf_len(sc, 3) %&gt;% spark_apply(function(x) { worker_log(&quot;processing &quot;, digest::digest(x), &quot; partition&quot;) # your code }) This will add an entry similar to: 18/11/03 14:48:32 INFO sparklyr: RScript (2566) processing f35b1c321df0162e3f914adfb70b5416 partition When executing this in your cluster, you will have to look in the logs for the task that is not finishing, once you have that digest, you can cancel the job. Then you can use that digest to retrieve that specific data frame to R with something like: broken_partition &lt;- sdf_len(sc, 3) %&gt;% spark_apply(function(x) { if (identical(digest::digest(x), &quot;f35b1c321df0162e3f914adfb70b5416&quot;)) x else x[0,] }) %&gt;% collect() Which you can then run in R to troubleshoot further. 11.10.4 Debugging Workers You can use a debugger, which is a tool to let you execute your code line-by-line, to troubleshoot spark_apply() for local connections. You can start spark_apply() in debug mode using the debug parameter and then following the instructions. sdf_len(sc, 1) %&gt;% spark_apply(function() { stop(&quot;Error!&quot;) }, debug = TRUE) Debugging spark_apply(), connect to worker debugging session as follows: 1. Find the workers &lt;sessionid&gt; and &lt;port&gt; in the worker logs, from RStudio click &#39;Log&#39; under the connection, look for the last entry with contents: &#39;Session (&lt;sessionid&gt;) is waiting for sparklyr client to connect to port &lt;port&gt;&#39; 2. From a new R session run: debugonce(sparklyr:::spark_worker_main) sparklyr:::spark_worker_main(&lt;sessionid&gt;, &lt;port&gt;) As the instructions indicate, you will need to connect “as the worker node” from a different R session and then step through the code. This method is not as straightforward as previous ones, since you will also need to step through some lines of sparklyr code; so this is something we only recommend as a last resort. You can also use the online resources that were described in in the Getting Started chapter. Lets now wrap up this chapter with a brief recap of all the functionality that was presented. 11.11 Recap This chapter presented spark_apply() as an advanced technique that you can use to fill gaps in functionality in Spark or its many extensions. We presented sensible use cases for spark_apply() to parse data, model in parallel many small datasets, perform grid search and call web APIs. You learned how partitions relate to spark_apply(), how to craete custom groups, distribute contextual information across all nodes, troubleshoot problems and presented limitations, cluster configuration caveats. We also introduced Apache Arrow as a library we strongly recommend when using Spark with R and presented installation, use cases and considerations you should be aware. Up to this chapter, we’ve only worked with large datasets of static data. As in, we’ve assumed our datasets do not change over time and remain invariant while analysing, modeing and visualizing them. In the next chapter, Streaming, we will introduce techniques to process datasets which, in addition to being large, are also growing an resemble a stream of information. "],
["streaming.html", "Chapter 12 Streaming 12.1 Spark Streaming 12.2 Working with Spark Streams 12.3 sparklyr extras 12.4 Intro example 12.5 Transformations 12.6 Shiny integration 12.7 Kafka", " Chapter 12 Streaming As the velocity of the generation of data increases, so does the need to real-time analysis. Real-time refers to the ability to continuously analyze data from a constantly updating data feed. This is usually called Stream Analytics. In Stream Analytics, data is analyzed as its generated, not retroactively as it happens with “everyday” analysis. This chapter will cover how to analyze a stream of data using R and Spark. It will also cover basics about stream analysis, along with how to implement these using sparklyr and other R packages. 12.1 Spark Streaming Spark Streaming is an extension of the core Spark API. It is used for processing live streams of data. It does this in a scalable, high-throughput, and fault tolerant way. It also allows for the current data to be joined with the historical data. Spark Streaming works by splitting the live input into small batches. Each batch is processed by Spark individually. The output from Spark is also in small batches. This process is not visible to the user. Spark displays streams as a DStream. The name stands for “discretize stream”. DStream represents the small batches as one continuous stream. Inside Spark, the DStream is represented as a sequence of Resilient Distributed Datasets (RDD). The best resource to learn how Spark analyzes streams is the Apache Spark’s Official site (“Spark Streaming Programming Guide” 2018). This chapter will cover just enough Spark Streaming concepts to help you understand the mechanics of what the R code is doing. It is recommended to read the official resource, specially if you need to implement solutions based on Spark Streaming. 12.2 Working with Spark Streams In practice, a Spark Stream update is a three stage operation. This is the breakdown of the three stages: Read - The stream is expected to append new files in a specified folder. Those new files contain the most recent information from the stream. Spark monitors the folder, and reads the data from the files. The following file formats are supported: CSV, text, JSON, parquet, and orc. Transform - Spark applies the desired operations on top of the data. No special sparklyr functions are needed to transform stream data. You can use same dplyr verbs, Spark transformers and even native R code (via spark_apply()). Write - The results of the transformed input are saved in a different folder. The following file formats are supported: CSV, text, JSON, parquet, and orc. Figure 12.1 provides a visual aid to what each stage does and how they connect: FIGURE 12.1: Working with Spark Streams Here is the breakdown of the available sparklyr functions for reading and writing: Format Read Write CSV stream_read_csv stream_write_csv JSON stream_read_json stream_write_json Kafka stream_read_kafka stream_write_kafka ORC stream_read_orc stream_write_orc Parquet stream_read_parquet stream_write_parquet Text stream_read_text stream_write_text Memory stream_write_memory In the same way all of the read and write operations in sparklyr for Spark Standalone, or in sparklyr’s local mode, the input and output folders are actual Operating System file system folders. For YARN managed clusters, these will be folder locations inside the Hadoop File System (HDFS). Non-file driven applications are also supported, such as Kafka. Kafka will be discussed in the last part of the chapter. 12.3 sparklyr extras The sparklyr package goes beyond providing an easy-to-use-interface to Spark Streaming. The R package includes features which provide a more complete integration with R: An out-of-the box graph visualization to monitor the stream. Stream generator for testing and learning purposes. A Shiny reactive function. It allows Shiny apps to read the contents of a steam. 12.3.1 Stream monitor The stream_view() function will generate a Shiny app which displays the current state, as well as the history, of the stream. An example of how to use it is available in the Intro Example section. 12.3.2 Stream generator The stream_generate_test() function creates a local test stream. This function works independently from a Spark connection. The following example will create five files in sub-folder called “source”. The files will be created one second apart from the previous file’s creation. library(sparklyr) stream_generate_test(iterations = 5, path = &quot;source&quot;, interval = 1) After the function completes, all of the files should show up in the “source” folder. Notice that the file size vary. This is so that it simulates what a true stream would do. file.info(file.path(&quot;source&quot;, list.files(&quot;source&quot;)))[1] ## size ## source/stream_1.csv 44 ## source/stream_2.csv 121 ## source/stream_3.csv 540 ## source/stream_4.csv 2370 ## source/stream_5.csv 7236 The stream_generate_test() by default will create a single numeric variable data frame. readr::read_csv(&quot;source/stream_5.csv&quot;) ## # A tibble: 1,489 x 1 ## x ## &lt;dbl&gt; ## 1 630 ## 2 631 ## 3 632 ## 4 633 ## 5 634 ## 6 635 ## 7 636 ## 8 637 ## 9 638 ## 10 639 ## # ... with 1,479 more rows 12.3.3 Shiny reactive Shiny’s reactive framework is well suited to support streaming information. The idea is that your Shiny app can automatically display the latest results as fast as Spark can process them. The reactiveSpark() function provides that integration. 12.4 Intro example This section will use a very simple example to introduce the mechanics of Spark Streaming, and how sparklyr interacts with it. This is very simple example. It will only move the input contents to the output contents without any transformations being done to it. Open a local Spark session sc &lt;- spark_connect(master = &quot;local&quot;) Remove the “source” and “destination” folders. This step ensures a clean slate if you try to run the example again. if(file.exists(&quot;source&quot;)) unlink(&quot;source&quot;, TRUE) if(file.exists(&quot;destination&quot;)) unlink(&quot;destination&quot;, TRUE) Just like with read_csv(), stream_read_csv() needs a file specification. To save ourselves from providing one, a single test file is generated. stream_generate_test(iterations = 1) stream_read_csv() starts the ingestion part of the job. It corresponds to the 1. Read stage described in the previous section. read_folder &lt;- stream_read_csv(sc, &quot;source&quot;) Set the output of the job to read the incoming data. That is done by passing the read_folder variable, set in the previous step. It corresponds to the 3. Read stage described in the previous section. write_output &lt;- stream_write_csv(read_folder, &quot;destination&quot;) The libraryfuture will allow the test generation to run in a asynchronous fashion. This is needed because the next step, stream_view() will start a Shiny app which takes over the R session. library(future) invisible(future(stream_generate_test(interval = 0.3))) stream_view() is the out-of-the box graph visualization to monitor the stream that was mentioned in the sparklyr Interface section. stream_view(write_output) The Shiny app shows up in the Viewer pane. The column bars will slowly accumulate in the app’s plot After the test generator completes, the plot should look like what Figure 12.2 shows. FIGURE 12.2: stream_view() output The final step is to clean up the stream and Spark connection stream_stop(write_output) spark_disconnect(sc) 12.5 Transformations Streams can be transformed using dplyr, SQL queries, ML Pipelines or R code. We can use as many transformations as needed in the same way that Spark data frames can be transformed with sparklyr. The transformation source can be streams or data frames but the output is always a stream. If needed, one can always take a snapshot from the destination stream and save the output as a data frame, which is what sparklyr will do for you if a destination stream is not specified. 12.5.1 dplyr The same dplyr verbs can be used on top of a Spark Stream. The following example shows how easy it is to filter rows and add columns to data from an input folder. sc &lt;- spark_connect(master = &quot;local&quot;) if(file.exists(&quot;source&quot;)) unlink(&quot;source&quot;, TRUE) stream_generate_test(iterations = 5) stream_read_csv(sc, &quot;source&quot;) %&gt;% filter(x &gt; 700) %&gt;% mutate(y = round(x / 100)) ## # Source: spark&lt;?&gt; [inf x 2] ## x y ## &lt;int&gt; &lt;dbl&gt; ## 1 701 7 ## 2 702 7 ## 3 703 7 ## 4 704 7 ## 5 705 7 ## 6 706 7 ## 7 707 7 ## 8 708 7 ## 9 709 7 ## 10 710 7 ## # ... with more rows It also is possible to perform aggregations over the entire history of the stream. The history could be filtered or not. stream_read_csv(sc, &quot;source&quot;) %&gt;% filter(x &gt; 700) %&gt;% mutate(y = round(x / 100)) %&gt;% count(y) ## # Source: spark&lt;?&gt; [inf x 2] ## y n ## &lt;dbl&gt; &lt;dbl&gt; ## 1 8 200 ## 2 9 200 ## 3 10 102 ## 4 7 98 Grouped aggregations of the latest data in the stream require a time stamp. The time stamp will be of when reading function, in this case stream_read_csv() , first “saw” that specific record. In Spark stream terms, that time stamp is called a “watermark”. The spark_watermark() function is used to add the time stamp. For this exercise, the watermark will be the same for all records. That is because the 5 files were read by the stream after they were created. Please note that only Kafka and memory outputs support watermarks. stream_read_csv(sc, &quot;source&quot;) %&gt;% stream_watermark() ## # Source: spark&lt;?&gt; [inf x 2] ## x timestamp ## &lt;int&gt; &lt;dttm&gt; ## 1 630 2019-04-07 15:44:50 ## 2 631 2019-04-07 15:44:50 ## 3 632 2019-04-07 15:44:50 ## 4 633 2019-04-07 15:44:50 ## 5 634 2019-04-07 15:44:50 ## 6 635 2019-04-07 15:44:50 ## 7 636 2019-04-07 15:44:50 ## 8 637 2019-04-07 15:44:50 ## 9 638 2019-04-07 15:44:50 ## 10 639 2019-04-07 15:44:50 ## # ... with more rows After the watermark is created, it can be used in the group_by() verb. It can then be piped into a summarise() function to get some stats of the stream. stream_read_csv(sc, &quot;source&quot;) %&gt;% stream_watermark() %&gt;% group_by(timestamp) %&gt;% summarise( max_x = max(x, na.rm = TRUE), min_x = min(x, na.rm = TRUE), count = n() ) ## # Source: spark&lt;?&gt; [inf x 4] ## timestamp max_x min_x count ## &lt;dttm&gt; &lt;int&gt; &lt;int&gt; &lt;dbl&gt; ## 1 2019-04-07 15:45:59 1000 1 2122 12.5.2 Transformer functions Transformer functions can also be used to modify a stream. They can also be combined with the regular dplyr functions. stream_read_csv(sc, &quot;source&quot;) %&gt;% mutate(x = as.numeric(x)) %&gt;% ft_bucketizer(&quot;x&quot;, &quot;buckets&quot;, splits = 0:10 * 100) %&gt;% count(buckets) %&gt;% arrange(buckets) ## # Source: spark&lt;?&gt; [inf x 2] ## # Ordered by: buckets ## buckets n ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 299 ## 2 1 220 ## 3 2 200 ## 4 3 200 ## 5 4 200 ## 6 5 200 ## 7 6 201 ## 8 7 200 ## 9 8 200 ## 10 9 202 12.5.3 R code Arbitrary R code can also be used to transform a stream with the use of spark_apply(). Following the same principles from executing R code over Spark data frames, for structured streams, spark_apply() runs R code over each executor in the cluster where data is available, this enables processing high-throughput streams and fulfill low-latency requirements. stream_read_csv(sc, &quot;source&quot;) %&gt;% spark_apply(~ nrow(.x), list(n=&quot;integer&quot;)) ## # Source: spark&lt;?&gt; [inf x 1] ## n ## &lt;int&gt; ## 1 1962 ## 2 148 ## 3 12 12.5.4 ML Pipelines Spark pipelines can be used for scoring streams, but not to train over streaming data. The former is fully supported while the latter is a feature under active development by the Spark community. In order to try scoring data in a stream, it is necessary to first create a Pipeline Model. The following build, fits and saves a simple pipeline. It also opens and closes the Spark connection. sc &lt;- spark_connect(master = &quot;local&quot;) cars &lt;- copy_to(sc, mtcars, &quot;mtcars_remote&quot;) sc %&gt;% ml_pipeline() %&gt;% ft_binarizer(&quot;mpg&quot;, &quot;over_30&quot;,30) %&gt;% ft_r_formula(over_30 ~ wt) %&gt;% ml_logistic_regression() %&gt;% ml_fit(cars) %&gt;% ml_save(&quot;cars_model&quot;) spark_disconnect(sc) A new connection of Spark is opened. The saved model is loaded into the new connection. sc &lt;- spark_connect(master = &quot;local&quot;) model &lt;- ml_load(sc, &quot;cars_model&quot;) Data that can be used for predictions is needed. The stream_generate_test() can be used for this as well. Instead of relying on the default output, the mtcars variable is passed to it. if(file.exists(&quot;source&quot;)) unlink(&quot;source&quot;, TRUE) stream_generate_test(mtcars, iterations = 5) The ml_transform() function can now be used on top of the stream. Because the function expects the model as the first function, the piping works a little different. Instead of starting with reading the stream, we start with the model, and use the stream input as the argument on ml_transform() model %&gt;% ml_transform(stream_read_csv(sc, &quot;source&quot;)) ## # Source: spark&lt;?&gt; [inf x 17] ## mpg cyl disp hp drat wt qsec vs am ## &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; ## 1 15.5 8 318 150 2.76 3.52 16.9 0 0 ## 2 15.2 8 304 150 3.15 3.44 17.3 0 0 ## 3 13.3 8 350 245 3.73 3.84 15.4 0 0 ## 4 19.2 8 400 175 3.08 3.84 17.0 0 0 ## 5 27.3 4 79 66 4.08 1.94 18.9 1 1 ## 6 26 4 120. 91 4.43 2.14 16.7 0 1 ## 7 30.4 4 95.1 113 3.77 1.51 16.9 1 1 ## 8 15.8 8 351 264 4.22 3.17 14.5 0 1 ## 9 19.7 6 145 175 3.62 2.77 15.5 0 1 ## 10 15 8 301 335 3.54 3.57 14.6 0 1 ## # ... with more rows, and 8 more variables: gear &lt;int&gt;, ## # carb &lt;int&gt;, over_30 &lt;dbl&gt;, features &lt;list&gt;, ## # label &lt;dbl&gt;, rawPrediction &lt;list&gt;, ## # probability &lt;list&gt;, prediction &lt;dbl&gt; 12.6 Shiny integration The reactiveSpark() provides a mechanism to process the transformations on a stream. It allows you to circumvent the need for writing an output. Also, because it does not depend on the stream writing functions, it is possible to to use watermark groups. This section’s example will result in a Shiny app. It will start to accumulate and display the current and historical results. The app’s output is shown on Figure 12.3. Start by opening a Spark connection and begin a test generation. sc &lt;- spark_connect(master = &quot;local&quot;) if(file.exists(&quot;source&quot;)) unlink(&quot;source&quot;, TRUE) invisible(future(stream_generate_test(interval = 0.2, iterations = 10))) Load the shiny library and create a simple UI function with one table output. library(shiny) ui &lt;- function() tableOutput(&quot;table&quot;) The server function contains a reactiveSpark() function. This function reads the stream, adds the watermark and then performs the aggregation. The results are then rendered via the table output. server &lt;- function(input, output, session){ ps &lt;- stream_read_csv(sc, &quot;source&quot;) %&gt;% stream_watermark() %&gt;% group_by(timestamp) %&gt;% summarise( max_x = max(x, na.rm = TRUE), min_x = min(x, na.rm = TRUE), count = n()) %&gt;% reactiveSpark() # Spark stream reactive output$table &lt;- renderTable( ps() %&gt;% mutate(timestamp = as.character(timestamp)) )} The Shiny app can be activated with runGadget(). runGadget(ui, server) FIGURE 12.3: Shiny reactive 12.7 Kafka Apache Kafka is to streaming as what Hadoop is to data storage/retrieval. Hadoop provides a a distributed, resilient and reliable architecture for large-scale data storage. Kafka does the same, but for large-scale streaming applications. 12.7.1 Workflow A most basic Kafka workflow is illustrated on Figure 12.4. An application streams data into Kafka, called a Producer. Kafka stores the stream as records. Each record has a key, a value and a timestamp. Kafka can handle multiple streams that contain different information. To properly categorize each stream, Kafka uses a mechanism called topic. A topic is a alpha-numeric identifier of the stream. A Consumer is an app that is external to Kafka. It reads what is stored in Kafka for a given topic. Because the Consumer app is constantly monitoring the topic, the term used for its integration with Kafka is subscribe. FIGURE 12.4: Basic workflow Kafka also allows for an application to read from one topic, process its data, and then write the results to a different topic. That is called a Stream Processor. In Figure 12.5, the Stream Processor reads topic A, and then writes results to topic B. This allows for a given Consumer application to read results instead of “raw” feed data. FIGURE 12.5: Kafka workflow 12.7.2 Spark integration Spark Streaming enables the integration of Spark with Kafka. Spark is able to both, read from and write into Kafka topics. This means that Spark could be a Consumer, Stream Processor or Producer application of a Kafka implementation. Unless there is a very specific need, using Spark as a Producer does not make much sense. That is because Spark Streaming reacts to a stream, it doesn’t generate it. Theoretically, there could be cases where Spark reads from one stream (Kafka or not) and streams to a Kafka cluster. To the target, Spark would look as a pure Producer, even though that’s not really the case. The more likely use case for this integration, is to use Spark to read (Consumer) from one, or several, topics and then reactively write (Producer) to a different topic with the results of the analysis. All within the same Kafka cluster. This effectively makes Spark a Stream Processor. There are nuances on how the Spark-to-Kafka write-back modes works. It is important to offer some clarification. There are three modes available: complete, update and append. The complete mode will provide the totals for every group every time there is a new batch. The update mode will provide totals for only the groups that have updates in the latest batch. The append mode is able to add raw records to the target topic. This mode is not meant for aggregates, but works well for passing a filtered subset to the target topic. 12.7.3 R integration The R integration is delivered via the sparklyr interface. There are a couple of things to keep in mind: The Kafka integration Spark package is required - The name should be org.apache.spark:spark-sql-kafka followed by the Kafka version, then the Scala version, and lastly the Spark package version. For example: org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0, requests the version 2.4.0 of the Spark package that supports Scala 2.11 and Kafka 10. config &lt;- spark_config() config$sparklyr.shell.packages &lt;- &quot;org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0&quot; sc &lt;- spark_connect(master = &quot;local&quot;, config = config) The Kafka writing and reading function in sparklyr rely on additional parameters - These parameters are passed via the options argument. The contents of the options argument are passed down to Kafka as-is. This means that the same Kafka options used in your other applications can be reused here. There are three basic Kafka options to keep in mind: kafka.bootstrap.server, topic and subscribe. The former expects a list of the of one or more hosts from the Kafka cluster. The other two set the topic that the function is either reading from or writing to. One is used at the exclusion of the other. For reading the subscribe option is used, and for writing, topic is used. stream_read_kafka( sc, options = list( kafka.bootstrap.server = &quot;host1:9092, host2:9092&quot;, subscribe = &quot;topic&quot; ) ) 12.7.4 Example For the example, the Producer will stream random, single letters of the alphabet into Kafka. The topic will be called “letters”. A Stream Processor will be built in Spark by having it read the “letters” topic, and then produce the count by unique letter passed throught the stream. The count will be passed back to Kafka in separate topic called “totals”. To see the results, the same Spark connection will be used to setup a Consumer that reads the “totals” topic. Figure 12.6 is a diagram of how this example will work. FIGURE 12.6: Kafka example The example will use the update mode for writing back into Kafka. This means that only the totals of the letters that changed will be sent to the “totals” topic. The change in totals is determined after each batch from the “letters” topic is received. Figure 12.7 offers an deeper look of what the Stream Processor (Spark) process is supposed to do. FIGURE 12.7: Stream Processor - Update mode The infrastructure used for this example was a local, single node Kafka cluster. The external Producer uses the Kafka CLI to send the stream of letters. The installation instructions that were used can be found in the Appendix under the Kafka section. Load libraries, and open the connection. Remember to load the Kafka integration Spark package. library(sparklyr) library(dplyr) config &lt;- spark_config() config$sparklyr.shell.packages &lt;- &quot;org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0&quot; sc &lt;- spark_connect(master = &quot;local&quot;, config = config) The local Kafka cluster is served on port 9092, by default. In order to keep the read and write calls a little cleaner, a couple of variables will contain the Kafka options. Notice that the read option has subscribe, while the write option contains topic. hosts &lt;- &quot;localhost:9092&quot; read_options &lt;- list(kafka.bootstrap.servers = hosts, subscribe = &quot;letters&quot;) write_options &lt;- list(kafka.bootstrap.servers = hosts, topic = &quot;totals&quot;) Typically, the following steps would be written together using a pipe. So that explanations can be shared, they have been broken into individual step variables. What is unique about this setup is the use of the read_options variable as the options argument. step_1 &lt;- stream_read_kafka(sc, options = read_options) This steps coerces the value field into a character. The resulting content of the field is a single letter entry from the stream. The letters are grouped and counted. A single field is permitted by Kafka to send back as results. It also expect that the results are in a field called value. The new value field is a concatenated field with the letter and the count. step_2 &lt;- step_1 %&gt;% mutate(value = as.character(value)) %&gt;% count(value) %&gt;% mutate(value = paste0(value, &quot;=&quot;, n)) The results are written to the “totals” topic. Themode argument is set to “update”. This sets the count behavior illustrated in Figure 12.8 step_3 &lt;- step_2 %&gt;% stream_write_kafka(mode = &quot;update&quot;, options = write_options) The last step starts a Spark job. The job will remain active until stopped or until Spark disconnects. At this point, there is no visible output. Even if there was an active Producer sending letters over. A simple Shiny routine can be used as a Consumer app. It will read the “totals” topic, select the latest count for a given letter, and then display the results on a table. library(shiny) ui &lt;- function() tableOutput(&quot;table&quot;) server &lt;- function(input, output, session){ totals_options &lt;- list(kafka.bootstrap.servers = hosts, subscribe = &quot;totals&quot;) ps &lt;- stream_read_kafka(sc, options = totals_options) %&gt;% mutate(value = as.character(value), letter = substr(value, 1,1), total = as.numeric(substr(value, 3, 100)) ) %&gt;% group_by(letter) %&gt;% summarise(total = max(total, na.rm = TRUE)) %&gt;% arrange(letter) %&gt;% reactiveSpark() output$table &lt;- renderTable(ps()) } runGadget(ui, server) A new terminal session is started. Kafka’s CLI provides a simple Producer program that runs in the console. Using that program, we can manually write a single letter, and then press enter. user@laptop:~/kafka_2.12-2.2.0$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic letters &gt;A &gt;B &gt;C &gt;A &gt;A &gt;C &gt;D The Shiny reactive function will poll and refresh the results as the letters are being entered. This is shown in figure 11.8. FIGURE 12.8: Shiny with Kafka References "],
["contributing.html", "Chapter 13 Contributing 13.1 Overview 13.2 Spark API 13.3 Spark Extensions 13.4 Scala Code 13.5 Recap", " Chapter 13 Contributing The previous chapter, Streaming, equipped you with tools to tackle large-scale and real-time data processing in Spark using R. In a way, the previous chapter was the last learning chapter, while this last chater is less focused on learning and more on giving back to the Spark and R communities or collegues in your professional career. There are many ways to contribute, from helping community members to opening GitHub issues, to providing new functionality for yourself, colleagues or the R and Spark community; however, this chapter will focus on writting and sharing code that extends Spark to help others use new functionality that you can provide as an author of Spark extensions using R. Specifically, in this chapter you will learn what an extension is, when to build one, what tools are available, the different types of extensions you can consider buildiung and how to build one from scratch. You will also learn how to make use of hundreds of extensions available in Spark and millions of components available in Java that you can use in R with ease. You will also learn how to create code natevely in Scala that makes use of Spark; as you might know, R is a great language to interface with other languages, like C++, SQL, Python and many other languages. It then to no surprise that working with Scala from R will follow similar practices that make R ideal to provide easy-to-use interfaces that make data-processing productive and that are loved by many of us. 13.1 Overview When thinking of contributing back, the most important question you can ask about the code above – but really, about any piece of code you write is: Would this code be useful to someone else? We can start by considering one of the first and simplest lines of code presented in this book, this code was used to load a simple CSV file: spark_read_csv(sc, &quot;cars.csv&quot;) For the code above, the answer is probably no, the code is not useful to someone else; however, a more useful example would be to tailor that same example to something someone will actually care about, perhaps: spark_read_csv(sc, &quot;/path/that/is/hard/to/remember/data.csv&quot;) The code above is quite similar to the original one; however, assuming that you work with others that care about this dataset, the answer to: Would this code be useful to someone else? Is now completely different: Yes, most likely! This is surprising since this means that not all useful code needs to be advanced nor complicated; however, for it to be useful to others, it does need to be packaged, presented and shared in a format that is easy to consume. One first attempt would be to save this into a teamdata.R file and write a function wrapping it: load_team_data &lt;- function() { spark_read_text(sc, &quot;/path/that/is/hard/to/remember/data.csv&quot;) } This is an improvement; however, it would require users to manually share this file over and over. Fortunately, this is a problem well solved in R through R Packages. An R package contains R code packaged in a format installable using the install.packages() function. sparklyr is an R package, but there are many other packages available in R and you can also create your own packages. For those of you new to creating R packages, I would encourage reading Hadley Wickam’s book on packages: R Packages: Organize, Test, Document, and Share Your Code. Creating an R package allows you to easily share your functions with others by sharing the package file in your organization. Once a package is created, there are many ways to share this with colleagues or the world. For instance, for packages meant to be private, you can consider using Drat or products like RStudio Package Manager. R packages meant for public consumption are made available to the R community in CRAN, which stands for the Comprehensive R Archive Network. These repositories of R packages make packages allow users to install packages through install.packages(\"teamdata\") without having to worry where to download the package from and allows other packages to reuse your package as well. In addition to using R packages like sparklyr, dplyr, broom, etc. to create new R packages that extend Spark; you can also make use of all the functionality available in the Spark API, Spark Extensions or write custom Scala code. For instance, suppose that there is a new file format similar to a CSV but not quite the same, we might want to write a function named spark_read_file() that would take a path to this new file type and read it in Spark. One approach would be to use dplyr to process each line of text or any other R library using spark_apply(). Another approach would be to use the Spark API to access methods provided by Spark. A third approach would be to find if someone in the Spark community has already provided an Spark Extension that supports this new file format. Last but not least, you can write our own custom Scala Code that makes use any Java library, including Spark and its extensions. This is illustrated in a Figure 13.1. FIGURE 13.1: Extending Spark using the Spark API, Spark extensions or Scala code We will focus first on extending Spark using the Spark API since the techniques required to call the Spark API are also aplicable while calling Spark extensions or custom Scala code. 13.2 Spark API Before we introduce the Spark API, lets consider a simple and well known problem. Suppose we want to count the number on lines in a distributed and potentially large text file, say, cars.csv that we initialize as follows: library(sparklyr) library(dplyr) sc &lt;- spark_connect(master = &quot;local&quot;) cars &lt;- copy_to(sc, mtcars) spark_write_csv(cars, &quot;cars.csv&quot;) Now, in order to count how many lines are available in this file we can run: spark_read_text(sc, &quot;cars.csv&quot;) %&gt;% count() # Source: spark&lt;?&gt; [?? x 1] n &lt;dbl&gt; 1 33 Easy enough, we used spark_read_text() to read the entire text file, followed by counting lines using dplyr’s count(). Now, suppose that spark_read_text(), dplyr nor any other Spark functionality is available to you; how would you ask Spark to count the number of rows in cars.csv? If you were to do this in Scala, you will be able to find in the Spark documentation that using the Spark API you can count lines in a file as follows: val textFile = spark.read.textFile(&quot;cars.csv&quot;) textFile.count() So, in order to use functionality available in the Spark API from R, like spark.read.textFile; you can use invoke(), invoke_static() or invoke_new(). As their names suggest, one invokes a method from an object, the second one invokes a method from a static object and the third creates a new object. We can then use these functions to call Spark’s API and execute similar code as the one provided in Scala: spark_context(sc) %&gt;% invoke(&quot;textFile&quot;, file, 1L) %&gt;% invoke(&quot;count&quot;) [1] 33 While the invoke() function was originally designed to call Spark code, it can call any code available in Java. For instance, we can create a Java BigInteger with the following code: invoke_new(sc, &quot;java.math.BigInteger&quot;, &quot;1000000000&quot;) As you can see, the object that gets created is not an R object but rather – a proper Java object. In R, this Java object is represented by the spark_jobj. These objects are only meant to be used with the invoke() functions or spark_dataframe() and spark_connection(). spark_dataframe() transforms a spark_jobj into a Spark DataFrame, when possible; while spark_connect() retrieves the original Spark connection object, which can be useful to avoid passing the sc object across functions. While calling the Spark API can be useful in some cases, most of the funtionality available in Spark is already supported in sparklyr; therefore, a more interesting way to extend Spark is by using one of its many existing extensions. 13.3 Spark Extensions Before we get started with this section, consider navigating to spark-packages.org – a site that tracks Spark extensions provided by the Spark community. Using the same techniques presented in the previous section, you can make use of these extensions from R. For instance, Apache Solr is a “blazing-fast, open source enterprise search platform built on Apache Lucene” (“Apache Solr” 2019). Solr is a system designed to perform full text search over large datasets which Apache Spark currently does not support natevely. Also, as of this writting, there is no extension for R to support Solr. So let’s try to solve this using an Spark extension. First, you would want to search “spark-packages.org” to find out that there is a Solr extension, you should be able to find spark-solr (“Spark-Solr Spark Package” 2019). The extension “How to” mentioned that the com.lucidworks.spark:spark-solr:2.0.1 should be loaded. We can accomplish this in R using the sparklyr.shell.packages configuration option: config &lt;- spark_config() config[&quot;sparklyr.shell.packages&quot;] &lt;- &quot;com.lucidworks.spark:spark-solr:3.6.3&quot; config[&quot;sparklyr.shell.repositories&quot;] &lt;- &quot;http://repo.spring.io/plugins-release/,http://central.maven.org/maven2/&quot; sc &lt;- spark_connect(master = &quot;local&quot;, config = config) While specifying the sparklyr.shell.packages parameter is usually enough, for this particular extension, dependencies failed to download from the Spark Packages repository. For the failed dependencies, you would have to manually find them in the Maven repo (mvnrepository.com) and add additional repositories under the sparklyr.shell.repositories parameter. Note: When using an extension, Spark connects to the Maven package repository to retrieve the extension, this can take significant time depending on the extension and your download speed. Therefore, you should consider using the sparklyr.connect.timeout configuration parameter which defines the total seconds to wait before set to several minutes if you experience a connection error. From the spark-solr documentation, you would find that this extension can be used with the following Scala code: val options = Map( &quot;collection&quot; -&gt; &quot;{solr_collection_name}&quot;, &quot;zkhost&quot; -&gt; &quot;{zk_connect_string}&quot; ) val df = spark.read.format(&quot;solr&quot;) .options(options) .load() Which we can translate to R code: spark_session(sc) %&gt;% invoke(&quot;read&quot;) %&gt;% invoke(&quot;format&quot;, &quot;solr&quot;) %&gt;% invoke(&quot;option&quot;, &quot;collection&quot;, &quot;&lt;collection&gt;&quot;) %&gt;% invoke(&quot;option&quot;, &quot;zkhost&quot;, &quot;&lt;host&gt;&quot;) %&gt;% invoke(&quot;load&quot;) The code above will fail since it would require a valid Solr instance and configuring Solr goes beyond the scope of this book; however, this example provides a useful insight as to how you can to use Spark extensions. It’s also worth mentioning that spark_read_source() can be used to read from generic sources to avoid writting custom invoke() code. As pointed out in the Overview section, you should consider sharing code with others using R package. While you could require users of your package to specify sparklyr.shell.packages, you can avoid this by registering dependencies in your package. Dependencies are declared under a spark_dependencies() function, for the the example in this section: spark_dependencies &lt;- function(spark_version, scala_version, ...) { spark_dependency( packages = &quot;com.lucidworks.spark:spark-solr:3.6.3&quot;, repositories = c( &quot;http://repo.spring.io/plugins-release/&quot;, &quot;http://central.maven.org/maven2/&quot;) ) } .onLoad &lt;- function(libname, pkgname) { sparklyr::register_extension(pkgname) } The onLoad function will be automatically called by R when you library loads, it should call register_extension() which will then call back spark_dependencies() to allow your extension to provide additional dependencies. The example above supports Spark 2.4 but you should also support a map of Spark and Scala versions to the correct Spark extension version. There are about 450 Spark extensions you can use; in addition, you can also use any Java library from a Maven repository where Maven Central has over 3M artifacts (“Maven Repository: Repositories” 2019). While not all the Maven Central libraries might be relevant to Spark, the combination of Spark extensions and Maven repositories certainly opens many interesting possibilities for you to consider! However, for those cases where no Spark extension is available, the next section will teach you how to use custom Scala code from your own R package. 13.4 Scala Code Scala code enables you to use any method in the Spark API, Spark extensions or Java library; in addition, writting Scala code when running in Spark can provide performance improvements over R code using spark_apply(). In general, the structure of your R package will contains R code and Scala code; however, the Scala code will be need to be compiled as JARs (Java ARchive files) and included in your package. Conceptually, your R package will look as shown in Figure 13.2. FIGURE 13.2: R package structure when using Scala code As usual, the R code should be placed under a top-level R folder, Scala code under a java folder while the compiled JARs are distributed under an inst/java folder. While you are certainly welcomed to manually compiled the Scala code, you can use helper functions to download the required compiler and compile Scala code. In order to compile Scala code, you will need the Java Development Kit 8 installed (JDK8 for short); the JDK can be downloaded from oracle.com/technetwork/java/javase/downloads/ and will require you to restart your R session. You also need a Scala compiler for Scala 2.11 and 2.12 from https://www.scala-lang.org/; the Scala compilers can be automatically downloaded and installed using download_scalac: download_scalac() Next you will need to compile your Scala sources using compile_package_jars(). By default, it uses spark_compilation_spec() which compiles your sources for the following Spark versions: ## [1] &quot;1.5.2&quot; &quot;1.6.0&quot; &quot;2.0.0&quot; &quot;2.3.0&quot; &quot;2.4.0&quot; You can also customize this specification by creating custom entries with spark_compilation_spec(). While you can create the project structure for Scala code by hand, you can also simply call spark_extension(path) to create an extension in the given path; this extension will be mostly empty but will contain the appropriate project structure to call Scala code. Since spark_extension() is registered as a custom project extension in RStudio; you can also create an R package that extends Spark usign Scala code from the File menu, New Project... and selecting R Package using sparklyr as shown in Figure 13.3. FIGURE 13.3: Creating an Scala extension package from RStudio Once you are ready to compile your package JARs, you can simply run: compile_package_jars() Since the JARs are compiled by default into the inst/ package path, when building the R package all the JARs will also get included within the package, this means that you can share or publish your R package and it will be fully functional by R users. For advanced Spark users with most of their expertise in Scala, it should be quite compelling to consider writting libraries for R users and the R community in Scala and then easily packaging into R packages that are easy to consume, use and share among them. 13.5 Recap This last chapter introduced you to an entire new set of tools you can use to expand Spark functionality beyond what R and R packages currently support, this vast new space of libraries includes over 450 spark extensions and millions of Java artifacts you can use in Spark from R. Beyond these resources, you also learned how to build your own Java artifacts using Scala code that can be easily embeded and compiled from R. This bring us back to purpose of this book presented in Introduction chapter, while we know that in this chapter and previous ones you’ve learned how to perform large-scale compuring using Spark in R; we are also confident that you have acquired the knowledged required to help other community members through Spark extensions – we can’t wait to see your new creations, which will surely help grow the Spark and R communities at large. To close and recap on the entire book, we hope the first chapters gave you a easy intro to Spark and R, followed by the Analysis and Modeling chapters which gave you the foundations for using Spark from the familiarity of R package you know and love. You moved then to learned how to perform large-scale computation in proper Spark clusters. The last third of this book focused on advanced topics on using extensions, distributing R code, processing realtime data and finally, contributing back Spark extensions using R and potentially, also Scala code. We tried presenting the best possible content; however, if there is room to improve this book please open a GitHub issue under github.com/javierluraschi/the-r-in-spark which we can address in upcoming revisions. We hope you enjoyed reading this book, that you’ve learned as much as we’ve learned while writting this book and that is has been worthy of your time. It has been an honor having you as our reader. References "],
["appendix.html", "Chapter 14 Appendix 14.1 Prerequisites 14.2 Diagrams 14.3 Formatting 14.4 List of ML Functions 14.5 Kafka", " Chapter 14 Appendix 14.1 Prerequisites 14.1.1 Installing R From r-project.org, download and launch the R installer for your platform, Windows, Macs or Linux available. FIGURE 14.1: The R Project for Statistical Computing 14.1.2 Installing Java From java.com/download, download and launch the installer for your platform, Windows, Macs or Linux are also available. FIGURE 14.2: Java Download Page Starting with Spark 2.1, Java 8 is required; however, previous versions of Spark support Java 7. Regardless, we recommend installing Java Runtime Engine 8, or JRE 8 for short. Note: For advanced readers that are already using the Java Development Kit, JDK for short. Please notice that JDK 9+ is currently unsupported so you will need to downgrade to JDK 8 by uninstalling JDK 9+ or by setting JAVA_HOME appropiately. 14.1.3 Installing RStudio While installing RStudio is not strictly required to work with Spark with R, it will make you much more productive and therefore, I would recommend you take the time to install RStudio from rstudio.com/download, then download and launch the installer for your platform: Windows, Macs or Linux. FIGURE 14.3: RStudio Downloads Page After launching RStudio, you can use RStudio’s console panel to execute the code provided in this chapter. 14.1.4 Using RStudio If you are not familiar with RStudio, you should make note of the following panes: Console: A standalone R console you can use to execute all the code presented in this book. Packages: This pane allows you to install sparklyr with ease, check its version, navigate to the help contents, etc. Connections: This pane allows you to connecto to Spark, manage your active connection and view the available datasets. FIGURE 14.4: RStudio Overview 14.2 Diagrams 14.2.1 Worlds Store Capacity library(tidyverse) read_csv(&quot;data/01-worlds-capacity-to-store-information.csv&quot;, skip = 8) %&gt;% gather(key = storage, value = capacity, analog, digital) %&gt;% mutate(year = X1, terabytes = capacity / 1e+12) %&gt;% ggplot(aes(x = year, y = terabytes, group = storage)) + geom_line(aes(linetype = storage)) + geom_point(aes(shape = storage)) + scale_y_log10( breaks = scales::trans_breaks(&quot;log10&quot;, function(x) 10^x), labels = scales::trans_format(&quot;log10&quot;, scales::math_format(10^x)) ) + theme_light() + theme(legend.position = &quot;bottom&quot;) 14.2.2 Daily downloads of CRAN packages downloads_csv &lt;- &quot;data/01-intro-r-cran-downloads.csv&quot; if (!file.exists(downloads_csv)) { downloads &lt;- cranlogs::cran_downloads(from = &quot;2014-01-01&quot;, to = &quot;2019-01-01&quot;) readr::write_csv(downloads, downloads_csv) } cran_downloads &lt;- readr::read_csv(downloads_csv) ggplot(cran_downloads, aes(date, count)) + geom_point(colour=&quot;black&quot;, pch = 21, size = 1) + scale_x_date() + xlab(&quot;&quot;) + ylab(&quot;&quot;) + theme_light() 14.2.3 Google trends for mainframes, cloud computing and kubernetes Data downloaded from https://trends.google.com/trends/explore?date=all&amp;q=cloud%20computing,mainframe,kubernetes. library(r2d3) lines &lt;- readLines(&quot;data/clusters-trends.csv&quot;) lines &lt;- gsub(&quot;&lt;1&quot;, 0, lines) writeLines(lines, &quot;data/clusters-trends.csv&quot;) read.csv(&quot;data/clusters-trends.csv&quot;, skip = 2) %&gt;% mutate(year = as.Date(paste(Month, &quot;-01&quot;, sep = &quot;&quot;))) %&gt;% mutate(`On-Premise` = `mainframe...Worldwide.`, Cloud = `cloud.computing...Worldwide.`, Kubernetes = `kubernetes...Worldwide.`) %&gt;% tidyr::gather(`On-Premise`, Cloud, Kubernetes, key = &quot;trend&quot;, value = &quot;popularity&quot;) %&gt;% ggplot(aes(x=year, y=popularity, group=trend)) + geom_line(aes(linetype = trend, color = trend)) + scale_x_date(date_breaks = &quot;2 year&quot;, date_labels = &quot;%Y&quot;) + labs(title = &quot;Cluster Computing Trends&quot;, subtitle = &quot;Search popularity for on-premise (mainframe), cloud computing and kubernetes &quot;) + scale_color_grey(start = 0.6, end = 0.2) + geom_hline(yintercept = 0, size = 1, colour = &quot;#333333&quot;) 14.3 Formatting The following ggplot2 theme was use to format plots in this book: plot_style &lt;- function() { font &lt;- &quot;Helvetica&quot; ggplot2::theme_classic() + ggplot2::theme( plot.title = ggplot2::element_text(family = font, size=14, color = &quot;#222222&quot;), plot.subtitle = ggplot2::element_text(family=font, size=12, color = &quot;#666666&quot;), legend.position = &quot;right&quot;, legend.background = ggplot2::element_blank(), legend.title = ggplot2::element_blank(), legend.key = ggplot2::element_blank(), legend.text = ggplot2::element_text(family=font, size=14, color=&quot;#222222&quot;), axis.title.y = ggplot2::element_text(margin = ggplot2::margin(t = 0, r = 8, b = 0, l = 0), size = 14, color=&quot;#666666&quot;), axis.title.x = ggplot2::element_text(margin = ggplot2::margin(t = -2, r = 0, b = 0, l = 0), size = 14, color = &quot;#666666&quot;), axis.text = ggplot2::element_text(family=font, size=14, color=&quot;#222222&quot;), axis.text.x = ggplot2::element_text(margin = ggplot2::margin(5, b = 10)), axis.ticks = ggplot2::element_blank(), axis.line = ggplot2::element_blank(), panel.grid.minor = ggplot2::element_blank(), panel.grid.major.y = ggplot2::element_line(color = &quot;#eeeeee&quot;), panel.grid.major.x = ggplot2::element_line(color = &quot;#ebebeb&quot;), panel.background = ggplot2::element_blank(), strip.background = ggplot2::element_rect(fill = &quot;white&quot;), strip.text = ggplot2::element_text(size = 20, hjust = 0) ) } Which you can then active with: ggplot2::theme_set(plot_style()) 14.4 List of ML Functions The following table exhibits the ML algorithms supported in sparklyr: 14.4.1 Classification Algorithm Function Decision Trees ml_decision_tree_classifier() Gradient-Boosted Trees ml_gbt_classifier() Linear Support Vector Machines ml_linear_svc() Logistic Regression ml_logistic_regression() Multilayer Perceptron ml_multilayer_perceptron_classifier() Naive-Bayes ml_naive_bayes() One vs Rest ml_one_vs_rest() Random Forests ml_random_forest_classifier() 14.4.2 Regression Algorithm Function Accelerated Failure Time Survival Regression ml_aft_survival_regression() Decision Trees ml_decision_tree_regressor() Generalized Linear Regression ml_generalized_linear_regression() Gradient-Boosted Trees ml_gbt_regressor() Isotonic Regression ml_isotonic_regression() Linear Regression ml_linear_regression() 14.4.3 Clustering Algorithm Function Bisecting K-Means Clustering ml_bisecting_kmeans() Gaussian Mixture Clustering ml_gaussian_mixture() K-Means Clustering ml_kmeans() Latent Dirichlet Allocation ml_lda() 14.4.4 Recommendation Algorithm Function Alternating Least Squares Factorization ml_als() 14.4.5 Frequent Pattern Mining Algorithm Function FPGrowth ml_fpgrowth() 14.4.6 Feature Transformers Transformer Function Binarizer ft_binarizer() Bucketizer ft_bucketizer() Chi-Squared Feature Selector ft_chisq_selector() Vocabulary from Document Collections ft_count_vectorizer() Discrete Cosine Transform ft_discrete_cosine_transform() Transformation using dplyr ft_dplyr_transformer() Hadamard Product ft_elementwise_product() Feature Hasher ft_feature_hasher() Term Frequencies using Hashing export(ft_hashing_tf) Inverse Document Frequency ft_idf() Imputation for Missing Values export(ft_imputer) Index to String ft_index_to_string() Feature Interaction Transform ft_interaction() Rescale to [-1, 1] Range ft_max_abs_scaler() Rescale to [min, max] Range ft_min_max_scaler() Locality Sensitive Hashing ft_minhash_lsh() Converts to n-grams ft_ngram() Normalize using the given P-Norm ft_normalizer() One-Hot Encoding ft_one_hot_encoder() Feature Expansion in Polynomial Space ft_polynomial_expansion() Maps to Binned Categorical Features ft_quantile_discretizer() SQL Transformation ft_sql_transformer() Standardizes Features using Corrected STD ft_standard_scaler() Filters out Stop Words ft_stop_words_remover() Map to Label Indices ft_string_indexer() Splits by White Spaces ft_tokenizer() Combine Vectors to Row Vector ft_vector_assembler() Indexing Categorical Feature ft_vector_indexer() Subarray of the Original Feature ft_vector_slicer() Transform Word into Code ft_word2vec() 14.5 Kafka This instructions were put together using information from the official Kafka site. Specifically from the Quickstart page, and at the time of writting this book. Newer versions of Kafka undoubtely will be available when reading this book. The idea is to “timestamp” the versions used in the example in the Streaming chapter. Download Kafka wget http://apache.claz.org/kafka/2.2.0/kafka_2.12-2.2.0.tgz Expand the tar file and enter the new folder tar -xzf kafka_2.12-2.2.0.tgz cd kafka_2.12-2.2.0 Start the Zookeeper service that comes with Kafka bin/zookeeper-server-start.sh config/zookeeper.properties Start the Kafka service bin/kafka-server-start.sh config/server.properties Make sure to always start Zookeper first, and then Kafka. "],
["references.html", "Chapter 15 References", " Chapter 15 References "]
]
