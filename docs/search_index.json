[
["index.html", "The R in Spark: Learning Apache Spark with R Welcome", " The R in Spark: Learning Apache Spark with R 2019-02-27 Welcome In this book you will learn how to use Apache Spark with R using the sparklyr R package. The book intends to take someone unfamiliar with Spark or R and help them become intermediate users by teaching a set of tools, skills and practices applicable to data science. This work is licensed under the Creative Commons Attribution-NonCommercial-NoDerivs 3.0 United States License. Note: While this book is being written, the content in this website will not be accessible, once published, this website will reopen. Contact javier@rstudio.com for early access. "],
["preface.html", "Preface Authors Formatting Acknowledgments", " Preface In a world where information is growing exponentially, leading tools like Apache Spark, provide support to solve many of the relevant problems we face today. From companies looking for ways to improve based on data driven decisions, to research organizations solving problems in healthcare, finance, education, energy and so on; Spark enables analyzing much more information, faster, and more reliably, than ever before. Various books have been written for learning Apache Spark; for instance, “Spark: The Definitive Guide: Big Data Processing Made Simple”(Chambers and Zaharia 2018) is a comprehensive resource while “Learning Spark: Lightning-Fast Big Data Analysis”(Karau et al. 2015) is an introductory book meant to help users get up and running. However, as of this writing, there is no book to learn Apache Spark using the R programming language and neither, a book specifically designed for the R user nor the aspiring R user. There are some resources online to learn Apache Spark with R, most notably, the spark.rstudio.com site and the Spark documentation site under spark.apache.org. Both sites are great online resources; however, the content is not intended to be read from start to finish and assumes the reader has some knowledge of Apache Spark, R and cluster computing. The goal of this book is to help anyone get started with Apache Spark using R. Additionally, since the R programming language was created to simplify data analysis, it is also our belief that this book provides the easiest path for anyone to learn the tools used to solve data analysis problems with Spark. The first chapter provides an introduction to help anyone get up to speed with these concepts and presents the tools required to work on these problems in your own computer. After the first chapter, we quickly ramp up to relevant data science topics, cluster computing, and advanced topics that should interest even the most advanced users. Therefore, this book is intended to be a useful resource for a wide range of users; from those of you curious to learn Apache Spark, to the experienced reader seeking to understand why and how to use Apache Spark from R. This book has the following general outline: Introduction: In the first chapter, you will learn about Apache Spark, R and the tools to perform data analysis with Spark and R. Analysis: In the Analysis chapter, you will learn how to analyse, explore, transform and visualize data in Apache Spark with R. Modeling: In the Modeling chapter, you will learn how to create statistical models with the purpose of extracting information and predicticting outcomes. Scaling: Up to this point, chapters will have focused on performing operations on your personal computer; the Clusters, Connections, Data and Tuning chapters, introduce distributed computing techniques required to perform analysis and modeling across many machines to tackle the large-scale data and computation problems that Apache Spark was designed for. Extensions: The extensions chapter describes optional components and extended functionality applicable to specific, yet relevant, use cases. You will learn about alternative modeling frameworks, graph processing at scale and model deployment topics that will be relevant to many readers at some point in time. Advanced Topics: This book closes with a set of advanced chapters, Distributed R, Streaming and Contributing, which the advanced users will be most interested in. However, by the time you reach this section, these chapters won’t seem as intimidating; instead, they will be equally relevant, useful and interesting as the previous chapters. Authors Javier Luraschi Javier is a Software Engineer with experience in technologies ranging from desktop, web, mobile and backend; to augmented reality and deep learning applications. He previously worked for Microsoft Research and SAP and holds a double degree in Mathematics and Software Engineering. Kevin Kuo Kevin is a software engineer working on open source packages for big data analytics and machine learning. He has held data science positions in a variety of industries and was a credentialed actuary. He likes mixing cocktails and studying about wine. Edgar Ruiz Edgar has a background in deploying enterprise reporting and Business Intelligence solutions. He has posted multiple articles and blog posts sharing analytics insights and server infrastructure for Data Science. He lives with his family near Biloxi, MS. Formatting Tables generated from a code command are formatted as follows: ## # A tibble: 3 x 2 ## numbers text ## &lt;dbl&gt; &lt;chr&gt; ## 1 1 one ## 2 2 two ## 3 3 three The dimensions of the table (number of rows and columns) are described in the first row, followed by column names in the second row and column types in the third row. There are also various subtle visual improvements provided by the tibble package that we make use of throughout this book. Plots will be rendered using the ggplot2 package; however, since this book is not focused on data visualization, some examples make use of R’s plot() function while the figures were rendered using ggplot. If you are interested to learn more about visualization in R, consider specialized books like “R graphics cookbook: practical recipes for visualizing data”(Chang 2012). Acknowledgments This project would not have been possible without the work put into building sparklyr by Javier Luraschi, Kevin Kuo, Kevin Ushey and JJ Allaire, dplyr by Romain François and Hadley Wickham, DBI by Kirill Mülller, colleagues and friends in the R community nor the Apache Spark project itself. References "],
["intro.html", "Chapter 1 Introduction 1.1 Information 1.2 Hadoop 1.3 Spark 1.4 R 1.5 sparklyr 1.6 Recap", " Chapter 1 Introduction With information growing at exponential rates, it’s no surprise that historians are referring to this period of history as the Information Age. The increasing speed at which data is being collected has created new opportunities and is certainly staged to create even more. This chapter presents the tools that have been used to solve large scale data challenges and introduces Apache Spark as a leading tool that is democratizing our ability to process data at large scale. We will then introduce the R computing language, which was specifically designed to simplify data analysis. It is then natural to ask what the outcome would be from combining the ease of use provided by R, with the compute power available through Apache Spark. This will lead us to introduce sparklyr, a project merging R and Spark into a powerful tool that is easily accessible to all. The next chapter, Getting Started, will present prerequisites, tools and steps you will need to have Spark and R working in your computer with ease. You will learn how to install Spark, initialize Spark, introduce you to common operations and help you get your very first data processing task done. It is the goal of that chapter to help anyone grasp the concepts and tools required to start tackling large scale data challenges which, until recently, were only accessible to just a few organizations. You will then move on into learning how to analyze large scale data, followed by building models capable of predicting trends and discover information hidden in vasts amounts of information. At which point, you will have the tools required to perform data analysis and modeling at scale. Subsequent chapters will help you move away from your local computer into computing clusters required to solve many real world problems. The last chapters will present additional topics, like real time data processing and graph analysis, which you will need to truly master the art of analyzing data at any scale. The last chapter of this book will give you tools and inspiration to consider contributing back to this project and many others. We hope this is a journey you will enjoy, that will help you solve problems in your professional career and with our efforts combined, nudge the world into taking better decisions that can benefit us all. 1.1 Information As humans, we have been storing, retrieving, manipulating, and communicating information since the Sumerians in Mesopotamia developed writing in about 3000 BC. Based on the storage and processing technologies employed, it is possible to distinguish four distinct phases of development: pre-mechanical (3000 BC – 1450 AD), mechanical (1450–1840), electromechanical (1840–1940), and electronic (1940–present)(Laudon, Traver, and Laudon 1996). Mathematician George Stibitz used the word digital to describe fast electric pulses back in 1942(Ceruzzi 2012) and, to this day, we describe information stored electronically as digital information. In contrast, analog information represents everything we have stored by any non-electronic means such as hand written notes, books, newspapers, and so on(Webster 2006). The world bank report on digital development provides an estimate of digital and analog information stored over the last decades(Group 2016). This report noted that digital information surpassed analog information around 2003. At that time, there were aboput 10 million terabytes of digital information, which is roughly about 10 million computer drives today. However, a more relevant finding from this report was that our footprint of digital information is growing at exponential rates. Figure 1.1 shows the findings of this report, notice that every other year, world information has grown tenfold. FIGURE 1.1: World’s capacity to store information. With the ambition to provide tools capable of searching all this new digital information, many companies attempted to provide such functionality with what we know today as search engines, used when searching the web. Given the vast amount of digital information, managing information at this scale was a challenging problem. Search engines were unble to store all the web page information required to support web searches in a single computer. This meant that they had to split information into several files and store them across many machines. This approach became known as the Google File System, a research paper published in 2003 by Google(Ghemawat, Gobioff, and Leung 2003). 1.2 Hadoop One year later, Google published a new paper describing how to perform operations across the Google File System, this approach came to be known as MapReduce(Dean and Ghemawat 2008). As you would expect, there are two operations in MapReduce: Map and Reduce. The map operation provides an arbitrary way to transform each file into a new file, while the reduce operation combines two files. Both operations require custom computer code, but the MapReduce framework takes care of automatically executing them across many computers at once. These two operations are sufficient to process all the data available in the web, while also providing enough flexibility to extract meaningful information from it. For example, as illustrated in Figure 1.2, we can use MapReduce to count words in two different text files. The mapping operation splits each word in the original file and outputs a new word-counting file with a mapping of words and counts. The reduce operation can be defined to take two word-counting files and combine them by aggregating the totals for each word, this last file will contain a list of word counts across all the original files. FIGURE 1.2: Simple MapReduce Example. Counting words is often the most basic MapReduce example, but it can be also used for much more sophisticated and interesting applications. For instance, MapReduce can be used to rank web pages in Google’s PageRank algorithm, which assigns ranks to web pages based on the count of hyperlinks linking to a web page and the rank of the page linking to it. After these papers were released by Google, a team in Yahoo worked on implementing the Google File System and MapReduce as a single open source project. This project was released in 2006 as Hadoop with the Google File System implemented as the Hadoop File System, or HDFS for short. The Hadoop project made distributed file-based computing accessible to a wider range of users and organizations which enabled them to make use of MapReduce beyond web data processing. While Hadoop provided support to perform MapReduce operations over a distributed file system, it still required MapReduce operations to be written with code every time a data analysis was run. To improve over this tedious process, the Hive project released in 2008 by Facebook, brought Structured Query Language (SQL) support to Hadoop. This meant that data analysis could now be performed at large-scale without the need to write code for each MapReduce operation; instead, one could write generic data analysis statements in SQL that are much easier to understand and write. 1.3 Spark In 2009, Apache Spark began as a research project at the UC Berkeley’s AMPLab to improve on MapReduce. Specifically, by providing a richer set of verbs beyond MapReduce that facilitate optimizing code running in multiple machines, and by loading data in-memory making operations much fasters than Hadoop’s on-disk storage. One of the earliest results showed that running logistic regression, a data modeling technique that will be introduced under the modeling chapter, allowed Spark to run 10 times faster than Hadoop by making use of in-memory datasets(Zaharia et al. 2010), a chart similar to Figure 1.3 was presented in the original research publication. FIGURE 1.3: Logistic regression performance in Hadoop and Spark. While Spark is well known for its in-memory performance, Spark was designed to be a general execution engine that works both in-memory and on-disk. For instance, Spark holds records in large-scale sorting, where data was not loaded in-memory; but rather, Spark made use of improvements in network serialization, network shuffling and efficient use of the CPU’s cache to dramatically improve performance. For comparison, one can sort 100 terabytes of data in 72min and 2100 computers using Hadoop, but only 206 computers in 23 minutes using Spark, it’s also the case that Spark holds the record in the cloud sorting benchmark, which makes Spark the most cost effective solution for large-scale sorting. Hadoop Record Spark Record Data Size 102.5 TB 100 TB Elapsed Time 72 mins 23 mins Nodes 2100 206 Cores 50400 6592 Disk 3150 GB/s 618 GB/s Network 10Gbps 10Gbps Sort rate 1.42 TB/min 4.27 TB/min Sort rate / node 0.67 GB/min 20.7 GB/min In 2010, Spark was released as an open source project and then donated to the Apache Software Foundation in 2013. Spark is licensed under the Apache 2.0, which allows you to freely use, modify, and distribute it. In 2015, Spark reaches more than 1000 contributors, making it one of the most active projects in the Apache Software Foundation. This gives an overview of how Spark came to be, which we can now use to formally introduce Apache Spark as follows: “Apache Spark is a fast and general engine for large-scale data processing.” — spark.apache.org To help us understand this definition of Apache Spark, we will break it down as follows: Data Processing: Data processing is the collection and manipulation of items of data to produce meaningful information(French 1996). General: Spark optimizes and executes parallel generic code, as in, there are no restrictions as to what type of code one can write in Spark. Large-Scale: One can interpret this as cluster-scale, as in, a set of connected computers working together to accomplish specific goals. Fast: Spark is much faster than its predecessor by making efficient use of memory, network and CPUs to speed data processing algorithms in computing cluster. Since Spark is general, you can use Spark to solve many problems, from calculating averages to approximating the value of Pi, predicting customer churn, aligning protein sequences or analyzing high energy physics at CERN. Describing Spark as large scale implies that a good use case for Spark is tackling problems that can be solved with multiple machines. For instance, when data does not fit in a single disk driver or does not fit into memory, Spark is a good candidate to consider. Since Spark is fast, it is worth considering for problems that may not be large-scale, but where using multiple processors could speed up computation. For instance, sorting large datasets or CPU intensive models could also bennefit from running in Spark. Therefore, Spark is good at tackling large-scale data processing problems, this usually known as big data (data sets that are more voluminous and complex that traditional ones), but also is good at tackling large-scale computation problems, known as big compute (tools and approaches using a large amount of CPU and memory resources in a coordinated way). Big data and big compute problems are usually easy to spot – if the data does not fit into a single machine, you might have a big data problem; if the data fits into a single machine but a process over the data takes days, weeks or even months to compute, you might have a big compute problem. However, there is also a third problem space where neither data nor compute are necessarily large-scale and yet, there are significant benefits to using Spark. For this third problem space, there are a few use cases this breaks to: Velocity: Suppose you have a dataset of 10 gigabytes in size and a process that takes 30 minutes to run over this data – this is by no means big-compute nor big data. However, if you happen to be researching ways to improve the accuracy of your models, reducing the runtime down to 3 minutes is a significant improvement, which can lead to significant advances and productivity gains by increasing the velocity at which you can analyze data. Alternatevely, you might need to process data faster, for stock trading for instance, while 3 minutes could seem as fast enough; it can be way too slow for realtime data processing, where you might need to process data in a few seconds – or even down to a few milliseconds. Variety: You could also have an efficient process to collect data from many sources into a single location, usually a database, this process could be already running efficiently and close to realtime. Such processes are known at ETL (Extract-Transform-Load); data is extracted from multiple sources, transformed to the required format and loaded in a single data store. While this has worked for years, the tradeoff from this approach is that adding a new data source is expensive. Since the system is centralized and tightly controlled, making changes could cause the entire process to halt; therefore, adding new data source usually takes too long to be implemented. Instead, one can store all data its natural format and process it as needed using cluster computing, this architecture is currently known as a data lake. In addition, storing data in its raw format allows you to process a variety of new file formats like images, audio and video; without having to figure out how to fit them into conventional structured storage systems. Veracity: Asserts that data can vary greatly in quality which might require special analysis methods to improve its accuracy. For instance, suppose you have a table of cities with values like San Francisco, Seattle and Boston, what happens when data contains a misspelled entry like “Bston”? In a relational database, this invalid entry might get dropped; however, dropping values is not necessarily the best approach in all cases, you might want to correct this field by making use of geocodes, cross referencing data sources or attempting a best-effort match. Therefore, understanding the veracity of the original data source and what accuracy your particular analysis needs, can get yield a better outcome in many cases. If we include “Volume” as a synonym for big data, you get the mnemonics people refer as the four ’V’s of big data; others have gone as far as expending this to five or even as the 10 Vs of Big Data. Mnemonics aside, cluster computing is being used today in more innovative ways and and is not uncommon to see organizations experimenting with new workflows and a variety of tasks that were traditionally uncommon for cluster computing. Much of the hype attributed to big data falls into this space where, strictly speaking, one is not handling big data but there are still beneffits from using tools designed for big data and big compute. Our hope is that this book will help you understand the opportunities and limitations of cluster computing, and specifically, the opportunities and limitations from using Apache Spark with R. 1.4 R The R computing language has its origins in the S language, created at Bell Laboratories. R was not created at Bell Labs, but its predecesor, the S computing language was. Rick Becker explained in useR 2016 that at that time in Bell Labs, computing was done by calling subroutines written in the Fortran language which, apparently, were not pleasant to deal with. The S computing language was designed as an interface language to solve particular problems without having to worry about other languages, such as Fortran. The creator of S, John Chambers, describes in Figure 1.4 how S was designed to provide an interface that simplifies data processing, this was presented during useR 2016 as the original diagram that inspired the creation of S. FIGURE 1.4: Interface language diagram by John Chambers - Rick Becker useR 2016. R is a modern and free implementation of S, specifically: R is a programming language and free software environment for statistical computing and graphics. — The R Project for Statistical Computing While working with data, I believe there are two strong arguments for using R: The R Language was designed by statisticians for statisticians, meaning, this is one of the few successful languages designed for non-programmers; so learning R will probably feel more natural. Additionally, since the R language was designed to be an interface to other tools and languages, R allows you to focus more on modeling and less on peculiarities of computer science and engineering. The R Community provides a rich package archive provided by CRAN (The Comprehensive R Archive Network) which allows you to install ready-to-use packages to perform many tasks; most notably, high-quality data manipulation, visualizations and statistic models, many of which are only available in R. In addition, the R community is a welcoming and active group of talented individuals motivated to help you succeed. Many packages provided by the R community make R, by far, the best option for statistical computing. Some of the most downloaded R packages include: dplyr to manipulate data, cluster to analyze clusters and ggplot2 to visualize data. Figure 1.5 quantifies the growth of the R community by plotting daily downloads of R packages in CRAN. FIGURE 1.5: Daily downloads of CRAN packages. Aside from statistics, R is also used in many other fields. The following ones are particularily relevant to this book: Data Science: Data science is based on knowledge and practices from statistics and computer science that turns raw data into understanding(Wickham and Grolemund 2016) by using data analysis and modeling techniques. Statistical methods provide a solid foundation to understand the world and perform predictions, while the automation provided by computing methods allows us to simplify statistical analysis and make it much more accessible. Some have advocated that statistics should be renamed data science(Wu 1997); however, data science goes beyond statistics by also incorporating advances in computing(Cleveland 2001). This book presents analysis and modeling techniques common in statistics, but applied to large datasets which requires incorporating advances in distributed computing. Machine Learning: Machine learning uses practices from statistics and computer science; however, it is heavily focused on automation and prediction. For instance, the term “machine learning” was coined by Arthur Samuel while automating a computer program to play checkers(Samuel 1959). While we could perform data science on particular games, we rather need to automate the entire process. Therefore, this falls in the realm of machine learning, not data science. Machine learning makes it possible for many users to take advantage of statistical methods without being aware of the statistical methods that are being used. One of the first important applications of machine learning was to filter spam emails; in this case, it’s just not feasible to perform data analysis and modeling over each email account; therefore, machine learning automates the entire process of finding spam and filtering it out without having to involve users at all. This book will present the methods to transition data science workflows into fully-automated machine learning methods through, for instance, providing support to build and export Spark pipelines that can be easily reused in automated environments. Deep Learning: Deep learning builds on knowledge of statistics, data science and machine learning to define models vaguely inspired on biological nervous systems. Deep learning models evolved from neural network models after the vanishing-gradient-problem was resolved by training one layer at a time(Hinton, Osindero, and Teh 2006) and have proven useful in image and speech recognition tasks. For instance, when using voice assistants like Siri, Alexa, Cortana or Google, the model performing the audio to text conversion is most likely to be based on deep learning models. While GPUs (Graphic Processing Units) have been successfully used to train deep learning models(Krizhevsky, Sutskever, and Hinton 2012); some datasets can not be processed in a single GPU. It is also the case that deep learning models require huge amounts of data, which needs to be preprocessed across many machines before they can be fed into a single GPU for training. This book won’t make any direct references to deep learning models; however, the methods presented in this book can be used to prepare data for deep learning and, in the years to come, using deep learning with large scale computing will become a common practice. In fact, recent versions of Spark have already introduced execution models optimized for training deep learning in Spark. While working in any of the previous fields, you will be faced with increasingly large datasets or increasingly complex computations that are slow to execute or at times, even impossible to process in a single computer. However, it is important to understand that Spark does not need to be the answer to all our computations problems; instead, when faced with computing challenges in R, the following techniques can be as effective: Sampling: A first approach to try is reduce the amount of data being handled, through sampling. However, data must be sampled properly by applying sound statistical principles. For instance, selecting the top results is not sufficient in sorted datasets; with simple random sampling, there might be underrepresented groups, which we could overcome with stratified sampling, which in turn adds complexity to properly select categories. It is out of the scope of this book to teach how to properly perform statistical sampling, but many online resources and literature is available on this subject. Profiling: One can try to understand why a computation is slow and make the necessary improvements. A profiler, is a tool capable of inspecting code execution to help identify bottlenecks. In R, the R profiler, the profvis R package(“Profvis” 2018) and RStudio profiler feature(“RStudio Profiler” 2018), allow you to easily to retrieve and visualize a profile; however, it’s not always trivial to optimize. Scaling Up: Speeding up computation is usually possible by buying faster or more capable hardware, say, increasing your machine memory, hard drive or procuring a machine with many more CPUs, this approach is known as “scaling up”. However, there are usually hard limits as to how much a single computer can scale up and even with significant CPUs, one needs to find frameworks that parallelize computation efficiently. Scaling Out: Finally, we can consider spreading computation and storage across multiple machines; this approach provides the highest degree of scalability since one can potentially use an arbitrary number of machines to perform a computation, this approach is commonly known as “scaling out”. However, spreading computation effectively across many machines is a complex endeavour, specially without using specialized tools and frameworks like Apache Spark. This last point brings us closer to the purpose of this book, which is to bring the power of distributed computing systems provided by Apache Spark, to solve meaningful computation problems in Data Science and related fields, using R. 1.5 sparklyr When you think of the computation power that Spark provides and the ease of use of the R language, it is natural to want them to work together through – seamlessly. This is also what the R community expected, an R package that would provide an interface to Spark that was, easy to use, compatible with other R packages and, available in CRAN; with this goal, we started developing sparklyr. The first version, sparklyr 0.4, was released during the useR! 2016 conference, this first version included support for dplyr, DBI, modeling with MLlib and an extensible API that enabled extensions like H2O’s rsparkling package. Since then, many new features and improvements have been made available through sparklyr 0.5, 0.6, 0.7, 0.8 and 0.9. Officially, sparklyr is an R interface for Apache Spark. It’s available in CRAN and works like any other CRAN package, meaning that: it’s agnostic to Spark versions, it’s easy to install, it serves the R community, it embraces other packages and practices from the R community and so on. It’s hosted in GitHub under github.com/rstudio/sparklyr and licensed under Apache 2.0 which is allows you to clone, modify and contribute back to this project. While thinking of who and why should use sparklyr, the following roles come to mind: New Users: For new users, sparklyr provides the easiest way to get started with Spark. Our hope is that the early chapters of this book will get you up running with ease and set you up for long term success. Data Scientists: For data scientists that already use and love R, sparklyr integrates with many other R practices and packages like dplyr, magrittr, broom, DBI, tibble and many others that will make you feel at home while working with Spark. For those new to R and Spark, the combination of high-level workflows available in sparklyr and low-level extensibility mechanisms make it a productive environment to match the needs and skills of every data scientist. Expert Users: For those users that are already immersed in Spark and can write code natively in Scala, consider making your libraries available as an sparklyr custom extension to the R community, a diverse and skilled community that can put your contributions to good use while moving open science forward. This book is titled “The R in Spark” as a way to describe and teach that area of overlap between Spark and R. sparklyr is the R package that materializes this overlap of communities, expectations, future directions, packages, and package extensions as well. Naming this book sparklyr or “Introduction to sparklyr” would have left behind a much more exciting opportunity – an opportunity to present this book as an intersection of the R and Spark communities. Both are solving very similar problems with a set of different skills and backgrounds; therefore, it is my hope that sparklyr can be a fertile ground for innovation, a welcoming place to newcomers, a productive place for experienced data scientists and an open community where cluster computing and modeling can come together. 1.6 Recap This chapter presented Spark as a modern and powerful computing platform, R as an easy-to-use computing language with solid foundations in statistical methods and, sparklyr, as a project bridging both technologies and communities together. In a world where the total amount of information is growing exponentailly, learning how to analyze data at scale will help you tackle the problems and opportunities humanity is facing today. However, before we start analzing data, the Getting Started chapter will equip you with the tools you will need through the rest of this book. We recommend you follow each step carefully and take the time to install the recommended tools which, we hope will become familiar tools that you use and love. References "],
["starting.html", "Chapter 2 Getting Started 2.1 Prerequisites 2.2 Installing sparklyr 2.3 Installing Spark 2.4 Connecting to Spark 2.5 Using Spark 2.6 Disconnecting 2.7 Using RStudio 2.8 Resources 2.9 Recap", " Chapter 2 Getting Started From R, getting started with Spark using sparklyr and a local cluster is as easy as running: spark_install() sc &lt;- spark_connect(master = &quot;local&quot;) To make sure we can all run the code above and understand it, this section will walk you through the prerequisites, installing sparklyr and Spark, connecting to a local Spark cluster and briefly explaining how to use Spark. However, if a Spark cluster and R environment have been made available to you, you do not need to install the prerequisites nor install Spark yourself. Instead, you should ask for the Spark master parameter and connect as follows; this parameter will be formally introduced under the clusters and connections chapters. sc &lt;- spark_connect(master = &quot;&lt;cluster-master&gt;&quot;) 2.1 Prerequisites R can run in many platforms and environments; therfore, whether you use Windows, Mac or Linux, the first step is to install R from the r-project.org, detailed instructions are provided in the Installing R appendix. Most people use programming languages with tools to make them more productive; for R, RStudio would be such tool. Strictly speaking, RStudio is an Integrated Development Environment (or IDE), which also happens to support many platforms and environments. We strongly recommend you get RStudio installed if you haven’t done so already, see details under the Installing RStudio appendix. Additionally, since Spark is built in the Scala programming language which is run by the Java Virtual Machine, you also need to install Java 8 in your system. It is likely that your system already has Java installed, but you should still check the version and update if needed as described in the Installing Java appendix. 2.2 Installing sparklyr As many other R packages, sparkylr is available to be installed from CRAN and can be easily installed as follows: install.packages(&quot;sparklyr&quot;) The CRAN release of sparklyr contains the most stable version and it’s the recommended version to use; however, to try out features being developed in sparklyr, you can install directly from GitHub using the devtools package. First, install the devtools package and then install sparklyr as follows: install.packages(&quot;remotes&quot;) remotes::install_github(&quot;rstudio/sparklyr&quot;) The examples in this book assume you are using the latest version of sparklyr, you can verify your version is as new as the one we are using by running: packageVersion(&quot;sparklyr&quot;) [1] ‘1.0.0’ 2.3 Installing Spark Start by loading sparklyr, library(sparklyr) This will makes all sparklyr functions available in R, which is really helpful; otherwise, we would have to run each sparklyr command prefixed with sparklyr::. Spark can be easily installed by running spark_install(); this will install the latest version of Spark locally in your computer, go ahead and run spark_install(). Notice that this command requires internet connectivity to download Spark. spark_install() All the versions of Spark that are available for installation can be displayed by running: spark_available_versions() ## spark ## 1 1.6 ## 2 2.0 ## 3 2.1 ## 4 2.2 ## 5 2.3 ## 6 2.4 A specific version can be installed using the Spark version and, optionally, by also specifying the Hadoop version. For instance, to install Spark 1.6.3, we would run: spark_install(version = &quot;1.6&quot;) You can also check which versions are installed by running: spark_installed_versions() spark hadoop dir 7 2.3.1 2.7 /spark/spark-2.3.1-bin-hadoop2.7 The path where Spark is installed is referenced as Spark’s home, which is defined in R code and system configuration settings with the SPARK_HOME identifier. When using a local Spark cluster installed with sparklyr, this path is already known and no additional configuration needs to take place. Finally, in order to uninstall an specific version of Spark you can run spark_uninstall() by specifying the Spark and Hadoop versions, for instance: spark_uninstall(version = &quot;1.6.3&quot;, hadoop = &quot;2.6&quot;) Note: The default installation paths are ~/spark for OS X and Linux and, %LOCALAPPDATA%/spark for Windows. To customize the installation path you can run options(spark.install.dir = \"&lt;installation-path&gt;\") before spark_install() and spark_connect(). 2.4 Connecting to Spark It’s important to mention that, so far, we’ve only installed a local Spark cluster. A local cluster is really helpful to get started, test code and troubleshoot with ease. Further chapters will explain where to find, install and connect to real Spark clusters with many machines, but for the first few chapters, we will focus on using local clusters. To connect to this local cluster we simply run: sc &lt;- spark_connect(master = &quot;local&quot;) The master parameter identifies which is the “main” machine from the Spark cluster; this machine is often called the driver node. While working with real clusters using many machines, most machines will be worker machines and one will be the master. Since we only have a local cluster with only one machine, we will default to use \"local\" for now. If connection fails, the connections chapter contains a troubleshooting section which can help you resolve your connection issue. 2.5 Using Spark Now that you are connected, we can run a few simple commands. For instance, let’s start by copying the mtcars dataset into Apache Spark using copy_to(). cars &lt;- copy_to(sc, mtcars) The data was copied into Spark but we can access it from R using the cars reference. To print it’s contents we can simply type cars. cars # Source: spark&lt;mtcars&gt; [?? x 11] mpg cyl disp hp drat wt qsec vs am gear carb &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 21 6 160 110 3.9 2.62 16.5 0 1 4 4 2 21 6 160 110 3.9 2.88 17.0 0 1 4 4 3 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 4 21.4 6 258 110 3.08 3.22 19.4 1 0 3 1 5 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 6 18.1 6 225 105 2.76 3.46 20.2 1 0 3 1 7 14.3 8 360 245 3.21 3.57 15.8 0 0 3 4 8 24.4 4 147. 62 3.69 3.19 20 1 0 4 2 9 22.8 4 141. 95 3.92 3.15 22.9 1 0 4 2 10 19.2 6 168. 123 3.92 3.44 18.3 1 0 4 4 # … with more rows Congrats! You have successfully connected and loaded your first dataset into Spark. Let’s explain what’s going on in copy_to(). The first parameter, sc, gives the function a reference to the active Spark Connection that was earlier created with spark_connect(). The second parameter specifies a dataset to load into Spark. Now, copy_to() returns a reference to the dataset in Spark which R automatically prints. Whenever a Spark dataset is printed, Spark will collect some of the records and display them for you. In this particular case, that dataset contains only a few rows describing automobile models and some of their specifications like Horse Power and expected Miles per Gallon. 2.5.1 Web Interface Most of the Spark commands are executed from the R console; however, monitoring and analyzing execution is done through Spark’s web interface, see Figure 2.1. This interface is a web application provided by Spark which can be accessed by running: spark_web(sc) FIGURE 2.1: Apache Spark Web Interface. Printing the cars dataset collected a few records to be displayed in the R console. You can see in the Spark web interface that a job was started to collect this information back from Spark. You can also select the storage tab to see the “mtcars” dataset cached in-memory in Spark, Figure 2.2. FIGURE 2.2: Apache Spark Web Interface - Storage Tab. Notice that this dataset is fully loaded into memory since the fraction cached is 100%, you can know exactly how much memory this dataset is using through the size in memory column. The executors tab, Figure 2.3, provides a view of your cluster resources. For local connections, you will find only one compute instance active with only 2GB of memory allocated to Spark and 384MB available for computation. The tunning chapter you will learn how request more compute instances, resources and learn how memory is allocated. FIGURE 2.3: Apache Spark Web Interface - Executors Tab. The last tab to explore is the environment tab, Figure 2.4, this tab lists all the settings for this Spark application which the tunning will also introduce them in detail. As you will learn, most settings don’t need to be configured explicitly, but in order to properly run at scale, you will have to become familiar with some of them, eventually. FIGURE 2.4: Apache Spark Web Interface - Environment Tab. 2.5.2 Analysis When using Spark from R to analyze data, you can use SQL (Structured Query Language) or dplyr (a grammar of data manipulation). SQL can be used through the DBI package; for instance, to count how many records are available in our cars dataset we can run: library(DBI) dbGetQuery(sc, &quot;SELECT count(*) FROM mtcars&quot;) count(1) 1 32 When using dplyr, you write less code and it’s often much easier to write than SQL; which is why we won’t make use SQL in this book; however, if you are profficient in SQL, this is a viable option to you. For instance, counting records in dplyr is more compact and easier to understand. library(dplyr) count(cars) # Source: spark&lt;?&gt; [?? x 1] n &lt;dbl&gt; 1 32 In general, we usually start by analysing data in Spark with dplyr, followed by sampling rows and selecting a subset of the available columns, the last step is to collect data from Spark to perform further data processing in R, like data visualization. Let’s perform a very simple data analysis example by selecting, sampling and plotting the cars dataset in Spark: select(cars, hp, mpg) %&gt;% sample_n(100) %&gt;% collect() %&gt;% plot() FIGURE 2.5: Horse Power vs Miles per Gallon. The plot in Figure 2.5, shows that as we increase the horse power in a vehicle, their fuel efficiency measured in miles per gallon gets reduced. While this is insightful, it’s hard to predict numerically how increased horse power would affect fuel effiency, modeling can help us overcome this. 2.5.3 Modeling While data analysis can take you quite far when understanding data, building a mathematical model that describes and generalizes the dataset is quite powerful. In the introduction chapter you learned that the fields of machine learning and data science make use of mathematical models to perform predictions and find additional insights. For instance, we can use a linear model to approximate the relationship between fuel efficiency and horse power: model &lt;- ml_linear_regression(cars, mpg ~ hp) This model can now be used to predict values that are not in the original datset. For instance, we can add entries for cars with horse power beyond 250 and also visualize the predicted values as shown in Figure 2.6. model %&gt;% ml_predict(copy_to(sc, data.frame(hp = 250 + 10 * 1:10))) %&gt;% transmute(hp = hp, mpg = prediction) %&gt;% full_join(select(cars, hp, mpg)) %&gt;% collect() %&gt;% plot() FIGURE 2.6: Horse power vs miles per gallon with predictions. In addition, we can refine our insights using the broom package to retrive additional statistics from our model that can help us asses it’s quality. broom::glance(model) # A tibble: 1 x 5 explained.varia… mean.absolute.e… mean.squared.er… r.squared &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 21.2 2.91 14.0 0.602 # … with 1 more variable: root.mean.squared.error &lt;dbl&gt; While the previous example lacks many of the appropriate techniques you should use while modeling, it’s also a simple example to briefly introduce the modeling capabilities of Spark. All the Spark models, techniques and best practices will be properly introduced in the modeling chapter. 2.5.4 Data For simplicity, we copied the mtcars dataset into Spark; however, data is usually not copied into Spark. Instead, data is read from existing data sources in a variety of formats like plain text, CSV, JSON, JDBC and many more which, the data chapter will introduce in detail. For instance, we can export our cars dataset as a CSV file: spark_write_csv(cars, &quot;cars.csv&quot;) In practice, we would read an existing dataset from a distributed storage system like HDFS, but we can also read back from the local file system: cars &lt;- spark_read_csv(sc, &quot;cars.csv&quot;) 2.5.5 Extensions In the same way that R is known for it’s vibrant community of package authors, at a smaller scale, many extensions for Spark and R have been written and are available to you. The extensions chapter will introduce many interesting ones to perform advanced modeling, graph analysis, preprocess datasets for deep learning, etc. For instance, the sparkly.nested extension is an R package that extends sparklyr to help you manage values that contain nested information. A common use case arises while dealing with JSON files which contain nested lists that require preprocessing before doing meaningful data analysis. To use this extension, we have to first install it as follows: install.packages(&quot;sparklyr.nested&quot;) Then we can use this extension to group all the horse power data points over the number of cylinders: sparklyr.nested::sdf_nest(cars, hp) %&gt;% group_by(cyl) %&gt;% summarize(data = collect_list(data)) # Source: spark&lt;?&gt; [?? x 2] cyl data &lt;int&gt; &lt;list&gt; 1 6 &lt;list [7]&gt; 2 4 &lt;list [11]&gt; 3 8 &lt;list [14]&gt; While nesting data makes it harder to read, it is a requirement while dealing with nested data formats like JSON using the spark_read_json() and spark_write_json() functions. 2.5.6 Distributed R For those few cases where a particular functionality is not available in Spark and no extension has been developed, you can consider distributing your own R code across the Spark cluster. This is a powerful tools but comes with additional complexity that you should only use as a last resort option. Suppose that we need to round all the values across all the columns in our dataset, one approach would be running custom R code making use of R’s round() function: cars %&gt;% spark_apply(~round(.x)) # Source: spark&lt;?&gt; [?? x 11] mpg cyl disp hp drat wt qsec vs am gear carb &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 21 6 160 110 4 3 16 0 1 4 4 2 21 6 160 110 4 3 17 0 1 4 4 3 23 4 108 93 4 2 19 1 1 4 1 4 21 6 258 110 3 3 19 1 0 3 1 5 19 8 360 175 3 3 17 0 0 3 2 6 18 6 225 105 3 3 20 1 0 3 1 7 14 8 360 245 3 4 16 0 0 3 4 8 24 4 147 62 4 3 20 1 0 4 2 9 23 4 141 95 4 3 23 1 0 4 2 10 19 6 168 123 4 3 18 1 0 4 4 # … with more rows If you are a profficient R user, it can be quite tempting to use spark_apply() for everything, but please, don’t! spark_apply() was designed for advanced use cases where Spark falls short; instead, you will learn how to do proper data analysis and modeling without having to distribute custom R code across your cluster. 2.5.7 Streaming While processing large static datasets is the most typical use case for Spark, processing dynamic datasets in realtime is also possible and for some applications, a requirement. You can think of a streaming dataset as a static data source with new data arriving continously, like stock market quotes. Streaming data is usually read from Kafka (an open-source stream-processing software platform) or from distributed storage that receives new data continuously. To try out streaming, lets first create an input/ folder with some data that we will use as the input for this stream: dir.create(&quot;input&quot;) write.csv(mtcars, &quot;input/cars_1.csv&quot;, row.names = F) Then we will define a stream that processes incoming data from the input/ folder, performs a custom transformation in R and, pushes the output into an output/ folder stream_read_csv(sc, &quot;input/&quot;) %&gt;% spark_apply(~sapply(.x, jitter)) %&gt;% stream_write_csv(&quot;output/&quot;) Stream: 720aac2a-d4aa-4e6c-828d-325d8b017fdb Status: Waiting for next trigger Active: TRUE As soon as the stream of realtime data starts, the input/ folder is processed and turned into a set of new files under the output/ folder containing the new transformed files. Since the input contained only one file, the output folder will also contain a single file resulting from applying the custom spark_apply() transformation. dir(&quot;output&quot;, pattern = &quot;.csv&quot;) [1] &quot;part-00000-eece04d8-7cfa-4231-b61e-f1aef8edeb97-c000.csv&quot; Up to this point, this resembles static data processing; however, we can keep adding files to the input/ location and Spark will parallelize and process data automatically. Let’s add one more file and validate that it’s automatically processed. # Write more data into the stream source write.csv(mtcars, &quot;input/cars_2.csv&quot;, row.names = F) # Wait for the input stream to be processed Sys.sleep(1) # Check the contents of the stream destination dir(&quot;output&quot;, pattern = &quot;.csv&quot;) [1] &quot;part-00000-2d8e5c07-a2eb-449d-a535-8a19c671477d-c000.csv&quot; [2] &quot;part-00000-eece04d8-7cfa-4231-b61e-f1aef8edeb97-c000.csv&quot; You can use dplyr, SQL, Spark models or distributed R to analyze streams in realtime, we will properly introduce you to all the interesting transformations you can perform to analyze realtime data during the streaming chapter. 2.5.8 Logs Logging is definetely less interesting that realtime data processing; however, it’s a tool you should be familiar with. A log is just a text file where Spark will append information relevant to the execution of tasks in the cluster. For local clusters, we can retrieve all the recent log by running: spark_log(sc) 18/10/09 19:41:46 INFO Executor: Finished task 0.0 in stage 5.0 (TID 5)... 18/10/09 19:41:46 INFO TaskSetManager: Finished task 0.0 in stage 5.0... 18/10/09 19:41:46 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose... 18/10/09 19:41:46 INFO DAGScheduler: ResultStage 5 (collect at utils... 18/10/09 19:41:46 INFO DAGScheduler: Job 3 finished: collect at utils... Or we can retrieve specific log entries containing, say sparklyr, by using the filter parameter as follows: spark_log(sc, filter = &quot;sparklyr&quot;) ## 18/10/09 18:53:23 INFO SparkContext: Submitted application: sparklyr ## 18/10/09 18:53:23 INFO SparkContext: Added JAR... ## 18/10/09 18:53:27 INFO Executor: Fetching spark://localhost:52930/... ## 18/10/09 18:53:27 INFO Utils: Fetching spark://localhost:52930/... ## 18/10/09 18:53:27 INFO Executor: Adding file:/private/var/folders/... Most of the time, you won’t need to worry about Spark logs, except in cases where you need to troubleshoot a failed computation; in those cases, logs are an invaluable resource to be aware of, now you know. 2.6 Disconnecting For local clusters (really, any cluster) once you are done processing data you should disconnect by running: spark_disconnect(sc) This will terminate the connection to the cluster as well as the cluster tasks . If multiple Spark connections are active, or if the conneciton instance sc is no longer available, you can also disconnect all your Spark connections by running: spark_disconnect_all() Notice that exiting R, RStudio or restarting your R session will also cause the Spark connection to terminate, which in turn terminates the Spark cluster and cached data that is not explicitly persisted. 2.7 Using RStudio Since it’s very common to use RStudio with R, sparklyr provides RStudio extensions to help simplify your workflows and increase your productivity while using Spark in RStudio. If you are not familiar with RStudio, take a quick look at the Using RStudio appendix section. Otherwise, there are a couple extensions worth highlighting. First, instead of starging a new connections using spark_connect() from RStudio’s R console, you can use the new connection action from the connections pane and then, select the Spark connection which will open the dialog shown in Figure 2.7. You can then customize the versions and connect to Spark which will simply generate the right spark_connect() command and execute this in the R console for you. FIGURE 2.7: RStudio New Spark Connection. Second, once connected to Spark, either by using the R console or through RStudio’s connections pane, RStudio will display your datasets available in the connections pane, see Figure 2.8. This is a useful way to track your existing datasets and provides an easy way to explore each of them. FIGURE 2.8: RStudio Connections Pane. Additionally, an active connection provides the following custom actions: Spark: Opens the Spark web interface, a shortcut to spark_ui(sc). Log: Opens the Spark web logs, a shortcut to spark_log(sc). SQL: Opens a new SQL query, see DBI and SQL support in the data analysis chapter. Help: Opens the reference documentation in a new web browser window. Disconnect: Disconnects from Spark, a shortcut to spark_disconnect(sc). The rest of this book will use plain R code, it is up to you to execute this code in the R console, RStudio, Jupyter Notebooks or any other tool that support executing R code since, the code provided in this book executes in any R environment. 2.8 Resources While we’ve put significant effort into simplifying the onboarding process, there are many additional resources that can help you troubleshoot particular issues while getting started and, in general, introduce you to the broader Spark and R communities to help you get specific answers, discuss topics and get connected with many users actevely using Spark with R. Documentation: This should be your first stop to learn more about Spark when using R. The documentation is kept up to date with examples, reference functions and many more relevant resources, spark.rstudio.com. Blog: To keep up to date with major sparklyr announcements, you can follow the RStudio blog, blog.rstudio.com/tags/sparklyr. Community: For general sparklyr questions, you can post then in the RStudio Community tagged as sparklyr, community.rstudio.com/tags/sparklyr. Stack Overflow: For general Spark questions, Stack Overflow is a great resource, stackoverflow.com/questions/tagged/apache-spark; there are also many topics specifically about sparklyr, stackoverflow.com/questions/tagged/sparklyr. Github: If you believe something needs to be fixed, open a GitHub issue or send us a pull request, github.com/rstudio/sparklyr. Gitter: For urgent issues, or to keep in touch, you can chat with us in Gitter, gitter.im/rstudio/sparklyr. 2.9 Recap In this chapter you learned about the prerequisites required to work with Spark, how to connect to Spark using spark_connect(), install a local cluster using spark_install(), load a simple dataset, launch the web interface and display logs using spark_web(sc) and spark_log(sc) respectively, disconnect from RStudio using spark_disconnect() and we closed this chapter presenting the RStudio extensions sparklyr provides. It is our hope that this chapter will help anyone interested in learning cluster computing using Spark and R getting started, ready to experiment on your own and ready to tackle actual data analysis and modeling problems which, the next two chapters will introduce you. The next chapter, analysis, will present data analysis as the process to inspect, clean, and transform data with the goal of discovering useful information. Modeling can be considered part of data analysis; however, it deserves it’s own chapter to truly understand and take advantage of the modeling functionality available in Spark. "],
["analysis.html", "Chapter 3 Analysis 3.1 Introduction 3.2 Import 3.3 Wrangle 3.4 Visualize 3.5 Model 3.6 Communicate 3.7 Later review", " Chapter 3 Analysis 3.1 Introduction Previous chapters focused on introducing Spark, R and helping you get started with the tools you need throughout this book. In this chapter you will learn how to do data analysis in Spark from R. Data analysis may become the most common task you will do when working with Spark. This chapter will serve as a foundation to later chapters because concepts from this chapter will apply to properly prepare data when doing modeling, graph processing, streaming and other related topics that might not be strictly considered data analysis. For those who are new to R, this might be a good time to consider complementing this chapter with R for Data Science and other online resources to learn R. This chapter will try to briefly introduce all the concepts it presents, but it would take an entire book to detail all the concepts used when doing data analysis in R. 3.1.1 Searching for insights In a data analysis project, the main goal is to search for insights that can be derived from the data. It is the results of the data transformations identified during the data analysis phase that can later be formalized into artifacts such as dashboard or model pipelines. The output of the data transformations could be models, aggregations or visualizations. Most data analysis projects follow a set of steps outlined in Figure 3.1. FIGURE 3.1: General steps of a data analysis 3.1.2 R as an interface to Spark For data analysis, the ideal approach is to let Spark do what its good at. It excels at being a parallel computation engine that works at a large scale. Spark goes beyond offering generic calculations. Out of the box, Spark includes libraries that actually can do a lot of what analysts usually do in R, but for large amounts of data. Figure 3.2 paraphrases the four main capabilities available to data analysts in Spark. FIGURE 3.2: Spark capabilities Thanks to Spark’s libraries, most of the Data Science project steps can be completed inside Spark. For example, selecting, transforming and modeling can all be done by Spark. The idea is to use R to tell Spark what data operations to run (import, tidy, transform, model), and then focus on only bringing back into R the results of the operation. FIGURE 3.3: R as an interface for Spark The sparklyr package focuses on implementing the principle mentioned in the previous section. Most of its functions are mainly wrappers on top of Spark API calls. The idea is take advantage of Spark’s analysis components instead of R’s. For example, if the analyst needs to fit a Linear Regression model, instead of using the familiar lm() function, for data available via Spark, the analyst would use the ml_linear_regression() function. The R function will actually run Scala code that runs the Spark’s API model. FIGURE 3.4: R as an interface for Spark For more common data manipulation tasks, sparklyr provides a back-end for dplyr. This means that already familiar dplyr verbs can be used in R, and then sparklyr and dplyr will translate those actions into Spark SQL statements, see figure 3.8. FIGURE 3.5: dplyr-to-SQL translation 3.2 Import The local master will be used for the exercises to allow you as the reader to replicate the code in your laptop. Please, make sure to already have sparklyr and a local copy of Spark installed, by using the utility that comes with the package. For more information on how to do that please see the Local section in the Connections chapter. First, load the sparklyr and dplyr packages, and open a new local connection. library(sparklyr) library(dplyr) sc &lt;- spark_connect(master = &quot;local&quot;) To aid in conceptualizing where the data is being process, please open the Spark UI of the session the was just created. Keep that tab open in the browser so it is easy to switch back and forth R and the Spark UI. This is not something that would happen in an everyday analysis, switching between both is just to become familiar with the process. spark_web(sc) 3.2.1 Data source Most commonly at an enterprise, Spark sessions are created on top of Hadoop clusters, so data would already be available to be accessed directly by Spark, via either a Hive table, or through the Hadoop File System (HDFS). Because of a local master session is being used in this section. The session itself does not have any data. So the next step is to prime the session with data, in this case mtcars. The copy_to() command from dplyr can be used for that. cars &lt;- copy_to(sc, mtcars, &quot;mtcars_remote&quot;) This operation transfered the data into Spark, under a variable called “mtcars_remote”. That name only exists within Spark and not R. R recognizes the newly uploaded data as cars. In other words, the data is not in R, cars is just a pointer that tells sparklyr what object inside the Spark session contains the data we are requested. cars ## # Source: spark&lt;mtcars_remote&gt; [?? x 11] ## mpg cyl disp hp drat wt qsec vs am gear carb ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 21 6 160 110 3.9 2.62 16.5 0 1 4 4 ## 2 21 6 160 110 3.9 2.88 17.0 0 1 4 4 ## 3 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 ## 4 21.4 6 258 110 3.08 3.22 19.4 1 0 3 1 ## 5 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 ## 6 18.1 6 225 105 2.76 3.46 20.2 1 0 3 1 ## 7 14.3 8 360 245 3.21 3.57 15.8 0 0 3 4 ## 8 24.4 4 147. 62 3.69 3.19 20 1 0 4 2 ## 9 22.8 4 141. 95 3.92 3.15 22.9 1 0 4 2 ## 10 19.2 6 168. 123 3.92 3.44 18.3 1 0 4 4 ## # ... with more rows The output in the R console indicates that it is not a tibble or data frame object, it is a remote Spark source. Because it is a rectangular table, and thanks to the dplyr back end provided by sparklyr, it can be treated as a local data frame when using dplyr verbs. 3.3 Wrangle The transformations will occurr not in R, but inside Spark, and then Spark will return the results. cars %&gt;% group_by(am) %&gt;% summarise(mpg_mean = mean(mpg, na.rm = TRUE)) ## # Source: spark&lt;?&gt; [?? x 2] ## am mpg_mean ## &lt;dbl&gt; &lt;dbl&gt; ## 1 0 17.1 ## 2 1 24.4 Using the show_query() command, it is possible to peer into the SQL statement that sparklyr and dplyr created and sent to Spark. cars %&gt;% group_by(am) %&gt;% summarise(mpg_mean = mean(mpg, na.rm = TRUE)) %&gt;% show_query() ## &lt;SQL&gt; ## SELECT `am`, AVG(`mpg`) AS `mpg_mean` ## FROM `mtcars_remote` ## GROUP BY `am` As it is evident, it will not be necessary to have to see the resulting query everytime, dplyr verbs are being used, but it is a very handy function that helps in familiarizing us with how the translations work. As the analysis matures, there may be a need to share with non-R users the needed transformations, and at that time the resulting SQL query can be shared which will reduce the friction of implementing a solution, because there is no need to re-code or convert the analysis into some other language. cars %&gt;% group_by(am) %&gt;% summarise( wt_mean = mean(wt, na.rm = TRUE), mpg_mean = mean(mpg, na.rm = TRUE) ) ## # Source: spark&lt;?&gt; [?? x 3] ## am wt_mean mpg_mean ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 0 3.77 17.1 ## 2 1 2.41 24.4 3.4 Visualize Visualizing data is a of vital importance to the success of data analysis. It helps us, as humans, to gain insights by allowing us to derive patterns from data. R excels at data visualizations, and its capabilities for creating plots is extended by the many R packages that focus on this analysis step. Unfortunately, the vast majority of R functions that create plots depend on the data already being in local memory within R, so they fail when using a remote table inside Spark. 3.4.1 Stages of a plot It is possible to create visualizations in R from data source from Spark. To understand how to do this, let’s first break down how computer programs build plots: FIGURE 3.6: Stages of a plot For example, to create a bar plot in R, we simply call a function: ggplot(aes(as.factor(cyl), mpg), data = mtcars) + geom_col() FIGURE 3.7: Plotting inside R In this case, the mtcars raw data was automatically transformed into three discreete aggregated numbers, then each result was mapped into an x and y plane, and then the plot was drawn, see figure 3.8. As R users, all of the stages of building the plot are conveniently abstracted for us. FIGURE 3.8: R plotting function 3.4.2 Transform remote, plot locally The key to building plots based on a large data set inside Spark is to separate the data transformation from the mapping and drawing of the plot. car_group &lt;- cars %&gt;% group_by(cyl) %&gt;% summarise(mpg = sum(mpg, na.rm = TRUE)) %&gt;% collect() car_group ## # A tibble: 3 x 2 ## cyl mpg ## &lt;dbl&gt; &lt;dbl&gt; ## 1 6 138. ## 2 4 293. ## 3 8 211. ggplot(aes(as.factor(cyl), mpg), data = car_group) + geom_col() FIGURE 3.9: Plot from Spark 3.5 Model 3.6 Communicate 3.7 Later review 3.7.1 Background Understanding and applying the concepts in this section are going to be key to a successful analysis using R and Spark. It provides the background information about why the methods used in later sections should be applied in practice. Please keep in mind that this chapter focuses on interactive analysis, not Production pipelines. The line between the two are sometimes blurred, because often there is an expectation for the analyst to use the insights gathered from the Data Science project, and implement a solution that is to be used in Production, such as Shiny application, or a Model Pipeline. As illustrated in Figure 3.1, a typical Data Science project is made up of multiple steps. Because its capabilities, every single step for most analyses can be completed inside R. FIGURE 3.10: Typical Data Science Project In addition, the large universe of R packages allow analyst to extend R capabilities in each of the steps. One good example is the ggplot2 package which enables the creation of effective and professional looking plots. 3.7.2 Working with Big Data R developers are used to a very specific cadence: Import the data into R, and analyze the data in-memory. But what happens when dealing with Big Data? FIGURE 3.11: Working with Big Data For the purposes of this book, let’s define Big Data as data that is too big to fit into RAM. A secondary definition of Big Data, is data located in a remote machine, and can only be accessed through a small conduit. An example would be a remote database that can only be accessed via a network connection. Given these definitions, it is actually very common for an R developer to encounter Big Data in their day-to-day work. Almost instinctively, the developer opts for one of the following strategies to handle such an encounter with Big Data: Sampling - Download into R, what the analyst hopes is, a representative sample of the data being analyzed. Most remote sources do not offer a true randomize way of selecting which records will be downloaded, so any alternative would not provide a data set from which inferences can safely be made. In parts - This may be the most common method. The data is downloaded in segments, and then saved into the local disk. The analyst then imports into memory the files recursively, and often importing only some of the columns from each file. An example of this would be an R developer using one R script to download one-day’s worth of transactions at a time, and saving them into a CSV file, and then using a second R script to compile and analyze the data. This approach is preferred because it allows the analyst to not have to go “back to the well” every time a new angle needs to be looked at, or the analyst needs to re-run the existing analysis. The main downside of this approach is that the copies saved in the local machine will eventually go stale because they won’t receive get any updates made in the original source data. Whole - It is surprising how many choose this option. The idea is to download all of the data, or at least as much as it is physically possible for the local machine. This usually means having to wait hours before the data is imported. Once in memory, each step of the Data Science project is painfully slow because the local machine struggles with processing complex calculations over vast amounts of local data. It is obvious that none of the three strategies are ideal, and this is more likely why an analyst will look into Spark as a way to scale the analysis. 3.7.3 Avoid running R inside Spark As it was personally, the usual first reaction is to try to run existing R code in a Spark cluster. The thought is that somehow Spark will take the R code and packages and divide the job between all of the executors in parallel. Unfortunately, that is not the case. Yes, it is true that sparklyr offers a way to run R code inside Spark. But that is reserved for a very specific use case: embarrassingly parallel jobs. In other words, if the data to be analyzed needs to be first divided into segments, and then the R code is applied to each segment. This is not due to anything lacking on sparklyr, or Spark for that matter. It boils down to how Spark divides its operations. When running R code, Spark can run one segment per Spark Executor. In the example on Figure 3.2, the data is split by the company’s customer data, and then a model is fitted for each customer. FIGURE 3.12: Works for embarassing parallel jobs The ability to split a job into multiple executors is a core strength of Spark. It allows the job to run faster because theoretically, each executor has its own discrete amount of machine resources it can use to complete its assigned part of the job. This means that if the data is not split, or segmented, then the entire R job will run on a single executor, and thus only using that executor’s limited resources will be used. Additionally, the job will take much longer to complete, if it completes at all, because the are no other executors helping to complete the job in parallel, see Figure 3.3. FIGURE 3.13: Running a single large job The authors of this book recommend that this approach should only be considered by advanced R users and experienced Spark/Big Data engineers. There are several infrastructure considerations that need to be taken into account when using this method in an enterprise cluster, please see the chapter Distributed R for more information. 3.7.4 R, under the hood First, lets mentally separate R the language, from R as an analysis engine. In reality, R does not have a calculation engine, it actually depends on other languages to run these operations. Under the hood, there are C++ and FORTRAN routines that R simply interfaces with in order to run a given calculation. What drives data analysts to R, is the fact that all of the complicated lower level code is now behind easy to use and understand R language functions. R is not limited to interacting with the languages mentioned above. There are R packages that allow data analysts to directly interact languages such as python (reticulate) and D3 (r2d3), or interact indirectly with other languages, such as how the shiny package interfaces with JavaScript and HTML on behalf of the developer. Given this pattern, running R inside Spark, is akin to thinking in terms of running R code using the browser’s HTML interpreter, it would not make much sense. With shiny, R writes HTML and JavaScript code to allow the browser do what its good at. ## Warning: package &#39;tibble&#39; was built under R version 3.5.2 ## Warning: package &#39;purrr&#39; was built under R version 3.5.2 "],
["modeling.html", "Chapter 4 Modeling 4.1 Overview 4.2 Supervised 4.3 Unsupervised 4.4 Broom 4.5 Pipelines", " Chapter 4 Modeling While this chatper has not been written, a few resources and basic examples were made available to help out until this chapter is completed. 4.1 Overview MLlib is Apache Spark’s scalable machine learning library and is available through sparklyr, mostly, with functions prefixed with ml_. The following table describes some of the modeling algorithms supported: Algorithm Function Accelerated Failure Time Survival Regression ml_aft_survival_regression() Alternating Least Squares Factorization ml_als() Bisecting K-Means Clustering ml_bisecting_kmeans() Chi-square Hypothesis Testing ml_chisquare_test() Correlation Matrix ml_corr() Decision Trees ml_decision_tree () Frequent Pattern Mining ml_fpgrowth() Gaussian Mixture Clustering ml_gaussian_mixture() Generalized Linear Regression ml_generalized_linear_regression() Gradient-Boosted Trees ml_gradient_boosted_trees() Isotonic Regression ml_isotonic_regression() K-Means Clustering ml_kmeans() Latent Dirichlet Allocation ml_lda() Linear Regression ml_linear_regression() Linear Support Vector Machines ml_linear_svc() Logistic Regression ml_logistic_regression() Multilayer Perceptron ml_multilayer_perceptron() Naive-Bayes ml_naive_bayes() One vs Rest ml_one_vs_rest() Principal Components Analysis ml_pca() Random Forests ml_random_forest() Survival Regression ml_survival_regression() To complement those algorithms, you will often also want to consider using the following feature transformers: Transformer Function Binarizer ft_binarizer() Bucketizer ft_bucketizer() Chi-Squared Feature Selector ft_chisq_selector() Vocabulary from Document Collections ft_count_vectorizer() Discrete Cosine Transform ft_discrete_cosine_transform() Transformation using dplyr ft_dplyr_transformer() Hadamard Product ft_elementwise_product() Feature Hasher ft_feature_hasher() Term Frequencies using Hashing export(ft_hashing_tf) Inverse Document Frequency ft_idf() Imputation for Missing Values export(ft_imputer) Index to String ft_index_to_string() Feature Interaction Transform ft_interaction() Rescale to [-1, 1] Range ft_max_abs_scaler() Rescale to [min, max] Range ft_min_max_scaler() Locality Sensitive Hashing ft_minhash_lsh() Converts to n-grams ft_ngram() Normalize using the given P-Norm ft_normalizer() One-Hot Encoding ft_one_hot_encoder() Feature Expansion in Polynomial Space ft_polynomial_expansion() Maps to Binned Categorical Features ft_quantile_discretizer() SQL Transformation ft_sql_transformer() Standardizes Features using Corrected STD ft_standard_scaler() Filters out Stop Words ft_stop_words_remover() Map to Label Indices ft_string_indexer() Splits by White Spaces export(ft_tokenizer) Combine Vectors to Row Vector ft_vector_assembler() Indexing Categorical Feature ft_vector_indexer() Subarray of the Original Feature ft_vector_slicer() Transform Word into Code ft_word2vec() 4.2 Supervised Examples are reosurces are available in spark.rstudio.com/mlib. 4.3 Unsupervised 4.3.1 K-Means Clustering Here is an example to get you started with K-Means: library(sparklyr) # Connect to Spark in local mode sc &lt;- spark_connect(master = &quot;local&quot;) # Copy iris to Spark iris_tbl &lt;- sdf_copy_to(sc, iris, overwrite = TRUE) # Run K-Means for Species using only Petal_Width and Petal_Length as features iris_tbl %&gt;% ml_kmeans(centers = 3, Species ~ Petal_Width + Petal_Length) 4.3.2 Gaussian Mixture Clustering Alternatevely, we can also cluster using Gaussian Mixture Models (GMMs). predictions &lt;- copy_to(sc, fueleconomy::vehicles) %&gt;% ml_gaussian_mixture(~ hwy + cty, k = 3) %&gt;% ml_predict() %&gt;% collect() predictions %&gt;% ggplot(aes(hwy, cty)) + geom_point(aes(hwy, cty, col = factor(prediction)), size = 2, alpha = 0.4) + scale_color_discrete(name = &quot;&quot;, labels = paste(&quot;Cluster&quot;, 1:3)) + labs(x = &quot;Highway&quot;, y = &quot;City&quot;) + theme_light() predictions &lt;- readRDS(&quot;data/03-gaussian-mixture-prediction.rds&quot;) predictions %&gt;% ggplot(aes(hwy, cty)) + geom_point(aes(hwy, cty, col = factor(prediction)), size = 2, alpha = 0.4) + scale_color_discrete(name = &quot;&quot;, labels = paste(&quot;Cluster&quot;, 1:3)) + labs(x = &quot;Highway&quot;, y = &quot;City&quot;) + theme_light() FIGURE 4.1: Fuel economy data for 1984-2015 from the US EPA 4.4 Broom You can turn your sparklyr models into data frames using the broom package: model &lt;- cars_tbl %&gt;% ml_linear_regression(mpg ~ wt + cyl) # Turn a model object into a data frame broom::tidy(model) ## # A tibble: 3 x 5 ## term estimate std.error statistic p.value ## &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 (Intercept) 39.7 1.71 23.1 0 ## 2 wt -3.19 0.757 -4.22 0.000222 ## 3 cyl -1.51 0.415 -3.64 0.00106 # Construct a single row summary broom::glance(model) ## # A tibble: 1 x 5 ## explained.varia… mean.absolute.e… mean.squared.er… r.squared ## &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 29.2 1.92 5.97 0.830 ## # … with 1 more variable: root.mean.squared.error &lt;dbl&gt; # Augments each observation in the dataset with the model broom::augment(model, cars_tbl) ## # A tibble: 32 x 14 ## `_c0` mpg cyl disp hp drat wt qsec vs am gear carb ## &lt;chr&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; ## 1 Mazda… 21 6 160 110 3.9 2.62 16.5 0 1 4 4 ## 2 Mazda… 21 6 160 110 3.9 2.88 17.0 0 1 4 4 ## 3 Datsu… 22.8 4 108 93 3.85 2.32 18.6 1 1 4 1 ## 4 Horne… 21.4 6 258 110 3.08 3.22 19.4 1 0 3 1 ## 5 Horne… 18.7 8 360 175 3.15 3.44 17.0 0 0 3 2 ## 6 Valia… 18.1 6 225 105 2.76 3.46 20.2 1 0 3 1 ## 7 Duste… 14.3 8 360 245 3.21 3.57 15.8 0 0 3 4 ## 8 Merc … 24.4 4 147. 62 3.69 3.19 20 1 0 4 2 ## 9 Merc … 22.8 4 141. 95 3.92 3.15 22.9 1 0 4 2 ## 10 Merc … 19.2 6 168. 123 3.92 3.44 18.3 1 0 4 4 ## # … with 22 more rows, and 2 more variables: fitted &lt;dbl&gt;, resid &lt;dbl&gt; 4.5 Pipelines Spark’s ML Pipelines provide a way to easily combine multiple transformations and algorithms into a single workflow, or pipeline. Take a look at spark.rstudio.com/guides/pipelines to learn about their purpose and functionality. "],
["clusters.html", "Chapter 5 Clusters 5.1 Overview 5.2 Managers 5.3 On-Premise 5.4 Cloud 5.5 Tools 5.6 Recap", " Chapter 5 Clusters Previous chapters focused on using Spark over a single computing instance, your personal computer. In this chapter we will introduce techniques to run Spark over multiple computing instances, also known as a computing cluster. This chapter and subsequent ones will introduce and make use of concepts applicable to computing clusters; however, it’s not required to use a computing cluster to follow along, you can still use your personal computer. It’s worth mentioning that while previous chapters focused on single computing instances, all the data analysis and modeling techniques previously presented, can also be used in a computing cluster without changing any code. For those of you already have a Spark cluster in your organization, you could consider skipping to the next chapter, Connections, which will teach you how to connect to an existing cluster. Otherwise, if you don’t have a cluster or are considering improvements to your existing infrastructure, this chapter will introduce the cluster trends, managers and providers available today. 5.1 Overview There are three major trends in cluster computing worth discussing: on-premise, cloud computing and kubernetes. Framing these trends over time will help us understand how they came to be, what they are and what their future might be. To illustrate this, Figure 5.1 plots these trends over time using data from Google trends. FIGURE 5.1: Google trends for on-premise (mainframe), cloud computing and kubernetes. For on-premise clusters, yourself or someone in your organization purchased physical computers that were intended to be used for cluster computing. The computers in this cluster are made of off-the-shelf hardware, meaning that someone placed an order to purchase computers usually found in stores shelves or, high-performance hardware, meaning that a computing vendor provided highly customized computing hardware which also comes optimized for high-performance network connectivity, power consumption, etc. When purchasing hundreds or thousands of computing instances, it doesn’t make sense to keep them in the usual computing case that we are all familiar with, instead, it makes sense to stack them as efficiently as possible on top of each other to minimize the space the use. This group of efficiently stacked computing instances is known as a rack. Once a cluster grows to thousands of computers, you will also need to host hundreds of racks of computing devices, at this scale, you would also need significant physical space to hosts those racks. A building that provides racks of computing instances is usually known as a data center. At the scale of a data center, you would also need to find ways to make the building more efficient, specially the cooling system, power supplies, network connectivity, and so on. Since this is time consuming, a few organization have come together to open source their infrastructure under the Open Compute Project initiative, which provides a set of data center blueprints free for anyone to use. There is nothing preventing you from building our own data center and in fact, many organizations have followed this path. For instance, Amazon started as an online book store, over the years Amazon grew to sell much more than just books and, with its online store growth, their data centers also grew in size. In 2002, Amazon considered renting servers in their data centers to the public, two year laters, Amazon Web Services launched as a way to let anyone rent servers in their data centers on-demand, meaning that, one did not have to purchase, configure, maintain nor teardown it’s own clusters but could rather rent them from Amazon directly. This on-demand compute model is what we know today as Cloud Computing. In the cloud, the cluster you use is not owned by you and it’s neither in your physical building, but rather, it’s a data center owned and managed by someone else. Today, there are many cloud providers in this space ranging from Amazon, Databricks, IBM, Google, Microsoft and many others. Most cloud computing platforms provide a user interface either through a web application and command line to request and manage resources. While the benefits of processing data in the cloud were obvious for many years, picking a cloud provider had the unintended side-effect of locking organizations with one particular provider, making it hard to switch between providers or back to on-premise clusters. Kubernetes, announced by Google in 2014, is an open source system for managing containerized applications across multiple hosts. In practice, it makes it easier to deploy across multiple cloud providers and on-premise as well. Many projects, included Spark, have added support for Kubernetes and, through sparklyr, is also available in R. 5.2 Managers In order to run Spark within a computing cluster, you will need to run software capable of initializing Spark over each compute instance and register all the available computing nodes, this software is known as a cluster manager. The available cluster managers in Spark are: Spark Standalone, YARN, Mesos and Kubernetes. 5.2.1 Standalone In Spark Standalone, Spark uses itself as its own cluster manager, which allows you to use Spark without installing additional software in your cluster. This can be useful if you are planning to use your cluster to only run Spark applications; if this cluster is not dedicated to Spark, a generic cluster manager like YARN, Mesos or Kubernetes would be more suitable. The landing page for Spark Standalone is available under spark.apache.org and contains detailed information on configuring, launching, monitoring and enabling high-availability, see Figure 5.2. FIGURE 5.2: Spark Standalone Site. However, since Spark Standalone is contained within a Spark installation; then, by completing the getting started chapter, you have now a local Spark installation available that we can use to initialize a local Spark Standalone cluster in a single machine. In practice, you would want to start the worker nodes in different machines but, for simplicity, we will present the code to start a standalone cluster in a single machine. First, retrieve the SPARK_HOME directory by running spark_home_dir() then, run start-master.sh and start-slave.sh as follows: # Retrieve the Spark installation directory spark_home &lt;- spark_home_dir() # Build path to start-master.sh start_master &lt;- file.path(spark_home, &quot;sbin&quot;, &quot;start-master.sh&quot;) # Execute start-master.sh to start the cluster manager master node system2(start_master) # Build path to start-slave start_slave &lt;- file.path(spark_home, &quot;sbin&quot;, &quot;start-slave.sh&quot;) # Execute start-slave.sh to start a worker and register in master node system2(start_slave, paste0(&quot;spark://&quot;, system2(&quot;hostname&quot;, stdout = TRUE), &quot;:7077&quot;)) The previous command initialized the master node and a worker node, the master node interface can be accessed under localhost:8080 as captured in Figure 5.3: FIGURE 5.3: Spark Standalone Web Interface. Notice that there is one worker register in Spark standalone, you can follow the link to this worker node to see, Figure 5.4, details for this particular worker like available memory and cores. FIGURE 5.4: Spark Standalone Worker Web Interface. Once you are done performing computations in this cluster, you can simply stop all the running nodes in this local cluster by running: # Build path to stop-all stop_all &lt;- file.path(spark_home, &quot;sbin&quot;, &quot;stop-all.sh&quot;) # Execute stop-all.sh to stop the workers and master nodes system2(stop_all) A similar approach can be followed to configure a cluster by running each start-slave.sh command over each machine in the cluster. Note: When running on a Mac, if you hit: ssh: connect to host localhost port 22: Connection refused, you will need to manually turn off the workers using system2(\"jps\") to list the running Java process and then, system2(\"kill\", c(\"-9\", \"&lt;process id&gt;\")) to stop the specific workers. 5.2.2 Yarn YARN for short, or Hadoop YARN, is the resource manager of the Hadoop project. It was originally developed in the Hadoop project but, refactored into it’s own project in Hadoop 2. As we mentioned in in the introduction chapter, Spark was built to speed up computation over Hadoop and therefore, it’s very common to find Spark intalled on Hadoop clusters. One advantage of YARN, is that it is likely to be already installed in many existing clusters that support Hadoop; which means that you can easily use Spark with many existing Hadoop clusters without requesting any major changes to the existing cluster infrastructure. It is also very common to find Spark deployed in YARN clusters since many started out as Hadoop clusters that were eventually upgraded to also support Spark. YARN applications can be submitted in two modes: yarn-client and yarn-cluster. In yarn-cluster mode the driver is running remotely, while in yarn-client mode, the driver is on the machine that started the job, both modes are supported and are explained further in the connections chapter. Since YARN is the cluster manager from the Hadoop project, the main documentation can be found under the hadoop.apache.org site captured in Figure 5.5, you can also reference the “Running Spark on YARN” guide from spark.apache.org. FIGURE 5.5: Hadoop YARN Site 5.2.3 Mesos Apache Mesos is an open-source project to manage computer clusters. Mesos began as a research project in the UC Berkeley RAD Lab and makes use of Linux Cgroups to provide isolation for CPU, memory, I/O and file system access. Mesos, like YARN, supports executing many cluster frameworks, including Spark. However, one advantage particular to Mesos is that, it allows cluster framework like Spark to implement custom task schedulers. An scheduler is the component that coordinates in a cluster which applications get execution time and which resources are assign to them. Spark uses a coarse-grained scheduler(“Spark on Mesos” 2018) which schedules resources for the duration of the application; however, other frameworks might use Mesos’ fine-grained scheduler, which can increase the overall efficiency in the cluster by scheduling tasks in shorter intervals allowing them to share resources between them. Mesos is an Apache project with its documentation available under mesos.apache.org, Figure 5.6. The “Running Spark on Mesos” guide from spark.apache.org is also a great resource if you choose to use Mesos as your cluster manager. FIGURE 5.6: Mesos Landing Site. 5.2.4 Kubernetes Kubernetes is an open-source container-orchestration system for automating deployment, scaling and management of containerized applications that was originally designed by Google and now maintained by the Cloud Native Computing Foundation. Kubernetes was originally based on Docker while, like Mesos, it’s also based on Linux Cgroups. Kubernetes can also execute many cluster frameworks, it’s based on container images which provide a comprehensive isolation from it’s operating system, this allows a single Kubernetes cluster to be used for many different purposes beyond data analysis, which in turn helps organizations manage their compute resources with ease. However, one trade offs is that it adds additional overhead and compared to Mesos, does not provide a custom scheduler, trading efficiency for convenience. Nevertheless, this convenience has proven to be instrumental to administrate with ease cluster resources in many organizations and, as shown in the overview section, it’s becoming a very popular cluster framework. You can learn more about kubernetes.io captured in Figure 5.7, and the “Running Spark on Kubernetes” guide from spark.apache.org. FIGURE 5.7: Kubernetes Landing Site. 5.3 On-Premise As mentioned in the overview section, on-premise clusters represent a set of computing instances procured and managed by staff members from your organization. These clusters can be highly customized and controlled; however, they can also incur higher initial expenses and maintenance costs. One can use a cluster manager in on-premise clusters as described in the previous section; however, many organizations choose to partner with companies providing additional management software, services and resources to help manage applications in their cluster; including, but not limited to, Apache Spark. Some of the on-premise cluster providers include: Cloudera, Hortonworks and MapR to mention a few which, next, we will be briefly introduce them. 5.3.1 Cloudera Cloudera, Inc. is a United States-based software company that provides Apache Hadoop and Apache Spark-based software, support and services, and training to business customers. Cloudera’s hybrid open-source Apache Hadoop distribution, CDH (Cloudera Distribution Including Apache Hadoop), targets enterprise-class deployments of that technology. Cloudera donates more than 50% of its engineering output to the various Apache-licensed open source projects (Apache Hive, Apache Avro, Apache HBase, and so on) that combine to form the Apache Hadoop platform. Cloudera is also a sponsor of the Apache Software Foundation(“Cloudera Wikipedia” 2018). Cloudera clusters make use of parcels, which are binary distributions containing program files and metadata(“Cloudera Documentation” 2018), Spark happens to be installed as a parcel in Cloudera. It’s beyond the scope of this book to present how to configure Cloudera clusters, resources and documentation can be found under cloudera.com, captured in Figure 5.8, and “Introducing sparklyr, an R Interface for Apache Spark”(“Cloudera Engineering” 2016) under Cloudera’s Engineering Blog. FIGURE 5.8: Cloudera Landing Site. sparklyr is certified with Cloudera(“Cloudera Partners” 2017); meaning that, Cloudera’s support is aware of sparklyr and can be effective helping organizations that are using Spark and R, the following table summarizes the versions currently certified. Cloudera Version Product Version Components Kerberos CDH5.9 sparklyr 0.5 HDFS, Spark Yes CDH5.9 sparklyr 0.6 HDFS, Spark Yes CDH5.9 sparklyr 0.7 HDFS, Spark Yes 5.3.2 Hortonworks Hortonworks is a big data software company based in Santa Clara, California. The company develops, supports, and provides expertise on an expansive set of entirely open source software designed to manage data and processing for everything from IOT, to advanced analytics and machine learning. Hortonworks believes it is a data management company bridging the cloud and the datacenter(“Hortonworks Wikipedia” 2018). Hortonworks partnered with Microsoft(“Hortonworks Microsoft” 2018) to improve support in Microsoft Windows for Hadoop and Spark, this used to be a differentiation point; however, comparing Hortonworks and Cloudera is less relevant today since both companies are merging in 2019(“Hortonworks Cloudera” 2018). While the companies are merging, support for the Cloudera and Hortonworks Spark distributions are still available. Additional resources to configure Spark under Hortonworks are available under hortonworks.com, see Figure 5.9. FIGURE 5.9: Hortonworks Landing Site. 5.3.3 MapR MapR is a business software company headquartered in Santa Clara, California. MapR provides access to a variety of data sources from a single computer cluster, including big data workloads such as Apache Hadoop and Apache Spark, a distributed file system, a multi-model database management system, and event stream processing, combining analytics in real-time with operational applications. Its technology runs on both commodity hardware and public cloud computing services(“MapR Wikipedia” 2018). MapR’s landing site is captured in Figure 5.10 and accesible at mapr.com. FIGURE 5.10: MapR Landing Site. 5.4 Cloud If you don’t have an on-prem cluster nor spare machines to reuse, starting with a cloud cluster can be quite convenient since it will allow you to access a proper cluster in a matter of minutes. This section will briefly mention some of the major cloud infrastructure providers and give you resources to help you get started if you choose to use a cloud provider. In cloud services, the compute instances are billed for as long the Spark cluster runs; you start getting billed when the cluster launches and stops when the cluster stops. This cost needs to be multiplied by the number of instances reserved for your cluster. SO for instance, if a cloud provider chargets $1.00USD per compute instance per hour and you start a three node cluster that you use for one hour and 10 minutes; it is likely that you’ll get billed $1.00 * 2 hours * 3 nodes = $6.00. Some cloud providers charge per minute but, at least, you can rely on all of them charging per compute hour. Please be aware that, while compute costs can be quite low for small clusters, accidentally leaving a cluster running can cause significant billing expenses. Therefore, is is worth taking the extra time to check twice that your cluster is terminated when you no longer need it. It’s also a good practice to monitor costs daily while using clusters to make sure your expectations match the daily bill. From past experience, you should also plan to request compute resources in advance while dealing with large scale projects; various cloud providers will not allow you to start a cluster with hundreds of machines before requesting them explicitly through a support request. While this can be cumbersome, it’s also a way to help you controll costs in your organization. Since the cluster size is flexible, it is a good practice to start with small clusters and scale compute resources as needed. Even if you know in advance that a cluster of significant size will be required, starting small provides an opportunity to troubleshoot issues at a lower cost since it’s unlikely that your data analysis will run at scale flawlessly on the first try. As a rule oh thumb, grow the instances exponentially; if you need to run a computation over an eight node cluster, start with one node and an eighth of the entire dataset, then two nodes with a fourth, then four nodes with a half the dataset and then, finally, eight nodes and the entire dataset. As you become more experienced, you’ll develop a good sense of how to troubleshoot issues, the size of the required cluster and you’ll be able to skip intermediate steps, but for starters, this is a good practice to follow. One can also use a cloud provider to acquire bare computing resources and then, install the on-premise distributions presented in the previous section yourself; for instance, you can run the Cloudera distribution on Amazon Elastic Compute Cloud (EC2). This model would avoid procuring colocated hardware, but still allow you to closely manage and customize your cluster. This book presents an overview of only the fully-managed Spark services available by cloud providers; however, you can usually find with ease instructions online on how to install on-premise distributions in the cloud. Some of the major providers of cloud computing infrastructure are: Amazon, Databricks, Google, IBM and Microsoft that this section will briefly introduce. 5.4.1 Amazon Amazon provides cloud services through Amazon Web Services(Amazon AWS); more specifically, provides an on-demand Spark cluster through Amazon Elastic MapReduce or EMR for short, see Figure 5.11. FIGURE 5.11: Amazon EMR Landing Site. Detailed instructions on using R with Amazon EMR was published under Amazon’s Big Data Blog: “Running sparklyr on Amazon EMR”(“AWS Blog” 2016), this post introduced the launch of sparklyr and instructions to configure EMR clusters with sparklyr. For instance, it suggests you can use the Amazon Command Line Interface to launch a cluster with three nodes as follows: aws emr create-cluster --applications Name=Hadoop Name=Spark Name=Hive --release-label emr-5.8.0 --service-role EMR_DefaultRole --instance-groups InstanceGroupType=MASTER,InstanceCount=1,InstanceType=m3.2xlarge InstanceGroupType=CORE,InstanceCount=2,InstanceType=m3.2xlarge --bootstrap-action Path=s3://aws-bigdata-blog/artifacts/aws-blog-emr-rstudio-sparklyr/rstudio_sparklyr_emr5.sh,Args=[&quot;--user-pw&quot;, &quot;&lt;password&gt;&quot;, &quot;--rstudio&quot;, &quot;--arrow&quot;] --ec2-attributes InstanceProfile=EMR_EC2_DefaultRole You can then see the cluster launching, and eventually running under the AWS portal, see Figure 5.12. FIGURE 5.12: Launching an Amazon EMR Cluster. You can then navigate to the Master Public DNS and find RStudio under port 8787, for instance: ec2-12-34-567-890.us-west-1.compute.amazonaws.com:8787, and then login with user hadoop and password &lt;password&gt;. It is also possible to launch the EMR cluster using the web interface, the same introductory post contains additional details and walkthroughs specifically designed for EMR. Please remember to turn off your cluster to avoid unnecessary charges and use appropriate security restrictions when starting EMR clusters for sensitive data analysis. Regarding cost, the most up to date information can be found under aws.amazon.com/emr/pricing. As of this writing, these are some of the instance types available in the us-west-1 region, it is meant to provide a glimpse of the resources and costs associated with cloud processing. Notice that the “EMR price is in addition to the Amazon EC2 price (the price for the underlying servers)”. Instance CPUs Memory Storage EC2 Cost EMR Cost c1.medium 2 1.7GB 350GB $0.148 USD/hr $0.030 USD/hr m3.2xlarge 8 30GB 160GB $0.616 USD/hr $0.140 USD/hr i2.8xlarge 32 244GB 6400GB $7.502 USD/hr $0.270 USD/hr Note: We are only presentring a subset of the available compute instances for Amazon and subsequent cloud providers during 2019; however, please note that hardware (CPU speed, hard drive speed, etc.) varies between vendors and locations; therefore, you can’t use these hardware tables as an accurate price comparison, an accurate comparison would require running your particular workloads and considering other aspects beyond compute instance cost. 5.4.2 Databricks Databricks is a company founded by the creators of Apache Spark, that aims to help clients with cloud-based big data processing using Spark, see Figure 5.13. Databricks grew out of the AMPLab project at University of California, Berkeley(“Databricks Wikipedia” 2018). FIGURE 5.13: Databricks Landing Site. Databricks provides enterprise-level cluster computing plans, while also providing a free/community tear to explore functionality and get familiar with their environment. Once a cluster is launched, R and sparklyr can be used from Databricks notebooks following the steps from the Getting Started chapter or, by installing RStudio on Databricks(“Databricks Rstudio” 2018). Figure 5.14 shows a Databricks notebook using Spark through sparkylr. FIGURE 5.14: Databricks community notebook running sparklyr. Additional resources are available under the Databricks Engineering Blog post: “Using sparklyr in Databricks”(“Databricks Blog” 2017) and the “Databricks Documentation for sparklyr”(“Databricks Documentation” 2018). The latest pricing information can be found under databricks.com/product/pricing, as of this writing, available plans Plan Basic Data Engineering Data Analytics AWS Standard $0.07 USD/DBU $0.20 USD/DBU $0.40 USD/DBU Azure Standard $0.20 USD/DBU $0.40 USD/DBU Azure Premium $0.35 USD/DBU $0.55 USD/DBU Notice that pricing is based on cost of DBU/hr. From Databricks, “A Databricks Unit (DBU) is a unit of Apache Spark processing capability per hour. For a varied set of instances, DBUs are a more transparent way to view usage instead of the node-hour”(“Databricks Units” 2018). 5.4.3 Google Google provides Gooble Cloud Dataproc as a cloud-based managed Spark and Hadoop service offered on Google Cloud Platform, see Figure 5.15. Dataproc utilizes many Google Cloud Platform technologies such as Google Compute Engine and Google Cloud Storage to offer fully managed clusters running popular data processing frameworks such as Apache Hadoop and Apache Spark(“Dataproc Wikipedia” 2018). FIGURE 5.15: Google Dataproc Landing Site. A cluster can be easily created from the Google Cloud console or the Google Cloud command line interface as illustrated in Figure 5.16. FIGURE 5.16: Launching a Dataproc cluster. Once created, ports can be forwarded to allow you to access this cluster from your machine; for instance, by launching Chrome to make use of this proxy and securely connect to the Dataproc cluster. Configuring this connection looks as follows: gcloud compute ssh sparklyr-m --project=&lt;project&gt; --zone=&lt;region&gt; -- -D 1080 -N &quot;&lt;path to chrome&gt;&quot; --proxy-server=&quot;socks5://localhost:1080&quot; --user-data-dir=&quot;/tmp/sparklyr-m&quot; http://sparklyr-m:8088 There are various tutorials available under cloud.google.com/dataproc/docs/tutorials, including, a comprehensive tutorial to configure RStudio and sparklyr(“Dataproc Sparklyr” 2018). The latest pricing information can be found under cloud.google.com/dataproc/pricing. Notice that the cost is split between Compute Engine and a Dataproc Premium. Instance CPUs Memory Compute Engine Dataproc Premium n1-standard-1 1 3.75GB $0.0475 USD/hr $0.010 USD/hr n1-standard-8 8 30GB $0.3800 USD/hr $0.080 USD/hr n1-standard-64 64 244GB $3.0400 USD/hr $0.640 USD/hr 5.4.4 IBM IBM cloud computing is a set of cloud computing services for business offered by the information technology company IBM, see Figure 5.17. IBM cloud includes infrastructure as a service (IaaS), software as a service (SaaS) and platform as a service (PaaS) offered through public, private and hybrid cloud delivery models, in addition to the components that make up those clouds(“IBM Cloud Wikipedia” 2018). FIGURE 5.17: IBM Landing Site. From within IBM Cloud, open Watson Studio and create a Data Science project, add a Spark cluster under the project settings and launch RStudio from the Launch IDE menu. Please note that, as of this writting, the provided version of sparklyr was not the latest version available in CRAN, since sparklyr was modified to run under the IBM Cloud. In any case, please follos IBMs documentation as an authoritative reference to run R and Spark on the IBM Cloud and particularily, on how to upgrade sparklyr appropiately. Figure 5.18 captures IBM’s Cloud portal launching a Spark cluster. FIGURE 5.18: IBM Watson Studio using RStudio and sparklyr The most up to date pricing information is available under ibm.com/cloud/pricing. In the following table, compute cost was normalized using 31 days from the per-month costs. Instance CPUs Memory Storage Cost C1.1x1x25 1 1GB 25GB $0.033 USD/hr C1.4x4x25 4 4GB 25GB $0.133 USD/hr C1.32x32x25 32 25GB 25GB $0.962 USD/hr 5.4.5 Microsoft Microsoft Azure is a cloud computing service created by Microsoft for building, testing, deploying, and managing applications and services through a global network of Microsoft-managed data centers, see Figure 5.19. It provides software as a service (SaaS), platform as a service (PaaS) and infrastructure as a service (IaaS) and supports many different programming languages, tools and frameworks, including both Microsoft-specific and third-party software and systems(“Azure Wikipedia” 2018). FIGURE 5.19: Azure HDInsight Landing Site. From the Azure portal, the Azure HDInsight service provides support for on-demand Spark clusters. An HDInsight cluster with support for Spark and RStudio can be easily created by selecting the ML Services cluster type. Please note that the provided version of sparklyr might not be the latest version available in CRAN since the default package repo seems to be initialized using an MRAN (Microsoft R Application Network) snapshot, not directly from CRAN. Figure 5.20 shows the Azure portal launching an Spark cluster with support for R. FIGURE 5.20: Creating an Azure HDInsight Spark Cluster. Up to date pricing for HDInsight is available under azure.microsoft.com/en-us/pricing/details/hdinsight. Instance CPUs Memory Total Cost D1 v2 1 3.5 GB $0.074/hour D4 v2 8 28 GB $0.59/hour G5 64 448 GB $9.298/hour 5.5 Tools While using only R and Spark can be sufficient for some clusters, it is common to install complementary tools in your cluster to improve: monitoring, sql analysis, workflow coordination, etc. with applications like Ganglia, Hue and Oozie respectively. This section is not meant to cover all, but rather mention the ones that are commonly to use. 5.5.1 RStudio From reading the Introduction chapter, you are aware that RStudio is a well known, free, desktop development environment for R; therefore, it is likely that you are following the examples in this book using RStudio Desktop; however, you might not be aware that RStudio can also be run as a web service inside an Spark cluster, this version of RStudio is known as RStudio Server. You can see RStudio Server running in Figure 5.21. In the same way that the Spark UI runs in the cluster, RStudio Server can be installed inside the cluster, then you can connect to RStudio Server and use RStudio in exactly the same way you use RStudio Desktop but with the ability to run code against the Spark cluster. As you can see on the following image, RStudio Server is running on a web browser inside a Spark cluster; it looks and feels just like RStudio Desktop, but adds support to run commands efficiently by being located within the cluster. FIGURE 5.21: RStudio Server Pro running inside Apache Spark. For those familiar with R, Shiny is a very popular tool for building interactive web applications from R; which it is also recommended you install directly in your Spark cluster. RStudio Server and Shiny Server are a free and open source; however, RStudio also provides professional producs, like: RStudio Server, RStudio Server Pro, Shiny Server Pro and RStudio Connect which can be installed within the cluster to support additional R workflows, while sparklyr does not require any additional tools, they provide significant productivity gains worth considering. You can learn more about them at rstudio.com/products/. 5.5.2 Jupyter Project Jupyter exists to develop open-source software, open-standards, and services for interactive computing across dozens of programming languages. A Jupyter notebook, provide support for various programming languages, including R. sparklyr can be used with Jupyter notebooks using the R Kernel. Figure 5.22 shows sparklyr running inside a local Jupyter notebook. FIGURE 5.22: Jupyter notebook running sparklyr. 5.5.3 Livy Apache Livy is an incubation project in Apache providing support to use Spark clusters remotely through a web interface, see Figure 5.23. It is ideal to connect directly into the Spark cluster; however, there are times where connecting directly to the cluster is not feasible. When facing those constraints, one can consider installing Livy in their cluster and secure it properly to enable remote use over web protocols. However, there is a significant performance overhead from using Livy in sparklyr. FIGURE 5.23: Apache Livy Landing Site. To help test Livy locally, sparklyr provides support to list, install, start and stop a local Livy instance by executing: ## livy ## 1 0.2.0 ## 2 0.3.0 ## 3 0.4.0 ## 4 0.5.0 Which lists the versions that you can install, we recommend installing the latest version and verifying the installed version as follows # Install default Livy version livy_install() # List installed Livy services livy_installed_versions() # Start the Livy service livy_service_start() You can then navigate to this locakl Livy session under http://localhost:8998, the Livy Connections section will detail how to connect to this local instance and also proper clusters with Livy enabled, once connected, you can navigate to the Livy web application as captured by Figure 5.24. FIGURE 5.24: Apache Livy running as a local service. Make sure you also stop the Livy service when working with local Livy instances, for proper Livy services running in a cluster, you won’t have to. # Stops the Livy service livy_service_stop() 5.6 Recap This chapter explained the history and tradeoffs of on-premise, cloud computing and presented Kubernetes as a promising framework to provide flexibility across on-premise and cloud providers. It also introduced cluster managers (Spark Standalone, YARN, Mesos and Kubernetes) as the software needed to run Spark as a cluster application. This chapter briefly mentioned on-premise cluster providers like Cloudera, Hortonworks and MapR as well as the major cloud providers: Amazon, Google and Microsoft. While this chapter provided a solid foundation to understand current cluster computing trends, tools and providers useful to perform data science at scale; it did not provide a comprehensive framework to decide which cluster technologies to choose. Instead, use this chapter as an overview and a starting point to reach out to additional resources to complement your understanding the cluster stack that best fits your organization needs. The next chapter, connections, will focus on understanding how to connect to existing clusters; therefore, it assumes a Spark cluster like the ones presented in this chapter, is already available to you. References "],
["connections.html", "Chapter 6 Connections 6.1 Overview 6.2 Local 6.3 Standalone 6.4 Yarn 6.5 Livy 6.6 Mesos 6.7 Kubernetes 6.8 Cloud 6.9 Multiple 6.10 Troubleshooting 6.11 Recap", " Chapter 6 Connections The previous chapter, Clusters, presented the major cluster computing paradigms, cluster managers and cluster providers to help you choose the Spark cluster distribution and provider that best suits your needs. In contras, this chapter presents the internal components of a Spark cluster and the how to connect to any cluster running Apache Spark, including but not limitted to, any distribution and provided presented in the previous chapter. In addition, this chapter provides various troubleshooting connection techniques which I hope you won’t need to use, but if you have to, you can use them as effective techniquest to resolve connectivity issues. While this chapter might feel a bit dry since, connecting and troubleshooting connections is definetely not the most fun part of large-scale data analysis, it is the first chapter that will introduce the components of a Spark cluster and how they interact, this is often known as the architecture of Apache Spark. This chapter, Data and Tunning chapter, will provide a detailed view of how Spark works, which will help you move towards becoming an intermediate Spark user that can go beyond analysis to dive into the realm of distributed computing, using Apache Spark. Note: As you know from previous chapters, a cluster is a collection of machines that work together to perform a computation. However, in distributed systems and clusters literature, we often refer to each physical machine as a compute instance, compute node, instance or node. It is helpful to remind this while reading through this chapter and making use of external resources. 6.1 Overview The overall connection architecture for a Spark cluster is composed of three type of compute instances: The driver node, the worker nodes and the cluster manager. A cluster manager is a service that allows Spark to be executed in the cluster as described in the previous chapter under the cluster managers section. The driver node is tasked with delegating work to the worker nodes, but also for aggregating their results and controlling computation flow. For the most part, aggregation happens in the worker nodes; however, even after the nodes aggregate data, it is often the case that the driver node would have to collect the worker’s results. Therefore, the driver node usually has at least, but often much more, compute resources (memory, CPUs, local storage, etc.) than the worker node. The worker nodes execute compute tasks over partitioned data and communicate intermediate results to other workers or back to the driver node, worker nodes are also referred as executors. Strictly speaking, the driver node and worker nodes are just names assigned to machines with particular roles, while the actual computation in the driver node is performed by the spark context. The Spark context is a Spark component tasked with scheduling tasks, managing data and so on. In the worker nodes, the actual computation is performed under a spark executor, which is also a Spark component tasked with executing subtasks against a data partition. We can illustrate this concepts in Figure 6.1, where the driver node orchestrates worker’s work through the cluster manager. FIGURE 6.1: Apache Spark Architecture If you already have a Spark cluster in your organization, you should request from your cluster administrator the connection information to this cluster, read carefully their usage policies and follow their advice. Since a cluster may be shared among many users, you want to make sure you only request the compute resources you need, you will learn how to request resources in the Tunning chapter. Your system administrator will describe if it’s an on-premise vs cloud cluster, the cluster manager being used, supported connections and supported tools. You can use this information to jump directly to Local, Standalone, YARN, Mesos, Livy or Kubernetes based on the information provided to you. Note: Once connected is performed with spark_connect(), you can use all techniques described in previous chapters using the sc connection; for instance, you can do data analysis or modeling with the same code previous chapters presented. 6.1.1 Edge Nodes Before connecting to Apache Spark, you will first have to connect to the cluster. Usually, by connecting to an edge node within the cluster. An edge node, is a machine that you can access from outside the cluster but which is also part of the cluster. There are two methods to connect to this edge instance: Terminal: Using a computer terminal application, one can use a secure shell to establish a remote connection into the cluster, once you connect into the cluster, you can launch R and then use sparklyr. However, a terminal can be cumbersome for some tasks, like exploratory data analysis, so it’s often only used while configuring the cluster or troubleshooting issues. Web Browser: While using sparklyr from a terminal is possible, it is usually more productive to install a web server in an edge node that provides access to run R with sparklyr from a web browser. Most likely, you will want to consider using RStudio or Jupyter rather than connecting from the terminal. Figure 6.2 explains these concepts visually. The left block is usually your web browser, the right block is the edge node, client and edge node communicate over HTTP when using a web browser or SSH when using the terminal. FIGURE 6.2: Connecting to Sparks Edge Node 6.1.2 Spark Home It is important to mention that, while connecting to a Spark cluster, you will need to find out the correct SPARK_HOME path which contains the installation of Spark in the given instance. The SPARK_HOME path must be specified by your system administrator as an environment variable or by yourself explicitly specified in spark_connect() using the spark_home parameter. If your cluster provider or cluster administrator already provided SPARK_HOME for you, the following code should return a path instead of an empty string. Sys.getenv(&quot;SPARK_HOME&quot;) For system administrators, we recommend setting SPARK_HOME for all the users in your cluster; however, if this is not set in your cluster, you can also specify SPARK_HOME while using spark_connect() as follows: sc &lt;- spark_connect(master = &quot;&lt;cluster-master&gt;&quot;, spark_home = &quot;local/path/to/spark&quot;) Where &lt;cluster-master&gt; is set to the correct cluster manager master for Spark Standalone, YARN, Mesos, Kubernetes or Livy. 6.2 Local When connecting to Spark in local mode, Spark starts as a single application simulating a cluster with a single node, this is not a proper computing cluster but it’s ideal to perform work offline and while troubleshooting issues. A local connection to Spark is represented in Figure 6.3. FIGURE 6.3: Local Connection Diagram Notice that in the local connections diagram, there is no cluster manager nor worker process since, in local mode, everything runs inside the driver application. It’s also worth noting that sparklyr starts the Spark Context through spark-submit, a script available in every Spark installation to enable users to submit custom application to Spark which, sparklyr makes use of to submit itself to Spark. For the curious reader, the Contributing chapter explains the internal processes that takes place in sparklyr to submit this application and connect properly from R. To perform this local connection, we can connect with the following familiar code used in previous chapters: # Connect to local Spark instance sc &lt;- spark_connect(master = &quot;local&quot;) By default, sparklyr, will connect using as many CPUs are available in your compute instance; however, this can be customized by connecting using master=\"local[n]\", where n is the desired number of cores to use. For example, we can connect using only 2 CPUs as follows: # Connect to local Spark instance using 2 cores sc &lt;- spark_connect(master = &quot;local[2]&quot;) 6.3 Standalone Connecting to a Spark Standalone cluster requires the location of the cluster manager’s master instance, this location can be found in the cluster manager web interface as described in the Standalone Clusters section, you can find this location by looking for a URL starting with spark://. A connection in standalone mode starts from sparklyr which launches spark-submit, which then submits the sparklyr application and creates the Spark Context, which requests executors from the Spark Standalone instance running under the given master address. Visually, this is described in Figure 6.4, which is quite similar to the overall connection architecture from Figure 6.1 but, with additional details that are particular to standalone clusters and sparklyr. FIGURE 6.4: Spark Standalone Connection Diagram In order to connect, use master = \"spark://hostname:port\" in spark_connect() as follows: sc &lt;- spark_connect(master = &quot;spark://hostname:port&quot;) 6.4 Yarn Hadoop YARN supports two connection modes: YARN Client and YARN Cluster. However, YARN Client mode is much more common that YARN Cluster since it’s more efficient and easier to set up. 6.4.1 Yarn Client When connecting in YARN Client mode, the driver instance runs R, sparklyr and the Spark Context which requests worker nodes from YARN to run Spark executors as shown in Figure 6.5. FIGURE 6.5: YARN Client Connection Diagram To connect, one can simply run with master = \"yarn\" as follows: sc &lt;- spark_connect(master = &quot;yarn-client&quot;) Behind the scenes, when running YARN in client mode, the cluster manager will do what you would expect a cluster manager would do; it will allocate resources from the cluster and assign them to your Spark application, which the Spark Context will manage for you. The important piece to notice in Figure 6.5 is that, the Spark Context resides in the same machine where you run R code, this is different when running YARN in cluster mode. 6.4.2 Yarn Cluster The main difference between YARN in cluster mode and running YARN in client mode is that, in cluster mode, the driver node is not required to be the node where R and sparklyr were launched; instead, the driver node remains the designated driver node which is usually a different node than the edge node where R is running. It can be helpful to consider using cluster mode when the edge node has too many concurrent users, when is lacking computing resources or where tools (like RStudio or Jupyter) need to be managed independently of other cluster resources. Figure 6.6 shows how the different components become decoupled when running in cluster mode. Notice there is still a line connecting the client with the cluster manager since, first of all, resources still need to be allocated from the cluster manager; however, once allocated, the client communicates directly with the driver node which will then communicate with the worker nodes. From this diagram, you might think that cluster mode looks much more complicated than the client mode diagram – this would be a correct assesment; therefore, it’s best to avoid cluster mode when possible due to additional configuration overhead that is best to avoid, if possible. FIGURE 6.6: YARN Cluster Connection Diagram To connect in YARN Cluster mode, we can simple run: sc &lt;- spark_connect(master = &quot;yarn-cluster&quot;) Cluster mode assumes that the node running spark_connect() is properly configured, meaning that, yarn-site.xml exists and the YARN_CONF_DIR environment variable is properly set. When using Hadoop as a file system, you will also need the HADOOP_CONF_DIR environment variable properly configured. In addition, you would need to have proper network connectivity between the client and the driver node, not just with sufficient bandwidth but also making sure both machines are reachable and no intermediate. This configuration is usually provided by your system administrator and is not something that you would have to manually configure. 6.5 Livy As opposed to other connection methods which require using an edge node in the cluster, Livy provides a Web API that makes the Spark cluster accessible from outside the cluster and does not require a Spark installation in the client. Once connected through the Web API, the Livy Service starts the Spark context by requesting resources from the cluster manager and distributing work as usual. FIGURE 6.7: Livy Connection Diagram Connecting through Livy requires the URL to the Livy service which should be similar to https://hostname:port/livy. Since remote connections are allowed, connections usually requires, at the very least, basic authentication: sc &lt;- spark_connect( master = &quot;https://hostname:port/livy&quot;, method = &quot;livy&quot;, config = livy_config( spark_version = &quot;2.4.0&quot;, username = &quot;&lt;username&gt;&quot;, password = &quot;&lt;password&gt;&quot; )) To try out Livy in your local machine, you can install and run a Livy service as described under the Livy Clusters section and then, connect as follows: sc &lt;- spark_connect( master = &quot;http://localhost:8998&quot;, method = &quot;livy&quot;, config = livy_config(spark_version = &quot;2.4.0&quot;)) Once connected through Livy, you can make use of any sparklyr feature; however, Livy is not suitable for exploratory data analysis since, executing commands has a significant performance cost; that said, while running long running computations, this overhead could be considered irrelevant. In general, it is preferred to avoid using Livy and work directly within an edge node in the cluster; when this is not feasible, using Livy could be a reasonable approach. Note: Specifying the Spark version through the spark_version parameter is optional; however, when the version is specified, performance is significantly improved by deploying precompiled Java binaries compatible with the given version. Therefore, it is a best practice to specify the Spark version when connecting to Spark using Livy. 6.6 Mesos Similar to YARN, Mesos supports client mode and a cluster mode, sparklyr currently only supports client mode for Mesos. FIGURE 6.8: Mesos Connection Diagram Connecting requires the address to the Mesos master node, usually in the form of mesos://host:port or mesos://zk://host1:2181,host2:2181,host3:2181/mesos for Mesos using ZooKeeper. sc &lt;- spark_connect(master = &quot;mesos://host:port&quot;) 6.7 Kubernetes Kubernetes cluster do not support client modes similar to Mesos or YARN, instead, the connection model is similar to YARN Cluster, where the driver node is assigned by Kubernetes. FIGURE 6.9: Kubernetes Connection Diagram Kubernetes support is scheduled to be added to sparklyr with sparklyr/issues/1525, please follow progress for this feature directly in github. Once Kubernetes becomes supported in sparklyr, connecting to Kubernetes will work as follows: sc &lt;- spark_connect( master = &quot;k8s://https://&lt;apiserver-host&gt;:&lt;apiserver-port&gt;&quot; config = list( spark.executor.instances = 2, spark.kubernetes.container.image = &quot;spark-image&quot; ) ) If your computer is already configured to use a Kubernetes cluster, you can use the following command to find the apiserver-host and apiserver-port: system2(&quot;kubectl&quot;, &quot;cluster-info&quot;) 6.8 Cloud When working with cloud providers, there are a few connection differences. For instance, connecting from Databricks requires the following connection method: library(sparklyr) sc &lt;- spark_connect(method = &quot;databricks&quot;) Connecting to EMR works fine through sc &lt;- spark_connect(master = \"yarn\"); however, to properly configure spark_web() and RStudio’s connection pane you can connect instead by configuring the proxy yourself, domain would have to be set to your driver node domain, usually something like http://ec2-12-345-678-9.us-west-2.compute.amazonaws.com. sc &lt;- spark_connect(master = &quot;yarn&quot;, config = list( sparklyr.web.spark = ~paste0(domain, &quot;:20888/proxy/&quot;, invoke(spark_context(sc), &quot;applicationId&quot;)), sparklyr.web.yarn = paste0(domain, &quot;:8088&quot;) )) Similarly, connections to Spark when using IBM’s Watson Studio require you to connect as follows: kernels &lt;- load_spark_kernels() sc &lt;- spark_connect(config = kernels[2]) Under Microsoft Azure HDInsights and when using ML Services (R Server), creating an sparklyr connection gets initialized through: library(RevoScaleR) cc &lt;- rxSparkConnect(reset = TRUE, interop = &quot;sparklyr&quot;) sc &lt;- rxGetSparklyrConnection(cc) Please reference your cloud provider documentation and their support channels if assistance is needed. 6.9 Multiple It is common to connect once, and only once, to Spark. However, you can also open multiple connections to Spark by connecting to different clusters or by specifying the app_name parameter, this can be helpful to compare Spark versions or validate you analysis before submitting to the cluster. The following example opens connections to Spark 1.6.3, 2.3.0 and Spark Standalone: # Connect to local Spark 1.6.3 sc_1_6_3 &lt;- spark_connect(master = &quot;local&quot;, version = &quot;1.6.3&quot;) # Connect to local Spark 2.3.0 sc_2_3_0 &lt;- spark_connect(master = &quot;local&quot;, version = &quot;2.3.0&quot;, appName = &quot;Spark23&quot;) # Connect to local Spark Standalone sc_standalone &lt;- spark_connect(master = &quot;spark://host:port&quot;) Finally, we can disconnect from each connection: spark_disconnect(sc_1_6_3) spark_disconnect(sc_2_3_0) spark_disconnect(sc_standalone) Alternatively, you can disconnect from all connections at once: spark_disconnect_all() 6.10 Troubleshooting Last but not least, we will introduce the following troubleshooting techniques for: Logging, Spark Submit and Windows. When in doubt of where to start, start with the Windows section when using Windows systems, followed by Logging and closing with Spark Submit. 6.10.1 Logging One first step is to troubleshoot connections is to run in verbose to print directly to the console additional error messages: sc &lt;- spark_connect(master = &quot;local&quot;, log = &quot;console&quot;) Verbose logging can also be enabled with the following option: options(sparklyr.verbose = TRUE) 6.10.2 Spark Submit If connections fail in sparklyr, first troubleshoot if this issue is specific to sparklyr or Spark in general. This can be accomplished by running an example spark-submit job and validating that no errors are thrown. # Find the spark directory using an environment variable spark_home &lt;- Sys.getenv(&quot;SPARK_HOME&quot;) # Or by getting the local spark installation spark_home &lt;- sparklyr::spark_home_dir() Then execute the sample compute Pi example by replacing \"local\" with the correct master parameter you are troubleshooting: # Launching a sample application to compute Pi system2( file.path(spark_home, &quot;bin&quot;, &quot;spark-submit&quot;), c( &quot;--master&quot;, &quot;local&quot;, &quot;--class&quot;, &quot;org.apache.spark.examples.SparkPi&quot;, file.path(spark_home, &quot;examples&quot;, &quot;jars&quot;, &quot;spark-examples_2.11-2.4.0.jar&quot;), 100) ) ... Pi is roughly 3.1415503141550314 ... If your Spark cluster is properly configured, you should see the message above; otherwise, you will need to investigate why your Spark cluster is not properly configured, which is beyond the scope of this book. 6.10.2.1 Detailed To troubleshoot the connection process in detail, you can manually replicate the two-step connection process, which is often very helpful to diagnose connection issues. Connecting to Spark is performed in two steps; first, spark-submit is triggered from R which submits the application to Spark, second, R connects to the running Spark application. First, identify the Spark installation directory and the path to the correct sparklyr*.jar by running: dir(system.file(&quot;java&quot;, package = &quot;sparklyr&quot;), pattern = &quot;sparklyr&quot;, full.names = T) Make sure you identify the correct version that matches your Spark cluster, for instance sparklyr-2.1-2.11.jar for Spark 2.1. Then, from the terminal, run: $SPARK_HOME/bin/spark-submit --class sparklyr.Shell $PATH_TO_SPARKLYR_JAR 8880 12345 18/06/11 12:13:53 INFO sparklyr: Session (12345) found port 8880 is available 18/06/11 12:13:53 INFO sparklyr: Gateway (12345) is waiting for sparklyr client to connect to port 8880 The parameter 8880 represents the default port to use in sparklyr while 12345 is the session number, this is a cryptographically secure number generated by sparklyr, but for troubleshooting purposes can be as simple as 12345. If this first connection step fails, it means that the cluster can’t accept the application. This usually means that there are not enough resources, there are permission restrictions, etc. The second step is to connect from R as follows, notice that there is a 60 seconds timeout, so you’ll have to run the R command after running the terminal command, if needed, this timeout can be configured as described in the Tunning chapter. library(sparklyr) sc &lt;- spark_connect(master = &quot;sparklyr://localhost:8880/12345&quot;, version = &quot;2.3&quot;) If this second connection step fails, it usually means that there is a connectivity problem between R and the driver node, you can try using a different connection port, for instance. 6.10.3 Windows Connecting from Windows is, in most cases, as straightforward as connecting from Linux and OS X; however, there are a few common connection issues you should be aware of: Firewalls and antivirus software might block ports for your connection. The default port used by sparklyr is 8880, double check this port is not being blocked. Long path names can cause issues in, specially in older Windows systems like Windows 7. When using these systems, try connecting with Spark installed with all folders using at most 8 characters and no spaces in their names. 6.11 Recap This chapter presented an overview of Spark’s architecture, connection concepts and examples to connect in local mode, standalone, YARN, Mesos, Kubernetes and Livy. It also presented edge nodes and their role while connecting to Spark clusters. This should have provided you with enough information to successfully connect to any Apache Spark cluster. To troubleshoot connection problems beyond the techniques described in this chpater, it is recommended to search for the connection problem in StackOverflow, the sparklyr github issues and, if needed, open a new GitHub issue in sparklyr to assist further. In the next chapter, Data, you will learn how to read and write over multiple data sources, you will understand how Spark makes use of Spark DataFrames and how to transfer data into and out of Spark clusters. "],
["data.html", "Chapter 7 Data 7.1 Overview 7.2 Data Frames 7.3 Formats 7.4 Data Types 7.5 Sources 7.6 Troubleshooting 7.7 Recap", " Chapter 7 Data While this chatper has not been written., a few resources are available to help explore these topics until this chapter gets written. 7.1 Overview 7.2 Data Frames TODO: Introduce Data Frames sparklyr provides access to Data Frame functionality, mostly for completeness and advanced use cases. As sparklyr keeps evolving, it is our goal to reduce the use of this functions and provide instead propert wrappers with practices known in the R community. For instnace, we will introduce a function to pivot data, which is quite useful but would rather preffer to support tidyr::spread() and tidyr::gather() implementations which are already well known in the R community. 7.2.1 Functions sdf_nrow() sdf_ncol() sdf_dim() sdf_len() sdf_pivot() sdf_schema() ... 7.2.2 Pivoting sensors &lt;- data.frame( sensor = c(&quot;sensor1&quot;, &quot;sensor1&quot;, &quot;sensor1&quot;, &quot;sensor2&quot;, &quot;sensor2&quot;, &quot;sensor2&quot;), time = c(1, 2, 3, 1, 2, 3), metric = c(1, 2, 3, 10, 20, 30) ) sensor time metric 1 sensor1 1 1 2 sensor1 2 2 3 sensor1 3 3 4 sensor2 1 10 5 sensor2 2 20 6 sensor2 3 30 sensors_tbl &lt;- sdf_copy_to(sc, sensors, overwrite = T) sensors_tbl %&gt;% sdf_pivot(time ~ sensor, fun.aggregate = c(&quot;avg&quot;, &quot;metric&quot;)) # Source: spark&lt;?&gt; [?? x 3] time sensor1 sensor2 * &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 3 3 30 2 1 1 10 3 2 2 20 7.3 Formats spark_read_csv() spark_read_jdbc() spark_read_json() spark_read_libsvm() spark_read_orc() spark_read_parquet() spark_read_source() spark_read_table() spark_read_text() 7.4 Data Types 7.4.1 Dates Note: Some Spark date/time functions make timezone assumtions, for instance, the following code makes use of to_date() which assumes the timestamp will be given in the local time zone. This is not to discourage use of date/time functions, but to be aware of time zones to be handled with care. sdf_len(sc, 1) %&gt;% transmute( date = timestamp(1419126103) %&gt;% from_utc_timestamp(&#39;UTC&#39;) %&gt;% to_date() %&gt;% as.character() ) 7.5 Sources 7.5.1 Amazon S3 7.5.2 Azure Storage wasb files 7.5.3 Cassandra See https://blog.rstudio.com/2017/07/31/sparklyr-0-6/#external-data-sources. 7.5.4 Databases See https://blog.rstudio.com/2017/07/31/sparklyr-0-6/#external-data-sources. 7.5.4.1 Switching You can query multiple databases registered in Spark using the . syntax, as in: DBI::dbSendQuery(&quot;SELECT * FROM databasename.table&quot;) However, if you preffer to switch to a particular database and make it the default, you can run: tbl_change_db(sc, “db_name”) which an alias over DBI::dbGetQuery(sc, \"use db_name”). 7.5.4.2 Schemas in_schema(&quot;database&quot;, &quot;table&quot;) 7.5.5 HBase 7.5.6 Nested Data See nested data extension. 7.6 Troubleshooting 7.6.1 Troubleshoot CSVs writeLines(c(&quot;bad&quot;, 1, 2, 3, &quot;broken&quot;), &quot;tmp/bad.csv&quot;) There are a couple modes that can help troubleshoot parsing issues: - PERMISSIVE: NULLs are inserted for missing tokens. - DROPMALFORMED: Drops lines which are malformed. - FAILFAST: Aborts if encounters any malformed line. Which can be used as follows: spark_read_csv( sc, &quot;bad&quot;, &quot;tmp/bad.csv&quot;, columns = list(foo = &quot;integer&quot;), infer_schema = FALSE, options = list(mode = &quot;DROPMALFORMED&quot;)) # Source: table&lt;bad&gt; [?? x 1] # Database: spark_connection foo &lt;int&gt; 1 1 2 2 3 3 In Spark 2.X, there is also a secret column _corrupt_record that can be used to output those incorrect records: spark_read_csv( sc, &quot;decimals&quot;, &quot;tmp/bad.csv&quot;, columns = list(foo = &quot;integer&quot;, &quot;_corrupt_record&quot; = &quot;character&quot;), infer_schema = FALSE, options = list(mode = &quot;PERMISIVE&quot;) ) # Source: table&lt;decimals&gt; [?? x 2] # Database: spark_connection foo `_corrupt_record` &lt;int&gt; &lt;chr&gt; 1 1 NA 2 2 NA 3 3 NA 4 NA sdfsdfds 5 NA 2.16027303300001e+31 7.6.2 Column Names By default, sparklyr sanitizes column names by translating characters like . to _, this was required in Spark 1.6.X to avoid couple nuances in Spark. However, to disable this functionality, you can run the following code: options(sparklyr.sanitize.column.names = FALSE) dplyr::copy_to(sc, iris, overwrite = TRUE) # Source: table&lt;iris&gt; [?? x 5] # Database: spark_connection Sepal.Length Sepal.Width Petal.Length Petal.Width Species &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;chr&gt; 1 5.1 3.5 1.4 0.2 setosa 2 4.9 3 1.4 0.2 setosa 3 4.7 3.2 1.3 0.2 setosa 4 4.6 3.1 1.5 0.2 setosa 5 5 3.6 1.4 0.2 setosa 6 5.4 3.9 1.7 0.4 setosa 7 4.6 3.4 1.4 0.3 setosa 8 5 3.4 1.5 0.2 setosa 9 4.4 2.9 1.4 0.2 setosa 10 4.9 3.1 1.5 0.1 setosa # ... with more rows 7.7 Recap In the next chapter, Tuning, you will learn in-detail how Spark works and use this knowledge to optimize it’s resource usage and performance. "],
["tuning.html", "Chapter 8 Tuning 8.1 Overview 8.2 Configuring 8.3 Partitioning 8.4 Caching 8.5 Shuffling 8.6 Serialization 8.7 Configuration Files 8.8 Recap", " Chapter 8 Tuning In previous chapters we’ve assumed that computation within a Spark cluster works efficiently. While this is true in some cases, it is often required to have some knowledge of the operations Spark runs internally to fine tune configuration settings that will make computations run efficiently. This chapter will explain how Spark computes data over large datasets and provide details on how to fine-tune its operations. For instance, you will learn how to request more compute nodes and increase the amount of memory which, if you remember, defaults to only 2GB in local instances. You will learn how Spark unifies computation through partioning, shuffling and caching. As mentioned a few chapters back, this is the last chapter describing the internals of Spark; once you complete this chapter, it is our belief that you would have acquired intermediate Spark skills necessary to be productive at using Spark. In subsequent chapters: Extensions, Distributed R and Streaming; you will learn exciting techniques to deal with specific modeling, scaling and computation problems. However, we must first understand how spark performs internal computations, what pieces we can control and why. 8.1 Overview Spark performs distributed computation by: configuring, partitioning, executing, shuffling, caching and serializing data, tasks and resources across multiple machines: Configuring requests the cluster manager for resources: total machines, memory, etc. Partitioning splits the data among various machines. Partitions can be either implicit or explicit. Executing means running an arbitrary transformation over each partition. Shuffling redistributes data when data to the correct machine. Caching preserves data in-memory across different computation cycles. Serializing transforms data partitions or data collection to be sent over the network to other workers or back to the driver node. The diagram in Figure 8.1 shows an example on how a sorting job would conceptually work across a cluster of machines. First, Spark would configure the cluster to use three worker machines. In this example, the numbers 1-9 are partitioned across three storage instances. Since the data is already partitioned, each worker node loads this implicit partition; for instance, 4,9,1 is loaded in the first worker node. Afterwards, a custom transformation is applied to each partition in each worker node, this is denoted by f(x) in the diagram below and is defined as a stage in Spark terminology. In this example, f(x) executes a sorting operation within a partition. Since Spark is general, execution over a partition can be as simple or complex as needed. Once the execution completes, the result is shuffled to the right machine to finish the sorting operation across the entire dataset. Once the data is sorted across the cluster, the sorted results can be optionally cached in memory to avoid rerunning this computation multiple times. Finally, a small subset of the cached results is serialized, through the network connecting the cluster machines, back to the driver node to print a preview of this sorting example. FIGURE 8.1: Sorting Distributed Data with Apache Spark. Notice that while Figure 8.1 describes a sorting operation, a similar approach applies to filtering or joining datasets and analyzing and modeling data at scale. Spark provides support to perform custom partitions, custom shuffling, etc; however, most of these lower level operations are not exposed in sparklyr, instead, sparklyr makes those operations available through higher level commands provided by data analysis tools like dplyr or DBI, modeling and by using many extensions. For those few cases where you might need to implement lowe-level operations, you can always use the Spark’s Scala API through an sparklyr extensions or run custom distributed R code. In order to effectively tune Spark, we will start by getting familiar with the graph visualization and the event timeline. Both are accessible through the Spark Web Interface by selecting a particular job and a particular state under this job. 8.1.1 Graph Visualization You can find a graph visualization under each Spark stage by expanding “DAG Visualization”. DAG stands for Directed Acyclic Graph, which means that all computations in Spark move computation forward without repeating previous steps, this helps Spark optimize computations effectively. What you will see in this visualization is a breakdown of the operations that Spark had to perform (or is performing if the stage is still active) to execute your computation. For instance, lets first sort a dataset using dplyr::arrange() and then take a look at the resulting graph from Figure 8.2. Notice that there are no arrows pointing back to previous steps, since Spark makes use of acyclic graphs; then, take a look at the individual steps comprised of mostly mapping operations. It’s hard to understand what they mean the first time you see them, but as you execute more Spark jobs, this graph will become more familiar and will help you identify unexpected steps to investigate further. copy_to(sc, iris) %&gt;% arrange(Sepal_Width) FIGURE 8.2: Spark graph visualization. 8.1.2 Event Timeline The event timeline is also available for each Spark stage in the Spark’s web interface, this timeline gives you a great summary of how Spark is spending computation cycles. In general, you want to see this timeline consisting of mostly CPU usage since other tasks can be considered overhead. Lets sort again a dataset, but this time, we will skip the graph visualization and look instead at the event timeline from Figure 8.3. copy_to(sc, iris, overwrite = T) %&gt;% arrange(Sepal_Width) FIGURE 8.3: Spark Event Timeline. Notice that a single executor is being used, for machines and clusters with multiple CPUs, you should rather expect computation to be parallelized. You can explicitly repartition data as will be explained in the partitioning section, but for now, let’s just copy again the dataset using two partitions and explore the event timeline of this operation. copy_to(sc, iris, repartition = 2, overwrite = T) %&gt;% arrange(Sepal_Width) FIGURE 8.4: Spark Event Timeline with . Figure 8.4 now shows two execution lanes with most time spent under “Executor Computing Time”, this means that this particular operation is making better use of our compute resources. When working with clusters, requesting more compute nodes from your cluster should shorten computation time. In contrast, for timelines heavy on read or write shuffling, requesting more compute nodes might not shorten time and might actualy makes everything slower. There is no concrete set of rules to follow; however, as you gain experience understanding this timeline over multiple operations – you will develop insight as to how to properly optimize Spark operations. 8.2 Configuring When tuning a Spark application, the most common resources to configure are memory and cores, specifically: Memory in Driver: The amount of memory required in the driver node. Memory per Worker: The amount of memory required in the worker nodes. Cores per Worker: The number of CPUs to required in the worker nodes. Number of Workers: The number of workers required for this session. Note: It is recommended to request significantly more memory for the driver than the memory available over each worker node. In most cases, you will want to request one core per worker. In local mode, spark_connect(master = \"local\"); as mentioned in the local connections section, there are no workers, but we can set the driver memory and cores to use through: # Initialize configuration with defaults config &lt;- spark_config() # Memory config[&quot;sparklyr.shell.driver-memory&quot;] &lt;- &quot;2g&quot; # Cores config[&quot;sparklyr.connect.cores.local&quot;] &lt;- 2 # Connect to local cluster with custom configuration sc &lt;- spark_connect(master = &quot;local&quot;, config = config) When using the Spark Standalone and the Mesos cluster managers, all the available memory and cores are assigned by default; therefore, there is no additional configuration changes required, unless, you want to restrict resources to allow multiple users to share this cluster, in which case you can use total-executor-cores to restrict the total executors requested. The “Spark Standalone”(“Spark Standalone Mode” 2018) and “Spark on Mesos”(“Running Spark on Mesos” 2018) guides provided additional information when sharing clusters. When running under YARN Client, you would configure memory and cores as follows: # Memory in Driver config[&quot;sparklyr.shell.driver-memory&quot;] &lt;- &quot;2g&quot; # Memory per Worker config[&quot;sparklyr.shell.executor-memory&quot;] &lt;- &quot;2g&quot; # Cores per Worker config[&quot;sparklyr.shell.executor-cores&quot;] &lt;- 1 # Number of Workers config[&quot;sparklyr.shell.num-executors&quot;] &lt;- 3 When using YARN in Cluster mode, sparklyr.shell.driver-cores can be used to configure total cores requested in the driver node, the “Spark on YARN”(“Running Spark on Yarn” 2018) guide provides additional configuration settings worth familiarizing with. There are a few types of configuration settings: Connect settings are set as parameters to spark_connect(), they are common settings used while connecting. Submit settings are set while sparklyr is being submitted to Spark through spark-submit, some dependent on the cluster manager being used. Runtime settings configure Spark when the Spark session is created, these settings are independent to the cluster manager and specific to Spark. sparklyr settings configure sparklyr behaviour, these settings are independent to the cluster manager and particular to R. The following subsections present extensive lists of all the available settings, it is not required to fully understand them all while tuning Spark, but skimming through them could prove useful in the future while troubleshooting issues. You can also consider skipping the following settings subsections and use them instead as reference material as needed. 8.2.1 Connect Settings name value master Spark cluster url to connect to. Use “local” to connect to a local instance of Spark installed via spark_install(). spark_home The path to a Spark installation. Defaults to the path provided by the SPARK_HOME environment variable. If SPARK_HOME is defined, it will be always be used unless the version parameter is specified to force the use of a locally installed version. method The method used to connect to Spark. Default connection method is “shell” to connect using spark-submit, use “livy” to perform remote connections using HTTP, or “databricks” when using a Databricks clusters. app_name The application name to be used while running in the Spark cluster. version The version of Spark to use. Only applicable to “local” Spark connections. hadoop_version The version of Hadoop to use. Only applicable to “local” Spark connections. config Custom configuration for the generated Spark connection. See spark_config for details. 8.2.2 Submit Settings Some settings must be specified when spark-submit (the terminal application that launches Spark) is run. For instance, since spark-submit launches driver node which runs as a Java instance, choosing how much memory is allocated needs to be specified as a parameter to spark-submit. You can list all the available spark-submit parameters by running: spark_home_dir() %&gt;% file.path(&quot;bin&quot;, &quot;spark-submit&quot;) %&gt;% system2() For readability, we’ve provided the output of this command in table format, replacing the spark-submit parameter with the appropriate spark_config() setting and removing the parameters that are not applicable or already presented in this chapter: name value sparklyr.shell.jars Specified as ‘jars’ parameter in ‘spark_connect()’. sparklyr.shell.packages Comma-separated list of maven coordinates of jars to include on the driver and executor classpaths. Will search the local maven repo, then maven central and any additional remote repositories given by ‘sparklyr.shell.repositories’. The format for the coordinates should be groupId:artifactId:version. sparklyr.shell.exclude-packages Comma-separated list of groupId:artifactId, to exclude while resolving the dependencies provided in ‘sparklyr.shell.packages’ to avoid dependency conflicts. sparklyr.shell.repositories Comma-separated list of additional remote repositories to search for the maven coordinates given with ‘sparklyr.shell.packages’ sparklyr.shell.files Comma-separated list of files to be placed in the working directory of each executor. File paths of these files in executors can be accessed via SparkFiles.get(fileName). sparklyr.shell.conf Arbitrary Spark configuration property set as PROP=VALUE. sparklyr.shell.properties-file Path to a file from which to load extra properties. If not specified, this will look for conf/spark-defaults.conf. sparklyr.shell.driver-java-options Extra Java options to pass to the driver. sparklyr.shell.driver-library-path Extra library path entries to pass to the driver. sparklyr.shell.driver-class-path Extra class path entries to pass to the driver. Note that jars added with ‘sparklyr.shell.jars’ are automatically included in the classpath. sparklyr.shell.proxy-user User to impersonate when submitting the application. This argument does not work with ‘sparklyr.shell.principal’ / ‘sparklyr.shell.keytab’. sparklyr.shell.verbose Print additional debug output. The remaining ones are specific to YARN, name value sparklyr.shell.queue The YARN queue to submit to (Default: “default”). sparklyr.shell.archives Comma separated list of archives to be extracted into the working directory of each executor. sparklyr.shell.principal Principal to be used to login to KDC, while running on secure HDFS. sparklyr.shell.keytab The full path to the file that contains the keytab for the principal specified above. This keytab will be copied to the node running the Application Master via the Secure Distributed Cache, for renewing the login tickets and the delegation tokens periodically. In general, any spark-submit setting is configured through sparklyr.shell.X, where X is the name of the spark-submit parameter without the -- prefix. 8.2.3 Runtime Settings As mentioned, some Spark settings configure the session runtime. The runtime settings are a superset of the submit settings since is usually helpful to retrieve the current configuration even if a setting can’t be changed. To list the Spark settings set in your current Spark session, you can run: spark_session_config(sc) name value spark.master local[4] spark.sql.shuffle.partitions 4 spark.driver.port 62314 spark.submit.deployMode client spark.executor.id driver spark.jars /Library/…/sparklyr/java/sparklyr-2.3-2.11.jar spark.app.id local-1545518234395 spark.env.SPARK_LOCAL_IP 127.0.0.1 spark.sql.catalogImplementation hive spark.spark.port.maxRetries 128 spark.app.name sparklyr spark.home /Users/…/spark/spark-2.3.2-bin-hadoop2.7 spark.driver.host localhost However, there are many more configuration settings available in Spark as described in the “Spark Configuration”(“Spark Configuration” 2018) guide. It is not in the scope of this book to describe them all so, if possible, take some time to identify the ones that might be of interest to your paritcular use cases. 8.2.4 sparklyr Settings Apart from Spark settings, there are a few settings particular to sparklyr listed below. sparklyr.connect.cores is useful to set the CPU cores to use in local mode; the remaining ones are not used as much while tuning, but they can prove helpful while troubleshooting other issues. spark_config_settings() name description sparklyr.apply.packages Configures default value for packages parameter in spark_apply(). sparklyr.apply.rlang Experimental feature. Turns on improved serialization for spark_apply(). sparklyr.apply.schema.infer Number of rows collected to infer schema when column types specified in spark_apply(). sparklyr.arrow Use Apache Arrow to serialize data? sparklyr.backend.interval Total seconds sparklyr will check on a backend operation. sparklyr.backend.timeout Total seconds before sparklyr will give up waiting for a backend operation to complete. sparklyr.collect.batch Total rows to collect when using batch collection, defaults to 100,000. sparklyr.cancellable Cancel spark jobs when the R session is interrupted? sparklyr.connect.aftersubmit R function to call after spark-submit executes. sparklyr.connect.app.jar The path to the sparklyr jar used in spark_connect(). sparklyr.connect.cores.local Number of cores to use in spark_connect(master = “local”), defaults to parallel::detectCores(). sparklyr.connect.csv.embedded Regular expression to match against versions of Spark that require package extension to support CSVs. sparklyr.connect.csv.scala11 Use Scala 2.11 jars when using embedded CSV jars in Spark 1.6.X. sparklyr.connect.jars Additional JARs to include while submitting application to Spark. sparklyr.connect.master The cluster master as spark_connect() master parameter, notice that the ‘spark.master’ setting is usually preferred. sparklyr.connect.packages Spark packages to include when connecting to Spark. sparklyr.connect.ondisconnect R function to call after spark_disconnect(). sparklyr.connect.sparksubmit Command executed instead of spark-submit when connecting. sparklyr.connect.timeout Total seconds before giving up connecting to the sparklyr gateway while initializing. sparklyr.dplyr.period.splits Should ‘dplyr’ split column names into database and table? sparklyr.extensions.catalog Catalog PATH where extension JARs are located. Defaults to ‘TRUE’, ‘FALSE’ to disable. sparklyr.gateway.address The address of the driver machine. sparklyr.gateway.config.retries Number of retries to retrieve port and address from config, useful when using functions to query port or address in kubernetes. sparklyr.gateway.interval Total of seconds sparkyr will check on a gateway connection. sparklyr.gateway.port The port the sparklyr gateway uses in the driver machine. sparklyr.gateway.remote Should the sparklyr gateway allow remote connections? This is required in yarn cluster, etc. sparklyr.gateway.routing Should the sparklyr gateway service route to other sessions? Consider disabling in kubernetes. sparklyr.gateway.service Should the sparklyr gateway be run as a service without shutting down when the last connection disconnects? sparklyr.gateway.timeout Total seconds before giving up connecting to the sparklyr gateway after initialization. sparklyr.gateway.wait Total seconds to wait before retrying to contact the sparklyr gateway. sparklyr.livy.auth Authentication method for Livy connections. sparklyr.livy.headers Additional HTTP headers for Livy connections. sparklyr.livy.sources Should sparklyr sources be sourced when connecting? If false, manually register sparklyr jars. sparklyr.log.invoke Should every call to invoke() be printed in the console? Can be set to ‘callstack’ to log call stack. sparklyr.log.console Should driver logs be printed in the console? sparklyr.progress Should job progress be reported to RStudio? sparklyr.progress.interval Total of seconds to wait before attempting to retrieve job progress in Spark. sparklyr.sanitize.column.names Should partially unsupported column names be cleaned up? sparklyr.stream.collect.timeout Total seconds before stopping collecting a stream sample in sdf_collect_stream(). sparklyr.stream.validate.timeout Total seconds before stopping to check if stream has errors while being created. sparklyr.verbose Use verbose logging across all sparklyr operations? sparklyr.verbose.na Use verbose logging when dealing with NAs? sparklyr.verbose.sanitize Use verbose logging while sanitizing columns and other objects? sparklyr.web.spark The URL to Spark’s web interface. sparklyr.web.yarn The URL to YARN’s web interface. sparklyr.worker.gateway.address The address of the worker machine, most likely localhost. sparklyr.worker.gateway.port The port the sparklyr gateway uses in the driver machine. sparklyr.yarn.cluster.accepted.timeout Total seconds before giving up waiting for cluster resources in yarn cluster mode. sparklyr.yarn.cluster.hostaddress.timeout Total seconds before giving up waiting for the cluster to assign a host address in yarn cluster mode. sparklyr.yarn.cluster.lookup.byname Should the current user name be used to filter yarn cluster jobs while searching for submitted one? sparklyr.yarn.cluster.lookup.prefix Application name prefix used to filter yarn cluster jobs while searching for submitted one. sparklyr.yarn.cluster.lookup.username The user name used to filter yarn cluster jobs while searching for submitted one. sparklyr.yarn.cluster.start.timeout Total seconds before giving up waiting for yarn cluster application to get registered. 8.3 Partitioning As mentioned in the introduction chapter, MapReduce and Spark were designed with the purpose of performing computations against data stored across many machines, the subset of the data available for computation over each compute instance is known as a partition. By default, Spark will compute over each existing implicit partition since it’s more effective to run computations were the data is already located. However, there are cases where you will want to set an explicit partition to help Spark use more efficient use of your cluster resources. 8.3.1 Implicit There is always an implicit partition for each computation that Spark will assign on its own. For instance, how your data is distributed in storage will match the default partitions Spark assigns. You can get the number of partitions a computation will require through sdf_num_partitions(): sdf_len(sc, 10) %&gt;% sdf_num_partitions() [1] 2 While in most cases the default partitions works just fine, there are cases where we you will need to be explicit on the partitions you choose. 8.3.2 Explicit There will be times when you have many more compute instances than data partitions, or much less compute instances than the number of partitions in your data. In both cases, it can help to repartition data to match your cluster resources. Various data functions, like spark_read_csv(), already support a repartition parameter to request Spark to repartition data appropriately. For instance, we can create a sequence of 10 numbers partitioned by 10 as follows: sdf_len(sc, 10, repartition = 10) %&gt;% sdf_num_partitions() [1] 10 For datasets that are already partitioned, we can also use sdf_repartition: sdf_len(sc, 10, repartition = 10) %&gt;% sdf_repartition(4) %&gt;% sdf_num_partitions() [1] 4 The number of partitions usually changes significantly the speed and resources being used; for instance, the following example calculates the mean over 10M rows with different partition sizes. library(microbenchmark) library(ggplot2) microbenchmark( &quot;1 Partition(s)&quot; = sdf_len(sc, 10^7, repartition = 1) %&gt;% summarise(mean(id)) %&gt;% collect(), &quot;2 Partition(s)&quot; = sdf_len(sc, 10^7, repartition = 2) %&gt;% summarise(mean(id)) %&gt;% collect(), times = 10 ) %&gt;% autoplot() + theme_light() FIGURE 8.5: Computation speed when using explicit Spark partitions. The results show that sorting data with two partitions is almost twice as fast, this is the case since two CPUs can be used to execute this operation. However, it is not necessarily the case that higher-partitions produce faster computation; instead, partitioning data is particular to your computing cluster and the data analysis operations being performed. 8.4 Caching From the introduction chapter, we know that Spark was designed to be faster than its predecessors by using memory instead of disk to store data, this is formally known as an Spark RDD and stands for resilient distributed dataset. An RDD is resilient by duplicating copies of the same data across many machines, such that, if one machine fails other can complete the task. Resiliency is important in distributed systems since, while things will usually work in one machine, when running over thousands of machines the likelihood of something failing is much higher; when a failure happens, it is preferable be fault tolerant to avoid losing the work of all the other machines. RDDs are fault tolerant by tracking data lineage information to rebuild lost data automatically on failure. In sparklyr, you can control when an RDD gets loaded or unloaded from memory using tbl_cache() and tbl_uncache(). Most sparklyr operations that retrieve a Spark DataFrame, cache the results in-memory, for instance, running spark_read_parquet() or sdf_copy_to() will provide a Spark DataFrame that is already cached in-memory. As a Spark DataFrame, this object can be used in most sparklyr functions, including data analysis with dplyr or machine learning. library(sparklyr) sc &lt;- spark_connect(master = &quot;local&quot;) iris_tbl &lt;- sdf_copy_to(sc, iris, overwrite = TRUE) You can inspect which tables are cached by navigating to the Spark UI using spark_web(sc), opening the storage tab, and clicking on a given RDD: FIGURE 8.6: Cached RDD in Spark Web Interface. Data loaded in memory will be released when the R session terminates either explicitly or implicitly with a restart or disconnection; however, to free up resources, you can use tbl_uncache(): tbl_uncache(sc, &quot;iris&quot;) 8.4.1 Checkpointing Checkpointing is a slightly different type of caching, while it also persists data it will, additionally, break the graph computation lineage. So for instance, if a cached partition is lost, it can be computed from the computation graph which is not possible while checkpointing since the source of computation is lost. When performing expensive computation graphs, it can make sense to checkpoint to persist and break the computation lineage, this to help Spark reduce graph computation resources; otherwise, Spark might try to over-optimize a computation graph that is really not useful to optimize. You can checkpoint explicitly by saving to CSV, Parquet, etc. files. Or let Spark checkpoint this for you using sdf_checkpoint() in sparklyr as follows. Notice that checkpointing truncates the computation lineage graph which can speed up performance if the same intermediate result is used multiple times. 8.4.2 Memory Memory in Spark is categorized into: reserved, user, execution or storage: Reserved: Reserved memory is the memory required by Spark to function and therefore, is overhead that is required and should not be configured. This value defaults to 300MB. User: User memory is the memory used to execute custom code, sparklyr only makes use of this memory indirectly when executing dplyr expressions or modeling a dataset. Execution: Execution memory is used to execute code by Spark, mostly, to process the results from the partition and perform shuffling. Storage: Storage memory is used to cache RDDs, for instance, when using tbl_cache() in sparklyr. As part of tuning execution, you can consider tweaking the amount of memory allocated for user, execution and storage by creating a Spark connection with different values than the defaults provided in Spark: config &lt;- spark_config() # define memory available for storage and execution config$spark.memory.fraction &lt;- 0.75 # define memory available for storage config$spark.memory.storageFraction &lt;- 0.5 For instance, if you want to use Spark to store large amounts of data in-memory with the purpose of filtering and retrieving subsets quickly, you can expect Spark to use little execution or user memory; therefore, to maximize storage memory, one can tune Spark as follows: config &lt;- spark_config() # define memory available for storage and execution config$spark.memory.fraction &lt;- 0.90 # define memory available for storage config$spark.memory.storageFraction &lt;- 0.90 However, notice that Spark will borrow execution memory from storage and viceversa if needed and if possible; therefore, in practice, there should be little need to tune the memory settings. 8.5 Shuffling Shuffling, is the operation that redistributes data across machines, it is usually an expensive operation and therefore, one we try to minimize. One can easily identify is significant time is being spent shuffling by looking at the event timeline. It is possible to reduce shuffling by reframing data analysis questions or hinting Spark appropriately. For instance, when joining data frames that differ in size significantly, as in, one set being orders of magnitude smaller than the other one. You can consider using sdf_broadcast() to mark a data frame as small enough for use in broadcast joins, meaning, it pushes one of the smaller data frames to each of the worker nodes to reduce shuffling the bigger dataframe. One example for sdf_broadcast() follows: sdf_len(sc, 10000) %&gt;% sdf_broadcast() %&gt;% left_join(sdf_len(sc, 100)) 8.6 Serialization It is not that common to have to adjust serialization when tuning Spark; however, it is worth mentioning there are alternative serialization modules like the Kryo Serializer that can provide performance improvements over the default Java Serializer. The Kryo Serializer can be enabled in sparklyr through: config &lt;- spark_config() config$spark.serializer &lt;- &quot;org.apache.spark.serializer.KryoSerializer&quot; sc &lt;- spark_connect(master = &quot;local&quot;, config = config) 8.7 Configuration Files Configuring the spark_config() settings before connecting is the most common approach while tuning Spark. However, once the desired connection is known, you should consider switching to use a configuration file since it will remove the clutter in your connection code and also allow you to share the configuration settings across projects and coworkers. For instance, instead of connecting to Spark through: config &lt;- spark_config() config[&quot;sparklyr.shell.driver-memory&quot;] &lt;- &quot;2G&quot; sc &lt;- spark_connect(master = &quot;local&quot;, config = config) You can instead define a config.yml with the desired settings. This file should be located in the current working directory or in parent directories. For example, you can create the following config.yml file to modify the default driver memory: default: sparklyr.shell.driver-memory: 2G Then, connecting with the same configuration settings becomes much cleaner by using instead: sc &lt;- spark_connect(master = &quot;local&quot;) You can also specify an alternate config file name or location by setting the file parameter in spark_config(). One additional benefit from using configuration files, is that a system administrator can change the default configuration file without modifying the original one and additional functionality provided by the config package, see github.com/rstudio/config. 8.8 Recap This chapter provided a broad overview of Spark internals and a detailed configuration settings to help you speed up computation and enable high computation loads, it provided the foundations to understand bottlenecks and guidance on common configuration considerations; however, fine-tuning Spark is a broad topic that would require many more chapters to cover extensively. Therefore, while troubleshooting Spark’s performance and scalability, searching the web and consulting online communities it is often necessary to fine-tune your particular environment. The next chapter, Extensions, introduces the exciting world of sparklyr extensions. It’s exciting since you might find out that the extensions can be much more relevant to your particular interests and needs. For instance, they can process nested data, perform graph analysis or use different modeling libraries like rsparkling from H20. Not only that, but the next few chapters introduce many advanced data analysis and modeling topics that are required to master large-scale computing in R. References "],
["extensions.html", "Chapter 9 Extensions 9.1 RSparkling 9.2 GraphFrames 9.3 Mleap 9.4 Nested Data", " Chapter 9 Extensions While this chatper has not been written., a few resources are available to help explore these topics until this chapter gets written. 9.1 RSparkling rsparkling provies H2O support in Spark using sparklyr: library(rsparkling) library(sparklyr) library(h2o) cars_h2o &lt;- as_h2o_frame(sc, cars_tbl, strict_version_check = FALSE) h2o.glm(x = c(&quot;wt&quot;, &quot;cyl&quot;), y = &quot;mpg&quot;, training_frame = mtcars_h2o, lambda_search = TRUE) See spark.rstudio.com/guides/h2o. 9.1.1 Troubleshooting Apache IVY is a popular dependency manager focusing on flexibility and simplicity, which happens to be used by Apache Spark while installing extensions. When connection fails while using rsparkling, consider clearing your IVY Cache by running: unlink(&quot;~/.ivy2/cache&quot;, recursive = TRUE) 9.2 GraphFrames GraphFrames provides graph algorithms: PageRank, ShortestPaths, etc. gf_graphframe(vertices_tbl, edges_tbl) %&gt;% gf_pagerank(reset_prob = 0.15, max_iter = 10L) GraphFrame Vertices: $ id &lt;dbl&gt; 12, 12, 59, 59, 1, 20, 20, 45, 45, 8, 8, 9, 9, 26, 26, 37, 37, 47, 47, 16, 16, 71, 71, ... $ pagerank &lt;dbl&gt; 0.0058199702, 0.0058199702, 0.0000000000, 0.0000000000, 0.1500000000, 0.0344953402, 0.0... Edges: $ src &lt;dbl&gt; 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 13, 58, 58, 58, 58, 58, 58, 5... $ dst &lt;dbl&gt; 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 22, 65, 65, 65, 65, 65, 65, 6... $ weight &lt;dbl&gt; 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0.0625, 0... FIGURE 9.1: Highschool ggraph dataset with pagerank highlighted. See also spark.rstudio.com/graphframes. 9.3 Mleap Mleap enables Spark pipelines in production. # Create pipeline pipeline_model &lt;- ml_pipeline(sc) %&gt;% ft_binarizer(&quot;hp&quot;, &quot;big_hp&quot;, threshold = 100) %&gt;% ft_vector_assembler(c(&quot;big_hp&quot;, &quot;wt&quot;, &quot;qsec&quot;), &quot;features&quot;) %&gt;% ml_gbt_regressor(label_col = &quot;mpg&quot;) %&gt;% ml_fit(cars_tbl) # Perform predictions predictions_tbl &lt;- ml_predict(pipeline_model, mtcars_tbl) # Export model with mleap ml_write_bundle(pipeline_model, predictions_tbl, &quot;mtcars_model.zip&quot;) Use model outside Spark and productions systems. For instance, in Java: import ml.combust.mleap.runtime.MleapContext; // Initialize BundleBuilder bundleBuilder = new BundleBuilder(); MleapContext context = (new ContextBuilder()).createMleapContext(); Bundle&lt;Transformer&gt; bundle = bundleBuilder.load(new File(request.get(&quot;mtcars_model.zip&quot;)), context); // Read into Mleap DataFrame DefaultLeapFrame inputLeapFrame = new DefaultLeapFrame(); // Perform Mleap transformation DefaultLeapFrame transformedLeapFrame = bundle.root().transform(inputLeapFrame).get(); See also spark.rstudio.com/guides/mleap. 9.4 Nested Data library(sparklyr.nested) "],
["distributed.html", "Chapter 10 Distributed R 10.1 Overview 10.2 Use Cases 10.3 Partitions 10.4 Grouping 10.5 Columns 10.6 Context 10.7 Packages 10.8 Requirements 10.9 Limitations 10.10 Troubleshooting 10.11 Clusters 10.12 Apache Arrow 10.13 Recap", " Chapter 10 Distributed R In previous chapters you learned how to perform data analysis and modeling in local Spark instances and proper Spark clusters, the previous Extensions chapter, described how to use functionality provided not in extensions developed by Spark and R contributors. In most cases, the combination of Spark functionality and extensions is more than enough to perform almost any computation. However, for those few cases where functionality is lacking in Spark and their extensions, you can consider distributing R computations to worker nodes yourself and to leverage any R packages. If you are already a familiar R user, you might be tempted to use this approach for all Spark operations; however, this is not the recommended use of spark_apply(). Previous chapters provided more efficient techiniques and tools to solve well known problems, in contrast, spark_apply() introduces additional cognitive overhead, additional troubleshooting steps, performance trade-offs and, in general, additional complexity that should be avoided. Not to say that spark_apply() should never used; but instead, spark_apply() is reserved to support use cases where previous tools and techniques fall short. 10.1 Overview The Introduction chapter introduced MapReduce as a technique capable of processing large scale datasets; it also described how Apache Spark provided a superset of operations to perform MapReduce computations with ease and more efficiently. The Tuning chapter presented insights into how Spark works by applying custom transformation over each partition of the distributed datasets. For instance, if we were to multiply by ten each element of a distributed numeric dataset, Spark would apply a mapping operation over each partition through multiple workers, conceptually similar to: FIGURE 10.1: Conceptual mapping operation when multiplying by ten. This chapter presents how to define a custom f(x) mapping operations using spark_apply(); for the example above, spark_apply() provides support to define 10 * x as follows: sdf_len(sc, 3) %&gt;% spark_apply(~ 10 * .x) # Source: spark&lt;?&gt; [?? x 1] id * &lt;dbl&gt; 1 10 2 20 3 30 Notice that ~ 10 * .x is plain R code executed across all worker nodes; the ~ operator is defined in the rlang package and provides a compact definition of a function equivalent to function(.x) 10 * .x or what is also known as an anonymous function or lambda expression. Now, the first thing to notice is that f(x) takes an R data frame as input and must also produce an R data frame as output, conceptually this looks as follows: FIGURE 10.2: Expected function signature in spark_apply() mappings. We can use the orginal MapReduce example from the Introduction chapter where, the map operations was defined to split sentences into words and then, the total unique words were counted as the reduce operation. In R, we could make use of the unnest_tokens() function in the tidytext R package, combining the functionality of tidytext with spark_apply() would allow us to tokenize those sentences into a table of words as follows: sentences_tbl &lt;- copy_to(sc, data_frame(text = c(&quot;I like apples&quot;, &quot;I like bananas&quot;))) sentences_tbl %&gt;% spark_apply(~tidytext::unnest_tokens(.x, word, text)) # Source: spark&lt;?&gt; [?? x 1] word * &lt;chr&gt; 1 i 2 like 3 apples 4 i 5 like 6 bananas Finally, we can reduce this dataset using dplyr to compute this original MapReduce word-count example using dplyr as follows: sentences_tbl %&gt;% spark_apply(~tidytext::unnest_tokens(., word, text)) %&gt;% group_by(word) %&gt;% summarise(count = count()) # Source: spark&lt;?&gt; [?? x 2] word count * &lt;chr&gt; &lt;dbl&gt; 1 i 2 2 apples 1 3 like 2 4 bananas 1 The rest of this chapter will explain in detail use cases, features, caveats, considerations and troubleshooting techniques required when defining custom mappings through spark_apply() Note: The previous sentence tokenizer example can be more efficiently implemented using concept from previous chapters, specifically using sentences_tbl %&gt;% ft_tokenizer(\"text\", \"words\") %&gt;% transmute(word = explode(words)). 10.2 Use Cases The overview spark_apply() examples were meant to help you understand how it works; this section will cover a few practical use case for spark_apply(): Parsing: When a file format is not natevely supported in Spark or it’s extensions, you can consider using R code to implement a distributed custom parser using R packages. Modeling: When data fits into a single machine you can make use of grid search to optimize their parameters in parallel, in cases where the data can be partitioned to create several models over subsets of the data you can use partitioned modeling in R to compute models across partitions. Interoperating: When large data needs to be evaluated by external systems you can use R to interoperate with those external systems. For instance, to process a large image dataset using deep learning models, you can consider calling an specialized web API like Google’s Vision API, Amazon’s Rekognition API, etc. 10.2.1 Custom Parsers While Spark and its various extensions provide support for many file formats: CSVs, JSON, Parquet, AVRO, etc. there are many more file formats that one might want to use at scale through spark_apply() and the right set of R packages available in CRAN. This section presents two use cases for parsing log and WARC files. 10.2.1.1 Log Parser It is common to use Spark to analize log files, say for instance, logs that track download data from Amazon S3. To parse logs, the webreadr package can simplify this process by providing support to load logs stored as: Amzon S3, Squid and Common format. For instance, an S3 log looks as follows: #Version: 1.0 #Fields: date time x-edge-location sc-bytes c-ip cs-method cs(Host) cs-uri-stem sc-status cs(Referer) cs(User-Agent) cs-uri-query cs(Cookie) x-edge-result-type x-edge-request-id x-host-header cs-protocol cs-bytes time-taken 2014-05-23 01:13:11 FRA2 182 192.0.2.10 GET d111111abcdef8.cloudfront.net /view/my/file.html 200 www.displaymyfiles.com Mozilla/4.0%20(compatible;%20MSIE%205.0b1;%20Mac_PowerPC) - zip=98101 RefreshHit MRVMF7KydIvxMWfJIglgwHQwZsbG2IhRJ07sn9AkKUFSHS9EXAMPLE== d111111abcdef8.cloudfront.net http - 0.001 Which can be parsed easily with read_aws() as follows: aws_log &lt;- system.file(&quot;extdata/log.aws&quot;, package = &quot;webreadr&quot;) webreadr::read_aws(aws_log) # A tibble: 2 x 18 date edge_location bytes_sent ip_address http_method host path &lt;dttm&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 2014-05-23 01:13:11 FRA2 182 192.0.2.10 GET d111… /vie… 2 2014-05-23 01:13:12 LAX1 2390282 192.0.2.2… GET d111… /sou… # ... with 11 more variables: status_code &lt;int&gt;, referer &lt;chr&gt;, user_agent &lt;chr&gt;, # query &lt;chr&gt;, cookie &lt;chr&gt;, result_type &lt;chr&gt;, request_id &lt;chr&gt;, host_header &lt;chr&gt;, # protocol &lt;chr&gt;, bytes_received &lt;chr&gt;, time_elapsed &lt;dbl&gt; We can make use of read_aws() as follows in Spark with spark_apply(): spark_read_text(sc, &quot;logs&quot;, aws_log, overwrite = TRUE, whole = TRUE) %&gt;% spark_apply(~webreadr::read_aws(.x$contents)) # Source: spark&lt;?&gt; [?? x 18] date edge_location bytes_sent ip_address http_method host path * &lt;dttm&gt; &lt;chr&gt; &lt;int&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; &lt;chr&gt; 1 2014-05-23 01:13:11 FRA2 182 192.0.2.10 GET d111… /vie… 2 2014-05-23 01:13:12 LAX1 2390282 192.0.2.2… GET d111… /sou… # ... with 11 more variables: status_code &lt;int&gt;, referer &lt;chr&gt;, user_agent &lt;chr&gt;, # query &lt;chr&gt;, cookie &lt;chr&gt;, result_type &lt;chr&gt;, request_id &lt;chr&gt;, # host_header &lt;chr&gt;, protocol &lt;chr&gt;, bytes_received &lt;chr&gt;, time_elapsed &lt;dbl&gt; 10.2.1.2 WARC Parser by parsing WARC (Web ARChive) file from the Common Crawl project. The Common Crawl project builds and maintains an open repository of web crawl data that can be accessed and analyzed by anyone. You can use their petabytes of data to analyzing the contents of the web without having to manually download each web page, which is a much more time consuming process. The challenge with WARC files is that multiple records are embedded within the same text file, conceptually, a WARC file looks as follows: WARC/1.0 &lt;html&gt;...&lt;/html&gt; WARC/1.0 &lt;html&gt;...&lt;/html&gt; We could read all those lines using spark_read_text(); however, they wouldn’t be grouped by web page response which is required to analyze properly the contents. Using the warc package, we can easily parse these files in R running, warc_example &lt;- system.file(&quot;samples/sample.warc.gz&quot;, package = &quot;warc&quot;) as_tibble(warc::read_warc(warc_example)) # A tibble: 17 x 1 content &lt;fct&gt; 1 &quot;WARC/1.0\\nWARC-Type: warcinfo\\nWARC-Date: 2016-12-13T03:16:04Z\\nWARC-Record-ID: &lt;ur… 2 &quot;WARC/1.0\\nWARC-Type: request\\nWARC-Date: 2016-12-11T14:00:57Z\\nWARC-Record-ID: &lt;urn… 3 &quot;WARC/1.0\\nWARC-Type: response\\nWARC-Date: 2016-12-11T14:00:57Z\\nWARC-Record-ID: &lt;ur… 4 &quot;WARC/1.0\\nWARC-Type: metadata\\nWARC-Date: 2016-12-11T14:00:57Z\\nWARC-Record-ID: &lt;ur… 5 &quot;WARC/1.0\\nWARC-Type: request\\nWARC-Date: 2016-12-11T14:08:53Z\\nWARC-Record-ID: &lt;urn… The warc package also allows you to easily extract specific lines efficiently to, for instance, extract the language of each web page: warc::read_warc(warc_example, line_filter = &quot;&lt;meta http-equiv=\\&quot;Content-Language\\&quot;&quot;) %&gt;% transmute(language = gsub(&quot;.*content=\\&quot;|\\&quot;.*&quot;, &quot;&quot;, content)) %&gt;% group_by(language) %&gt;% summarise(count = n()) # A tibble: 1 x 2 language count &lt;chr&gt; &lt;int&gt; 1 ru-RU 5 Now, in order to use this in Spark we can simply run: paths &lt;- readLines(system.file(&quot;samples/warc.paths&quot;, package = &quot;warc&quot;)) paths_tbl &lt;- copy_to(sc, paths, repartition = length(paths)) paths_tbl %&gt;% head() %&gt;% spark_apply(function(entry) { path &lt;- sub(&quot;s3n://commoncrawl/&quot;, &quot;https://commoncrawl.s3.amazonaws.com/&quot;, entry[1,]) download.file(url = path, destfile = temp_warc) warc::read_warc(path) }) You can remove head() to process a quite large dataset of WARC files, 10.2.2 Partitioned Modeling There are many modeling packages available in R that can also be run at scale by partitioning the data into manegable groups that do fit in the resources of a single machine. For instance, suppose that you have a 1TB dataset for sales data across multiple cities and you are tasked with creating sales predictions oever each city. Yopu can then considered partitioning the original dataset per city say, into 10GB of data per city, which can be managed by a single compute instance. For this kind of partitionable model task, you can also consider using spark_apply() by training each model over each city in parallel with access to all the packages available in R. For instance, we could run a linear regression over the iris dataset over each Species as follows: iris_tbl &lt;- sdf_copy_to(sc, iris) iris_tbl %&gt;% spark_apply(nrow, group_by = &quot;Species&quot;) ## # Source: table&lt;sparklyr_tmp_378c1b8155f3&gt; [?? x 2] ## # Database: spark_connection ## Species Sepal_Length ## &lt;chr&gt; &lt;int&gt; ## 1 versicolor 50 ## 2 virginica 50 ## 3 setosa 50 iris_tbl %&gt;% spark_apply( function(e) summary(lm(Petal_Length ~ Petal_Width, e))$r.squared, names = &quot;r.squared&quot;, group_by = &quot;Species&quot;) ## # Source: table&lt;sparklyr_tmp_378c30e6155&gt; [?? x 2] ## # Database: spark_connection ## Species r.squared ## &lt;chr&gt; &lt;dbl&gt; ## 1 versicolor 0.6188467 ## 2 virginica 0.1037537 ## 3 setosa 0.1099785 These covers some of the common use cases for spark_apply(), but you are certainly welcome to find others that may fit your particular needs. 10.2.3 Grid Search Many R packages provide models that make require defining multiple input parameters, when the value of the input parameters is not known, we can use grid search across a distributed cluster of machines to find the optimal parameter combination. For example, we can define a grid of parameters to optimize decision tree models as follows: grid &lt;- list(minsplit = c(2, 5, 10), maxdepth = c(1, 3, 8)) %&gt;% purrr:::cross_df() %&gt;% sdf_copy_to(sc, ., repartition = 9) grid # Source: spark&lt;sparklyr_1064056dd37da&gt; [?? x 2] minsplit maxdepth &lt;dbl&gt; &lt;dbl&gt; 1 2 1 2 5 1 3 10 1 4 2 3 5 5 3 6 10 3 7 2 8 8 5 8 9 10 8 The grid dataset was copied with repartition = 9 to ensure that each partition is contained in one machine since the grid has also nine rows. Now, assuming that the original data set fits in every machine, like mtcars, we can distribute this dataset to many machines and perform parameter search to find the model that best fits this data. spark_apply( grid, function(grid, cars) { model &lt;- rpart::rpart( am ~ hp + mpg, data = cars, control = rpart::rpart.control(minsplit = grid$minsplit, maxdepth = grid$maxdepth) ) dplyr::mutate( grid, accuracy = mean(round(predict(model, dplyr::select(cars, -am))) == cars$am) ) }, context = mtcars) # Source: spark&lt;?&gt; [?? x 3] minsplit maxdepth accuracy &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; 1 2 1 0.812 2 5 1 0.812 3 10 1 0.812 4 2 3 0.938 5 5 3 0.938 6 10 3 0.812 7 2 8 1 8 5 8 0.938 9 10 8 0.812 For this particular model, minsplit = 2 and maxdepth = 8 produces the most accurate results; now you can use these parameters to properly train models. 10.2.4 Web APIs The following example makes use of the googleAuthR package to authenticate to Google Cloud, the RoogleVision package to perform labeling over the Google Vision API, and spark_apply() to interoperate between Spark and this deep learning service by calling thi API a distributed dataset. If you want to run the following example you will need to disconnect first from Spark and download your cloudml.json from the Google developer portal. sc &lt;- spark_connect( master = &quot;local&quot;, config = list(sparklyr.shell.files = &quot;cloudml.json&quot;)) images &lt;- copy_to(sc, data.frame( image = &quot;http://pbs.twimg.com/media/DwzcM88XgAINkg-.jpg&quot; )) spark_apply(images, function(df) { googleAuthR::gar_auth_service( scope = &quot;https://www.googleapis.com/auth/cloud-platform&quot;, json_file = &quot;cloudml.json&quot;) RoogleVision::getGoogleVisionResponse( df$image, download = FALSE) }) # Source: spark&lt;?&gt; [?? x 4] mid description score topicality &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; 1 /m/04rky Mammal 0.973 0.973 2 /m/0bt9lr Dog 0.958 0.958 3 /m/01z5f Canidae 0.956 0.956 4 /m/0kpmf Dog breed 0.909 0.909 5 /m/05mqq3 Snout 0.891 0.891 In order to successfully run a large distirbuted computation over a Web API, the Web API would have to be able to scale to support the load from all the Spark executors. One can trust that major service providers are likely to support all the request incoming from your cluster. However, when calling internal Web APIs, make sure the API can handle the load. Also, when using third-party services, consider the cost of calling their API across all the executors in your cluster to avoid potentially expensive and unexpected charges. 10.3 Partitions As mentioned in the Tuning chapter, Spark partitions data and operates across partitions; therefore, spark_apply() receives a data frame partition; for instance, consider the following code, should we expect the output to be the total number of rows since nrow() counts the total rows in a data frame? sdf_len(sc, 10) %&gt;% spark_apply(~nrow(.x)) # Source: spark&lt;?&gt; [?? x 1] result * &lt;int&gt; 1 5 2 5 In general the answer is no, we should not expect spark_apply() to operate over a single partition, from the example above, we can find that sdf_len(sc, 10) contains two partitions by running: sdf_len(sc, 10) %&gt;% sdf_num_partitions() [1] 2 Which explains why counting rows through nrow() under spark_apply() retrieves two counts of five rows; instead of ten rows as the total. For this particular example, we could further aggregate these partitions by repartitioning and then adding up: sdf_len(sc, 10) %&gt;% spark_apply(~nrow(.x)) %&gt;% sdf_repartition(1) %&gt;% spark_apply(~sum(.x)) # Source: spark&lt;?&gt; [?? x 1] result * &lt;int&gt; 1 10 It was the intent of this section is to make the reader aware of partitions while using spark_apply(), the next section presents group_by as a way to control partitions. Without using grouping, spark_apply() is better use for mapping datasets row-by-row. 10.4 Grouping When using spark_apply(), we can request explicit partitions to work against. For instance, if we wanted to process numbers all numbers less than or equal than three in one partition and the remaining ones in a second partition, we could create this group as follows: sdf_len(sc, 10) %&gt;% transmute(groups = id &lt;= 3) %&gt;% spark_apply(~nrow(.x), group_by = &quot;groups&quot;) # Source: spark&lt;?&gt; [?? x 2] groups result * &lt;lgl&gt; &lt;int&gt; 1 TRUE 3 2 FALSE 7 As mentioned under “Use Cases”, we can use group_by to partition data into manageable datasets that can be used for modeling, one partition at a time. 10.5 Columns By default, spark_apply(), will inspect the data frame being produced to find out column names and types automatically, for example: sdf_len(sc, 1) %&gt;% spark_apply(~ data.frame(numbers = 1, names = &quot;abc&quot;)) # Source: spark&lt;?&gt; [?? x 2] numbers names * &lt;dbl&gt; &lt;chr&gt; 1 1 abc However, this is inneficient since spark_apply() needs to run more than once, first to find columns and then to compute the actual desired values. Instead, the columns can be specified explicitly through the columns parameters by defining each name and type from the resulting data frame, for the example above: sdf_len(sc, 1) %&gt;% spark_apply( ~ data.frame(numbers = 1, names = &quot;abc&quot;), columns = list(numbers = &quot;double&quot;, names = &quot;character&quot;)) # Source: spark&lt;?&gt; [?? x 2] numbers names * &lt;dbl&gt; &lt;chr&gt; 1 1 abc 10.6 Context sdf_len(sc, 3, repartition = 3) %&gt;% spark_apply(function(data, context) context, context = data.frame(something = c(&quot;foo&quot;, &quot;bar&quot;))) # Source: spark&lt;?&gt; [?? x 1] something * &lt;chr&gt; 1 foo 2 bar 3 foo 4 bar 5 foo 6 bar sdf_len(sc, 3, repartition = 3) %&gt;% spark_apply( function(data, context) context$numbers * context$constant, context = list( numbers = c(2, 3, 5), constant = 10 ) ) # Source: spark&lt;?&gt; [?? x 1] result * &lt;dbl&gt; 1 20 2 30 3 50 4 20 5 30 6 50 7 20 8 30 9 50 10.7 Packages With spark_apply() you can use any R package inside Spark. For instance, you can use the broom package to create a tidy data frame from linear regression output. spark_apply( iris_tbl, function(e) broom::tidy(lm(Petal_Length ~ Petal_Width, e)), names = c(&quot;term&quot;, &quot;estimate&quot;, &quot;std.error&quot;, &quot;statistic&quot;, &quot;p.value&quot;), group_by = &quot;Species&quot;) ## # Source: table&lt;sparklyr_tmp_378c5502500b&gt; [?? x 6] ## # Database: spark_connection ## Species term estimate std.error statistic p.value ## &lt;chr&gt; &lt;chr&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; ## 1 versicolor (Intercept) 1.7812754 0.2838234 6.276000 9.484134e-08 ## 2 versicolor Petal_Width 1.8693247 0.2117495 8.827999 1.271916e-11 ## 3 virginica (Intercept) 4.2406526 0.5612870 7.555230 1.041600e-09 ## 4 virginica Petal_Width 0.6472593 0.2745804 2.357267 2.253577e-02 ## 5 setosa (Intercept) 1.3275634 0.0599594 22.141037 7.676120e-27 ## 6 setosa Petal_Width 0.5464903 0.2243924 2.435422 1.863892e-02 The first time you call spark_apply() all of the contents in your local .libPaths(), which contains all R packages, will be copied into each Spark worker node. Packages will only be copied once and will persist as long as the connection remains open. It’s not uncommon for R libraries to be several gigabytes in size, so be prepared for a one-time tax while the R packages are copied over to your Spark cluster. You can disable package distribution by setting packages = FALSE. Note: R packages are not copied in local mode (master=“local”) because the packages already exist on the local system. 10.8 Requirements The R Runtime is expected to be pre-installed in the cluster for spark_apply to function. Failure to install the cluster will trigger a Cannot run program, no such file or directory error while attempting to use spark_apply(). Contact your cluster administrator to consider making the R runtime available throughout the entire cluster. If R is already installed, you can specify the installation path to use using the spark.r.command configuration setting, as in: config &lt;- spark_config() config[&quot;spark.r.command&quot;] &lt;- &quot;&lt;path-to-r-version&gt;&quot; sc &lt;- spark_connect(master = &quot;local&quot;, config = config) sdf_len(sc, 10) %&gt;% spark_apply(function(e) e) A Homogeneous Cluster is required since the driver node distributes, and potentially compiles, packages to the workers. For instance, the driver and workers must have the same processor architecture, system libraries, etc. 10.9 Limitations 10.9.1 Functions The function passed to spark_apply() are serialized using serialize(), which is described as “a simple low-level interface for serializing to connections.”. One of the current limitations of serialize is that it wont serialize objects being referenced outside of it’s environment. For instance, the following function will error out since the closures references external_value: external_value &lt;- 1 spark_apply(iris_tbl, function(e) e + external_value) 10.9.2 Livy Currently, Livy connections do not support distributing packages since the client machine where the libraries are precompiled might not have the same processor architecture, nor operating systems that the cluster machines. 10.9.3 Grouping While performing computations over groups, spark_apply() will provide partitions over the selected column; however, this implies that each partition can fit into a worker node, if this is not the case an exception will be thrown. To perform operations over groups that exceed the resources of a single node, one can consider partitioning to smaller units or use dplyr::do which is currently optimized for large partitions. 10.9.4 Packages Since packages are copied only once for the duration of the spark_connect() connection, installing additional packages is not supported while the connection is active. Therefore, if a new package needs to be installed, spark_disconnect() the connection, modify packages and reconnect. 10.10 Troubleshooting When an error is detected, spark_apply() automatically reruns an retrieves errors from worker nodes to be easily presented to you; consider the following example: sdf_len(sc, 1) %&gt;% spark_apply(function(df) { stop(&quot;an error&quot;) }) There are a few other more advanced troubleshooting techniquest applicable to spark_apply(), the following sections present these techniques in-order; meaning, we should try to troubleshoot using worker logs first, followed by identifying partitioning errors and finally, attempting to debug a worker node. 10.10.1 Worker Logs Whenever spark_apply() is executed, information regarding execution is written over each worker node. You can use this log to write custom messages o help you diagnose and fine-tune your code. For instance, suppose that you don’t know what the first column name of df is, we can write a custom log message executed from the worker nodes using worker_log() as follows: sdf_len(sc, 1) %&gt;% spark_apply(function(df) { worker_log(&quot;the first column in the data frame is named &quot;, names(df)[[1]]) df }) # Source: spark&lt;?&gt; [?? x 1] id * &lt;int&gt; 1 1 When running locally, we can filter the log entries for the worker as follows: spark_log(sc, filter = &quot;sparklyr: RScript&quot;) 18/12/18 11:33:47 INFO sparklyr: RScript (3513) the first column in the dataframe is named id 18/12/18 11:33:47 INFO sparklyr: RScript (3513) computed closure 18/12/18 11:33:47 INFO sparklyr: RScript (3513) updating 1 rows 18/12/18 11:33:47 INFO sparklyr: RScript (3513) updated 1 rows 18/12/18 11:33:47 INFO sparklyr: RScript (3513) finished apply 18/12/18 11:33:47 INFO sparklyr: RScript (3513) finished Notice that the logs show out custom log entry showing that id is the name of the first column in the given data frame. This functionality is useful when troubleshooting errors, for instance, if we force an error using the stop() function: sdf_len(sc, 1) %&gt;% spark_apply(function(df) { stop(&quot;force an error&quot;) }) You will get an error similar to, Error in force(code) : sparklyr worker rscript failure, check worker logs for details As suggested in the error, we can look in the worker logs for the specific errors as follows: spark_log(sc) This will show an entry containing the error and the callstack: 18/12/18 11:26:47 INFO sparklyr: RScript (1860) computing closure 18/12/18 11:26:47 ERROR sparklyr: RScript (1860) terminated unexpectedly: force an error 18/12/18 11:26:47 ERROR sparklyr: RScript (1860) collected callstack: 11: stop(&quot;force and error&quot;) 10: (function (df) { stop(&quot;force and error&quot;) })(structure(list(id = 1L), class = &quot;data.frame&quot;, row.names = c(NA, -1L))) Notice that, spark_log(sc) only retrieves the worker logs when using local clusters, when running in proper clusters with multiple machines, you will have to use the tools and user interface provided by the cluster manager to find these log entries. 10.10.2 Partition Errors If a particular partition fails, you can detect the broken partition by computing a digest, and then retrieving that particular partition as follows: sdf_len(sc, 3) %&gt;% spark_apply(function(x) { worker_log(&quot;processing &quot;, digest::digest(x), &quot; partition&quot;) # your code }) This will add an entry similar to: 18/11/03 14:48:32 INFO sparklyr: RScript (2566) processing f35b1c321df0162e3f914adfb70b5416 partition When executing this in your cluster, you will have to look in the logs for the task that is not finishing, once you have that digest, you can cancel the job. Then you can use that digest to retrieve that specific data frame to R with something like: broken_partition &lt;- sdf_len(sc, 3) %&gt;% spark_apply(function(x) { if (identical(digest::digest(x), &quot;f35b1c321df0162e3f914adfb70b5416&quot;)) x else x[0,] }) %&gt;% collect() Which you can then run in R to troubleshoot further. 10.10.3 Debugging Workers sdf_len(sc, 1) %&gt;% spark_apply(function() { stop(&quot;Error!&quot;) }, debug = TRUE) Debugging spark_apply(), connect to worker debugging session as follows: 1. Find the workers &lt;sessionid&gt; and &lt;port&gt; in the worker logs, from RStudio click &#39;Log&#39; under the connection, look for the last entry with contents: &#39;Session (&lt;sessionid&gt;) is waiting for sparklyr client to connect to port &lt;port&gt;&#39; 2. From a new R session run: debugonce(sparklyr:::spark_worker_main) sparklyr:::spark_worker_main(&lt;sessionid&gt;, &lt;port&gt;) 10.11 Clusters When using spark_apply(), R needs to be properly installed in each worker node. Different cluster managers, distributions and services, proivide different solutions to install additional software; those instructions should be followed when installing R over each worker node. To mention a few, Spark Standalone: Requires connecting to each machine and installing R; there are tools like pssh that allow you to run a single installation command against multiple machines. Cloudera: Provides an R parcel, see “How to Distribute your R code with sparklyr and Cloudera Data Science Workbench”(???), which enables R over each worker node. Amazon EMR: R is pre-installed when starting an EMR cluster as mentioned in the Amazon EMR section. Microsoft HDInsight: R is pre-installed and no additional steps are needed. 10.12 Apache Arrow Apache Arrow is strongly adviced while working spark_apply(), it’s available since Spark 2.3.0 and requires system administrators to install the Apache Arrow runtime in every node. For instance, in Linux (Ubuntu), you will need to install this in your cluster as follows: sudo yum install -y https://packages.red-data-tools.org/centos/red-data-tools-release-latest.noarch.rpm sudo sed -i &#39;s/\\$releasever/6/g&#39; /etc/yum.repos.d/red-data-tools.repo sudo yum install -y --enablerepo=red-data-tools arrow-devel To use Apache Arrow with sparklyr you need to install the arrow package: devtools::install_github(&quot;apache/arrow&quot;, subdir = &quot;r&quot;, ref = &quot;apache-arrow-0.12.0&quot;) Once installed, to enable Arrow simply include the library: library(arrow) You can validate that significant performance improvements become available by measuring time of spark_apply() operations. system.time(sdf_len(sc, 10^5) %&gt;% spark_apply(nrow)) Then you can dettach arrow and rerun this task. detach(&quot;package:arrow&quot;, unload = TRUE) system.time(sdf_len(sc, 10^5) %&gt;% spark_apply(nrow)) Notice the performance degradation when not using arrow, to enable it again, run library(arrow). Most functionality in arrow simply works on the background improving performance and data serialization; however, there is one setting you should be aware of. The spark.sql.execution.arrow.maxRecordsPerBatch configuiration settings specified the default size of each arrow data transfer. It’s shared with other Spark components and defaults to 10,000 rows. Compare the results from the following two operation when using and not using arrow(). detach(&quot;package:arrow&quot;, unload = TRUE) sdf_len(sc, 10^6) %&gt;% spark_apply(nrow) Now using arrow, library(arrow) sdf_len(sc, 10^6) %&gt;% spark_apply(nrow) You might need to tweak this number based on how much data your system can handle, making it smaller for large dataset or bigger for operations that require records to be processed together. You can specify an unlimitted batch size by setting spark.sql.execution.arrow.maxRecordsPerBatch to zero. spark_disconnect(sc) config &lt;- spark_config() config$spark.sql.execution.arrow.maxRecordsPerBatch &lt;- 0 sc &lt;- spark_connect(master = &quot;local&quot;) sdf_len(sc, 10^6) %&gt;% spark_apply(nrow) 10.13 Recap This chapter presented spark_apply() as an advanced technique to be used to fill gaps in functionality in Spark or its many extensions. It presented use cases to map data frames with R packages and group data into manaegable partitions. It detailed how partitions relate to spark_apply(), how to craete custom groups distribute contextual information across all nodes, troubleshoot problems and presented limitations, cluster configuration caveats and improved performance optimization using Apache Arrow. "],
["streaming.html", "Chapter 11 Streaming 11.1 Overview 11.2 Transformations 11.3 Shiny 11.4 Formats", " Chapter 11 Streaming 11.1 Overview One can understand a stream as an unbounded data frame, meaning, a data frame with finite columns but infinite rows. Streams are most relevant when processing real time data; for example, when analyzing a Twitter feed or stock prices. Both examples have well defined columns, like ‘tweet’ or ‘price’, but there are always new rows of data to be analyzed. Spark provided initial support for streams with Spark’s DStreams; however, a more versatile and efficient replacement is available through Spark structured streams. Structured streams provide scalable and fault-torerant data processing over streams of data. That means, one can use many machines to process multiple streaming sources, perform joins with other streams or static sources, and recover from failures with at-least-once guarantees (where each message is certain to be delivered, but may do so multiple times). In order to use structured streams in sparklyr, one needs to define the sources, transformations and a destination: The sources are defined using any of the stream_read_*() functions to read streams of data from various data sources. The transformations can be specified using dplyr, SQL, scoring pipelines or R code through spark_apply(). The destination is defined with the stream_write_*() functions, it often also referenced as a sink. Since the transformation step is optional, the simplest stream we can define is to continuously process files, which would effectively copy text files between source and destination. We can define this copy-stream in sparklyr as follows: library(sparklyr) sc &lt;- spark_connect(master = &quot;local&quot;) stream &lt;- stream_read_text(sc, &quot;source/&quot;) %&gt;% stream_write_text(&quot;destination/&quot;) The streams starts running with stream_write_*(); once executed, the stream will monitor the source path and process data into the destination/ path as it arrives. We can use view_stream() to track the rows per second (rps) being processed in the source, destination and their latest values over time: stream_view(stream) FIGURE 11.1: Viewing a Spark Stream with sparklyr Notice that the rows-per-second in the destination stream are higher than the rows-per-second in the source stream; this is expected and desireable since Spark measures incoming rates from the source, but actual row processing times in the destination stream. For example, if 10 rows-per-second are written to the source/ path, the incoming rate is 10 RPS. However, if it takes Spark only 0.01 seconds to write all those 10 rows, the output rate is 100 RPS. Use stream_stop() to properly stop processing data from this stream: stream_stop(stream) In order to reproduce the above example, one needs to feed streaming data into the source/ path. This was accomplished by running stream_generate_test() to produce a file every second containing lines of text that follow overlapping binomial distributions. In practice, you would connect to existing sources without having to generate data artificially. See ?stream_generate_test for additional details and make sure the later package is installed. stream_generate_test(paste(&quot;Row&quot;, 1:1000), &quot;source/&quot;) For the subsequent examples, a stream with one hundred rows of text will be used: writeLines(paste(&quot;Row&quot;, 1:100), &quot;source/rows.txt&quot;) 11.2 Transformations Streams can be transformed using dplyr, SQL, pipelines or R code. We can use as many transformations as needed in the same way that Spark data frames can be transformed with sparklyr. The transformation source can be streams or data frames but the output is always a stream. If needed, one can always take a snapshot from the destination stream and save the output as a data frame, which is what sparklyr will do for you if a destination stream is not specified. Conceptually, this looks as follows: 11.2.1 dplyr Using dplyr, we can process each row of the stream; for example, we can filter the stream to only the rows containing a number one: library(dplyr, warn.conflicts = FALSE) stream_read_text(sc, &quot;source/&quot;) %&gt;% filter(line %like% &quot;%1%&quot;) Since the destination was not specified, sparklyr creates a temporary memory stream and previews the contents of a stream by capturing a few seconds of streaming data. We can also aggregate data with dplyr, stream_read_text(sc, &quot;source/&quot;) %&gt;% summarise(n = n()) and even join across many concurrent streams: left_join( stream_read_text(sc, &quot;source/&quot;) %&gt;% stream_watermark(), stream_read_text(sc, &quot;source/&quot;) %&gt;% stream_watermark() %&gt;% mutate(random = rand()), ) However, some operations, require watermarks to define when to stop waiting for late data. You can specify watermarks in sparklyr using stream_watermak(), see also handling late data in Spark’s documentation. 11.2.2 Pipelines Spark pipelines can be used for scoring streams, but not to train over streaming data. The former is fully supported while the latter is a feature under active development by the Spark community. To use a pipeline for scoring a stream, first train a Spark pipeline over a static dataset. Once trained, save the pipeline, then reload and score over a stream as follows: fitted_pipeline &lt;- ml_load(sc, &quot;iris-fitted/&quot;) stream_read_csv(sc, &quot;iris-in&quot;) %&gt;% sdf_transform(fitted_pipeline) %&gt;% stream_write_csv(&quot;iris-out&quot;) 11.2.3 R Code Arbitrary R code can also be used to transform a stream with the use of spark_apply(). Following the same principles from executing R code over Spark data frames, for structured streams, spark_apply() runs R code over each executor in the cluster where data is available, this enables processing high-throughput streams and fullfill low-latency requirements. The following example splits a stream of Row # line entries and adds jitter using R code: stream_read_text(sc, &quot;source/&quot;) %&gt;% spark_apply(~ jitter(as.numeric(gsub(&quot;Row &quot;, &quot;&quot;, .x$text)))) 11.3 Shiny Streams can be used with Shiny by making use of the reactiveSpark() to retrieve the stream as a reactive data source. Internally, reactiveSpark() makes use of reactivePoll() to check the stream’s timestamp and collect the stream contents when needed. The following Shiny application makes use of reactiveSpark() to view a Spark stream summarized with dplyr: library(shiny) library(sparklyr) library(dplyr) sc &lt;- spark_connect(master = &quot;local&quot;) ui &lt;- fluidPage( sidebarLayout( mainPanel( tableOutput(&quot;table&quot;) ) ) ) server &lt;- function(input, output, session) { pollData &lt;- stream_read_text(sc, &quot;source/&quot;) %&gt;% summarise(n = n()) %&gt;% reactiveSpark(session = session) output$table &lt;- renderTable({ pollData() }) } shinyApp(ui = ui, server = server) 11.4 Formats The following formats are available to read and write streaming data: Format Read Write CSV stream_read_csv stream_write_csv JSON stream_read_json stream_write_json Kafka stream_read_kafka stream_write_kafka ORC stream_read_orc stream_write_orc Parquet stream_read_parquet stream_write_parquet Text stream_read_text stream_write_text Memory stream_write_memory "],
["contributing.html", "Chapter 12 Contributing 12.1 Overview 12.2 R Extensions 12.3 Scala Extensions 12.4 Spark Extensions 12.5 R Packages 12.6 sparklyr 12.7 Recap", " Chapter 12 Contributing There are many ways to contribute, from helping community members to opening GitHub issues, to providing new functionality for yourself, colleagues or the R and Spark community; this last chapter will focus on writting and sharing code that extends Spark in many useful and probably also, awesome, ways. Specifically, in this chapter you will learn what an extension is, when to build one, what tools are available, how to build an extension and when to consider contributing to sparklyr itself. 12.1 Overview While working with R and therefore, while working with R and Spark, you will write R code. In fact, you have already written R code throught most of the previous chaters in this book. Writting code can be as simple as loading data from a text file to writting distributed R code. But for the sake of the argument, lets consider one of the first lines of code presented in this book: spark_read_text(sc, &quot;hello&quot;, &quot;hello.txt&quot;) When thinking of contributing back, the most important question you can ask about the code above, but really, about any piece of code you write is: Would this code be useful to someone else? For the code above, the answer is probably no, it’s just too generic and can be easiy found online; however, a more realistic example would be to tailor something the code above for something that you actually care about, perhaps: spark_read_text(sc, &quot;stuff-that-matters&quot;, &quot;/secret/path/which/was/hard/to/figure/out/&quot;) The code above is quite similar to the original one, but assuming that you work with colleages, the answer to: Would this code be useful to someone else? Is now completely different: Yes, most likely! Which is surprising since this means that not all useful code needs to be very advanced or complicated; however, for it to be useful to others, it does need to be packaged, presented and shared in a usable format. One first attempt would be to wrap this into a file useful.R and then write a function over it, as in: load_useful_data &lt;- function() { spark_read_text(sc, &quot;stuff-that-matters&quot;, &quot;/secret/path/which/was/hard/to/figure/out/&quot;) } Which is an improvement but it would require users to manually share this file over and over. Fortunately, this is a problem already solved in R quite well through R Packages. An R package contains R code packaged in a format installable using the install.packages() function. sparklyr is an R package, but there are many other packages available in R and you can also create your own packages. For those of you new to creating R packages, I would encourage reading Hadley Wickam’s book on packages: R Packages: Organize, Test, Document, and Share Your Code. Creating an R package allows you to easily share your functions with others by sharing the package file in your organization. Once a package is created, there are many ways to share this with colleagues or the world. For instance, for packages meant to be private, you can consider using Drat or products like RStudio Package Manager. R packages meant for public consumption are made available to the R community in CRAN, which stands for the Comprehensive R Archive Network. These repositories of R packages make packages allow users to install packages through install.packages(\"usefulness\") without having to worry where to download the package from and allows other packages to reuse your package in their packages as well. While this was a very brief introduction to R packages in the context of Spark, it should be more or less obvious that you should be thinking of writting R packages while extending Spark from R. The rest of this chapter will present the tools and concepts require to extend functionality in sparklyr. There are three different ways in which sparklyr extensions can be written: R Extensions: These extensions make use of only R code and are the easiest one to get started with. Scala Extensions: These extensions make use of R code but also Scala code to get access to all the functionality available in Spark. Spark Extensions: These extensions make use of R code, Scala code and also Spark extensions on their own and while they could be seen as the most complex of all, they are also some of the most useful extensions we can write. Then we can wrap those extensions into an [R Package] or consider the functionality to be added back into sparklyr. 12.2 R Extensions R extensions make use of three functions in sparklyr: invoke_new(), invoke_static() and invoke(). For the most part, that’s all you need to extend Spark’s functionality in R. spark_context(sc) %&gt;% invoke(&quot;textFile&quot;, &quot;my-file.txt&quot;, 1L) %&gt;% invoke(&quot;count&quot;) 12.3 Scala Extensions 12.3.1 Prerequisites Changes in the scala sources require the Scala compilers to be installed. You can install the required compilers by running: library(sparklyr) download_scalac() Which will download the correct compilers from https://www.scala-lang.org/. 12.4 Spark Extensions 12.5 R Packages 12.5.1 RStudio Projects You can create an sparklyr extension with ease from RStudio. This feature requires RStudio 1.1 or newer and the sparklyr package to be installed. Then, from the File menu, New Project..., select R Packag using sparklyr: 12.5.2 Troubleshooting We can trace all the calls made to invoke(), invoke_new() and invoke_static() using the sparklyr.invoke.trace and sparklyr.invoke.trace.callstack options as follows: config &lt;- spark_config() config$sparklyr.invoke.trace &lt;- TRUE spark_connect(master = &quot;local&quot;, config = config) 12.6 sparklyr First of all, it’s worth mentioning that, sparklyr is just another R package; a package which contains R code packaged in a format installable using the install.packages() function. Now, since Spark was built in the Scala programming language, sparklyr also contains Scala code to provide the functionality required to interoperate with Spark efficiently. CRAN, which stands for the Comprehensive R Archive Network, provides a repository of packages that can be easily installable and which are carefully reviewed before they are made available. You can read more about the release process under releasing a package and Hadley Wickam’s book on packages: R Packages: Organize, Test, Document, and Share Your Code. CRAN users are always encouraged to install the latest version of a package, which means that sparklyr also needs to support multiple versions of Spark. Therefore, at a high level, sparklyr is composed of: R code and versioned Scala code: FIGURE 12.1: Sparklyr Package Architecture The sparklyr sources are built with R CMD build or from the RStudio’s build menu, this topic extensevely covered in the R Packages book. The Scala code is compiled into JAVA Archive files which will eventually be executed by Spark through the Java Virtual Machine. Compilation can be manually performed with the Scala compiler using compile_package_jars(), a sparklyr function that invokes the Scala compiler over a set of supported versions. When connecting to Spark using spark_connect(), sparklyr submits the correct version of the sparklyr JAR to Spark which then Spark executes. We will refer to this submitted application as the sparklyr backend and the R code as the sparklyr frontend. Fontend and backend are common software engineer concepts related to separating the user interface, the R console in this case, with the data layer, Spark data proessing in this context. For most connections, the backend is usually submitted by sparklyr to Spark using spark-submit, which is a command line tool well-known in Spark; however, for others connections, like Livy, the backend is submitted through Livy’s HTTP interface. Once the backend is submitted to Spark, the frontend communicates to the backend using socket connections, expect for Livy connections where this happens over HTTP: FIGURE 12.2: Sparklyr Connection Architecture 12.6.1 Compiling To compile sparklyr, make sure the prerequisites described in the Scala Extensions Prerequisites section are fullfilled. Then you can recompile all the jars by running configure.R in the root of the sparklyr sources. Once the jars are compiled, you can build the R package as described in the R Extensions section. 12.6.2 Serialization 12.6.3 Invocations 12.6.4 R Packages (dbi, dplyr, broom, etc) 12.6.5 Connections 12.6.6 Distributed R 12.7 Recap "],
["appendix.html", "Appendix 12.8 Prerequisites 12.9 Diagrams", " Appendix 12.8 Prerequisites 12.8.1 Installing R From r-project.org, download and launch the R installer for your platform, Windows, Macs or Linux available. FIGURE 12.3: The R Project for Statistical Computing. 12.8.2 Installing Java From java.com/download, download and launch the installer for your platform, Windows, Macs or Linux are also available. FIGURE 12.4: Java Download Page. Starting with Spark 2.1, Java 8 is required; however, previous versions of Spark support Java 7. Regardless, we recommend installing Java 8. 12.8.3 Installing RStudio While installing RStudio is not strictly required to work with Spark with R, it will make you much more productive and therefore, I would recommend you take the time to install RStudio from rstudio.com/download, then download and launch the installer for your platform: Windows, Macs or Linux. FIGURE 12.5: RStudio Downloads Page. After launching RStudio, you can use RStudio’s console panel to execute the code provided in this chapter. 12.8.4 Using RStudio If you are not familiar with RStudio, you should make note of the following panes: Console: A standalone R console you can use to execute all the code presented in this book. Packages: This pane allows you to install sparklyr with ease, check its version, navigate to the help contents, etc. Connections: This pane allows you to connecto to Spark, manage your active connection and view the available datasets. FIGURE 12.6: RStudio Overview. 12.9 Diagrams 12.9.1 Worlds Store Capacity library(tidyverse) read_csv(&quot;data/01-worlds-capacity-to-store-information.csv&quot;, skip = 8) %&gt;% gather(key = storage, value = capacity, analog, digital) %&gt;% mutate(year = X1, terabytes = capacity / 1e+12) %&gt;% ggplot(aes(x = year, y = terabytes, group = storage)) + geom_line(aes(linetype = storage)) + geom_point(aes(shape = storage)) + scale_y_log10( breaks = scales::trans_breaks(&quot;log10&quot;, function(x) 10^x), labels = scales::trans_format(&quot;log10&quot;, scales::math_format(10^x)) ) + theme_light() + theme(legend.position = &quot;bottom&quot;) 12.9.2 Daily downloads of CRAN packages downloads_csv &lt;- &quot;data/01-intro-r-cran-downloads.csv&quot; if (!file.exists(downloads_csv)) { downloads &lt;- cranlogs::cran_downloads(from = &quot;2014-01-01&quot;, to = &quot;2019-01-01&quot;) readr::write_csv(downloads, downloads_csv) } cran_downloads &lt;- readr::read_csv(downloads_csv) ggplot(cran_downloads, aes(date, count)) + geom_point(colour=&quot;black&quot;, pch = 21, size = 1) + scale_x_date() + xlab(&quot;&quot;) + ylab(&quot;&quot;) + theme_light() 12.9.3 Google trends for mainframes, cloud computing and kubernetes library(r2d3) read.csv(&quot;data/05-cluster-trends.csv&quot;) %&gt;% mutate(month = as.Date(paste(month, &quot;-01&quot;, sep = &quot;&quot;))) %&gt;% r2d3(script=&quot;images/05-clusters-trends.js&quot;) "],
["references.html", "Chapter 13 References", " Chapter 13 References "]
]
