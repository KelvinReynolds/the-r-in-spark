<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 10 Distributed R | The R in Spark: Learning Apache Spark with R</title>
  <meta name="description" content="A book to learn Apache Spark with R using the sparklyr R package.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 10 Distributed R | The R in Spark: Learning Apache Spark with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A book to learn Apache Spark with R using the sparklyr R package." />
  <meta name="github-repo" content="javierluraschi/the-r-in-spark" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 10 Distributed R | The R in Spark: Learning Apache Spark with R" />
  
  <meta name="twitter:description" content="A book to learn Apache Spark with R using the sparklyr R package." />
  



<meta name="date" content="2019-03-22">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="extensions.html">
<link rel="next" href="streaming.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.3/htmlwidgets.js"></script>
<script src="libs/r2d3-render-0.1.0/r2d3-render.js"></script>
<script src="libs/webcomponents-2.0.0/webcomponents.js"></script>
<script src="libs/r2d3-binding-0.2.3/r2d3.js"></script>
<script src="libs/d3v5-5.0.0/d3.min.js"></script>
<script src="libs/dagre-0.0.1/dagre.min.js"></script>
<script src="libs/lodash-3.7.0/lodash.js"></script>
<script src="libs/nomnoml-0.2.0/nomnoml.js"></script>
<script src="libs/nomnoml-binding-0.1.0/nomnoml.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-119986300-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-119986300-1');
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./">Learning Apache Spark with R</a></strong></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#authors"><i class="fa fa-check"></i>Authors</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#formatting"><i class="fa fa-check"></i>Formatting</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#intro-background"><i class="fa fa-check"></i><b>1.1</b> Information</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#intro-hadoop"><i class="fa fa-check"></i><b>1.2</b> Hadoop</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#intro-spark"><i class="fa fa-check"></i><b>1.3</b> Spark</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#intro-r"><i class="fa fa-check"></i><b>1.4</b> R</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#intro-sparklyr"><i class="fa fa-check"></i><b>1.5</b> sparklyr</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#intro-recap"><i class="fa fa-check"></i><b>1.6</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="starting.html"><a href="starting.html"><i class="fa fa-check"></i><b>2</b> Getting Started</a><ul>
<li class="chapter" data-level="2.1" data-path="starting.html"><a href="starting.html#starting-prerequisites"><i class="fa fa-check"></i><b>2.1</b> Prerequisites</a></li>
<li class="chapter" data-level="2.2" data-path="starting.html"><a href="starting.html#starting-install-sparklyr"><i class="fa fa-check"></i><b>2.2</b> Installing sparklyr</a></li>
<li class="chapter" data-level="2.3" data-path="starting.html"><a href="starting.html#starting-installing-spark"><i class="fa fa-check"></i><b>2.3</b> Installing Spark</a></li>
<li class="chapter" data-level="2.4" data-path="starting.html"><a href="starting.html#starting-connect-to-spark"><i class="fa fa-check"></i><b>2.4</b> Connecting to Spark</a></li>
<li class="chapter" data-level="2.5" data-path="starting.html"><a href="starting.html#starting-sparklyr-hello-world"><i class="fa fa-check"></i><b>2.5</b> Using Spark</a><ul>
<li class="chapter" data-level="2.5.1" data-path="starting.html"><a href="starting.html#starting-spark-web-interface"><i class="fa fa-check"></i><b>2.5.1</b> Web Interface</a></li>
<li class="chapter" data-level="2.5.2" data-path="starting.html"><a href="starting.html#starting-analysis"><i class="fa fa-check"></i><b>2.5.2</b> Analysis</a></li>
<li class="chapter" data-level="2.5.3" data-path="starting.html"><a href="starting.html#starting-modeling"><i class="fa fa-check"></i><b>2.5.3</b> Modeling</a></li>
<li class="chapter" data-level="2.5.4" data-path="starting.html"><a href="starting.html#starting-data"><i class="fa fa-check"></i><b>2.5.4</b> Data</a></li>
<li class="chapter" data-level="2.5.5" data-path="starting.html"><a href="starting.html#starting-extensions"><i class="fa fa-check"></i><b>2.5.5</b> Extensions</a></li>
<li class="chapter" data-level="2.5.6" data-path="starting.html"><a href="starting.html#starting-distributed-r"><i class="fa fa-check"></i><b>2.5.6</b> Distributed R</a></li>
<li class="chapter" data-level="2.5.7" data-path="starting.html"><a href="starting.html#starting-streaming"><i class="fa fa-check"></i><b>2.5.7</b> Streaming</a></li>
<li class="chapter" data-level="2.5.8" data-path="starting.html"><a href="starting.html#starting-logs"><i class="fa fa-check"></i><b>2.5.8</b> Logs</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="starting.html"><a href="starting.html#starting-disconnecting"><i class="fa fa-check"></i><b>2.6</b> Disconnecting</a></li>
<li class="chapter" data-level="2.7" data-path="starting.html"><a href="starting.html#starting-using-spark-from-rstudio"><i class="fa fa-check"></i><b>2.7</b> Using RStudio</a></li>
<li class="chapter" data-level="2.8" data-path="starting.html"><a href="starting.html#starting-resources"><i class="fa fa-check"></i><b>2.8</b> Resources</a></li>
<li class="chapter" data-level="2.9" data-path="starting.html"><a href="starting.html#starting-recap"><i class="fa fa-check"></i><b>2.9</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="analysis.html"><a href="analysis.html"><i class="fa fa-check"></i><b>3</b> Analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="analysis.html"><a href="analysis.html#introduction"><i class="fa fa-check"></i><b>3.1</b> Introduction</a><ul>
<li class="chapter" data-level="3.1.1" data-path="analysis.html"><a href="analysis.html#searching-for-insights"><i class="fa fa-check"></i><b>3.1.1</b> Searching for insights</a></li>
<li class="chapter" data-level="3.1.2" data-path="analysis.html"><a href="analysis.html#r-as-an-interface-to-spark"><i class="fa fa-check"></i><b>3.1.2</b> R as an interface to Spark</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="analysis.html"><a href="analysis.html#exercise"><i class="fa fa-check"></i><b>3.2</b> Exercise</a></li>
<li class="chapter" data-level="3.3" data-path="analysis.html"><a href="analysis.html#import-access"><i class="fa fa-check"></i><b>3.3</b> Import / Access</a></li>
<li class="chapter" data-level="3.4" data-path="analysis.html"><a href="analysis.html#wrangle"><i class="fa fa-check"></i><b>3.4</b> Wrangle</a><ul>
<li class="chapter" data-level="3.4.1" data-path="analysis.html"><a href="analysis.html#transformations-using-dplyr"><i class="fa fa-check"></i><b>3.4.1</b> Transformations using <code>dplyr</code></a></li>
<li class="chapter" data-level="3.4.2" data-path="analysis.html"><a href="analysis.html#correlations"><i class="fa fa-check"></i><b>3.4.2</b> Correlations</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="analysis.html"><a href="analysis.html#visualize"><i class="fa fa-check"></i><b>3.5</b> Visualize</a><ul>
<li class="chapter" data-level="3.5.1" data-path="analysis.html"><a href="analysis.html#transform-remotely-plot-locally"><i class="fa fa-check"></i><b>3.5.1</b> Transform remotely, plot locally</a></li>
<li class="chapter" data-level="3.5.2" data-path="analysis.html"><a href="analysis.html#simple-plots"><i class="fa fa-check"></i><b>3.5.2</b> Simple plots</a></li>
<li class="chapter" data-level="3.5.3" data-path="analysis.html"><a href="analysis.html#complex-plots"><i class="fa fa-check"></i><b>3.5.3</b> Complex plots</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="analysis.html"><a href="analysis.html#model"><i class="fa fa-check"></i><b>3.6</b> Model</a></li>
<li class="chapter" data-level="3.7" data-path="analysis.html"><a href="analysis.html#communicate"><i class="fa fa-check"></i><b>3.7</b> Communicate</a></li>
<li class="chapter" data-level="3.8" data-path="analysis.html"><a href="analysis.html#later-review"><i class="fa fa-check"></i><b>3.8</b> Later review</a><ul>
<li class="chapter" data-level="3.8.1" data-path="analysis.html"><a href="analysis.html#background"><i class="fa fa-check"></i><b>3.8.1</b> Background</a></li>
<li class="chapter" data-level="3.8.2" data-path="analysis.html"><a href="analysis.html#working-with-big-data"><i class="fa fa-check"></i><b>3.8.2</b> Working with Big Data</a></li>
<li class="chapter" data-level="3.8.3" data-path="analysis.html"><a href="analysis.html#avoid-running-r-inside-spark"><i class="fa fa-check"></i><b>3.8.3</b> Avoid running R inside Spark</a></li>
<li class="chapter" data-level="3.8.4" data-path="analysis.html"><a href="analysis.html#r-under-the-hood"><i class="fa fa-check"></i><b>3.8.4</b> R, under the hood</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="modeling.html"><a href="modeling.html"><i class="fa fa-check"></i><b>4</b> Modeling</a><ul>
<li class="chapter" data-level="4.1" data-path="modeling.html"><a href="modeling.html#the-data"><i class="fa fa-check"></i><b>4.1</b> The Data</a></li>
<li class="chapter" data-level="4.2" data-path="modeling.html"><a href="modeling.html#exploratory-data-analysis-eda"><i class="fa fa-check"></i><b>4.2</b> Exploratory Data Analysis (EDA)</a></li>
<li class="chapter" data-level="4.3" data-path="modeling.html"><a href="modeling.html#feature-engineering"><i class="fa fa-check"></i><b>4.3</b> Feature Engineering</a></li>
<li class="chapter" data-level="4.4" data-path="modeling.html"><a href="modeling.html#model-building"><i class="fa fa-check"></i><b>4.4</b> Model Building</a><ul>
<li class="chapter" data-level="4.4.1" data-path="modeling.html"><a href="modeling.html#logistic-regression-as-a-generalized-linear-regression"><i class="fa fa-check"></i><b>4.4.1</b> Logistic Regression as a Generalized Linear Regression</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="modeling.html"><a href="modeling.html#more-machine-learning-algorithms"><i class="fa fa-check"></i><b>4.5</b> More Machine Learning Algorithms</a></li>
<li class="chapter" data-level="4.6" data-path="modeling.html"><a href="modeling.html#functionlist"><i class="fa fa-check"></i><b>4.6</b> List of ML Functions</a><ul>
<li class="chapter" data-level="4.6.1" data-path="modeling.html"><a href="modeling.html#classification"><i class="fa fa-check"></i><b>4.6.1</b> Classification</a></li>
<li class="chapter" data-level="4.6.2" data-path="modeling.html"><a href="modeling.html#regression"><i class="fa fa-check"></i><b>4.6.2</b> Regression</a></li>
<li class="chapter" data-level="4.6.3" data-path="modeling.html"><a href="modeling.html#clustering"><i class="fa fa-check"></i><b>4.6.3</b> Clustering</a></li>
<li class="chapter" data-level="4.6.4" data-path="modeling.html"><a href="modeling.html#recommendation"><i class="fa fa-check"></i><b>4.6.4</b> Recommendation</a></li>
<li class="chapter" data-level="4.6.5" data-path="modeling.html"><a href="modeling.html#frequent-pattern-mining"><i class="fa fa-check"></i><b>4.6.5</b> Frequent Pattern Mining</a></li>
<li class="chapter" data-level="4.6.6" data-path="modeling.html"><a href="modeling.html#feature-transformers"><i class="fa fa-check"></i><b>4.6.6</b> Feature Transformers</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="5" data-path="clusters.html"><a href="clusters.html"><i class="fa fa-check"></i><b>5</b> Clusters</a><ul>
<li class="chapter" data-level="5.1" data-path="clusters.html"><a href="clusters.html#clusters-overview"><i class="fa fa-check"></i><b>5.1</b> Overview</a></li>
<li class="chapter" data-level="5.2" data-path="clusters.html"><a href="clusters.html#on-premise"><i class="fa fa-check"></i><b>5.2</b> On-Premise</a><ul>
<li class="chapter" data-level="5.2.1" data-path="clusters.html"><a href="clusters.html#clusters-manager"><i class="fa fa-check"></i><b>5.2.1</b> Managers</a></li>
<li class="chapter" data-level="5.2.2" data-path="clusters.html"><a href="clusters.html#distributions"><i class="fa fa-check"></i><b>5.2.2</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="clusters.html"><a href="clusters.html#cloud"><i class="fa fa-check"></i><b>5.3</b> Cloud</a><ul>
<li class="chapter" data-level="5.3.1" data-path="clusters.html"><a href="clusters.html#clusters-amazon-emr"><i class="fa fa-check"></i><b>5.3.1</b> Amazon</a></li>
<li class="chapter" data-level="5.3.2" data-path="clusters.html"><a href="clusters.html#databricks"><i class="fa fa-check"></i><b>5.3.2</b> Databricks</a></li>
<li class="chapter" data-level="5.3.3" data-path="clusters.html"><a href="clusters.html#google"><i class="fa fa-check"></i><b>5.3.3</b> Google</a></li>
<li class="chapter" data-level="5.3.4" data-path="clusters.html"><a href="clusters.html#ibm"><i class="fa fa-check"></i><b>5.3.4</b> IBM</a></li>
<li class="chapter" data-level="5.3.5" data-path="clusters.html"><a href="clusters.html#microsoft"><i class="fa fa-check"></i><b>5.3.5</b> Microsoft</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="clusters.html"><a href="clusters.html#kubernetes"><i class="fa fa-check"></i><b>5.4</b> Kubernetes</a></li>
<li class="chapter" data-level="5.5" data-path="clusters.html"><a href="clusters.html#tools"><i class="fa fa-check"></i><b>5.5</b> Tools</a><ul>
<li class="chapter" data-level="5.5.1" data-path="clusters.html"><a href="clusters.html#rstudio"><i class="fa fa-check"></i><b>5.5.1</b> RStudio</a></li>
<li class="chapter" data-level="5.5.2" data-path="clusters.html"><a href="clusters.html#jupyter"><i class="fa fa-check"></i><b>5.5.2</b> Jupyter</a></li>
<li class="chapter" data-level="5.5.3" data-path="clusters.html"><a href="clusters.html#clusters-livy"><i class="fa fa-check"></i><b>5.5.3</b> Livy</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="clusters.html"><a href="clusters.html#recap"><i class="fa fa-check"></i><b>5.6</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="connections.html"><a href="connections.html"><i class="fa fa-check"></i><b>6</b> Connections</a><ul>
<li class="chapter" data-level="6.1" data-path="connections.html"><a href="connections.html#connections-overview"><i class="fa fa-check"></i><b>6.1</b> Overview</a><ul>
<li class="chapter" data-level="6.1.1" data-path="connections.html"><a href="connections.html#connections-spark-edge-nodes"><i class="fa fa-check"></i><b>6.1.1</b> Edge Nodes</a></li>
<li class="chapter" data-level="6.1.2" data-path="connections.html"><a href="connections.html#connections-spark-home"><i class="fa fa-check"></i><b>6.1.2</b> Spark Home</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="connections.html"><a href="connections.html#connections-local"><i class="fa fa-check"></i><b>6.2</b> Local</a></li>
<li class="chapter" data-level="6.3" data-path="connections.html"><a href="connections.html#connections-standalone"><i class="fa fa-check"></i><b>6.3</b> Standalone</a></li>
<li class="chapter" data-level="6.4" data-path="connections.html"><a href="connections.html#connections-yarn"><i class="fa fa-check"></i><b>6.4</b> Yarn</a><ul>
<li class="chapter" data-level="6.4.1" data-path="connections.html"><a href="connections.html#connections-yarn-client"><i class="fa fa-check"></i><b>6.4.1</b> Yarn Client</a></li>
<li class="chapter" data-level="6.4.2" data-path="connections.html"><a href="connections.html#connections-yarn-cluster"><i class="fa fa-check"></i><b>6.4.2</b> Yarn Cluster</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="connections.html"><a href="connections.html#connections-livy"><i class="fa fa-check"></i><b>6.5</b> Livy</a></li>
<li class="chapter" data-level="6.6" data-path="connections.html"><a href="connections.html#connections-mesos"><i class="fa fa-check"></i><b>6.6</b> Mesos</a></li>
<li class="chapter" data-level="6.7" data-path="connections.html"><a href="connections.html#connections-kubernetes"><i class="fa fa-check"></i><b>6.7</b> Kubernetes</a></li>
<li class="chapter" data-level="6.8" data-path="connections.html"><a href="connections.html#cloud-1"><i class="fa fa-check"></i><b>6.8</b> Cloud</a></li>
<li class="chapter" data-level="6.9" data-path="connections.html"><a href="connections.html#batches"><i class="fa fa-check"></i><b>6.9</b> Batches</a></li>
<li class="chapter" data-level="6.10" data-path="connections.html"><a href="connections.html#tools-1"><i class="fa fa-check"></i><b>6.10</b> Tools</a></li>
<li class="chapter" data-level="6.11" data-path="connections.html"><a href="connections.html#multiple"><i class="fa fa-check"></i><b>6.11</b> Multiple</a></li>
<li class="chapter" data-level="6.12" data-path="connections.html"><a href="connections.html#connections-troubleshooting"><i class="fa fa-check"></i><b>6.12</b> Troubleshooting</a><ul>
<li class="chapter" data-level="6.12.1" data-path="connections.html"><a href="connections.html#logging"><i class="fa fa-check"></i><b>6.12.1</b> Logging</a></li>
<li class="chapter" data-level="6.12.2" data-path="connections.html"><a href="connections.html#troubleshoot-spark-submit"><i class="fa fa-check"></i><b>6.12.2</b> Spark Submit</a></li>
<li class="chapter" data-level="6.12.3" data-path="connections.html"><a href="connections.html#windows"><i class="fa fa-check"></i><b>6.12.3</b> Windows</a></li>
</ul></li>
<li class="chapter" data-level="6.13" data-path="connections.html"><a href="connections.html#recap-1"><i class="fa fa-check"></i><b>6.13</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>7</b> Data</a><ul>
<li class="chapter" data-level="7.1" data-path="data.html"><a href="data.html#overview"><i class="fa fa-check"></i><b>7.1</b> Overview</a></li>
<li class="chapter" data-level="7.2" data-path="data.html"><a href="data.html#data-frames"><i class="fa fa-check"></i><b>7.2</b> Data Frames</a><ul>
<li class="chapter" data-level="7.2.1" data-path="data.html"><a href="data.html#data-sdf-functions"><i class="fa fa-check"></i><b>7.2.1</b> Functions</a></li>
<li class="chapter" data-level="7.2.2" data-path="data.html"><a href="data.html#pivoting"><i class="fa fa-check"></i><b>7.2.2</b> Pivoting</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="data.html"><a href="data.html#formats"><i class="fa fa-check"></i><b>7.3</b> Formats</a></li>
<li class="chapter" data-level="7.4" data-path="data.html"><a href="data.html#data-types"><i class="fa fa-check"></i><b>7.4</b> Data Types</a><ul>
<li class="chapter" data-level="7.4.1" data-path="data.html"><a href="data.html#dates"><i class="fa fa-check"></i><b>7.4.1</b> Dates</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="data.html"><a href="data.html#sources"><i class="fa fa-check"></i><b>7.5</b> Sources</a><ul>
<li class="chapter" data-level="7.5.1" data-path="data.html"><a href="data.html#amazon-s3"><i class="fa fa-check"></i><b>7.5.1</b> Amazon S3</a></li>
<li class="chapter" data-level="7.5.2" data-path="data.html"><a href="data.html#azure-storage"><i class="fa fa-check"></i><b>7.5.2</b> Azure Storage</a></li>
<li class="chapter" data-level="7.5.3" data-path="data.html"><a href="data.html#cassandra"><i class="fa fa-check"></i><b>7.5.3</b> Cassandra</a></li>
<li class="chapter" data-level="7.5.4" data-path="data.html"><a href="data.html#databases"><i class="fa fa-check"></i><b>7.5.4</b> Databases</a></li>
<li class="chapter" data-level="7.5.5" data-path="data.html"><a href="data.html#hbase"><i class="fa fa-check"></i><b>7.5.5</b> HBase</a></li>
<li class="chapter" data-level="7.5.6" data-path="data.html"><a href="data.html#nested-data"><i class="fa fa-check"></i><b>7.5.6</b> Nested Data</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="data.html"><a href="data.html#troubleshooting"><i class="fa fa-check"></i><b>7.6</b> Troubleshooting</a><ul>
<li class="chapter" data-level="7.6.1" data-path="data.html"><a href="data.html#troubleshoot-csvs"><i class="fa fa-check"></i><b>7.6.1</b> Troubleshoot CSVs</a></li>
<li class="chapter" data-level="7.6.2" data-path="data.html"><a href="data.html#column-names"><i class="fa fa-check"></i><b>7.6.2</b> Column Names</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="data.html"><a href="data.html#recap-2"><i class="fa fa-check"></i><b>7.7</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="tuning.html"><a href="tuning.html"><i class="fa fa-check"></i><b>8</b> Tuning</a><ul>
<li class="chapter" data-level="8.1" data-path="tuning.html"><a href="tuning.html#overview-1"><i class="fa fa-check"></i><b>8.1</b> Overview</a><ul>
<li class="chapter" data-level="8.1.1" data-path="tuning.html"><a href="tuning.html#tuning-graph-visualization"><i class="fa fa-check"></i><b>8.1.1</b> Graph</a></li>
<li class="chapter" data-level="8.1.2" data-path="tuning.html"><a href="tuning.html#tuning-event-timeline"><i class="fa fa-check"></i><b>8.1.2</b> Timeline</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="tuning.html"><a href="tuning.html#tuning-configuring"><i class="fa fa-check"></i><b>8.2</b> Configuring</a><ul>
<li class="chapter" data-level="8.2.1" data-path="tuning.html"><a href="tuning.html#connect-settings"><i class="fa fa-check"></i><b>8.2.1</b> Connect Settings</a></li>
<li class="chapter" data-level="8.2.2" data-path="tuning.html"><a href="tuning.html#submit-settings"><i class="fa fa-check"></i><b>8.2.2</b> Submit Settings</a></li>
<li class="chapter" data-level="8.2.3" data-path="tuning.html"><a href="tuning.html#runtime-settings"><i class="fa fa-check"></i><b>8.2.3</b> Runtime Settings</a></li>
<li class="chapter" data-level="8.2.4" data-path="tuning.html"><a href="tuning.html#sparklyr-settings"><i class="fa fa-check"></i><b>8.2.4</b> sparklyr Settings</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="tuning.html"><a href="tuning.html#tuning-partitioning"><i class="fa fa-check"></i><b>8.3</b> Partitioning</a><ul>
<li class="chapter" data-level="8.3.1" data-path="tuning.html"><a href="tuning.html#implicit"><i class="fa fa-check"></i><b>8.3.1</b> Implicit</a></li>
<li class="chapter" data-level="8.3.2" data-path="tuning.html"><a href="tuning.html#explicit"><i class="fa fa-check"></i><b>8.3.2</b> Explicit</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="tuning.html"><a href="tuning.html#tuning-caching"><i class="fa fa-check"></i><b>8.4</b> Caching</a><ul>
<li class="chapter" data-level="8.4.1" data-path="tuning.html"><a href="tuning.html#checkpointing"><i class="fa fa-check"></i><b>8.4.1</b> Checkpointing</a></li>
<li class="chapter" data-level="8.4.2" data-path="tuning.html"><a href="tuning.html#tuning-memory"><i class="fa fa-check"></i><b>8.4.2</b> Memory</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="tuning.html"><a href="tuning.html#tuning-shuffling"><i class="fa fa-check"></i><b>8.5</b> Shuffling</a></li>
<li class="chapter" data-level="8.6" data-path="tuning.html"><a href="tuning.html#tuning-serialization"><i class="fa fa-check"></i><b>8.6</b> Serialization</a></li>
<li class="chapter" data-level="8.7" data-path="tuning.html"><a href="tuning.html#configuration-files"><i class="fa fa-check"></i><b>8.7</b> Configuration Files</a></li>
<li class="chapter" data-level="8.8" data-path="tuning.html"><a href="tuning.html#recap-3"><i class="fa fa-check"></i><b>8.8</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="extensions.html"><a href="extensions.html"><i class="fa fa-check"></i><b>9</b> Extensions</a><ul>
<li class="chapter" data-level="9.1" data-path="extensions.html"><a href="extensions.html#rsparkling"><i class="fa fa-check"></i><b>9.1</b> RSparkling</a><ul>
<li class="chapter" data-level="9.1.1" data-path="extensions.html"><a href="extensions.html#troubleshooting-1"><i class="fa fa-check"></i><b>9.1.1</b> Troubleshooting</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="extensions.html"><a href="extensions.html#graphframes"><i class="fa fa-check"></i><b>9.2</b> GraphFrames</a></li>
<li class="chapter" data-level="9.3" data-path="extensions.html"><a href="extensions.html#mleap"><i class="fa fa-check"></i><b>9.3</b> Mleap</a></li>
<li class="chapter" data-level="9.4" data-path="extensions.html"><a href="extensions.html#extensions-nested-data"><i class="fa fa-check"></i><b>9.4</b> Nested Data</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="distributed.html"><a href="distributed.html"><i class="fa fa-check"></i><b>10</b> Distributed R</a><ul>
<li class="chapter" data-level="10.1" data-path="distributed.html"><a href="distributed.html#overview-2"><i class="fa fa-check"></i><b>10.1</b> Overview</a></li>
<li class="chapter" data-level="10.2" data-path="distributed.html"><a href="distributed.html#use-cases"><i class="fa fa-check"></i><b>10.2</b> Use Cases</a><ul>
<li class="chapter" data-level="10.2.1" data-path="distributed.html"><a href="distributed.html#custom-parsers"><i class="fa fa-check"></i><b>10.2.1</b> Custom Parsers</a></li>
<li class="chapter" data-level="10.2.2" data-path="distributed.html"><a href="distributed.html#partitioned-modeling"><i class="fa fa-check"></i><b>10.2.2</b> Partitioned Modeling</a></li>
<li class="chapter" data-level="10.2.3" data-path="distributed.html"><a href="distributed.html#distributed-grid-search"><i class="fa fa-check"></i><b>10.2.3</b> Grid Search</a></li>
<li class="chapter" data-level="10.2.4" data-path="distributed.html"><a href="distributed.html#web-apis"><i class="fa fa-check"></i><b>10.2.4</b> Web APIs</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="distributed.html"><a href="distributed.html#partitions"><i class="fa fa-check"></i><b>10.3</b> Partitions</a></li>
<li class="chapter" data-level="10.4" data-path="distributed.html"><a href="distributed.html#grouping"><i class="fa fa-check"></i><b>10.4</b> Grouping</a></li>
<li class="chapter" data-level="10.5" data-path="distributed.html"><a href="distributed.html#columns"><i class="fa fa-check"></i><b>10.5</b> Columns</a></li>
<li class="chapter" data-level="10.6" data-path="distributed.html"><a href="distributed.html#context"><i class="fa fa-check"></i><b>10.6</b> Context</a></li>
<li class="chapter" data-level="10.7" data-path="distributed.html"><a href="distributed.html#packages"><i class="fa fa-check"></i><b>10.7</b> Packages</a></li>
<li class="chapter" data-level="10.8" data-path="distributed.html"><a href="distributed.html#requirements"><i class="fa fa-check"></i><b>10.8</b> Requirements</a></li>
<li class="chapter" data-level="10.9" data-path="distributed.html"><a href="distributed.html#limitations"><i class="fa fa-check"></i><b>10.9</b> Limitations</a><ul>
<li class="chapter" data-level="10.9.1" data-path="distributed.html"><a href="distributed.html#functions"><i class="fa fa-check"></i><b>10.9.1</b> Functions</a></li>
<li class="chapter" data-level="10.9.2" data-path="distributed.html"><a href="distributed.html#livy"><i class="fa fa-check"></i><b>10.9.2</b> Livy</a></li>
<li class="chapter" data-level="10.9.3" data-path="distributed.html"><a href="distributed.html#grouping-1"><i class="fa fa-check"></i><b>10.9.3</b> Grouping</a></li>
<li class="chapter" data-level="10.9.4" data-path="distributed.html"><a href="distributed.html#packages-1"><i class="fa fa-check"></i><b>10.9.4</b> Packages</a></li>
</ul></li>
<li class="chapter" data-level="10.10" data-path="distributed.html"><a href="distributed.html#troubleshooting-2"><i class="fa fa-check"></i><b>10.10</b> Troubleshooting</a><ul>
<li class="chapter" data-level="10.10.1" data-path="distributed.html"><a href="distributed.html#worker-logs"><i class="fa fa-check"></i><b>10.10.1</b> Worker Logs</a></li>
<li class="chapter" data-level="10.10.2" data-path="distributed.html"><a href="distributed.html#partition-timeouts"><i class="fa fa-check"></i><b>10.10.2</b> Partition Timeouts</a></li>
<li class="chapter" data-level="10.10.3" data-path="distributed.html"><a href="distributed.html#partition-errors"><i class="fa fa-check"></i><b>10.10.3</b> Partition Errors</a></li>
<li class="chapter" data-level="10.10.4" data-path="distributed.html"><a href="distributed.html#debugging-workers"><i class="fa fa-check"></i><b>10.10.4</b> Debugging Workers</a></li>
</ul></li>
<li class="chapter" data-level="10.11" data-path="distributed.html"><a href="distributed.html#clusters-1"><i class="fa fa-check"></i><b>10.11</b> Clusters</a></li>
<li class="chapter" data-level="10.12" data-path="distributed.html"><a href="distributed.html#apache-arrow"><i class="fa fa-check"></i><b>10.12</b> Apache Arrow</a></li>
<li class="chapter" data-level="10.13" data-path="distributed.html"><a href="distributed.html#recap-4"><i class="fa fa-check"></i><b>10.13</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="streaming.html"><a href="streaming.html"><i class="fa fa-check"></i><b>11</b> Streaming</a><ul>
<li class="chapter" data-level="11.1" data-path="streaming.html"><a href="streaming.html#overview-3"><i class="fa fa-check"></i><b>11.1</b> Overview</a></li>
<li class="chapter" data-level="11.2" data-path="streaming.html"><a href="streaming.html#streaming-treansform"><i class="fa fa-check"></i><b>11.2</b> Transformations</a><ul>
<li class="chapter" data-level="11.2.1" data-path="streaming.html"><a href="streaming.html#streams-dplyr"><i class="fa fa-check"></i><b>11.2.1</b> dplyr</a></li>
<li class="chapter" data-level="11.2.2" data-path="streaming.html"><a href="streaming.html#streams-pipelines"><i class="fa fa-check"></i><b>11.2.2</b> Pipelines</a></li>
<li class="chapter" data-level="11.2.3" data-path="streaming.html"><a href="streaming.html#streams-r"><i class="fa fa-check"></i><b>11.2.3</b> R Code</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="streaming.html"><a href="streaming.html#shiny"><i class="fa fa-check"></i><b>11.3</b> Shiny</a></li>
<li class="chapter" data-level="11.4" data-path="streaming.html"><a href="streaming.html#formats-1"><i class="fa fa-check"></i><b>11.4</b> Formats</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="contributing.html"><a href="contributing.html"><i class="fa fa-check"></i><b>12</b> Contributing</a><ul>
<li class="chapter" data-level="12.1" data-path="contributing.html"><a href="contributing.html#overview-4"><i class="fa fa-check"></i><b>12.1</b> Overview</a></li>
<li class="chapter" data-level="12.2" data-path="contributing.html"><a href="contributing.html#contributing-r-extension"><i class="fa fa-check"></i><b>12.2</b> R Extensions</a></li>
<li class="chapter" data-level="12.3" data-path="contributing.html"><a href="contributing.html#scala-extensions"><i class="fa fa-check"></i><b>12.3</b> Scala Extensions</a><ul>
<li class="chapter" data-level="12.3.1" data-path="contributing.html"><a href="contributing.html#scala-extension-prereq"><i class="fa fa-check"></i><b>12.3.1</b> Prerequisites</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="contributing.html"><a href="contributing.html#spark-extensions"><i class="fa fa-check"></i><b>12.4</b> Spark Extensions</a></li>
<li class="chapter" data-level="12.5" data-path="contributing.html"><a href="contributing.html#r-packages"><i class="fa fa-check"></i><b>12.5</b> R Packages</a><ul>
<li class="chapter" data-level="12.5.1" data-path="contributing.html"><a href="contributing.html#rstudio-projects"><i class="fa fa-check"></i><b>12.5.1</b> RStudio Projects</a></li>
<li class="chapter" data-level="12.5.2" data-path="contributing.html"><a href="contributing.html#troubleshooting-3"><i class="fa fa-check"></i><b>12.5.2</b> Troubleshooting</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="contributing.html"><a href="contributing.html#contributing-sparklyr"><i class="fa fa-check"></i><b>12.6</b> sparklyr</a><ul>
<li class="chapter" data-level="12.6.1" data-path="contributing.html"><a href="contributing.html#compiling"><i class="fa fa-check"></i><b>12.6.1</b> Compiling</a></li>
<li class="chapter" data-level="12.6.2" data-path="contributing.html"><a href="contributing.html#serialization"><i class="fa fa-check"></i><b>12.6.2</b> Serialization</a></li>
<li class="chapter" data-level="12.6.3" data-path="contributing.html"><a href="contributing.html#invocations"><i class="fa fa-check"></i><b>12.6.3</b> Invocations</a></li>
<li class="chapter" data-level="12.6.4" data-path="contributing.html"><a href="contributing.html#r-packages-1"><i class="fa fa-check"></i><b>12.6.4</b> R Packages</a></li>
<li class="chapter" data-level="12.6.5" data-path="contributing.html"><a href="contributing.html#connections-1"><i class="fa fa-check"></i><b>12.6.5</b> Connections</a></li>
<li class="chapter" data-level="12.6.6" data-path="contributing.html"><a href="contributing.html#distributed-r"><i class="fa fa-check"></i><b>12.6.6</b> Distributed R</a></li>
</ul></li>
<li class="chapter" data-level="12.7" data-path="contributing.html"><a href="contributing.html#recap-5"><i class="fa fa-check"></i><b>12.7</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i>Appendix</a><ul>
<li class="chapter" data-level="12.8" data-path="appendix.html"><a href="appendix.html#appendix-prerequisites"><i class="fa fa-check"></i><b>12.8</b> Prerequisites</a><ul>
<li class="chapter" data-level="12.8.1" data-path="appendix.html"><a href="appendix.html#appendix-install-r"><i class="fa fa-check"></i><b>12.8.1</b> Installing R</a></li>
<li class="chapter" data-level="12.8.2" data-path="appendix.html"><a href="appendix.html#appendix-install-java"><i class="fa fa-check"></i><b>12.8.2</b> Installing Java</a></li>
<li class="chapter" data-level="12.8.3" data-path="appendix.html"><a href="appendix.html#appendix-install-rstudio"><i class="fa fa-check"></i><b>12.8.3</b> Installing RStudio</a></li>
<li class="chapter" data-level="12.8.4" data-path="appendix.html"><a href="appendix.html#appendix-using-rstudio"><i class="fa fa-check"></i><b>12.8.4</b> Using RStudio</a></li>
</ul></li>
<li class="chapter" data-level="12.9" data-path="appendix.html"><a href="appendix.html#diagrams"><i class="fa fa-check"></i><b>12.9</b> Diagrams</a><ul>
<li class="chapter" data-level="12.9.1" data-path="appendix.html"><a href="appendix.html#appendix-storage-capacity"><i class="fa fa-check"></i><b>12.9.1</b> Worlds Store Capacity</a></li>
<li class="chapter" data-level="12.9.2" data-path="appendix.html"><a href="appendix.html#appendix-cran-downloads"><i class="fa fa-check"></i><b>12.9.2</b> Daily downloads of CRAN packages</a></li>
<li class="chapter" data-level="12.9.3" data-path="appendix.html"><a href="appendix.html#appendix-cluster-trends"><i class="fa fa-check"></i><b>12.9.3</b> Google trends for mainframes, cloud computing and kubernetes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>13</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The R in Spark: Learning Apache Spark with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="distributed" class="section level1">
<h1><span class="header-section-number">Chapter 10</span> Distributed R</h1>
<p>In previous chapters you learned how to perform data analysis and modeling in local Spark instances and proper Spark clusters, the previous <a href="extensions.html#extensions">Extensions</a> chapter, described how to make use of additional functionality provided by the Spark and R communities at large. In most cases, the combination of Spark functionality and extensions is more than enough to perform almost any computation. However, for those cases where functionality is lacking in Spark and their extensions, you can consider distributing R computations to worker nodes yourself while leveraging any existing R package.</p>
<p>If you are already a familiar R user, you might be tempted to use this approach for all Spark operations; however, this is not the recommended use of <code>spark_apply()</code>. Previous chapters provided more efficient techiniques and tools to solve well known problems, in contrast, <code>spark_apply()</code> introduces additional cognitive overhead, additional troubleshooting steps, performance trade-offs and, in general, additional complexity that should be avoided. Not to say that <code>spark_apply()</code> should never used; but instead, <code>spark_apply()</code> is reserved to support use cases where previous tools and techniques fall short.</p>
<div id="overview-2" class="section level2">
<h2><span class="header-section-number">10.1</span> Overview</h2>
<p>The <a href="intro.html#intro">Introduction</a> chapter introduced MapReduce as a technique capable of processing large scale datasets; it also described how Apache Spark provided a superset of operations to perform MapReduce computations with ease and more efficiently. The <a href="tuning.html#tuning">Tuning</a> chapter presented insights into how Spark works by applying custom transformation over each partition of the distributed datasets. For instance, if we were to multiply by ten each element of a distributed numeric dataset, Spark would apply a mapping operation over each partition through multiple workers, conceptually, this is illustrated in Figure <a href="distributed.html#fig:distributed-times-ten">10.1</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:distributed-times-ten"></span>
<img src="images/distributed-r-times-ten-resized.png" alt="Map Operation when Multiplying by Ten" width="1500" />
<p class="caption">
FIGURE 10.1: Map Operation when Multiplying by Ten
</p>
</div>
<p>This chapter presents how to define a custom <code>f(x)</code> mapping operation using <code>spark_apply()</code>; for the previous example, <code>spark_apply()</code> provides support to define <code>10 * x</code> as follows:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sdf_len</span>(sc, <span class="dv">3</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">spark_apply</span>(<span class="op">~</span><span class="st"> </span><span class="dv">10</span> <span class="op">*</span><span class="st"> </span>.x)</code></pre>
<pre><code># Source: spark&lt;?&gt; [?? x 1]
     id
* &lt;dbl&gt;
1    10
2    20
3    30</code></pre>
<p>Notice that <code>~ 10 * .x</code> is plain R code executed across all worker nodes; the <code>~</code> operator is defined in the <code>rlang</code> package and provides a compact definition of a function equivalent to <code>function(.x) 10 * .x</code> – this compact form is also known as an anonymous function or lambda expression.</p>
<p>Now, the first thing to notice is that <code>f(x)</code> takes an R data frame as input and must also produce an R data frame as output, conceptually illustrated in Figure <a href="distributed.html#fig:distributed-spark-apply-input-output">10.2</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:distributed-spark-apply-input-output"></span>
<img src="images/distributed-r-spark-apply-input-output.png" alt="Expected Function Signature in spark_apply() Mappings" width="936" />
<p class="caption">
FIGURE 10.2: Expected Function Signature in spark_apply() Mappings
</p>
</div>
<p>We can use the orginal MapReduce example from the <a href="intro.html#intro">Introduction</a> chapter where, the map operation was defined to split sentences into words and then, the total unique words were counted as the reduce operation. In R, we could make use of the <code>unnest_tokens()</code> function in the <code>tidytext</code> R package, combining the functionality of <code>tidytext</code> with <code>spark_apply()</code> would allow us to tokenize those sentences into a table of words as follows:</p>
<pre class="sourceCode r"><code class="sourceCode r">sentences &lt;-<span class="st"> </span><span class="kw">copy_to</span>(sc, <span class="kw">data_frame</span>(<span class="dt">text =</span> <span class="kw">c</span>(<span class="st">&quot;I like apples&quot;</span>, <span class="st">&quot;I like bananas&quot;</span>)))

sentences <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spark_apply</span>(<span class="op">~</span>tidytext<span class="op">::</span><span class="kw">unnest_tokens</span>(.x, word, text))</code></pre>
<pre><code># Source: spark&lt;?&gt; [?? x 1]
  word   
* &lt;chr&gt;  
1 i      
2 like   
3 apples 
4 i      
5 like   
6 bananas</code></pre>
<p>Finally, we can reduce this dataset using <code>dplyr</code> to compute this original MapReduce word-count example using <code>dplyr</code> as follows:</p>
<pre class="sourceCode r"><code class="sourceCode r">sentences <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spark_apply</span>(<span class="op">~</span>tidytext<span class="op">::</span><span class="kw">unnest_tokens</span>(., word, text)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(word) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">count =</span> <span class="kw">count</span>())</code></pre>
<pre><code># Source: spark&lt;?&gt; [?? x 2]
  word    count
* &lt;chr&gt;   &lt;dbl&gt;
1 i           2
2 apples      1
3 like        2
4 bananas     1</code></pre>
<p>The rest of this chapter will explain in detail use cases, features, caveats, considerations and troubleshooting techniques required when defining custom mappings through <code>spark_apply()</code></p>
<p><strong>Note:</strong> The previous sentence tokenizer example can be more efficiently implemented using concepts from previous chapters, specifically through <code>sentences %&gt;% ft_tokenizer("text", "words") %&gt;% transmute(word = explode(words))</code>.</p>
</div>
<div id="use-cases" class="section level2">
<h2><span class="header-section-number">10.2</span> Use Cases</h2>
<p>In the overview section we presented an example to help you understand how <code>spark_apply()</code> works; this section will cover a few practical use case for <code>spark_apply()</code>:</p>
<ul>
<li><strong>Parsing</strong>: When a file format is not natevely supported in Spark or its extensions, you can consider using R code to implement a distributed <strong>custom parser</strong> using R packages.</li>
<li><strong>Modeling</strong>: When data fits into a single machine you can make use of <strong>grid search</strong> to optimize their parameters in parallel, in cases where the data can be partitioned to create several models over subsets of the data you can use <strong>partitioned modeling</strong> in R to compute models across partitions.</li>
<li><strong>Interoperating</strong>: When large data needs to be evaluated by external systems you can use R to interoperate with those external systems. For instance, to process a large image dataset using deep learning models, you can consider calling an specialized <strong>web API</strong> like Google’s Vision API, Amazon’s Rekognition API, etc.</li>
</ul>
<div id="custom-parsers" class="section level3">
<h3><span class="header-section-number">10.2.1</span> Custom Parsers</h3>
<p>While Spark and its various extensions provide support for many file formats: CSVs, JSON, Parquet, AVRO, etc. there are many more file formats that you might need to use at scale. through You can parse these additional formats using <code>spark_apply()</code> and many of the existing R packages. This section presents two use cases for parsing logs and WARC files.</p>
<div id="log-parser" class="section level4">
<h4><span class="header-section-number">10.2.1.1</span> Log Parser</h4>
<p>It is common to use Spark to analize log files; for instance, logs that track download data from Amazon S3. To parse logs, the <code>webreadr</code> package can simplify this process by providing support to load logs stored as: Amazon S3, Squid and the Common log format. An Amazon S3 log looks as follows:</p>
<pre><code>#Version: 1.0
#Fields: date time x-edge-location sc-bytes c-ip cs-method cs(Host) cs-uri-stem
  sc-status cs(Referer) cs(User-Agent) cs-uri-query cs(Cookie) x-edge-result-type
  x-edge-request-id x-host-header cs-protocol cs-bytes time-taken 
  
2014-05-23  01:13:11    FRA2    182 192.0.2.10  GET d111111abcdef8.cloudfront.net
  /view/my/file.html    200 www.displaymyfiles.com  Mozilla/4.0%20
  (compatible;%20MSIE%205.0b1;%20Mac_PowerPC)   -   zip=98101   RefreshHit
  MRVMF7KydIvxMWfJIglgwHQwZsbG2IhRJ07sn9AkKUFSHS9EXAMPLE==
  d111111abcdef8.cloudfront.net http    -   0.001</code></pre>
<p>Which can be parsed easily with <code>read_aws()</code> as follows:</p>
<pre class="sourceCode r"><code class="sourceCode r">aws_log &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;extdata/log.aws&quot;</span>, <span class="dt">package =</span> <span class="st">&quot;webreadr&quot;</span>)
webreadr<span class="op">::</span><span class="kw">read_aws</span>(aws_log)</code></pre>
<pre><code># A tibble: 2 x 18
  date                edge_location bytes_sent ip_address http_method host  path 
  &lt;dttm&gt;              &lt;chr&gt;              &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;
1 2014-05-23 01:13:11 FRA2                 182 192.0.2.10 GET         d111… /vie…
2 2014-05-23 01:13:12 LAX1             2390282 192.0.2.2… GET         d111… /sou…
# ... with 11 more variables: status_code &lt;int&gt;, referer &lt;chr&gt;, user_agent &lt;chr&gt;,
#   query &lt;chr&gt;, cookie &lt;chr&gt;, result_type &lt;chr&gt;, request_id &lt;chr&gt;, host_header &lt;chr&gt;,
#   protocol &lt;chr&gt;, bytes_received &lt;chr&gt;, time_elapsed &lt;dbl&gt;</code></pre>
<p>To scale this operations, we can make use of <code>read_aws()</code> using <code>spark_apply()</code>:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">spark_read_text</span>(sc, <span class="st">&quot;logs&quot;</span>, aws_log, <span class="dt">overwrite =</span> <span class="ot">TRUE</span>, <span class="dt">whole =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spark_apply</span>(<span class="op">~</span>webreadr<span class="op">::</span><span class="kw">read_aws</span>(.x<span class="op">$</span>contents))</code></pre>
<pre><code># Source: spark&lt;?&gt; [?? x 18]
  date                edge_location bytes_sent ip_address http_method host  path 
* &lt;dttm&gt;              &lt;chr&gt;              &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;
1 2014-05-23 01:13:11 FRA2                 182 192.0.2.10 GET         d111… /vie…
2 2014-05-23 01:13:12 LAX1             2390282 192.0.2.2… GET         d111… /sou…
# ... with 11 more variables: status_code &lt;int&gt;, referer &lt;chr&gt;, user_agent &lt;chr&gt;,
#   query &lt;chr&gt;, cookie &lt;chr&gt;, result_type &lt;chr&gt;, request_id &lt;chr&gt;,
#   host_header &lt;chr&gt;, protocol &lt;chr&gt;, bytes_received &lt;chr&gt;, time_elapsed &lt;dbl&gt;</code></pre>
<p>The code between plain R and <code>spark_apply()</code> is similar; howerver, when using <code>spark_apply()</code> logs are parsed in parallel across all the worker nodes available in your cluster.</p>
</div>
<div id="warc-parser" class="section level4">
<h4><span class="header-section-number">10.2.1.2</span> WARC Parser</h4>
<p>The Common Crawl project builds and maintains an open repository of web crawl data that can be accessed and analyzed by anyone. You can use their petabytes of data to analyze the contents of the web without having to manually download each web page, which would be a time consuming and error-prone process.</p>
<p>The challenge with WARC files is that multiple records are embedded within the same text file, conceptually, a WARC file looks as follows:</p>
<pre><code>WARC/1.0
&lt;html&gt;...&lt;/html&gt;
WARC/1.0
&lt;html&gt;...&lt;/html&gt;</code></pre>
<p>We could read all those lines using <code>spark_read_text()</code>; however, they won’t be grouped by web page which is required to analyze properly the contents. Using the <code>warc</code> package, we can easily parse these files in R running,</p>
<pre class="sourceCode r"><code class="sourceCode r">warc_example &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;samples/sample.warc.gz&quot;</span>, <span class="dt">package =</span> <span class="st">&quot;warc&quot;</span>)
<span class="kw">as_tibble</span>(warc<span class="op">::</span><span class="kw">read_warc</span>(warc_example))</code></pre>
<pre><code># A tibble: 17 x 1
   content                                                                              
   &lt;fct&gt;                                                                              
 1 &quot;WARC/1.0\nWARC-Type: warcinfo\nWARC-Date: 2016-12-13T03:16:04Z\nWARC-Record-ID: &lt;ur…
 2 &quot;WARC/1.0\nWARC-Type: request\nWARC-Date: 2016-12-11T14:00:57Z\nWARC-Record-ID: &lt;urn…
 3 &quot;WARC/1.0\nWARC-Type: response\nWARC-Date: 2016-12-11T14:00:57Z\nWARC-Record-ID: &lt;ur…
 4 &quot;WARC/1.0\nWARC-Type: metadata\nWARC-Date: 2016-12-11T14:00:57Z\nWARC-Record-ID: &lt;ur…
 5 &quot;WARC/1.0\nWARC-Type: request\nWARC-Date: 2016-12-11T14:08:53Z\nWARC-Record-ID: &lt;urn…</code></pre>
<p>Suppose that you were interested in understanding the language web pages are written in. Since the <code>warc</code> package can also extract specific lines efficiently, you could use this feature to extract specific html tags that describe the language a page was written in:</p>
<pre class="sourceCode r"><code class="sourceCode r">warc<span class="op">::</span><span class="kw">read_warc</span>(warc_example, <span class="dt">line_filter =</span> <span class="st">&quot;&lt;meta http-equiv=</span><span class="ch">\&quot;</span><span class="st">Content-Language</span><span class="ch">\&quot;</span><span class="st">&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">transmute</span>(<span class="dt">language =</span> <span class="kw">gsub</span>(<span class="st">&quot;.*content=</span><span class="ch">\&quot;</span><span class="st">|</span><span class="ch">\&quot;</span><span class="st">.*&quot;</span>, <span class="st">&quot;&quot;</span>, content)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(language) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarise</span>(<span class="dt">count =</span> <span class="kw">n</span>())</code></pre>
<pre><code># A tibble: 1 x 2
  language count
  &lt;chr&gt;    &lt;int&gt;
1 ru-RU        5</code></pre>
<p>Now, in order to use this in Spark we can simply run:</p>
<pre class="sourceCode r"><code class="sourceCode r">paths &lt;-<span class="st"> </span><span class="kw">readLines</span>(<span class="kw">system.file</span>(<span class="st">&quot;samples/warc.paths&quot;</span>, <span class="dt">package =</span> <span class="st">&quot;warc&quot;</span>))
paths_tbl &lt;-<span class="st"> </span><span class="kw">copy_to</span>(sc, paths, <span class="dt">repartition =</span> <span class="kw">length</span>(paths))

paths_tbl <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">head</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spark_apply</span>(<span class="cf">function</span>(entry) {
    path &lt;-<span class="st"> </span><span class="kw">sub</span>(<span class="st">&quot;s3n://commoncrawl/&quot;</span>, <span class="st">&quot;https://commoncrawl.s3.amazonaws.com/&quot;</span>, entry[<span class="dv">1</span>,])
    <span class="kw">download.file</span>(<span class="dt">url =</span> path, <span class="dt">destfile =</span> temp_warc)
    warc<span class="op">::</span><span class="kw">read_warc</span>(path)
  })</code></pre>
<p>You can remove <code>head()</code> to process several gigabytes of WARC files; however, this would require a proper Spark cluster.</p>
</div>
</div>
<div id="partitioned-modeling" class="section level3">
<h3><span class="header-section-number">10.2.2</span> Partitioned Modeling</h3>
<p>There are many modeling packages available in R that can also be run at scale by partitioning the data into manegable groups that do fit in the resources of a single machine.</p>
<p>For instance, suppose that you have a 1TB dataset for sales data across multiple cities and you are tasked with creating sales predictions over each city. For this case, you can then considered partitioning the original dataset per city say, into 10GB of data per city, which could be managed by a single compute instance. For this kind of partitionable dataset, you can also consider using <code>spark_apply()</code> by training each model over each city.</p>
<p>As a simple example of partitoned modeling, we can run a linear regression using the <code>iris</code> dataset partiotioning by Species:</p>
<pre class="sourceCode r"><code class="sourceCode r">iris_tbl &lt;-<span class="st"> </span><span class="kw">sdf_copy_to</span>(sc, iris)

iris_tbl <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spark_apply</span>(nrow, <span class="dt">group_by =</span> <span class="st">&quot;Species&quot;</span>)</code></pre>
<pre><code>## # Source:   table&lt;sparklyr_tmp_378c1b8155f3&gt; [?? x 2]
## # Database: spark_connection
##      Species Sepal_Length
##        &lt;chr&gt;        &lt;int&gt;
## 1 versicolor           50
## 2  virginica           50
## 3     setosa           50</code></pre>
<p>Then you can run a linear regression over each species using <code>spark_apply()</code>:</p>
<pre class="sourceCode r"><code class="sourceCode r">iris_tbl <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spark_apply</span>(
    <span class="cf">function</span>(e) <span class="kw">summary</span>(<span class="kw">lm</span>(Petal_Length <span class="op">~</span><span class="st"> </span>Petal_Width, e))<span class="op">$</span>r.squared,
    <span class="dt">names =</span> <span class="st">&quot;r.squared&quot;</span>,
    <span class="dt">group_by =</span> <span class="st">&quot;Species&quot;</span>)</code></pre>
<pre><code>## # Source:   table&lt;sparklyr_tmp_378c30e6155&gt; [?? x 2]
## # Database: spark_connection
##      Species r.squared
##        &lt;chr&gt;     &lt;dbl&gt;
## 1 versicolor 0.6188467
## 2  virginica 0.1037537
## 3     setosa 0.1099785</code></pre>
</div>
<div id="distributed-grid-search" class="section level3">
<h3><span class="header-section-number">10.2.3</span> Grid Search</h3>
<p>Many R packages provide models that make require defining multiple input parameters, when the value of the input parameters is not known, we can use grid search across a distributed cluster of machines to find the optimal parameter combination. For example, we can define a grid of parameters to optimize decision tree models as follows:</p>
<pre class="sourceCode r"><code class="sourceCode r">grid &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">minsplit =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">5</span>, <span class="dv">10</span>), <span class="dt">maxdepth =</span> <span class="kw">c</span>(<span class="dv">1</span>, <span class="dv">3</span>, <span class="dv">8</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span>purrr<span class="op">:::</span><span class="kw">cross_df</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">sdf_copy_to</span>(sc, ., <span class="dt">repartition =</span> <span class="dv">9</span>)
grid</code></pre>
<pre><code># Source: spark&lt;sparklyr_1064056dd37da&gt; [?? x 2]
  minsplit maxdepth
     &lt;dbl&gt;    &lt;dbl&gt;
1        2        1
2        5        1
3       10        1
4        2        3
5        5        3
6       10        3
7        2        8
8        5        8
9       10        8</code></pre>
<p>The grid dataset was copied with <code>repartition = 9</code> to ensure that each partition is contained in one machine since the grid has also nine rows. Now, assuming that the original data set fits in every machine, we can distribute this dataset to many machines and perform parameter search to find the model that best fits this data.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">spark_apply</span>(
  grid,
  <span class="cf">function</span>(grid, cars) {
    model &lt;-<span class="st"> </span>rpart<span class="op">::</span><span class="kw">rpart</span>(
      am <span class="op">~</span><span class="st"> </span>hp <span class="op">+</span><span class="st"> </span>mpg,
      <span class="dt">data =</span> cars,
      <span class="dt">control =</span> rpart<span class="op">::</span><span class="kw">rpart.control</span>(<span class="dt">minsplit =</span> grid<span class="op">$</span>minsplit, <span class="dt">maxdepth =</span> grid<span class="op">$</span>maxdepth)
    )
    dplyr<span class="op">::</span><span class="kw">mutate</span>(
      grid,
      <span class="dt">accuracy =</span> <span class="kw">mean</span>(<span class="kw">round</span>(<span class="kw">predict</span>(model, dplyr<span class="op">::</span><span class="kw">select</span>(cars, <span class="op">-</span>am))) <span class="op">==</span><span class="st"> </span>cars<span class="op">$</span>am)
    )
  },
  <span class="dt">context =</span> mtcars)</code></pre>
<pre><code># Source: spark&lt;?&gt; [?? x 3]
  minsplit maxdepth accuracy
     &lt;dbl&gt;    &lt;dbl&gt;    &lt;dbl&gt;
1        2        1    0.812
2        5        1    0.812
3       10        1    0.812
4        2        3    0.938
5        5        3    0.938
6       10        3    0.812
7        2        8    1    
8        5        8    0.938
9       10        8    0.812</code></pre>
<p>For this particular model, <code>minsplit = 2</code> and <code>maxdepth = 8</code> produces the most accurate results. You can now use this specific parameter combination to properly a train model.</p>
</div>
<div id="web-apis" class="section level3">
<h3><span class="header-section-number">10.2.4</span> Web APIs</h3>
<p>A web API is a program that can fo something useful through an web interface that other programs can reuse; for instance, services like Twitter provide web APIs that allow you to automate reading tweets from a program written in R and other programming languages. You can make use of we APIs using <code>spark_apply()</code> by sending programatic requests to external services using R code.</p>
<p>For example, Google provides a web API to label images using deep learning techniques; you can make use of this API from R, but for larger datasets, you would need to access their APIs from Spark. You can use Spark to prepare data to be consumed by a webAPI, then use <code>spark_apply()</code> to perform this call and process all the incoming results back in Spark.</p>
<p>The following example makes use of the <code>googleAuthR</code> package to authenticate to Google Cloud, the <code>RoogleVision</code> package to perform labeling over the Google Vision API, and <code>spark_apply()</code> to interoperate between Spark and Google’s deep learning service. If you want to run the following example you will need to disconnect first from Spark and download your <code>cloudml.json</code> from the Google developer portal.</p>
<pre class="sourceCode r"><code class="sourceCode r">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(
  <span class="dt">master =</span> <span class="st">&quot;local&quot;</span>,
  <span class="dt">config =</span> <span class="kw">list</span>(<span class="dt">sparklyr.shell.files =</span> <span class="st">&quot;cloudml.json&quot;</span>))

images &lt;-<span class="st"> </span><span class="kw">copy_to</span>(sc, <span class="kw">data.frame</span>(
  <span class="dt">image =</span> <span class="st">&quot;http://pbs.twimg.com/media/DwzcM88XgAINkg-.jpg&quot;</span>
))

<span class="kw">spark_apply</span>(images, <span class="cf">function</span>(df) {
  googleAuthR<span class="op">::</span><span class="kw">gar_auth_service</span>(
    <span class="dt">scope =</span> <span class="st">&quot;https://www.googleapis.com/auth/cloud-platform&quot;</span>,
    <span class="dt">json_file =</span> <span class="st">&quot;cloudml.json&quot;</span>)
  
  RoogleVision<span class="op">::</span><span class="kw">getGoogleVisionResponse</span>(
    df<span class="op">$</span>image,
    <span class="dt">download =</span> <span class="ot">FALSE</span>)
})</code></pre>
<pre><code># Source: spark&lt;?&gt; [?? x 4]
  mid       description score topicality
  &lt;chr&gt;     &lt;chr&gt;       &lt;dbl&gt;      &lt;dbl&gt;
1 /m/04rky  Mammal      0.973      0.973
2 /m/0bt9lr Dog         0.958      0.958
3 /m/01z5f  Canidae     0.956      0.956
4 /m/0kpmf  Dog breed   0.909      0.909
5 /m/05mqq3 Snout       0.891      0.891</code></pre>
<p>In order to successfully run a large distirbuted computation over a Web API, the Web API would have to be able to scale to support the load from all the Spark executors. One can trust that major service providers are likely to support all the request incoming from your cluster. However, when calling internal Web APIs, make sure the API can handle the load. Also, when using third-party services, consider the cost of calling their API across all the executors in your cluster to avoid potentially expensive and unexpected charges.</p>
<p>These covers some of the common use cases for <code>spark_apply()</code>, but you are certainly welcome to find other use cases of <code>spark_apply()</code> for your particular needs.</p>
</div>
</div>
<div id="partitions" class="section level2">
<h2><span class="header-section-number">10.3</span> Partitions</h2>
<p>As mentioned in the <a href="tuning.html#tuning">Tuning</a> chapter, Spark operates over partitioned data. <code>spark_apply()</code> receives each of these partitions as a data frame and allows you to perform any transformation.</p>
<p>To help us understand how partitions are represented in <code>spark_apply()</code>, consider the following code. Should we expect the output to be the total number of rows?</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sdf_len</span>(sc, <span class="dv">10</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spark_apply</span>(<span class="op">~</span><span class="kw">nrow</span>(.x))</code></pre>
<pre><code># Source: spark&lt;?&gt; [?? x 1]
  result
*  &lt;int&gt;
1      5
2      5</code></pre>
<p>As you can see from the results, the general the answer is no. We should not expect <code>spark_apply()</code> to operate over a single partition; so let’s find out how many partitions <code>sdf_len(sc, 10)</code> contains:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sdf_len</span>(sc, <span class="dv">10</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">sdf_num_partitions</span>()</code></pre>
<pre><code>[1] 2</code></pre>
<p>Which explains why counting rows through <code>nrow()</code> under <code>spark_apply()</code> retrieves two counts of five rows. It retrieves the count of rows for each partition, 5 rows over each partitoions; not ten rows as you might have expected.</p>
<p>For this particular example, we could further aggregate these partitions by repartitioning and then adding up, this would resemple a simple MapReduce operation using <code>spark_apply()</code>:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sdf_len</span>(sc, <span class="dv">10</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spark_apply</span>(<span class="op">~</span><span class="kw">nrow</span>(.x)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">sdf_repartition</span>(<span class="dv">1</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spark_apply</span>(<span class="op">~</span><span class="kw">sum</span>(.x))</code></pre>
<pre><code># Source: spark&lt;?&gt; [?? x 1]
  result
*  &lt;int&gt;
1     10</code></pre>
<p>It was the intent of this section is to make the reader aware of partitions while using <code>spark_apply()</code>, the next section presents <code>group_by</code> as a way to control partitions.</p>
</div>
<div id="grouping" class="section level2">
<h2><span class="header-section-number">10.4</span> Grouping</h2>
<p>When using <code>spark_apply()</code>, we can request explicit partitions from Spark. For instance, if we had to process numbers less than four in one partition and the remaining ones in a second partition, we could create this groups explicitly and then request <code>spark_apply()</code> to use this group:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sdf_len</span>(sc, <span class="dv">10</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">transmute</span>(<span class="dt">groups =</span> id <span class="op">&lt;</span><span class="st"> </span><span class="dv">4</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spark_apply</span>(<span class="op">~</span><span class="kw">nrow</span>(.x), <span class="dt">group_by =</span> <span class="st">&quot;groups&quot;</span>)</code></pre>
<pre><code># Source: spark&lt;?&gt; [?? x 2]
  groups result
* &lt;lgl&gt;   &lt;int&gt;
1 TRUE        3
2 FALSE       7</code></pre>
<p>Notice that <code>spark_apply()</code> is still processing two partitions, but in this case, we expect these partitions since you explicitly requested them in <code>spark_apply()</code>; therefore, you can safely interpret the results as “there are three integers less than four”.</p>
<p>The takeaway from this section is to always consider partitions when dealing with <code>spark_apply()</code>. Next, we will zoom inside <code>spark_apply()</code> to understand how columns are interpreted.</p>
</div>
<div id="columns" class="section level2">
<h2><span class="header-section-number">10.5</span> Columns</h2>
<p>By default, <code>spark_apply()</code>, will inspect the data frame being produced to find out column names and types automatically, for example:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sdf_len</span>(sc, <span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">spark_apply</span>(<span class="op">~</span><span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">numbers =</span> <span class="dv">1</span>, <span class="dt">names =</span> <span class="st">&quot;abc&quot;</span>))</code></pre>
<pre><code># Source: spark&lt;?&gt; [?? x 2]
  numbers names
*   &lt;dbl&gt; &lt;chr&gt;
1       1 abc </code></pre>
<p>However, this is inneficient since <code>spark_apply()</code> needs to run twice. First to find columns by computing <code>spark_apply()</code> against a subset of all the data, and then to compute the actual desired values.</p>
<p>To improve performance, the columns can be specified explicitly through the <code>columns</code> parameters. The <code>columns</code> expects a named list of types expected in the resulting data frame.</p>
<p>We can then rewrite the previous example to specify the correct type for the <code>numbers</code> column:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sdf_len</span>(sc, <span class="dv">1</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spark_apply</span>(
    <span class="op">~</span><span class="st"> </span><span class="kw">data.frame</span>(<span class="dt">numbers =</span> <span class="dv">1</span>, <span class="dt">names =</span> <span class="st">&quot;abc&quot;</span>),
    <span class="dt">columns =</span> <span class="kw">list</span>(<span class="dt">numbers =</span> <span class="st">&quot;double&quot;</span>, <span class="dt">names =</span> <span class="st">&quot;character&quot;</span>))</code></pre>
<pre><code># Source: spark&lt;?&gt; [?? x 2]
  numbers names
*   &lt;dbl&gt; &lt;chr&gt;
1       1 abc </code></pre>
<p>In this section and the previous one, we presented how rows and columns interact with <code>spark_apply()</code>. The following section will allow us to make use of contextual information that is sometimes required when processing distributed datasets.</p>
</div>
<div id="context" class="section level2">
<h2><span class="header-section-number">10.6</span> Context</h2>
<p>In order to process partitions using <code>spark_apply()</code>, you might need to include auxility data that is small-enough to fit in each node. This was the case in the <a href="distributed-grid-search">Grid Search</a> use case, where the dataset was passed to all partitions and remained unpartitioned itself.</p>
<p>We can modify the initial <code>f(x) = 10 * x</code> example in this chapter to allow us to customize the multiplier, it was originally set to <code>10</code> but we can make this parameter configurable by specifying it as the <code>context</code> parameter.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sdf_len</span>(sc, <span class="dv">4</span>, <span class="dt">repartition =</span> <span class="dv">2</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spark_apply</span>(
    <span class="cf">function</span>(data, context) context <span class="op">*</span><span class="st"> </span>data,
    <span class="dt">context =</span> <span class="dv">100</span>
  )</code></pre>
<pre><code># Source: spark&lt;?&gt; [?? x 1]
     id
  &lt;dbl&gt;
1   100
2   200
3   300
4   400</code></pre>
<p>Figure <a href="distributed.html#fig:distributed-times-context">10.3</a> illustrates this example conceptually. Notice that the data partitions are still variable; however, the contextual parameter is distributed to all the nodes.</p>
<div class="figure" style="text-align: center"><span id="fig:distributed-times-context"></span>
<img src="images/distributed-r-context.png" alt="Map Operation when Multiplying with Context" width="2556" />
<p class="caption">
FIGURE 10.3: Map Operation when Multiplying with Context
</p>
</div>
<p>The grid search example used this parameter to pass a data frame to each worker node; however, since the context parameter is serialized as an R object, it can contain anything. For instance, if you need to pass multiple values – or even multiple datasets – you can pass a list with values. The following example defines a <code>f(x) = m * x + b</code> function and runs <code>m = 10</code> and <code>b = 2</code>:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sdf_len</span>(sc, <span class="dv">4</span>, <span class="dt">repartition =</span> <span class="dv">2</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spark_apply</span>(
    <span class="cf">function</span>(data, context) context<span class="op">$</span>m <span class="op">*</span><span class="st"> </span>data <span class="op">+</span><span class="st"> </span>context<span class="op">$</span>b,
    <span class="dt">context =</span> <span class="kw">list</span>(
      <span class="dt">b =</span> <span class="dv">2</span>,
      <span class="dt">m =</span> <span class="dv">10</span>
    )
  )</code></pre>
<pre><code># Source: spark&lt;?&gt; [?? x 1]
     id
  &lt;dbl&gt;
1    12
2    22
3    32
4    42</code></pre>
<p>Up to this point, you’ve learned all the functionality available in <code>spark_apply()</code> but we still need to present important considerations that you’ll have to consider while creating custom transformations.</p>
</div>
<div id="packages" class="section level2">
<h2><span class="header-section-number">10.7</span> Packages</h2>
<p>With <code>spark_apply()</code> you can use any R package inside Spark. For instance, you can use the broom package to create a tidy data frame from linear regression output.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">spark_apply</span>(
  iris_tbl,
  <span class="cf">function</span>(e) broom<span class="op">::</span><span class="kw">tidy</span>(<span class="kw">lm</span>(Petal_Length <span class="op">~</span><span class="st"> </span>Petal_Width, e)),
  <span class="dt">names =</span> <span class="kw">c</span>(<span class="st">&quot;term&quot;</span>, <span class="st">&quot;estimate&quot;</span>, <span class="st">&quot;std.error&quot;</span>, <span class="st">&quot;statistic&quot;</span>, <span class="st">&quot;p.value&quot;</span>),
  <span class="dt">group_by =</span> <span class="st">&quot;Species&quot;</span>)</code></pre>
<pre><code>## # Source:   table&lt;sparklyr_tmp_378c5502500b&gt; [?? x 6]
## # Database: spark_connection
##      Species        term  estimate std.error statistic      p.value
##        &lt;chr&gt;       &lt;chr&gt;     &lt;dbl&gt;     &lt;dbl&gt;     &lt;dbl&gt;        &lt;dbl&gt;
## 1 versicolor (Intercept) 1.7812754 0.2838234  6.276000 9.484134e-08
## 2 versicolor Petal_Width 1.8693247 0.2117495  8.827999 1.271916e-11
## 3  virginica (Intercept) 4.2406526 0.5612870  7.555230 1.041600e-09
## 4  virginica Petal_Width 0.6472593 0.2745804  2.357267 2.253577e-02
## 5     setosa (Intercept) 1.3275634 0.0599594 22.141037 7.676120e-27
## 6     setosa Petal_Width 0.5464903 0.2243924  2.435422 1.863892e-02</code></pre>
<p>The first time you call <code>spark_apply()</code> all of the contents in your local <code>.libPaths()</code>, which contains all R packages, will be copied into each Spark worker node. Packages will only be copied once and will persist as long as the connection remains open. It’s not uncommon for R libraries to be several gigabytes in size, so be prepared for a one-time tax while the R packages are copied over to your Spark cluster. You can disable package distribution by setting <code>packages = FALSE</code>.</p>
<p><strong>Note:</strong> R packages are not copied in local mode because the packages already exist on the local system.</p>
</div>
<div id="requirements" class="section level2">
<h2><span class="header-section-number">10.8</span> Requirements</h2>
<p>The R Runtime is expected to be pre-installed in <strong>every</strong> node in the cluster, this is a requirement specific to <code>spark_apply()</code>.</p>
<p>Failure to install R in every node will trigger a <code>Cannot run program, no such file or directory</code> error while attempting to use <code>spark_apply()</code>.</p>
<p>Contact your cluster administrator to consider making the R runtime available throughout the entire cluster. If R is already installed, you can specify the installation path to use using the <code>spark.r.command</code> configuration setting, as in:</p>
<pre class="sourceCode r"><code class="sourceCode r">config &lt;-<span class="st"> </span><span class="kw">spark_config</span>()
config[<span class="st">&quot;spark.r.command&quot;</span>] &lt;-<span class="st"> &quot;&lt;path-to-r-version&gt;&quot;</span>

sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>, <span class="dt">config =</span> config)
<span class="kw">sdf_len</span>(sc, <span class="dv">10</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">spark_apply</span>(<span class="cf">function</span>(e) e)</code></pre>
<p>A Homogeneous Cluster is required since the driver node distributes, and potentially compiles, packages to the workers. For instance, the driver and workers must have the same processor architecture, system libraries, etc.</p>
</div>
<div id="limitations" class="section level2">
<h2><span class="header-section-number">10.9</span> Limitations</h2>
<div id="functions" class="section level3">
<h3><span class="header-section-number">10.9.1</span> Functions</h3>
<p>The function passed to <code>spark_apply()</code> are serialized using <code>serialize()</code>, which is described as “a simple low-level interface for serializing to connections.”. One of the current limitations of serialize is that it wont serialize objects being referenced outside of it’s environment. For instance, the following function will error out since the closures references external_value:</p>
<pre class="sourceCode r"><code class="sourceCode r">external_value &lt;-<span class="st"> </span><span class="dv">1</span>
<span class="kw">spark_apply</span>(iris_tbl, <span class="cf">function</span>(e) e <span class="op">+</span><span class="st"> </span>external_value)</code></pre>
<p>As a workarounds to this limitation, you can add the functions your clousure needs into the <code>context</code> followed by assigning the functions into the global environment:</p>
<pre class="sourceCode r"><code class="sourceCode r">func_a &lt;-<span class="st"> </span><span class="cf">function</span>() <span class="dv">40</span>
func_b &lt;-<span class="st"> </span><span class="cf">function</span>() <span class="kw">func_a</span>() <span class="op">+</span><span class="st"> </span><span class="dv">1</span>
func_c &lt;-<span class="st"> </span><span class="cf">function</span>() <span class="kw">func_b</span>() <span class="op">+</span><span class="st"> </span><span class="dv">1</span>

<span class="kw">sdf_len</span>(sc, <span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">spark_apply</span>(<span class="cf">function</span>(df, context) {
  <span class="cf">for</span> (name <span class="cf">in</span> <span class="kw">names</span>(context)) <span class="kw">assign</span>(name, context[[name]], <span class="dt">envir =</span> .GlobalEnv)
  <span class="kw">func_c</span>()
}, <span class="dt">context =</span> <span class="kw">list</span>(
  <span class="dt">func_a =</span> func_a,
  <span class="dt">func_b =</span> func_b,
  <span class="dt">func_c =</span> func_c
))</code></pre>
<pre><code># Source: spark&lt;?&gt; [?? x 1]
  result
   &lt;dbl&gt;
1     42</code></pre>
<p>When this is not feasible, you can also create your own R package with the functionality you need and then use your package in <code>spark_apply()</code>.</p>
</div>
<div id="livy" class="section level3">
<h3><span class="header-section-number">10.9.2</span> Livy</h3>
<p>Currently, Livy connections do not support distributing packages since the client machine where the libraries are precompiled might not have the same processor architecture, nor operating systems that the cluster machines.</p>
</div>
<div id="grouping-1" class="section level3">
<h3><span class="header-section-number">10.9.3</span> Grouping</h3>
<p>While performing computations over groups, spark_apply() will provide partitions over the selected column; however, this implies that each partition can fit into a worker node, if this is not the case an exception will be thrown. To perform operations over groups that exceed the resources of a single node, one can consider partitioning to smaller units or use dplyr::do which is currently optimized for large partitions.</p>
</div>
<div id="packages-1" class="section level3">
<h3><span class="header-section-number">10.9.4</span> Packages</h3>
<p>Since packages are copied only once for the duration of the spark_connect() connection, installing additional packages is not supported while the connection is active. Therefore, if a new package needs to be installed, spark_disconnect() the connection, modify packages and reconnect.</p>
</div>
</div>
<div id="troubleshooting-2" class="section level2">
<h2><span class="header-section-number">10.10</span> Troubleshooting</h2>
<p>A custom transformation can fail for many reasons, to learn how to troubleshoot errors effectevely, lets simulate a problem by triggering an errors ourselves:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sdf_len</span>(sc, <span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">spark_apply</span>(<span class="op">~</span><span class="kw">stop</span>(<span class="st">&quot;force an error&quot;</span>))</code></pre>
<pre><code>Error in force(code) : 
  sparklyr worker rscript failure, check worker logs for details
    Log: /var/folders/ks/wm_bx4cn70s6h0r5vgqpsldm0000gn/T//Rtmpob83LD/file2aac1a6188_spark.log

---- Output Log ----
19/03/11 14:12:24 INFO sparklyr: Worker (8057) completed wait using lock for RScript</code></pre>
<p>Notice that the error message points out to inspect the logs. When running in local mode, you can simply run:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">spark_log</span>(sc, <span class="dt">filter =</span> <span class="st">&quot;terminated unexpectedly&quot;</span>)</code></pre>
<pre><code>19/03/11 14:12:24 ERROR sparklyr: RScript (8057) terminated unexpectedly: force an error </code></pre>
<p>Which points out to the artifial <code>stop("force an error")</code> error we introduced ourselves. However, if you are not working in local mode, you will have to retrieve the worker logs from your cluster manager. Since this can be cumbersome, one alternative is to rerun <code>spark_apply()</code> but return the error message yourself:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sdf_len</span>(sc, <span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">spark_apply</span>(<span class="op">~</span><span class="kw">tryCatch</span>(
    <span class="kw">stop</span>(<span class="st">&quot;force an error&quot;</span>),
    <span class="dt">error =</span> <span class="cf">function</span>(e) e<span class="op">$</span>message
))</code></pre>
<pre><code># Source: spark&lt;?&gt; [?? x 1]
  result        
  &lt;chr&gt;         
1 force an error</code></pre>
<p>There are a few other more advanced troubleshooting techniquest applicable to <code>spark_apply()</code>, the following sections present these techniques in-order; meaning, you should try to troubleshoot using worker logs first, followed by identifying partitioning errors and finally, attempting to debug a worker node.</p>
<div id="worker-logs" class="section level3">
<h3><span class="header-section-number">10.10.1</span> Worker Logs</h3>
<p>Whenever <code>spark_apply()</code> is executed, information regarding execution is written over each worker node. You can use this log to write custom messages o help you diagnose and fine-tune your code.</p>
<p>For instance, suppose that you don’t know what the first column name of <code>df</code> is, we can write a custom log message executed from the worker nodes using <code>worker_log()</code> as follows:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sdf_len</span>(sc, <span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">spark_apply</span>(<span class="cf">function</span>(df) {
  <span class="kw">worker_log</span>(<span class="st">&quot;the first column in the data frame is named &quot;</span>, <span class="kw">names</span>(df)[[<span class="dv">1</span>]])
  df
})</code></pre>
<pre><code># Source: spark&lt;?&gt; [?? x 1]
     id
* &lt;int&gt;
1     1</code></pre>
<p>When running locally, we can filter the log entries for the worker as follows:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">spark_log</span>(sc, <span class="dt">filter =</span> <span class="st">&quot;sparklyr: RScript&quot;</span>)</code></pre>
<pre><code>18/12/18 11:33:47 INFO sparklyr: RScript (3513) the first column in the dataframe is named id 
18/12/18 11:33:47 INFO sparklyr: RScript (3513) computed closure 
18/12/18 11:33:47 INFO sparklyr: RScript (3513) updating 1 rows 
18/12/18 11:33:47 INFO sparklyr: RScript (3513) updated 1 rows 
18/12/18 11:33:47 INFO sparklyr: RScript (3513) finished apply 
18/12/18 11:33:47 INFO sparklyr: RScript (3513) finished </code></pre>
<p>Notice that the logs show out custom log entry showing that <code>id</code> is the name of the first column in the given data frame.</p>
<p>This functionality is useful when troubleshooting errors, for instance, if we force an error using the <code>stop()</code> function:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sdf_len</span>(sc, <span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">spark_apply</span>(<span class="cf">function</span>(df) {
  <span class="kw">stop</span>(<span class="st">&quot;force an error&quot;</span>)
})</code></pre>
<p>You will get an error similar to,</p>
<pre><code> Error in force(code) : 
  sparklyr worker rscript failure, check worker logs for details</code></pre>
<p>As suggested in the error, we can look in the worker logs for the specific errors as follows:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">spark_log</span>(sc)</code></pre>
<p>This will show an entry containing the error and the callstack:</p>
<pre><code>18/12/18 11:26:47 INFO sparklyr: RScript (1860) computing closure 
18/12/18 11:26:47 ERROR sparklyr: RScript (1860) terminated unexpectedly: force an error 
18/12/18 11:26:47 ERROR sparklyr: RScript (1860) collected callstack: 
11: stop(&quot;force and error&quot;)
10: (function (df) 
{
    stop(&quot;force and error&quot;)
})(structure(list(id = 1L), class = &quot;data.frame&quot;, row.names = c(NA, 
-1L)))</code></pre>
<p>Notice that, spark_log(sc) only retrieves the worker logs when using local clusters, when running in proper clusters with multiple machines, you will have to use the tools and user interface provided by the cluster manager to find these log entries.</p>
</div>
<div id="partition-timeouts" class="section level3">
<h3><span class="header-section-number">10.10.2</span> Partition Timeouts</h3>
<p>When running with several hundred executors, it becomes more likely that some tasks will hang indefinetely. You might be in this situation if most of the tasks in your job complete successfully, but a handful of them are still running and do not fail nor succeed.</p>
<p>Suppose that we need to calculate the size of many web pages, we could use <code>spark_apply()</code> with something similar to:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sdf_len</span>(sc, <span class="dv">3</span>, <span class="dt">repartition =</span> <span class="dv">3</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spark_apply</span>(<span class="op">~</span><span class="st"> </span><span class="kw">download.file</span>(<span class="st">&quot;https://google.com&quot;</span>, <span class="st">&quot;index.html&quot;</span>) <span class="op">+</span><span class="st"> </span><span class="kw">file.size</span>(<span class="st">&quot;index.html&quot;</span>))</code></pre>
<p>Some web pages might not exist or take too long to download. In which case, most tasks will succeed, but a few will hang. To prevent a few tasks from blocking all computations, you can use the <code>spark.speculation</code> Spark setting. When this setting is enabled, once 75% of all tasks succeed, Spark will look for tasks taking longer than the median task execution time and retry this tasks. You can use the <code>spark.speculation.multiplier</code> setting to configure the time multiplier used to consider a task slow.</p>
<p>Therefore, for the previous example, you can consider configuring Spark to retry tasks that take two times longer than the median as follows:</p>
<pre class="sourceCode r"><code class="sourceCode r">config &lt;-<span class="st"> </span><span class="kw">spark_config</span>()
config[<span class="st">&quot;spark.speculation&quot;</span>] &lt;-<span class="st"> </span><span class="ot">TRUE</span>
config[<span class="st">&quot;spark.speculation.multiplier&quot;</span>] &lt;-<span class="st"> </span><span class="dv">4</span></code></pre>
</div>
<div id="partition-errors" class="section level3">
<h3><span class="header-section-number">10.10.3</span> Partition Errors</h3>
<p>If a particular partition fails, you can detect the broken partition by computing a digest, and then retrieving that particular partition as follows:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sdf_len</span>(sc, <span class="dv">3</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">spark_apply</span>(<span class="cf">function</span>(x) {
    <span class="kw">worker_log</span>(<span class="st">&quot;processing &quot;</span>, digest<span class="op">::</span><span class="kw">digest</span>(x), <span class="st">&quot; partition&quot;</span>)
    <span class="co"># your code</span>
})</code></pre>
<p>This will add an entry similar to:</p>
<pre><code>18/11/03 14:48:32 INFO sparklyr: RScript (2566) processing f35b1c321df0162e3f914adfb70b5416 partition </code></pre>
<p>When executing this in your cluster, you will have to look in the logs for the task that is not finishing, once you have that digest, you can cancel the job.</p>
<p>Then you can use that digest to retrieve that specific data frame to R with something like:</p>
<pre class="sourceCode r"><code class="sourceCode r">broken_partition &lt;-<span class="st"> </span><span class="kw">sdf_len</span>(sc, <span class="dv">3</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">spark_apply</span>(<span class="cf">function</span>(x) {
    <span class="cf">if</span> (<span class="kw">identical</span>(digest<span class="op">::</span><span class="kw">digest</span>(x), <span class="st">&quot;f35b1c321df0162e3f914adfb70b5416&quot;</span>)) x <span class="cf">else</span> x[<span class="dv">0</span>,]
}) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">collect</span>()</code></pre>
<p>Which you can then run in R to troubleshoot further.</p>
</div>
<div id="debugging-workers" class="section level3">
<h3><span class="header-section-number">10.10.4</span> Debugging Workers</h3>
<p>You can use a debugger, which is a tool to let you execute line-by-line your code, to troubleshoot <code>spark_apply()</code> for local connections. You can start <code>spark_apply()</code> in debug mode using the <code>debug</code> parameter and then following the instructions.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">sdf_len</span>(sc, <span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">spark_apply</span>(<span class="cf">function</span>() {
  <span class="kw">stop</span>(<span class="st">&quot;Error!&quot;</span>)
}, <span class="dt">debug =</span> <span class="ot">TRUE</span>)</code></pre>
<pre><code>Debugging spark_apply(), connect to worker debugging session as follows:
  1. Find the workers &lt;sessionid&gt; and &lt;port&gt; in the worker logs, from RStudio click
     &#39;Log&#39; under the connection, look for the last entry with contents:
     &#39;Session (&lt;sessionid&gt;) is waiting for sparklyr client to connect to port &lt;port&gt;&#39;
  2. From a new R session run:
     debugonce(sparklyr:::spark_worker_main)
     sparklyr:::spark_worker_main(&lt;sessionid&gt;, &lt;port&gt;)</code></pre>
<p>As the instructions indicate, you will need to connect “as the worker node” from a different R session and then step through the code. This method is not as straightforward as previous ones so this is something we only recommend as a last resort.</p>
<p>There are a few considerations we should mention when workin with specific clusters, which we are about to present.</p>
</div>
</div>
<div id="clusters-1" class="section level2">
<h2><span class="header-section-number">10.11</span> Clusters</h2>
<p>When using <code>spark_apply()</code>, R needs to be properly installed in each worker node. Different cluster managers, distributions and services, proivide different solutions to install additional software; those instructions should be followed when installing R over each worker node. To mention a few,</p>
<ul>
<li><strong>Spark Standalone</strong>: Requires connecting to each machine and installing R; there are tools like <code>pssh</code> that allow you to run a single installation command against multiple machines.</li>
<li><strong>Cloudera</strong>: Provides an R parcel, see <a href="https://blog.cloudera.com/blog/2017/09/how-to-distribute-your-r-code-with-sparklyr-and-cdsw/">“How to Distribute your R code with sparklyr and Cloudera Data Science Workbench”</a> <span class="citation">(<span class="citeproc-not-found" data-reference-id="cloudera-sparklyr-parcel"><strong>???</strong></span>)</span>, which enables R over each worker node.</li>
<li><strong>Amazon EMR</strong>: R is pre-installed when starting an EMR cluster as mentioned in the <a href="clusters.html#clusters-amazon-emr">Amazon EMR</a> section.</li>
<li><strong>Microsoft HDInsight</strong>: R is pre-installed and no additional steps are needed.</li>
</ul>
<p>Strictly speaking, this completes this chapter. However, we strongly recommend you use Apache Arrow with <code>spark_apply()</code> tio support large scale computation with minimal overhead.</p>
</div>
<div id="apache-arrow" class="section level2">
<h2><span class="header-section-number">10.12</span> Apache Arrow</h2>
<p>Apache Arrow is a cross-language development platform for in-memory data. Arrow is strongly adviced while working <code>spark_apply()</code>, it’s available since Spark 2.3.0 and requires system administrators to install the Apache Arrow runtime in every node.</p>
<p>For instance, in Linux (Ubuntu), you will need to install this in your cluster as follows:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="fu">sudo</span> yum install -y https://packages.red-data-tools.org/centos/red-data-tools-release-latest.noarch.rpm
<span class="fu">sudo</span> sed -i <span class="st">&#39;s/\$releasever/6/g&#39;</span> /etc/yum.repos.d/red-data-tools.repo
<span class="fu">sudo</span> yum install -y --enablerepo=red-data-tools arrow-devel</code></pre>
<p>To use Apache Arrow with <code>sparklyr</code> you need to install the <code>arrow</code> package:</p>
<pre class="sourceCode r"><code class="sourceCode r">devtools<span class="op">::</span><span class="kw">install_github</span>(<span class="st">&quot;apache/arrow&quot;</span>, <span class="dt">subdir =</span> <span class="st">&quot;r&quot;</span>, <span class="dt">ref =</span> <span class="st">&quot;apache-arrow-0.12.0&quot;</span>)</code></pre>
<p>Once installed, to enable Arrow simply include the library:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(arrow)</code></pre>
<p>You can validate that significant performance improvements become available by measuring time of <code>spark_apply()</code> operations.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">system.time</span>(<span class="kw">sdf_len</span>(sc, <span class="dv">10</span><span class="op">^</span><span class="dv">5</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">spark_apply</span>(nrow))</code></pre>
<p>Then you can dettach <code>arrow</code> and rerun this task.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">detach</span>(<span class="st">&quot;package:arrow&quot;</span>, <span class="dt">unload =</span> <span class="ot">TRUE</span>)
<span class="kw">system.time</span>(<span class="kw">sdf_len</span>(sc, <span class="dv">10</span><span class="op">^</span><span class="dv">5</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">spark_apply</span>(nrow))</code></pre>
<p>Notice the performance degradation when not using <code>arrow</code>, run <code>library(arrow)</code> to reneable Arrow.</p>
<p>Most functionality in <code>arrow</code> simply works on the background improving performance and data serialization; however, there is one setting you should be aware of. The <code>spark.sql.execution.arrow.maxRecordsPerBatch</code> configuiration settings specified the default size of each arrow data transfer. It’s shared with other Spark components and defaults to 10,000 rows. Compare the results from the following two operation when using and not using <code>arrow()</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">detach</span>(<span class="st">&quot;package:arrow&quot;</span>, <span class="dt">unload =</span> <span class="ot">TRUE</span>)
<span class="kw">sdf_len</span>(sc, <span class="dv">10</span><span class="op">^</span><span class="dv">6</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">spark_apply</span>(nrow)</code></pre>
<p>Now using <code>arrow</code>,</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(arrow)
<span class="kw">sdf_len</span>(sc, <span class="dv">10</span><span class="op">^</span><span class="dv">6</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">spark_apply</span>(nrow)</code></pre>
<p>You might need to adjust this number based on how much data your system can handle, making it smaller for large dataset or bigger for operations that require records to be processed together. You can specify an unlimitted batch size by setting <code>spark.sql.execution.arrow.maxRecordsPerBatch</code> to zero.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">spark_disconnect</span>(sc)

config &lt;-<span class="st"> </span><span class="kw">spark_config</span>()
config<span class="op">$</span>spark.sql.execution.arrow.maxRecordsPerBatch &lt;-<span class="st"> </span><span class="dv">0</span>

sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>)
<span class="kw">sdf_len</span>(sc, <span class="dv">10</span><span class="op">^</span><span class="dv">6</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">spark_apply</span>(nrow)</code></pre>
</div>
<div id="recap-4" class="section level2">
<h2><span class="header-section-number">10.13</span> Recap</h2>
<p>This chapter presented <code>spark_apply()</code> as an advanced technique that you can use to fill gaps in functionality in Spark or its many extensions. We presented sensible use cases for <code>spark_apply()</code> to parse data, model in parallel many small datasets, perform grid search and call web APIs. We detailed how partitions relate to <code>spark_apply()</code>, how to craete custom groups, distribute contextual information across all nodes, troubleshoot problems and presented limitations, cluster configuration caveats.</p>
<p>We also introduced Apache Arrow as a library we strongly recommend when using Spark with R and presented installation, use cases and considerations you should be aware.</p>
<p>Up to this chapter, we’ve only worked with large datasets of static data. As in, we’ve assumed our datasets do not change over time, they remain invariant while analysing, modeing and visualizing them. In the next chapter, we will introduce techniques to process datasets which, in addition to being large, are also growing an resemble a stream of information.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="extensions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="streaming.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
