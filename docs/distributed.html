<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>The R in Spark: Learning Apache Spark with R</title>
  <meta name="description" content="A book to learn Apache Spark with R using the sparklyr R package.">
  <meta name="generator" content="bookdown 0.7 and GitBook 2.6.7">

  <meta property="og:title" content="The R in Spark: Learning Apache Spark with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="A book to learn Apache Spark with R using the sparklyr R package." />
  <meta name="github-repo" content="javierluraschi/the-r-in-spark" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="The R in Spark: Learning Apache Spark with R" />
  
  <meta name="twitter:description" content="A book to learn Apache Spark with R using the sparklyr R package." />
  



<meta name="date" content="2018-12-28">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="extensions.html">
<link rel="next" href="streaming.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.3/htmlwidgets.js"></script>
<script src="libs/dagre-0.0.1/dagre.min.js"></script>
<script src="libs/lodash-3.7.0/lodash.js"></script>
<script src="libs/nomnoml-0.2.0/nomnoml.js"></script>
<script src="libs/nomnoml-binding-0.1.0/nomnoml.js"></script>
<script src="libs/r2d3-render-0.1.0/r2d3-render.js"></script>
<script src="libs/webcomponents-2.0.0/webcomponents.js"></script>
<script src="libs/r2d3-binding-0.2.3/r2d3.js"></script>
<script src="libs/d3v5-5.0.0/d3.min.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-119986300-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-119986300-1');
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(data-line-number);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./">Learning Apache Spark with R</a></strong></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html"><i class="fa fa-check"></i>Preface</a><ul>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#structure"><i class="fa fa-check"></i>Structure</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#authors"><i class="fa fa-check"></i>Authors</a></li>
<li class="chapter" data-level="" data-path="preface.html"><a href="preface.html#acknowledgments"><i class="fa fa-check"></i>Acknowledgments</a></li>
</ul></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#intro-background"><i class="fa fa-check"></i><b>1.1</b> Background</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#intro-spark"><i class="fa fa-check"></i><b>1.2</b> Spark</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#intro-r"><i class="fa fa-check"></i><b>1.3</b> R</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#intro-sparklyr"><i class="fa fa-check"></i><b>1.4</b> sparklyr</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="starting.html"><a href="starting.html"><i class="fa fa-check"></i><b>2</b> Getting Started</a><ul>
<li class="chapter" data-level="2.1" data-path="starting.html"><a href="starting.html#starting-prerequisites"><i class="fa fa-check"></i><b>2.1</b> Prerequisites</a><ul>
<li class="chapter" data-level="2.1.1" data-path="starting.html"><a href="starting.html#starting-install-r"><i class="fa fa-check"></i><b>2.1.1</b> Install R</a></li>
<li class="chapter" data-level="2.1.2" data-path="starting.html"><a href="starting.html#starting-install-java"><i class="fa fa-check"></i><b>2.1.2</b> Install Java</a></li>
<li class="chapter" data-level="2.1.3" data-path="starting.html"><a href="starting.html#starting-install-rstudio"><i class="fa fa-check"></i><b>2.1.3</b> Install RStudio</a></li>
<li class="chapter" data-level="2.1.4" data-path="starting.html"><a href="starting.html#starting-install-sparklyr"><i class="fa fa-check"></i><b>2.1.4</b> Install sparklyr</a></li>
</ul></li>
<li class="chapter" data-level="2.2" data-path="starting.html"><a href="starting.html#starting-installing-spark"><i class="fa fa-check"></i><b>2.2</b> Installing Spark</a></li>
<li class="chapter" data-level="2.3" data-path="starting.html"><a href="starting.html#starting-connect-to-spark"><i class="fa fa-check"></i><b>2.3</b> Connecting to Spark</a></li>
<li class="chapter" data-level="2.4" data-path="starting.html"><a href="starting.html#starting-sparklyr-hello-world"><i class="fa fa-check"></i><b>2.4</b> Using Spark</a><ul>
<li class="chapter" data-level="2.4.1" data-path="starting.html"><a href="starting.html#starting-spark-web-interface"><i class="fa fa-check"></i><b>2.4.1</b> Web Interface</a></li>
<li class="chapter" data-level="2.4.2" data-path="starting.html"><a href="starting.html#starting-logs"><i class="fa fa-check"></i><b>2.4.2</b> Logs</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="starting.html"><a href="starting.html#starting-disconnecting"><i class="fa fa-check"></i><b>2.5</b> Disconnecting</a></li>
<li class="chapter" data-level="2.6" data-path="starting.html"><a href="starting.html#starting-using-spark-from-rstudio"><i class="fa fa-check"></i><b>2.6</b> RStudio</a></li>
<li class="chapter" data-level="2.7" data-path="starting.html"><a href="starting.html#starting-resources"><i class="fa fa-check"></i><b>2.7</b> Resources</a></li>
<li class="chapter" data-level="2.8" data-path="starting.html"><a href="starting.html#starting-recap"><i class="fa fa-check"></i><b>2.8</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="analysis.html"><a href="analysis.html"><i class="fa fa-check"></i><b>3</b> Analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="analysis.html"><a href="analysis.html#background"><i class="fa fa-check"></i><b>3.1</b> Background</a><ul>
<li class="chapter" data-level="3.1.1" data-path="analysis.html"><a href="analysis.html#working-with-big-data"><i class="fa fa-check"></i><b>3.1.1</b> Working with Big Data</a></li>
<li class="chapter" data-level="3.1.2" data-path="analysis.html"><a href="analysis.html#avoid-running-r-inside-spark"><i class="fa fa-check"></i><b>3.1.2</b> Avoid running R inside Spark</a></li>
<li class="chapter" data-level="3.1.3" data-path="analysis.html"><a href="analysis.html#r-under-the-hood"><i class="fa fa-check"></i><b>3.1.3</b> R, under the hood</a></li>
<li class="chapter" data-level="3.1.4" data-path="analysis.html"><a href="analysis.html#use-r-as-an-interface-to-spark"><i class="fa fa-check"></i><b>3.1.4</b> Use R as an interface to Spark</a></li>
<li class="chapter" data-level="3.1.5" data-path="analysis.html"><a href="analysis.html#using-sparklyr-for-analysis"><i class="fa fa-check"></i><b>3.1.5</b> Using <code>sparklyr</code> for analysis</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="4" data-path="modeling.html"><a href="modeling.html"><i class="fa fa-check"></i><b>4</b> Modeling</a><ul>
<li class="chapter" data-level="4.1" data-path="modeling.html"><a href="modeling.html#overview"><i class="fa fa-check"></i><b>4.1</b> Overview</a></li>
<li class="chapter" data-level="4.2" data-path="modeling.html"><a href="modeling.html#supervised"><i class="fa fa-check"></i><b>4.2</b> Supervised</a></li>
<li class="chapter" data-level="4.3" data-path="modeling.html"><a href="modeling.html#unsupervised"><i class="fa fa-check"></i><b>4.3</b> Unsupervised</a><ul>
<li class="chapter" data-level="4.3.1" data-path="modeling.html"><a href="modeling.html#k-means-clustering"><i class="fa fa-check"></i><b>4.3.1</b> K-Means Clustering</a></li>
<li class="chapter" data-level="4.3.2" data-path="modeling.html"><a href="modeling.html#gaussian-mixture-clustering"><i class="fa fa-check"></i><b>4.3.2</b> Gaussian Mixture Clustering</a></li>
</ul></li>
<li class="chapter" data-level="4.4" data-path="modeling.html"><a href="modeling.html#broom"><i class="fa fa-check"></i><b>4.4</b> Broom</a></li>
<li class="chapter" data-level="4.5" data-path="modeling.html"><a href="modeling.html#pipelines"><i class="fa fa-check"></i><b>4.5</b> Pipelines</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="clusters.html"><a href="clusters.html"><i class="fa fa-check"></i><b>5</b> Clusters</a><ul>
<li class="chapter" data-level="5.1" data-path="clusters.html"><a href="clusters.html#clusters-overview"><i class="fa fa-check"></i><b>5.1</b> Overview</a></li>
<li class="chapter" data-level="5.2" data-path="clusters.html"><a href="clusters.html#clusters-manager"><i class="fa fa-check"></i><b>5.2</b> Managers</a><ul>
<li class="chapter" data-level="5.2.1" data-path="clusters.html"><a href="clusters.html#clusters-standalone"><i class="fa fa-check"></i><b>5.2.1</b> Standalone</a></li>
<li class="chapter" data-level="5.2.2" data-path="clusters.html"><a href="clusters.html#yarn"><i class="fa fa-check"></i><b>5.2.2</b> Yarn</a></li>
<li class="chapter" data-level="5.2.3" data-path="clusters.html"><a href="clusters.html#mesos"><i class="fa fa-check"></i><b>5.2.3</b> Mesos</a></li>
<li class="chapter" data-level="5.2.4" data-path="clusters.html"><a href="clusters.html#kubernetes"><i class="fa fa-check"></i><b>5.2.4</b> Kubernetes</a></li>
</ul></li>
<li class="chapter" data-level="5.3" data-path="clusters.html"><a href="clusters.html#on-premise"><i class="fa fa-check"></i><b>5.3</b> On-Premise</a><ul>
<li class="chapter" data-level="5.3.1" data-path="clusters.html"><a href="clusters.html#cloudera"><i class="fa fa-check"></i><b>5.3.1</b> Cloudera</a></li>
<li class="chapter" data-level="5.3.2" data-path="clusters.html"><a href="clusters.html#hortonworks"><i class="fa fa-check"></i><b>5.3.2</b> Hortonworks</a></li>
<li class="chapter" data-level="5.3.3" data-path="clusters.html"><a href="clusters.html#mapr"><i class="fa fa-check"></i><b>5.3.3</b> MapR</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="clusters.html"><a href="clusters.html#cloud"><i class="fa fa-check"></i><b>5.4</b> Cloud</a><ul>
<li class="chapter" data-level="5.4.1" data-path="clusters.html"><a href="clusters.html#clusters-amazon-emr"><i class="fa fa-check"></i><b>5.4.1</b> Amazon</a></li>
<li class="chapter" data-level="5.4.2" data-path="clusters.html"><a href="clusters.html#databricks"><i class="fa fa-check"></i><b>5.4.2</b> Databricks</a></li>
<li class="chapter" data-level="5.4.3" data-path="clusters.html"><a href="clusters.html#google"><i class="fa fa-check"></i><b>5.4.3</b> Google</a></li>
<li class="chapter" data-level="5.4.4" data-path="clusters.html"><a href="clusters.html#ibm"><i class="fa fa-check"></i><b>5.4.4</b> IBM</a></li>
<li class="chapter" data-level="5.4.5" data-path="clusters.html"><a href="clusters.html#microsoft"><i class="fa fa-check"></i><b>5.4.5</b> Microsoft</a></li>
</ul></li>
<li class="chapter" data-level="5.5" data-path="clusters.html"><a href="clusters.html#tools"><i class="fa fa-check"></i><b>5.5</b> Tools</a><ul>
<li class="chapter" data-level="5.5.1" data-path="clusters.html"><a href="clusters.html#rstudio"><i class="fa fa-check"></i><b>5.5.1</b> RStudio</a></li>
<li class="chapter" data-level="5.5.2" data-path="clusters.html"><a href="clusters.html#jupyter"><i class="fa fa-check"></i><b>5.5.2</b> Jupyter</a></li>
<li class="chapter" data-level="5.5.3" data-path="clusters.html"><a href="clusters.html#clusters-livy"><i class="fa fa-check"></i><b>5.5.3</b> Livy</a></li>
</ul></li>
<li class="chapter" data-level="5.6" data-path="clusters.html"><a href="clusters.html#recap"><i class="fa fa-check"></i><b>5.6</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="connections.html"><a href="connections.html"><i class="fa fa-check"></i><b>6</b> Connections</a><ul>
<li class="chapter" data-level="6.1" data-path="connections.html"><a href="connections.html#connections-overview"><i class="fa fa-check"></i><b>6.1</b> Overview</a><ul>
<li class="chapter" data-level="6.1.1" data-path="connections.html"><a href="connections.html#connections-spark-edge-nodes"><i class="fa fa-check"></i><b>6.1.1</b> Edge Nodes</a></li>
<li class="chapter" data-level="6.1.2" data-path="connections.html"><a href="connections.html#connections-spark-home"><i class="fa fa-check"></i><b>6.1.2</b> Spark Home</a></li>
</ul></li>
<li class="chapter" data-level="6.2" data-path="connections.html"><a href="connections.html#connections-local"><i class="fa fa-check"></i><b>6.2</b> Local</a></li>
<li class="chapter" data-level="6.3" data-path="connections.html"><a href="connections.html#connections-standalone"><i class="fa fa-check"></i><b>6.3</b> Standalone</a></li>
<li class="chapter" data-level="6.4" data-path="connections.html"><a href="connections.html#connections-yarn"><i class="fa fa-check"></i><b>6.4</b> Yarn</a><ul>
<li class="chapter" data-level="6.4.1" data-path="connections.html"><a href="connections.html#connections-yarn-client"><i class="fa fa-check"></i><b>6.4.1</b> Yarn Client</a></li>
<li class="chapter" data-level="6.4.2" data-path="connections.html"><a href="connections.html#connections-yarn-cluster"><i class="fa fa-check"></i><b>6.4.2</b> Yarn Cluster</a></li>
</ul></li>
<li class="chapter" data-level="6.5" data-path="connections.html"><a href="connections.html#connections-livy"><i class="fa fa-check"></i><b>6.5</b> Livy</a></li>
<li class="chapter" data-level="6.6" data-path="connections.html"><a href="connections.html#connections-mesos"><i class="fa fa-check"></i><b>6.6</b> Mesos</a></li>
<li class="chapter" data-level="6.7" data-path="connections.html"><a href="connections.html#connections-kubernetes"><i class="fa fa-check"></i><b>6.7</b> Kubernetes</a></li>
<li class="chapter" data-level="6.8" data-path="connections.html"><a href="connections.html#cloud-1"><i class="fa fa-check"></i><b>6.8</b> Cloud</a></li>
<li class="chapter" data-level="6.9" data-path="connections.html"><a href="connections.html#multiple"><i class="fa fa-check"></i><b>6.9</b> Multiple</a></li>
<li class="chapter" data-level="6.10" data-path="connections.html"><a href="connections.html#troubleshooting"><i class="fa fa-check"></i><b>6.10</b> Troubleshooting</a><ul>
<li class="chapter" data-level="6.10.1" data-path="connections.html"><a href="connections.html#logging"><i class="fa fa-check"></i><b>6.10.1</b> Logging</a></li>
<li class="chapter" data-level="6.10.2" data-path="connections.html"><a href="connections.html#troubleshoot-spark-submit"><i class="fa fa-check"></i><b>6.10.2</b> Spark Submit</a></li>
<li class="chapter" data-level="6.10.3" data-path="connections.html"><a href="connections.html#windows"><i class="fa fa-check"></i><b>6.10.3</b> Windows</a></li>
</ul></li>
<li class="chapter" data-level="6.11" data-path="connections.html"><a href="connections.html#recap-1"><i class="fa fa-check"></i><b>6.11</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>7</b> Data</a><ul>
<li class="chapter" data-level="7.1" data-path="data.html"><a href="data.html#overview-1"><i class="fa fa-check"></i><b>7.1</b> Overview</a></li>
<li class="chapter" data-level="7.2" data-path="data.html"><a href="data.html#data-frames"><i class="fa fa-check"></i><b>7.2</b> Data Frames</a><ul>
<li class="chapter" data-level="7.2.1" data-path="data.html"><a href="data.html#data-sdf-functions"><i class="fa fa-check"></i><b>7.2.1</b> Functions</a></li>
<li class="chapter" data-level="7.2.2" data-path="data.html"><a href="data.html#pivoting"><i class="fa fa-check"></i><b>7.2.2</b> Pivoting</a></li>
</ul></li>
<li class="chapter" data-level="7.3" data-path="data.html"><a href="data.html#formats"><i class="fa fa-check"></i><b>7.3</b> Formats</a></li>
<li class="chapter" data-level="7.4" data-path="data.html"><a href="data.html#data-types"><i class="fa fa-check"></i><b>7.4</b> Data Types</a><ul>
<li class="chapter" data-level="7.4.1" data-path="data.html"><a href="data.html#dates"><i class="fa fa-check"></i><b>7.4.1</b> Dates</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="data.html"><a href="data.html#sources"><i class="fa fa-check"></i><b>7.5</b> Sources</a><ul>
<li class="chapter" data-level="7.5.1" data-path="data.html"><a href="data.html#amazon-s3"><i class="fa fa-check"></i><b>7.5.1</b> Amazon S3</a></li>
<li class="chapter" data-level="7.5.2" data-path="data.html"><a href="data.html#azure-storage"><i class="fa fa-check"></i><b>7.5.2</b> Azure Storage</a></li>
<li class="chapter" data-level="7.5.3" data-path="data.html"><a href="data.html#cassandra"><i class="fa fa-check"></i><b>7.5.3</b> Cassandra</a></li>
<li class="chapter" data-level="7.5.4" data-path="data.html"><a href="data.html#databases"><i class="fa fa-check"></i><b>7.5.4</b> Databases</a></li>
<li class="chapter" data-level="7.5.5" data-path="data.html"><a href="data.html#hbase"><i class="fa fa-check"></i><b>7.5.5</b> HBase</a></li>
<li class="chapter" data-level="7.5.6" data-path="data.html"><a href="data.html#nested-data"><i class="fa fa-check"></i><b>7.5.6</b> Nested Data</a></li>
</ul></li>
<li class="chapter" data-level="7.6" data-path="data.html"><a href="data.html#troubleshooting-1"><i class="fa fa-check"></i><b>7.6</b> Troubleshooting</a><ul>
<li class="chapter" data-level="7.6.1" data-path="data.html"><a href="data.html#troubleshoot-csvs"><i class="fa fa-check"></i><b>7.6.1</b> Troubleshoot CSVs</a></li>
<li class="chapter" data-level="7.6.2" data-path="data.html"><a href="data.html#column-names"><i class="fa fa-check"></i><b>7.6.2</b> Column Names</a></li>
</ul></li>
<li class="chapter" data-level="7.7" data-path="data.html"><a href="data.html#recap-2"><i class="fa fa-check"></i><b>7.7</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="tuning.html"><a href="tuning.html"><i class="fa fa-check"></i><b>8</b> Tuning</a><ul>
<li class="chapter" data-level="8.1" data-path="tuning.html"><a href="tuning.html#overview-2"><i class="fa fa-check"></i><b>8.1</b> Overview</a><ul>
<li class="chapter" data-level="8.1.1" data-path="tuning.html"><a href="tuning.html#tuning-graph-visualization"><i class="fa fa-check"></i><b>8.1.1</b> Graph Visualization</a></li>
<li class="chapter" data-level="8.1.2" data-path="tuning.html"><a href="tuning.html#tuning-event-timeline"><i class="fa fa-check"></i><b>8.1.2</b> Event Timeline</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="tuning.html"><a href="tuning.html#tuning-configuring"><i class="fa fa-check"></i><b>8.2</b> Configuring</a><ul>
<li class="chapter" data-level="8.2.1" data-path="tuning.html"><a href="tuning.html#connect-settings"><i class="fa fa-check"></i><b>8.2.1</b> Connect Settings</a></li>
<li class="chapter" data-level="8.2.2" data-path="tuning.html"><a href="tuning.html#submit-settings"><i class="fa fa-check"></i><b>8.2.2</b> Submit Settings</a></li>
<li class="chapter" data-level="8.2.3" data-path="tuning.html"><a href="tuning.html#runtime-settings"><i class="fa fa-check"></i><b>8.2.3</b> Runtime Settings</a></li>
<li class="chapter" data-level="8.2.4" data-path="tuning.html"><a href="tuning.html#sparklyr-settings"><i class="fa fa-check"></i><b>8.2.4</b> sparklyr Settings</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="tuning.html"><a href="tuning.html#tuning-partitioning"><i class="fa fa-check"></i><b>8.3</b> Partitioning</a><ul>
<li class="chapter" data-level="8.3.1" data-path="tuning.html"><a href="tuning.html#implicit"><i class="fa fa-check"></i><b>8.3.1</b> Implicit</a></li>
<li class="chapter" data-level="8.3.2" data-path="tuning.html"><a href="tuning.html#explicit"><i class="fa fa-check"></i><b>8.3.2</b> Explicit</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="tuning.html"><a href="tuning.html#tuning-caching"><i class="fa fa-check"></i><b>8.4</b> Caching</a><ul>
<li class="chapter" data-level="8.4.1" data-path="tuning.html"><a href="tuning.html#checkpointing"><i class="fa fa-check"></i><b>8.4.1</b> Checkpointing</a></li>
<li class="chapter" data-level="8.4.2" data-path="tuning.html"><a href="tuning.html#tuning-memory"><i class="fa fa-check"></i><b>8.4.2</b> Memory</a></li>
</ul></li>
<li class="chapter" data-level="8.5" data-path="tuning.html"><a href="tuning.html#tuning-shuffling"><i class="fa fa-check"></i><b>8.5</b> Shuffling</a></li>
<li class="chapter" data-level="8.6" data-path="tuning.html"><a href="tuning.html#tuning-serialization"><i class="fa fa-check"></i><b>8.6</b> Serialization</a></li>
<li class="chapter" data-level="8.7" data-path="tuning.html"><a href="tuning.html#configuration-files"><i class="fa fa-check"></i><b>8.7</b> Configuration Files</a></li>
<li class="chapter" data-level="8.8" data-path="tuning.html"><a href="tuning.html#recap-3"><i class="fa fa-check"></i><b>8.8</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="extensions.html"><a href="extensions.html"><i class="fa fa-check"></i><b>9</b> Extensions</a><ul>
<li class="chapter" data-level="9.1" data-path="extensions.html"><a href="extensions.html#rsparkling"><i class="fa fa-check"></i><b>9.1</b> RSparkling</a><ul>
<li class="chapter" data-level="9.1.1" data-path="extensions.html"><a href="extensions.html#troubleshooting-2"><i class="fa fa-check"></i><b>9.1.1</b> Troubleshooting</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="extensions.html"><a href="extensions.html#graphframes"><i class="fa fa-check"></i><b>9.2</b> GraphFrames</a></li>
<li class="chapter" data-level="9.3" data-path="extensions.html"><a href="extensions.html#mleap"><i class="fa fa-check"></i><b>9.3</b> Mleap</a></li>
<li class="chapter" data-level="9.4" data-path="extensions.html"><a href="extensions.html#extensions-nested-data"><i class="fa fa-check"></i><b>9.4</b> Nested Data</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="distributed.html"><a href="distributed.html"><i class="fa fa-check"></i><b>10</b> Distributed R</a><ul>
<li class="chapter" data-level="10.1" data-path="distributed.html"><a href="distributed.html#overview-3"><i class="fa fa-check"></i><b>10.1</b> Overview</a></li>
<li class="chapter" data-level="10.2" data-path="distributed.html"><a href="distributed.html#use-cases"><i class="fa fa-check"></i><b>10.2</b> Use Cases</a><ul>
<li class="chapter" data-level="10.2.1" data-path="distributed.html"><a href="distributed.html#custom-parsers"><i class="fa fa-check"></i><b>10.2.1</b> Custom Parsers</a></li>
<li class="chapter" data-level="10.2.2" data-path="distributed.html"><a href="distributed.html#partitioned-modeling"><i class="fa fa-check"></i><b>10.2.2</b> Partitioned Modeling</a></li>
</ul></li>
<li class="chapter" data-level="10.3" data-path="distributed.html"><a href="distributed.html#partitions"><i class="fa fa-check"></i><b>10.3</b> Partitions</a></li>
<li class="chapter" data-level="10.4" data-path="distributed.html"><a href="distributed.html#columns"><i class="fa fa-check"></i><b>10.4</b> Columns</a></li>
<li class="chapter" data-level="10.5" data-path="distributed.html"><a href="distributed.html#grouping"><i class="fa fa-check"></i><b>10.5</b> Grouping</a></li>
<li class="chapter" data-level="10.6" data-path="distributed.html"><a href="distributed.html#packages"><i class="fa fa-check"></i><b>10.6</b> Packages</a></li>
<li class="chapter" data-level="10.7" data-path="distributed.html"><a href="distributed.html#context"><i class="fa fa-check"></i><b>10.7</b> Context</a></li>
<li class="chapter" data-level="10.8" data-path="distributed.html"><a href="distributed.html#restrictions"><i class="fa fa-check"></i><b>10.8</b> Restrictions</a><ul>
<li class="chapter" data-level="10.8.1" data-path="distributed.html"><a href="distributed.html#troubleshooting-3"><i class="fa fa-check"></i><b>10.8.1</b> Troubleshooting</a></li>
</ul></li>
<li class="chapter" data-level="10.9" data-path="distributed.html"><a href="distributed.html#clusters-1"><i class="fa fa-check"></i><b>10.9</b> Clusters</a></li>
<li class="chapter" data-level="10.10" data-path="distributed.html"><a href="distributed.html#apache-arrow"><i class="fa fa-check"></i><b>10.10</b> Apache Arrow</a></li>
<li class="chapter" data-level="10.11" data-path="distributed.html"><a href="distributed.html#recap-4"><i class="fa fa-check"></i><b>10.11</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="streaming.html"><a href="streaming.html"><i class="fa fa-check"></i><b>11</b> Streaming</a><ul>
<li class="chapter" data-level="11.1" data-path="streaming.html"><a href="streaming.html#overview-4"><i class="fa fa-check"></i><b>11.1</b> Overview</a></li>
<li class="chapter" data-level="11.2" data-path="streaming.html"><a href="streaming.html#streaming-treansform"><i class="fa fa-check"></i><b>11.2</b> Transformations</a><ul>
<li class="chapter" data-level="11.2.1" data-path="streaming.html"><a href="streaming.html#streams-dplyr"><i class="fa fa-check"></i><b>11.2.1</b> dplyr</a></li>
<li class="chapter" data-level="11.2.2" data-path="streaming.html"><a href="streaming.html#streams-pipelines"><i class="fa fa-check"></i><b>11.2.2</b> Pipelines</a></li>
<li class="chapter" data-level="11.2.3" data-path="streaming.html"><a href="streaming.html#streams-r"><i class="fa fa-check"></i><b>11.2.3</b> R Code</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="streaming.html"><a href="streaming.html#shiny"><i class="fa fa-check"></i><b>11.3</b> Shiny</a></li>
<li class="chapter" data-level="11.4" data-path="streaming.html"><a href="streaming.html#formats-1"><i class="fa fa-check"></i><b>11.4</b> Formats</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="contributing.html"><a href="contributing.html"><i class="fa fa-check"></i><b>12</b> Contributing</a><ul>
<li class="chapter" data-level="12.1" data-path="contributing.html"><a href="contributing.html#overview-5"><i class="fa fa-check"></i><b>12.1</b> Overview</a></li>
<li class="chapter" data-level="12.2" data-path="contributing.html"><a href="contributing.html#contributing-r-extension"><i class="fa fa-check"></i><b>12.2</b> R Extensions</a></li>
<li class="chapter" data-level="12.3" data-path="contributing.html"><a href="contributing.html#scala-extensions"><i class="fa fa-check"></i><b>12.3</b> Scala Extensions</a><ul>
<li class="chapter" data-level="12.3.1" data-path="contributing.html"><a href="contributing.html#scala-extension-prereq"><i class="fa fa-check"></i><b>12.3.1</b> Prerequisites</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="contributing.html"><a href="contributing.html#spark-extensions"><i class="fa fa-check"></i><b>12.4</b> Spark Extensions</a></li>
<li class="chapter" data-level="12.5" data-path="contributing.html"><a href="contributing.html#r-packages"><i class="fa fa-check"></i><b>12.5</b> R Packages</a><ul>
<li class="chapter" data-level="12.5.1" data-path="contributing.html"><a href="contributing.html#rstudio-projects"><i class="fa fa-check"></i><b>12.5.1</b> RStudio Projects</a></li>
<li class="chapter" data-level="12.5.2" data-path="contributing.html"><a href="contributing.html#troubleshooting-4"><i class="fa fa-check"></i><b>12.5.2</b> Troubleshooting</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="contributing.html"><a href="contributing.html#contributing-sparklyr"><i class="fa fa-check"></i><b>12.6</b> sparklyr</a><ul>
<li class="chapter" data-level="12.6.1" data-path="contributing.html"><a href="contributing.html#compiling"><i class="fa fa-check"></i><b>12.6.1</b> Compiling</a></li>
<li class="chapter" data-level="12.6.2" data-path="contributing.html"><a href="contributing.html#serialization"><i class="fa fa-check"></i><b>12.6.2</b> Serialization</a></li>
<li class="chapter" data-level="12.6.3" data-path="contributing.html"><a href="contributing.html#invocations"><i class="fa fa-check"></i><b>12.6.3</b> Invocations</a></li>
<li class="chapter" data-level="12.6.4" data-path="contributing.html"><a href="contributing.html#r-packages-1"><i class="fa fa-check"></i><b>12.6.4</b> R Packages</a></li>
<li class="chapter" data-level="12.6.5" data-path="contributing.html"><a href="contributing.html#connections-1"><i class="fa fa-check"></i><b>12.6.5</b> Connections</a></li>
<li class="chapter" data-level="12.6.6" data-path="contributing.html"><a href="contributing.html#distributed-r"><i class="fa fa-check"></i><b>12.6.6</b> Distributed R</a></li>
</ul></li>
<li class="chapter" data-level="12.7" data-path="contributing.html"><a href="contributing.html#recap-5"><i class="fa fa-check"></i><b>12.7</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>13</b> Appendix</a><ul>
<li class="chapter" data-level="13.1" data-path="appendix.html"><a href="appendix.html#diagrams"><i class="fa fa-check"></i><b>13.1</b> Diagrams</a><ul>
<li class="chapter" data-level="13.1.1" data-path="appendix.html"><a href="appendix.html#appendix-storage-capacity"><i class="fa fa-check"></i><b>13.1.1</b> Worlds Store Capacity</a></li>
<li class="chapter" data-level="13.1.2" data-path="appendix.html"><a href="appendix.html#appendix-cran-downloads"><i class="fa fa-check"></i><b>13.1.2</b> Daily downloads of CRAN packages</a></li>
<li class="chapter" data-level="13.1.3" data-path="appendix.html"><a href="appendix.html#appendix-cluster-trends"><i class="fa fa-check"></i><b>13.1.3</b> Google trends for mainframes, cloud computing and kubernetes</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="14" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>14</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">The R in Spark: Learning Apache Spark with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="distributed" class="section level1">
<h1><span class="header-section-number">Chapter 10</span> Distributed R</h1>
<p>Previous chapters presented how to perform many operations in Spark, the previous <a href="extensions.html#extensions">Extensions</a> chapter, described how to use functionality provided not in Spark itself, but in extensions developed by Spark and R contributors. In most cases, the combination of Spark and extensions is more than enough to scale almost any computation. However, for those few cases where functionality is lacking, you can consider distributing R computations to worker nodes to leverage R and R packages.</p>
<div id="overview-3" class="section level2">
<h2><span class="header-section-number">10.1</span> Overview</h2>
<p>The <a href="intro.html#intro">Introduction</a> chapter introduced MapReduce as a technique capable of processing large scale datasets; it also described how Apache Spark provided a superset of operations to perform MapReduce computations with ease and more efficiently. The <a href="tuning.html#tuning">Tuning</a> chapter presented insights into how Spark works by applying custom transformation over each partition of the distributed datasets. For instance, if we were to multiply by ten each element of a distributed numeric dataset, Spark would apply a mapping operation over each partition through multiple workers, conceptually similar to:</p>
<div class="figure" style="text-align: center"><span id="fig:distributed-times-ten"></span>
<div id="htmlwidget-4e1d1fba4192aea8b166" style="width:580pt;height:220pt;" class="nomnoml html-widget"></div>
<script type="application/json" data-for="htmlwidget-4e1d1fba4192aea8b166">{"x":{"code":"\n  #fill: #FEFEFF\n  #lineWidth: 2\n  #zoom: 4\n   \n#padding: 16\n#fontSize: 18\n#direction: right\n#lineWidth:2\n#leading:1\n#spacing: 20\n\n\n[Data |\n  [1]\n  [2]\n  [3]\n]->[Worker |\n  [1] -> [f(x) = 10 * x]\n  [2] -> [f(x) = 10 * x]\n  [3] -> [f(x) = 10 * x]\n  [f(x) = 10 * x] -> [10]\n  [f(x) = 10 * x] -> [20]\n  [f(x) = 10 * x] -> [30]\n]","className":null,"svg":false},"evals":[],"jsHooks":[]}</script>
<p class="caption">
FIGURE 10.1: Conceptual mapping operation when multiplying by ten.
</p>
</div>
<p>This chapter presents how to define custom <code>f(x)</code> mapping operations using <code>spark_apply()</code>; for the example above, <code>spark_apply()</code> provides support to define <code>x + 1</code> as follows:</p>
<div class="sourceCode" id="cb113"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb113-1" data-line-number="1"><span class="kw">sdf_len</span>(sc, <span class="dv">3</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">spark_apply</span>(<span class="op">~</span><span class="st"> </span><span class="dv">10</span> <span class="op">*</span><span class="st"> </span>.x)</a></code></pre></div>
<pre><code># Source: spark&lt;?&gt; [?? x 1]
     id
* &lt;dbl&gt;
1     2
2     3
3     4</code></pre>
<p>Notice that <code>~ 10 * .x</code> is plain R code executed across all worker nodes; the <code>~</code> character is defined in the <code>rlang</code> package and provides a compact definition of a function equivalent to <code>function(.x) 10 * .x</code> or what is also known as an anonymous function or lambda expression.</p>
<p>Now, the first thing to notice is that <code>f(x)</code> takes an R data frame as input and must also produce an R data frame as output, conceptually this looks as follows:</p>
<div class="figure" style="text-align: center"><span id="fig:distributed-spark-apply-input-output"></span>
<div id="htmlwidget-71f88bb05807e745e324" style="width:580pt;height:120pt;" class="nomnoml html-widget"></div>
<script type="application/json" data-for="htmlwidget-71f88bb05807e745e324">{"x":{"code":"\n  #fill: #FEFEFF\n  #lineWidth: 2\n  #zoom: 4\n   \n#padding: 16\n#fontSize: 18\n#direction: right\n#lineWidth:2\n#leading:1\n#spacing: 20\n\n\n[DataFrame]->[f(x)]\n[f(x)]->[DataFrame']\n","className":null,"svg":false},"evals":[],"jsHooks":[]}</script>
<p class="caption">
FIGURE 10.2: Expected function signature in spark_apply() mappings.
</p>
</div>
<p>We can use the orginal MapReduce example from the <a href="intro.html#intro">Introduction</a> chapter where, the map operations was defined to split sentences into words and then, the total unique words were counted as the reduce operation. We can compute this using knowledge from previous chapters as follows:</p>
<div class="sourceCode" id="cb115"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb115-1" data-line-number="1">sentences &lt;-<span class="st"> </span><span class="kw">data_frame</span>(<span class="dt">text =</span> <span class="kw">c</span>(</a>
<a class="sourceLine" id="cb115-2" data-line-number="2">  <span class="st">&quot;I like apples&quot;</span>,</a>
<a class="sourceLine" id="cb115-3" data-line-number="3">  <span class="st">&quot;I like bananas&quot;</span></a>
<a class="sourceLine" id="cb115-4" data-line-number="4">))</a>
<a class="sourceLine" id="cb115-5" data-line-number="5"></a>
<a class="sourceLine" id="cb115-6" data-line-number="6">sentences_tbl &lt;-<span class="st"> </span><span class="kw">copy_to</span>(sc, sentences)</a>
<a class="sourceLine" id="cb115-7" data-line-number="7"></a>
<a class="sourceLine" id="cb115-8" data-line-number="8">sentences_tbl <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb115-9" data-line-number="9"><span class="st">  </span><span class="kw">ft_tokenizer</span>(<span class="st">&quot;text&quot;</span>, <span class="st">&quot;words&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb115-10" data-line-number="10"><span class="st">  </span><span class="kw">transmute</span>(<span class="dt">word =</span> <span class="kw">explode</span>(words)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb115-11" data-line-number="11"><span class="st">  </span><span class="kw">group_by</span>(word) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb115-12" data-line-number="12"><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">count =</span> <span class="kw">count</span>())</a></code></pre></div>
<pre><code># Source: spark&lt;?&gt; [?? x 2]
  word    count
* &lt;chr&gt;   &lt;dbl&gt;
1 i           2
2 apples      1
3 like        2
4 bananas     1</code></pre>
<p>In this example, the operation that provides the mapping from text into words is a combination of <code>ft_tokenizer()</code> and <code>explode()</code>; however, if those operations were not available, we could use instead the <code>tidytext</code> R package to tokenize sentences into words, this would look as follows in R:</p>
<div class="sourceCode" id="cb117"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb117-1" data-line-number="1">tidytext<span class="op">::</span><span class="kw">unnest_tokens</span>(sentences, word, text)</a></code></pre></div>
<pre><code># A tibble: 6 x 1
  word   
  &lt;chr&gt;  
1 i      
2 like   
3 apples 
4 i      
5 like   
6 bananas</code></pre>
<p>To make use of <code>tidytext</code> in Spark, we would define a custom mapping operation using <code>spark_apply()</code> and <code>unnest_tokens()</code> as follows:</p>
<div class="sourceCode" id="cb119"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb119-1" data-line-number="1">sentences_tbl <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb119-2" data-line-number="2"><span class="st">  </span><span class="kw">spark_apply</span>(<span class="op">~</span>tidytext<span class="op">::</span><span class="kw">unnest_tokens</span>(., word, text))</a></code></pre></div>
<pre><code># Source: spark&lt;?&gt; [?? x 1]
  word   
* &lt;chr&gt;  
1 i      
2 like   
3 apples 
4 i      
5 like   
6 bananas</code></pre>
<p>Finally, we can reduce this dataset using <code>dplyr</code> to compute this original MapReduce word-count example using <code>dplyr</code> as follows:</p>
<div class="sourceCode" id="cb121"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb121-1" data-line-number="1">sentences_tbl <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb121-2" data-line-number="2"><span class="st">  </span><span class="kw">spark_apply</span>(<span class="op">~</span>tidytext<span class="op">::</span><span class="kw">unnest_tokens</span>(., word, text)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb121-3" data-line-number="3"><span class="st">  </span><span class="kw">group_by</span>(word) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb121-4" data-line-number="4"><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">count =</span> <span class="kw">count</span>())</a></code></pre></div>
<pre><code># Source: spark&lt;?&gt; [?? x 2]
  word    count
* &lt;chr&gt;   &lt;dbl&gt;
1 i           2
2 apples      1
3 like        2
4 bananas     1</code></pre>
<p>The rest of this chapter will explain in detail use cases, features, caveats, considerations and troubleshooting techniques required when defining custom mappings through <code>spark_apply()</code></p>
<p><strong>Warning:</strong> The proffient R reader can be tempted to use this approach for all Spark operations since it enables using all the functionality from many familiar R packages; however, this is NOT the recommended use of <code>spark_apply()</code> since it introduces additional cognitive overhead to the reader, additional troubleshooting steps, performance degradation and, in general, additional complexity that should be avoided.</p>
</div>
<div id="use-cases" class="section level2">
<h2><span class="header-section-number">10.2</span> Use Cases</h2>
<p>The overview <code>spark_apply()</code> examples were meant to help you understand how it works; this section will cover a few practical use case for <code>spark_apply()</code>, mostly, for parsing and loading custom file formats and modeling data over partitioned sets.</p>
<div id="custom-parsers" class="section level3">
<h3><span class="header-section-number">10.2.1</span> Custom Parsers</h3>
<p>While Spark and its various extensions provide support for many file formats: CSVs, JSON, Parquet, AVRO, etc. there are many more file formats that one might want to use at scale through <code>spark_apply()</code> and the right set of R packages available in CRAN. This section presents two use cases for parsing log and WARC files.</p>
<div id="log-parser" class="section level4">
<h4><span class="header-section-number">10.2.1.1</span> Log Parser</h4>
<p>It is common to use Spark to analize log files, say for instance, logs that track download data from Amazon S3. To parse logs, the <code>webreadr</code> package can simplify this process by providing support to load logs stored as: Amzon S3, Squid and Common format. For instance, an S3 log looks as follows:</p>
<pre><code>#Version: 1.0
#Fields: date time x-edge-location sc-bytes c-ip cs-method cs(Host) cs-uri-stem sc-status cs(Referer) cs(User-Agent) cs-uri-query cs(Cookie) x-edge-result-type x-edge-request-id x-host-header cs-protocol cs-bytes time-taken 
2014-05-23  01:13:11    FRA2    182 192.0.2.10  GET d111111abcdef8.cloudfront.net   /view/my/file.html  200 www.displaymyfiles.com  Mozilla/4.0%20(compatible;%20MSIE%205.0b1;%20Mac_PowerPC)   -   zip=98101   RefreshHit  MRVMF7KydIvxMWfJIglgwHQwZsbG2IhRJ07sn9AkKUFSHS9EXAMPLE==    d111111abcdef8.cloudfront.net   http    -   0.001</code></pre>
<p>Which can be parsed easily with <code>read_aws()</code> as follows:</p>
<div class="sourceCode" id="cb124"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb124-1" data-line-number="1">aws_log &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;extdata/log.aws&quot;</span>, <span class="dt">package =</span> <span class="st">&quot;webreadr&quot;</span>)</a>
<a class="sourceLine" id="cb124-2" data-line-number="2">webreadr<span class="op">::</span><span class="kw">read_aws</span>(aws_log)</a></code></pre></div>
<pre><code># A tibble: 2 x 18
  date                edge_location bytes_sent ip_address http_method host  path 
  &lt;dttm&gt;              &lt;chr&gt;              &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;
1 2014-05-23 01:13:11 FRA2                 182 192.0.2.10 GET         d111… /vie…
2 2014-05-23 01:13:12 LAX1             2390282 192.0.2.2… GET         d111… /sou…
# ... with 11 more variables: status_code &lt;int&gt;, referer &lt;chr&gt;, user_agent &lt;chr&gt;,
#   query &lt;chr&gt;, cookie &lt;chr&gt;, result_type &lt;chr&gt;, request_id &lt;chr&gt;, host_header &lt;chr&gt;,
#   protocol &lt;chr&gt;, bytes_received &lt;chr&gt;, time_elapsed &lt;dbl&gt;</code></pre>
<p>We can make use of <code>read_aws()</code> as follows in Spark with <code>spark_apply()</code>:</p>
<div class="sourceCode" id="cb126"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb126-1" data-line-number="1"><span class="kw">spark_read_text</span>(sc, <span class="st">&quot;logs&quot;</span>, aws_log, <span class="dt">overwrite =</span> <span class="ot">TRUE</span>, <span class="dt">whole =</span> <span class="ot">TRUE</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb126-2" data-line-number="2"><span class="st">  </span><span class="kw">spark_apply</span>(<span class="op">~</span>webreadr<span class="op">::</span><span class="kw">read_aws</span>(.x<span class="op">$</span>contents))</a></code></pre></div>
<pre><code># Source: spark&lt;?&gt; [?? x 18]
  date                edge_location bytes_sent ip_address http_method host  path 
* &lt;dttm&gt;              &lt;chr&gt;              &lt;int&gt; &lt;chr&gt;      &lt;chr&gt;       &lt;chr&gt; &lt;chr&gt;
1 2014-05-23 01:13:11 FRA2                 182 192.0.2.10 GET         d111… /vie…
2 2014-05-23 01:13:12 LAX1             2390282 192.0.2.2… GET         d111… /sou…
# ... with 11 more variables: status_code &lt;int&gt;, referer &lt;chr&gt;, user_agent &lt;chr&gt;,
#   query &lt;chr&gt;, cookie &lt;chr&gt;, result_type &lt;chr&gt;, request_id &lt;chr&gt;,
#   host_header &lt;chr&gt;, protocol &lt;chr&gt;, bytes_received &lt;chr&gt;, time_elapsed &lt;dbl&gt;</code></pre>
</div>
<div id="warc-parser" class="section level4">
<h4><span class="header-section-number">10.2.1.2</span> WARC Parser</h4>
<p>by parsing WARC (Web ARChive) file from the Common Crawl project.</p>
<p>The Common Crawl project builds and maintains an open repository of web crawl data that can be accessed and analyzed by anyone. You can use their petabytes of data to analyzing the contents of the web without having to manually download each web page, which is a much more time consuming process.</p>
<p>The challenge with WARC files is that multiple records are embedded within the same text file, conceptually, a WARC file looks as follows:</p>
<pre><code>WARC/1.0
&lt;html&gt;...&lt;/html&gt;
WARC/1.0
&lt;html&gt;...&lt;/html&gt;</code></pre>
<p>We could read all those lines using <code>spark_read_text()</code>; however, they wouldn’t be grouped by web page response which is required to analyze properly the contents. Using the <code>warc</code> package, we can easily parse these files in R running,</p>
<div class="sourceCode" id="cb129"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb129-1" data-line-number="1">warc_example &lt;-<span class="st"> </span><span class="kw">system.file</span>(<span class="st">&quot;samples/sample.warc.gz&quot;</span>, <span class="dt">package =</span> <span class="st">&quot;warc&quot;</span>)</a>
<a class="sourceLine" id="cb129-2" data-line-number="2"><span class="kw">as_tibble</span>(warc<span class="op">::</span><span class="kw">read_warc</span>(warc_example))</a></code></pre></div>
<pre><code># A tibble: 17 x 1
   content                                                                              
   &lt;fct&gt;                                                                              
 1 &quot;WARC/1.0\nWARC-Type: warcinfo\nWARC-Date: 2016-12-13T03:16:04Z\nWARC-Record-ID: &lt;ur…
 2 &quot;WARC/1.0\nWARC-Type: request\nWARC-Date: 2016-12-11T14:00:57Z\nWARC-Record-ID: &lt;urn…
 3 &quot;WARC/1.0\nWARC-Type: response\nWARC-Date: 2016-12-11T14:00:57Z\nWARC-Record-ID: &lt;ur…
 4 &quot;WARC/1.0\nWARC-Type: metadata\nWARC-Date: 2016-12-11T14:00:57Z\nWARC-Record-ID: &lt;ur…
 5 &quot;WARC/1.0\nWARC-Type: request\nWARC-Date: 2016-12-11T14:08:53Z\nWARC-Record-ID: &lt;urn…</code></pre>
<p>The <code>warc</code> package also allows you to easily extract specific lines efficiently to, for instance, extract the language of each web page:</p>
<div class="sourceCode" id="cb131"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb131-1" data-line-number="1">warc<span class="op">::</span><span class="kw">read_warc</span>(warc_example, <span class="dt">line_filter =</span> <span class="st">&quot;&lt;meta http-equiv=</span><span class="ch">\&quot;</span><span class="st">Content-Language</span><span class="ch">\&quot;</span><span class="st">&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb131-2" data-line-number="2"><span class="st">  </span><span class="kw">transmute</span>(<span class="dt">language =</span> <span class="kw">gsub</span>(<span class="st">&quot;.*content=</span><span class="ch">\&quot;</span><span class="st">|</span><span class="ch">\&quot;</span><span class="st">.*&quot;</span>, <span class="st">&quot;&quot;</span>, content)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb131-3" data-line-number="3"><span class="st">  </span><span class="kw">group_by</span>(language) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb131-4" data-line-number="4"><span class="st">  </span><span class="kw">summarise</span>(<span class="dt">count =</span> <span class="kw">n</span>())</a></code></pre></div>
<pre><code># A tibble: 1 x 2
  language count
  &lt;chr&gt;    &lt;int&gt;
1 ru-RU        5</code></pre>
<p>Now, in order to use this in Spark we can simply run:</p>
<div class="sourceCode" id="cb133"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb133-1" data-line-number="1">paths &lt;-<span class="st"> </span><span class="kw">readLines</span>(<span class="kw">system.file</span>(<span class="st">&quot;samples/warc.paths&quot;</span>, <span class="dt">package =</span> <span class="st">&quot;warc&quot;</span>))</a>
<a class="sourceLine" id="cb133-2" data-line-number="2">paths_tbl &lt;-<span class="st"> </span><span class="kw">copy_to</span>(sc, paths, <span class="dt">repartition =</span> <span class="kw">length</span>(paths))</a>
<a class="sourceLine" id="cb133-3" data-line-number="3"></a>
<a class="sourceLine" id="cb133-4" data-line-number="4">paths_tbl <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb133-5" data-line-number="5"><span class="st">  </span><span class="kw">head</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb133-6" data-line-number="6"><span class="st">  </span><span class="kw">spark_apply</span>(<span class="cf">function</span>(entry) {</a>
<a class="sourceLine" id="cb133-7" data-line-number="7">    path &lt;-<span class="st"> </span><span class="kw">sub</span>(<span class="st">&quot;s3n://commoncrawl/&quot;</span>, <span class="st">&quot;https://commoncrawl.s3.amazonaws.com/&quot;</span>, entry[<span class="dv">1</span>,])</a>
<a class="sourceLine" id="cb133-8" data-line-number="8">    <span class="kw">download.file</span>(<span class="dt">url =</span> path, <span class="dt">destfile =</span> temp_warc)</a>
<a class="sourceLine" id="cb133-9" data-line-number="9">    sparkwarc<span class="op">::</span><span class="kw">spark_rcpp_read_warc</span>(path, match_warc, match_line)</a>
<a class="sourceLine" id="cb133-10" data-line-number="10">  })</a></code></pre></div>
<p>You can remove <code>head()</code> to process a quite large dataset of WARC files,</p>
</div>
</div>
<div id="partitioned-modeling" class="section level3">
<h3><span class="header-section-number">10.2.2</span> Partitioned Modeling</h3>
<p>There are many modeling packages available in R that can also be run at scale by partitioning the data into manegable groups that do fit in the resources of a single machine. For instance, suppose that you have a 1TB dataset for sales data across multiple cities and you are tasked with creating sales predictions oever each city. Yopu can then considered partitioning the original dataset per city say, into 10GB of data per city, which can be managed by a single compute instance. For this kind of partitionable model task, you can also consider using <code>spark_apply()</code> by training each model over each city in parallel with access to all the packages available in R.</p>
<p>For instance, we could run a linear regression over the <code>iris</code> dataset over each Species as follows:</p>
<div class="sourceCode" id="cb134"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb134-1" data-line-number="1">iris_tbl &lt;-<span class="st"> </span><span class="kw">sdf_copy_to</span>(sc, iris)</a>
<a class="sourceLine" id="cb134-2" data-line-number="2"></a>
<a class="sourceLine" id="cb134-3" data-line-number="3">iris_tbl <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb134-4" data-line-number="4"><span class="st">  </span><span class="kw">spark_apply</span>(nrow, <span class="dt">group_by =</span> <span class="st">&quot;Species&quot;</span>)</a></code></pre></div>
<pre><code>## # Source:   table&lt;sparklyr_tmp_378c1b8155f3&gt; [?? x 2]
## # Database: spark_connection
##      Species Sepal_Length
##        &lt;chr&gt;        &lt;int&gt;
## 1 versicolor           50
## 2  virginica           50
## 3     setosa           50</code></pre>
<div class="sourceCode" id="cb136"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb136-1" data-line-number="1">iris_tbl <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb136-2" data-line-number="2"><span class="st">  </span><span class="kw">spark_apply</span>(</a>
<a class="sourceLine" id="cb136-3" data-line-number="3">    <span class="cf">function</span>(e) <span class="kw">summary</span>(<span class="kw">lm</span>(Petal_Length <span class="op">~</span><span class="st"> </span>Petal_Width, e))<span class="op">$</span>r.squared,</a>
<a class="sourceLine" id="cb136-4" data-line-number="4">    <span class="dt">names =</span> <span class="st">&quot;r.squared&quot;</span>,</a>
<a class="sourceLine" id="cb136-5" data-line-number="5">    <span class="dt">group_by =</span> <span class="st">&quot;Species&quot;</span>)</a></code></pre></div>
<pre><code>## # Source:   table&lt;sparklyr_tmp_378c30e6155&gt; [?? x 2]
## # Database: spark_connection
##      Species r.squared
##        &lt;chr&gt;     &lt;dbl&gt;
## 1 versicolor 0.6188467
## 2  virginica 0.1037537
## 3     setosa 0.1099785</code></pre>
<p>These covers some of the common use cases for <code>spark_apply()</code>, but you are certainly welcome to find others that may fit your particular needs.</p>
</div>
</div>
<div id="partitions" class="section level2">
<h2><span class="header-section-number">10.3</span> Partitions</h2>
</div>
<div id="columns" class="section level2">
<h2><span class="header-section-number">10.4</span> Columns</h2>
<p>Inference vs Excplicit</p>
<div class="sourceCode" id="cb138"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb138-1" data-line-number="1">iris_tbl &lt;-<span class="st"> </span><span class="kw">spark_apply</span>(</a>
<a class="sourceLine" id="cb138-2" data-line-number="2">  I,</a>
<a class="sourceLine" id="cb138-3" data-line-number="3">  <span class="dt">columns =</span> <span class="kw">lapply</span>(iris, class)</a>
<a class="sourceLine" id="cb138-4" data-line-number="4">)</a></code></pre></div>
</div>
<div id="grouping" class="section level2">
<h2><span class="header-section-number">10.5</span> Grouping</h2>
<div class="sourceCode" id="cb139"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb139-1" data-line-number="1"><span class="kw">sdf_len</span>(sc, <span class="dv">10</span>, <span class="dt">repartition =</span> <span class="dv">1</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb139-2" data-line-number="2"><span class="st">  </span><span class="kw">transmute</span>(<span class="dt">groups =</span> <span class="kw">floor</span>(id <span class="op">/</span><span class="st"> </span><span class="dv">2</span>)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb139-3" data-line-number="3"><span class="st">  </span><span class="kw">spark_apply</span>(<span class="op">~</span><span class="kw">nrow</span>(.x))</a></code></pre></div>
<pre><code># Source: spark&lt;?&gt; [?? x 1]
  result
*  &lt;int&gt;
1      5
2      5</code></pre>
<div class="sourceCode" id="cb141"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb141-1" data-line-number="1"><span class="kw">sdf_len</span>(sc, <span class="dv">10</span>, <span class="dt">repartition =</span> <span class="dv">1</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb141-2" data-line-number="2"><span class="st">  </span><span class="kw">transmute</span>(<span class="dt">groups =</span> <span class="kw">floor</span>(id <span class="op">/</span><span class="st"> </span><span class="dv">2</span>)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb141-3" data-line-number="3"><span class="st">  </span><span class="kw">sdf_repartition</span>(<span class="dt">partition_by =</span> <span class="st">&quot;groups&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb141-4" data-line-number="4"><span class="st">  </span><span class="kw">spark_apply</span>(<span class="op">~</span><span class="kw">nrow</span>(.x))</a></code></pre></div>
<pre><code># Source: spark&lt;?&gt; [?? x 1]
  result
*  &lt;int&gt;
1      4
2      1
3      4
4      1</code></pre>
<div class="sourceCode" id="cb143"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb143-1" data-line-number="1"><span class="kw">sdf_len</span>(sc, <span class="dv">10</span>, <span class="dt">repartition =</span> <span class="dv">1</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb143-2" data-line-number="2"><span class="st">  </span><span class="kw">transmute</span>(<span class="dt">groups =</span> <span class="kw">floor</span>(id <span class="op">/</span><span class="st"> </span><span class="dv">2</span>)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb143-3" data-line-number="3"><span class="st">  </span><span class="kw">sdf_repartition</span>(<span class="dt">partition_by =</span> <span class="st">&quot;groups&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb143-4" data-line-number="4"><span class="st">  </span><span class="kw">spark_apply</span>(<span class="op">~</span><span class="kw">nrow</span>(.x), <span class="dt">group_by =</span> <span class="st">&quot;groups&quot;</span>)</a></code></pre></div>
<pre><code># Source: spark&lt;?&gt; [?? x 2]
  groups result
*  &lt;dbl&gt;  &lt;int&gt;
1      1      2
2      2      2
3      5      1
4      3      2
5      4      2
6      0      1</code></pre>
<p>Notice that <code>spark_apply()</code> does not repartition data automatically, so optimizing how data is repartitioned mus be considered using <code>sdf_repartition()</code>.</p>
</div>
<div id="packages" class="section level2">
<h2><span class="header-section-number">10.6</span> Packages</h2>
</div>
<div id="context" class="section level2">
<h2><span class="header-section-number">10.7</span> Context</h2>
<div class="sourceCode" id="cb145"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb145-1" data-line-number="1"><span class="kw">sdf_len</span>(sc, <span class="dv">3</span>, <span class="dt">repartition =</span> <span class="dv">3</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb145-2" data-line-number="2"><span class="st">  </span><span class="kw">spark_apply</span>(<span class="cf">function</span>(data, context) context, <span class="dt">context =</span> <span class="kw">data.frame</span>(<span class="dt">something =</span> <span class="kw">c</span>(<span class="st">&quot;foo&quot;</span>, <span class="st">&quot;bar&quot;</span>)))</a></code></pre></div>
<pre><code># Source: spark&lt;?&gt; [?? x 1]
  something
* &lt;chr&gt;    
1 foo      
2 bar      
3 foo      
4 bar      
5 foo      
6 bar    </code></pre>
<div class="sourceCode" id="cb147"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb147-1" data-line-number="1"><span class="kw">sdf_len</span>(sc, <span class="dv">3</span>, <span class="dt">repartition =</span> <span class="dv">3</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb147-2" data-line-number="2"><span class="st">  </span><span class="kw">spark_apply</span>(</a>
<a class="sourceLine" id="cb147-3" data-line-number="3">    <span class="cf">function</span>(data, context) context<span class="op">$</span>numbers <span class="op">*</span><span class="st"> </span>context<span class="op">$</span>constant,</a>
<a class="sourceLine" id="cb147-4" data-line-number="4">    <span class="dt">context =</span> <span class="kw">list</span>(</a>
<a class="sourceLine" id="cb147-5" data-line-number="5">      <span class="dt">numbers =</span> <span class="kw">c</span>(<span class="dv">2</span>, <span class="dv">3</span>, <span class="dv">5</span>),</a>
<a class="sourceLine" id="cb147-6" data-line-number="6">      <span class="dt">constant =</span> <span class="dv">10</span></a>
<a class="sourceLine" id="cb147-7" data-line-number="7">    )</a>
<a class="sourceLine" id="cb147-8" data-line-number="8">  )</a></code></pre></div>
<pre><code># Source: spark&lt;?&gt; [?? x 1]
  result
*  &lt;dbl&gt;
1     20
2     30
3     50
4     20
5     30
6     50
7     20
8     30
9     50</code></pre>
</div>
<div id="restrictions" class="section level2">
<h2><span class="header-section-number">10.8</span> Restrictions</h2>
<div id="troubleshooting-3" class="section level3">
<h3><span class="header-section-number">10.8.1</span> Troubleshooting</h3>
<p>There are a couple common troubleshooting techniquest in <code>spark_apply()</code></p>
<div id="worker-logs" class="section level4">
<h4><span class="header-section-number">10.8.1.1</span> Worker Logs</h4>
<p>Whenever <code>spark_apply()</code> is executed, information regarding execution is written over each worker node. You can use this log to write custom messages o help you diagnose and fine-tune your code.</p>
<p>For instance, suppose that you don’t know what the first column name of <code>df</code> is, we can write a custom log message executed from the worker nodes using <code>worker_log()</code> as follows:</p>
<div class="sourceCode" id="cb149"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb149-1" data-line-number="1"><span class="kw">sdf_len</span>(sc, <span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">spark_apply</span>(<span class="cf">function</span>(df) {</a>
<a class="sourceLine" id="cb149-2" data-line-number="2">  <span class="kw">worker_log</span>(<span class="st">&quot;the first column in the data frame is named &quot;</span>, <span class="kw">names</span>(df)[[<span class="dv">1</span>]])</a>
<a class="sourceLine" id="cb149-3" data-line-number="3">  df</a>
<a class="sourceLine" id="cb149-4" data-line-number="4">})</a></code></pre></div>
<pre><code># Source: spark&lt;?&gt; [?? x 1]
     id
* &lt;int&gt;
1     1</code></pre>
<p>When running locally, we can filter the log entries for the worker as follows:</p>
<div class="sourceCode" id="cb151"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb151-1" data-line-number="1"><span class="kw">spark_log</span>(sc, <span class="dt">filter =</span> <span class="st">&quot;sparklyr: RScript&quot;</span>)</a></code></pre></div>
<pre><code>18/12/18 11:33:47 INFO sparklyr: RScript (3513) the first column in the dataframe is named id 
18/12/18 11:33:47 INFO sparklyr: RScript (3513) computed closure 
18/12/18 11:33:47 INFO sparklyr: RScript (3513) updating 1 rows 
18/12/18 11:33:47 INFO sparklyr: RScript (3513) updated 1 rows 
18/12/18 11:33:47 INFO sparklyr: RScript (3513) finished apply 
18/12/18 11:33:47 INFO sparklyr: RScript (3513) finished </code></pre>
<p>Notice that the logs show out custom log entry showing that <code>id</code> is the name of the first column in the given data frame.</p>
<p>This functionality is useful when troubleshooting errors, for instance, if we force an error using the <code>stop()</code> function:</p>
<div class="sourceCode" id="cb153"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb153-1" data-line-number="1"><span class="kw">sdf_len</span>(sc, <span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">spark_apply</span>(<span class="cf">function</span>(df) {</a>
<a class="sourceLine" id="cb153-2" data-line-number="2">  <span class="kw">stop</span>(<span class="st">&quot;force an error&quot;</span>)</a>
<a class="sourceLine" id="cb153-3" data-line-number="3">})</a></code></pre></div>
<p>You will get an error similar to,</p>
<pre><code> Error in force(code) : 
  sparklyr worker rscript failure, check worker logs for details</code></pre>
<p>As suggested in the error, we can look in the worker logs for the specific errors as follows:</p>
<div class="sourceCode" id="cb155"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb155-1" data-line-number="1"><span class="kw">spark_log</span>(sc)</a></code></pre></div>
<p>This will show an entry containing the error and the callstack:</p>
<pre><code>18/12/18 11:26:47 INFO sparklyr: RScript (1860) computing closure 
18/12/18 11:26:47 ERROR sparklyr: RScript (1860) terminated unexpectedly: force an error 
18/12/18 11:26:47 ERROR sparklyr: RScript (1860) collected callstack: 
11: stop(&quot;force and error&quot;)
10: (function (df) 
{
    stop(&quot;force and error&quot;)
})(structure(list(id = 1L), class = &quot;data.frame&quot;, row.names = c(NA, 
-1L)))</code></pre>
<p>Notice that, spark_log(sc) only retrieves the worker logs when using local clusters, when running in proper clusters with multiple machines, you will have to use the tools and user interface provided by the cluster manager to find these log entries.</p>
</div>
<div id="worker-error" class="section level4">
<h4><span class="header-section-number">10.8.1.2</span> Worker Error</h4>
<div class="sourceCode" id="cb157"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb157-1" data-line-number="1"><span class="kw">sdf_len</span>(sc, <span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">spark_apply</span>(<span class="cf">function</span>(df) {</a>
<a class="sourceLine" id="cb157-2" data-line-number="2">    <span class="kw">tryCatch</span>({</a>
<a class="sourceLine" id="cb157-3" data-line-number="3">        <span class="kw">stop</span>(<span class="st">&quot;an error&quot;</span>)</a>
<a class="sourceLine" id="cb157-4" data-line-number="4">    }, <span class="dt">error =</span> <span class="cf">function</span>(e) {</a>
<a class="sourceLine" id="cb157-5" data-line-number="5">        e<span class="op">$</span>message</a>
<a class="sourceLine" id="cb157-6" data-line-number="6">    })</a>
<a class="sourceLine" id="cb157-7" data-line-number="7">})</a></code></pre></div>
</div>
<div id="worker-partitions" class="section level4">
<h4><span class="header-section-number">10.8.1.3</span> Worker Partitions</h4>
<p>If a particular partition fails, you can detect the broken partition by computing a digest, and then retrieving that particular partition as follows:</p>
<div class="sourceCode" id="cb158"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb158-1" data-line-number="1"><span class="kw">sdf_len</span>(sc, <span class="dv">3</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">spark_apply</span>(<span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb158-2" data-line-number="2">    <span class="kw">worker_log</span>(<span class="st">&quot;processing &quot;</span>, digest<span class="op">::</span><span class="kw">digest</span>(x), <span class="st">&quot; partition&quot;</span>)</a>
<a class="sourceLine" id="cb158-3" data-line-number="3">    <span class="co"># your code</span></a>
<a class="sourceLine" id="cb158-4" data-line-number="4">})</a></code></pre></div>
<p>This will add an entry similar to:</p>
<pre><code>18/11/03 14:48:32 INFO sparklyr: RScript (2566) processing f35b1c321df0162e3f914adfb70b5416 partition </code></pre>
<p>When executing this in your cluster, you will have to look in the logs for the task that is not finishing, once you have that digest, you can cancel the job.</p>
<p>Then you can use that digest to retrieve that specific data frame to R with something like:</p>
<div class="sourceCode" id="cb160"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb160-1" data-line-number="1">broken_partition &lt;-<span class="st"> </span><span class="kw">sdf_len</span>(sc, <span class="dv">3</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">spark_apply</span>(<span class="cf">function</span>(x) {</a>
<a class="sourceLine" id="cb160-2" data-line-number="2">    <span class="cf">if</span> (<span class="kw">identical</span>(digest<span class="op">::</span><span class="kw">digest</span>(x), <span class="st">&quot;f35b1c321df0162e3f914adfb70b5416&quot;</span>)) x <span class="cf">else</span> x[<span class="dv">0</span>,]</a>
<a class="sourceLine" id="cb160-3" data-line-number="3">}) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">collect</span>()</a></code></pre></div>
<p>WHich you can then run in R to troubleshoot further.</p>
</div>
<div id="worker-debugger" class="section level4">
<h4><span class="header-section-number">10.8.1.4</span> Worker Debugger</h4>
<div class="sourceCode" id="cb161"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb161-1" data-line-number="1"><span class="kw">sdf_len</span>(sc, <span class="dv">1</span>) <span class="op">%&gt;%</span><span class="st"> </span><span class="kw">spark_apply</span>(<span class="cf">function</span>() {</a>
<a class="sourceLine" id="cb161-2" data-line-number="2">  <span class="kw">stop</span>(<span class="st">&quot;Error!&quot;</span>)</a>
<a class="sourceLine" id="cb161-3" data-line-number="3">}, <span class="dt">debug =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
<pre><code>Debugging spark_apply(), connect to worker debugging session as follows:
  1. Find the workers &lt;sessionid&gt; and &lt;port&gt; in the worker logs, from RStudio click
     &#39;Log&#39; under the connection, look for the last entry with contents:
     &#39;Session (&lt;sessionid&gt;) is waiting for sparklyr client to connect to port &lt;port&gt;&#39;
  2. From a new R session run:
     debugonce(sparklyr:::spark_worker_main)
     sparklyr:::spark_worker_main(&lt;sessionid&gt;, &lt;port&gt;)</code></pre>
</div>
</div>
</div>
<div id="clusters-1" class="section level2">
<h2><span class="header-section-number">10.9</span> Clusters</h2>
<p>When using <code>spark_apply()</code>, R needs to be properly installed in each worker node. Different cluster managers, distributions and services, proivide different solutions to install additional software; those instructions should be followed when installing R over each worker node. To mention a few,</p>
<ul>
<li><strong>Spark Standalone</strong>: Requires connecting to each machine and installing R; there are tools like <code>pssh</code> that allow you to run a single installation command against multiple machines.</li>
<li><strong>Cloudera</strong>: Provides an R parcel, see <a href="https://blog.cloudera.com/blog/2017/09/how-to-distribute-your-r-code-with-sparklyr-and-cdsw/">“How to Distribute your R code with sparklyr and Cloudera Data Science Workbench”</a><span class="citation">(<span class="citeproc-not-found" data-reference-id="cloudera-sparklyr-parcel"><strong>???</strong></span>)</span>, which enables R over each worker node.</li>
<li><strong>Amazon EMR</strong>: R is pre-installed when starting an EMR cluster as mentioned in the <a href="clusters.html#clusters-amazon-emr">Amazon EMR</a> section.</li>
<li><strong>Microsoft HDInsight</strong>: R is pre-installed and no additional steps are needed.</li>
</ul>
</div>
<div id="apache-arrow" class="section level2">
<h2><span class="header-section-number">10.10</span> Apache Arrow</h2>
<p>Apache Arrow is strongly adviced while working <code>spark_apply()</code>, it’s available since Spark 2.3.0.</p>
</div>
<div id="recap-4" class="section level2">
<h2><span class="header-section-number">10.11</span> Recap</h2>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="extensions.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="streaming.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

</body>

</html>
