<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 6 Clusters | Mastering Apache Spark with R</title>
  <meta name="description" content="The complete guide to large-scale analysis and modeling.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 6 Clusters | Mastering Apache Spark with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The complete guide to large-scale analysis and modeling." />
  <meta name="github-repo" content="javierluraschi/the-r-in-spark" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 6 Clusters | Mastering Apache Spark with R" />
  
  <meta name="twitter:description" content="The complete guide to large-scale analysis and modeling." />
  

<meta name="author" content="Javier Luraschi, Kevin Kuo, Edgar Ruiz">


<meta name="date" content="2019-06-02">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="pipelines.html">
<link rel="next" href="connections.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.3/htmlwidgets.js"></script>
<script src="libs/dagre-0.0.1/dagre.min.js"></script>
<script src="libs/lodash-3.7.0/lodash.js"></script>
<script src="libs/nomnoml-0.2.0/nomnoml.js"></script>
<script src="libs/nomnoml-binding-0.1.0/nomnoml.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-119986300-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-119986300-1');
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./">Learning Apache Spark with R</a></strong></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#intro-background"><i class="fa fa-check"></i><b>1.1</b> Information</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#intro-hadoop"><i class="fa fa-check"></i><b>1.2</b> Hadoop</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#intro-spark"><i class="fa fa-check"></i><b>1.3</b> Spark</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#intro-r"><i class="fa fa-check"></i><b>1.4</b> R</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#intro-sparklyr"><i class="fa fa-check"></i><b>1.5</b> sparklyr</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#intro-recap"><i class="fa fa-check"></i><b>1.6</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="starting.html"><a href="starting.html"><i class="fa fa-check"></i><b>2</b> Getting Started</a><ul>
<li class="chapter" data-level="2.1" data-path="starting.html"><a href="starting.html#starting-prerequisites"><i class="fa fa-check"></i><b>2.1</b> Prerequisites</a></li>
<li class="chapter" data-level="2.2" data-path="starting.html"><a href="starting.html#starting-install-sparklyr"><i class="fa fa-check"></i><b>2.2</b> Installing sparklyr</a></li>
<li class="chapter" data-level="2.3" data-path="starting.html"><a href="starting.html#starting-installing-spark"><i class="fa fa-check"></i><b>2.3</b> Installing Spark</a></li>
<li class="chapter" data-level="2.4" data-path="starting.html"><a href="starting.html#starting-connect-to-spark"><i class="fa fa-check"></i><b>2.4</b> Connecting to Spark</a></li>
<li class="chapter" data-level="2.5" data-path="starting.html"><a href="starting.html#starting-sparklyr-hello-world"><i class="fa fa-check"></i><b>2.5</b> Using Spark</a><ul>
<li class="chapter" data-level="2.5.1" data-path="starting.html"><a href="starting.html#starting-spark-web-interface"><i class="fa fa-check"></i><b>2.5.1</b> Web Interface</a></li>
<li class="chapter" data-level="2.5.2" data-path="starting.html"><a href="starting.html#starting-analysis"><i class="fa fa-check"></i><b>2.5.2</b> Analysis</a></li>
<li class="chapter" data-level="2.5.3" data-path="starting.html"><a href="starting.html#starting-modeling"><i class="fa fa-check"></i><b>2.5.3</b> Modeling</a></li>
<li class="chapter" data-level="2.5.4" data-path="starting.html"><a href="starting.html#starting-data"><i class="fa fa-check"></i><b>2.5.4</b> Data</a></li>
<li class="chapter" data-level="2.5.5" data-path="starting.html"><a href="starting.html#starting-extensions"><i class="fa fa-check"></i><b>2.5.5</b> Extensions</a></li>
<li class="chapter" data-level="2.5.6" data-path="starting.html"><a href="starting.html#starting-distributed-r"><i class="fa fa-check"></i><b>2.5.6</b> Distributed R</a></li>
<li class="chapter" data-level="2.5.7" data-path="starting.html"><a href="starting.html#starting-streaming"><i class="fa fa-check"></i><b>2.5.7</b> Streaming</a></li>
<li class="chapter" data-level="2.5.8" data-path="starting.html"><a href="starting.html#starting-logs"><i class="fa fa-check"></i><b>2.5.8</b> Logs</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="starting.html"><a href="starting.html#starting-disconnecting"><i class="fa fa-check"></i><b>2.6</b> Disconnecting</a></li>
<li class="chapter" data-level="2.7" data-path="starting.html"><a href="starting.html#starting-using-spark-from-rstudio"><i class="fa fa-check"></i><b>2.7</b> Using RStudio</a></li>
<li class="chapter" data-level="2.8" data-path="starting.html"><a href="starting.html#starting-resources"><i class="fa fa-check"></i><b>2.8</b> Resources</a></li>
<li class="chapter" data-level="2.9" data-path="starting.html"><a href="starting.html#starting-recap"><i class="fa fa-check"></i><b>2.9</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="analysis.html"><a href="analysis.html"><i class="fa fa-check"></i><b>3</b> Analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="analysis.html"><a href="analysis.html#r-as-an-interface-to-spark"><i class="fa fa-check"></i><b>3.1</b> R as an Interface to Spark</a><ul>
<li class="chapter" data-level="3.1.1" data-path="analysis.html"><a href="analysis.html#exercise"><i class="fa fa-check"></i><b>3.1.1</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="analysis.html"><a href="analysis.html#import-access"><i class="fa fa-check"></i><b>3.2</b> Import / Access</a></li>
<li class="chapter" data-level="3.3" data-path="analysis.html"><a href="analysis.html#wrangle"><i class="fa fa-check"></i><b>3.3</b> Wrangle</a><ul>
<li class="chapter" data-level="3.3.1" data-path="analysis.html"><a href="analysis.html#correlations"><i class="fa fa-check"></i><b>3.3.1</b> Correlations</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="analysis.html"><a href="analysis.html#visualize"><i class="fa fa-check"></i><b>3.4</b> Visualize</a><ul>
<li class="chapter" data-level="3.4.1" data-path="analysis.html"><a href="analysis.html#recommended-approach"><i class="fa fa-check"></i><b>3.4.1</b> Recommended approach</a></li>
<li class="chapter" data-level="3.4.2" data-path="analysis.html"><a href="analysis.html#simple-plots"><i class="fa fa-check"></i><b>3.4.2</b> Simple Plots</a></li>
<li class="chapter" data-level="3.4.3" data-path="analysis.html"><a href="analysis.html#histograms"><i class="fa fa-check"></i><b>3.4.3</b> Histograms</a></li>
<li class="chapter" data-level="3.4.4" data-path="analysis.html"><a href="analysis.html#scatter-vs-raster-plots"><i class="fa fa-check"></i><b>3.4.4</b> Scatter vs Raster Plots</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="analysis.html"><a href="analysis.html#model"><i class="fa fa-check"></i><b>3.5</b> Model</a><ul>
<li class="chapter" data-level="3.5.1" data-path="analysis.html"><a href="analysis.html#cache-model-data"><i class="fa fa-check"></i><b>3.5.1</b> Cache model data</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="analysis.html"><a href="analysis.html#communicate"><i class="fa fa-check"></i><b>3.6</b> Communicate</a><ul>
<li class="chapter" data-level="3.6.1" data-path="analysis.html"><a href="analysis.html#reports"><i class="fa fa-check"></i><b>3.6.1</b> Reports</a></li>
<li class="chapter" data-level="3.6.2" data-path="analysis.html"><a href="analysis.html#presentation-decks"><i class="fa fa-check"></i><b>3.6.2</b> Presentation decks</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="analysis.html"><a href="analysis.html#recap"><i class="fa fa-check"></i><b>3.7</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="modeling.html"><a href="modeling.html"><i class="fa fa-check"></i><b>4</b> Modeling</a><ul>
<li class="chapter" data-level="4.1" data-path="modeling.html"><a href="modeling.html#overview"><i class="fa fa-check"></i><b>4.1</b> Overview</a></li>
<li class="chapter" data-level="4.2" data-path="modeling.html"><a href="modeling.html#the-data"><i class="fa fa-check"></i><b>4.2</b> The Data</a></li>
<li class="chapter" data-level="4.3" data-path="modeling.html"><a href="modeling.html#exploratory-data-analysis"><i class="fa fa-check"></i><b>4.3</b> Exploratory Data Analysis</a></li>
<li class="chapter" data-level="4.4" data-path="modeling.html"><a href="modeling.html#feature-engineering"><i class="fa fa-check"></i><b>4.4</b> Feature Engineering</a></li>
<li class="chapter" data-level="4.5" data-path="modeling.html"><a href="modeling.html#model-building"><i class="fa fa-check"></i><b>4.5</b> Model Building</a><ul>
<li class="chapter" data-level="4.5.1" data-path="modeling.html"><a href="modeling.html#logistic-regression-as-a-generalized-linear-regression"><i class="fa fa-check"></i><b>4.5.1</b> Logistic Regression as a Generalized Linear Regression</a></li>
<li class="chapter" data-level="4.5.2" data-path="modeling.html"><a href="modeling.html#more-machine-learning-algorithms"><i class="fa fa-check"></i><b>4.5.2</b> More Machine Learning Algorithms</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="modeling.html"><a href="modeling.html#working-with-textual-data"><i class="fa fa-check"></i><b>4.6</b> Working with Textual Data</a><ul>
<li class="chapter" data-level="4.6.1" data-path="modeling.html"><a href="modeling.html#data-prep"><i class="fa fa-check"></i><b>4.6.1</b> Data Prep</a></li>
<li class="chapter" data-level="4.6.2" data-path="modeling.html"><a href="modeling.html#topic-modeling"><i class="fa fa-check"></i><b>4.6.2</b> Topic Modeling</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="modeling.html"><a href="modeling.html#conclusion"><i class="fa fa-check"></i><b>4.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="pipelines.html"><a href="pipelines.html"><i class="fa fa-check"></i><b>5</b> Pipelines</a><ul>
<li class="chapter" data-level="5.1" data-path="pipelines.html"><a href="pipelines.html#estimators-and-transformers"><i class="fa fa-check"></i><b>5.1</b> Estimators and Transformers</a></li>
<li class="chapter" data-level="5.2" data-path="pipelines.html"><a href="pipelines.html#pipelines-and-pipeline-models"><i class="fa fa-check"></i><b>5.2</b> Pipelines and Pipeline Models</a></li>
<li class="chapter" data-level="5.3" data-path="pipelines.html"><a href="pipelines.html#applying-pipelines-to-okcupid-data"><i class="fa fa-check"></i><b>5.3</b> Applying Pipelines to OKCupid Data</a><ul>
<li class="chapter" data-level="5.3.1" data-path="pipelines.html"><a href="pipelines.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>5.3.1</b> Hyperparameter Tuning</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="pipelines.html"><a href="pipelines.html#operating-modes-of-pipelines-functions"><i class="fa fa-check"></i><b>5.4</b> Operating Modes of Pipelines Functions</a></li>
<li class="chapter" data-level="5.5" data-path="pipelines.html"><a href="pipelines.html#model-persistence-and-interoperability"><i class="fa fa-check"></i><b>5.5</b> Model Persistence and Interoperability</a><ul>
<li class="chapter" data-level="5.5.1" data-path="pipelines.html"><a href="pipelines.html#sparklyr-ml-models"><i class="fa fa-check"></i><b>5.5.1</b> Sparklyr ML Models</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="6" data-path="clusters.html"><a href="clusters.html"><i class="fa fa-check"></i><b>6</b> Clusters</a><ul>
<li class="chapter" data-level="6.1" data-path="clusters.html"><a href="clusters.html#clusters-overview"><i class="fa fa-check"></i><b>6.1</b> Overview</a></li>
<li class="chapter" data-level="6.2" data-path="clusters.html"><a href="clusters.html#on-premise"><i class="fa fa-check"></i><b>6.2</b> On-Premise</a><ul>
<li class="chapter" data-level="6.2.1" data-path="clusters.html"><a href="clusters.html#clusters-manager"><i class="fa fa-check"></i><b>6.2.1</b> Managers</a></li>
<li class="chapter" data-level="6.2.2" data-path="clusters.html"><a href="clusters.html#distributions"><i class="fa fa-check"></i><b>6.2.2</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="clusters.html"><a href="clusters.html#cloud"><i class="fa fa-check"></i><b>6.3</b> Cloud</a><ul>
<li class="chapter" data-level="6.3.1" data-path="clusters.html"><a href="clusters.html#clusters-amazon-emr"><i class="fa fa-check"></i><b>6.3.1</b> Amazon</a></li>
<li class="chapter" data-level="6.3.2" data-path="clusters.html"><a href="clusters.html#databricks"><i class="fa fa-check"></i><b>6.3.2</b> Databricks</a></li>
<li class="chapter" data-level="6.3.3" data-path="clusters.html"><a href="clusters.html#google"><i class="fa fa-check"></i><b>6.3.3</b> Google</a></li>
<li class="chapter" data-level="6.3.4" data-path="clusters.html"><a href="clusters.html#ibm"><i class="fa fa-check"></i><b>6.3.4</b> IBM</a></li>
<li class="chapter" data-level="6.3.5" data-path="clusters.html"><a href="clusters.html#microsoft"><i class="fa fa-check"></i><b>6.3.5</b> Microsoft</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="clusters.html"><a href="clusters.html#kubernetes"><i class="fa fa-check"></i><b>6.4</b> Kubernetes</a></li>
<li class="chapter" data-level="6.5" data-path="clusters.html"><a href="clusters.html#tools"><i class="fa fa-check"></i><b>6.5</b> Tools</a><ul>
<li class="chapter" data-level="6.5.1" data-path="clusters.html"><a href="clusters.html#rstudio"><i class="fa fa-check"></i><b>6.5.1</b> RStudio</a></li>
<li class="chapter" data-level="6.5.2" data-path="clusters.html"><a href="clusters.html#jupyter"><i class="fa fa-check"></i><b>6.5.2</b> Jupyter</a></li>
<li class="chapter" data-level="6.5.3" data-path="clusters.html"><a href="clusters.html#clusters-livy"><i class="fa fa-check"></i><b>6.5.3</b> Livy</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="clusters.html"><a href="clusters.html#recap-1"><i class="fa fa-check"></i><b>6.6</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="connections.html"><a href="connections.html"><i class="fa fa-check"></i><b>7</b> Connections</a><ul>
<li class="chapter" data-level="7.1" data-path="connections.html"><a href="connections.html#connections-overview"><i class="fa fa-check"></i><b>7.1</b> Overview</a><ul>
<li class="chapter" data-level="7.1.1" data-path="connections.html"><a href="connections.html#connections-spark-edge-nodes"><i class="fa fa-check"></i><b>7.1.1</b> Edge Nodes</a></li>
<li class="chapter" data-level="7.1.2" data-path="connections.html"><a href="connections.html#connections-spark-home"><i class="fa fa-check"></i><b>7.1.2</b> Spark Home</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="connections.html"><a href="connections.html#connections-local"><i class="fa fa-check"></i><b>7.2</b> Local</a></li>
<li class="chapter" data-level="7.3" data-path="connections.html"><a href="connections.html#connections-standalone"><i class="fa fa-check"></i><b>7.3</b> Standalone</a></li>
<li class="chapter" data-level="7.4" data-path="connections.html"><a href="connections.html#connections-yarn"><i class="fa fa-check"></i><b>7.4</b> Yarn</a><ul>
<li class="chapter" data-level="7.4.1" data-path="connections.html"><a href="connections.html#connections-yarn-client"><i class="fa fa-check"></i><b>7.4.1</b> Yarn Client</a></li>
<li class="chapter" data-level="7.4.2" data-path="connections.html"><a href="connections.html#connections-yarn-cluster"><i class="fa fa-check"></i><b>7.4.2</b> Yarn Cluster</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="connections.html"><a href="connections.html#connections-livy"><i class="fa fa-check"></i><b>7.5</b> Livy</a></li>
<li class="chapter" data-level="7.6" data-path="connections.html"><a href="connections.html#connections-mesos"><i class="fa fa-check"></i><b>7.6</b> Mesos</a></li>
<li class="chapter" data-level="7.7" data-path="connections.html"><a href="connections.html#connections-kubernetes"><i class="fa fa-check"></i><b>7.7</b> Kubernetes</a></li>
<li class="chapter" data-level="7.8" data-path="connections.html"><a href="connections.html#cloud-1"><i class="fa fa-check"></i><b>7.8</b> Cloud</a></li>
<li class="chapter" data-level="7.9" data-path="connections.html"><a href="connections.html#batches"><i class="fa fa-check"></i><b>7.9</b> Batches</a></li>
<li class="chapter" data-level="7.10" data-path="connections.html"><a href="connections.html#tools-1"><i class="fa fa-check"></i><b>7.10</b> Tools</a></li>
<li class="chapter" data-level="7.11" data-path="connections.html"><a href="connections.html#multiple"><i class="fa fa-check"></i><b>7.11</b> Multiple</a></li>
<li class="chapter" data-level="7.12" data-path="connections.html"><a href="connections.html#connections-troubleshooting"><i class="fa fa-check"></i><b>7.12</b> Troubleshooting</a><ul>
<li class="chapter" data-level="7.12.1" data-path="connections.html"><a href="connections.html#logging"><i class="fa fa-check"></i><b>7.12.1</b> Logging</a></li>
<li class="chapter" data-level="7.12.2" data-path="connections.html"><a href="connections.html#troubleshoot-spark-submit"><i class="fa fa-check"></i><b>7.12.2</b> Spark Submit</a></li>
<li class="chapter" data-level="7.12.3" data-path="connections.html"><a href="connections.html#windows"><i class="fa fa-check"></i><b>7.12.3</b> Windows</a></li>
</ul></li>
<li class="chapter" data-level="7.13" data-path="connections.html"><a href="connections.html#recap-2"><i class="fa fa-check"></i><b>7.13</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>8</b> Data</a><ul>
<li class="chapter" data-level="8.1" data-path="data.html"><a href="data.html#source-types-and-file-systems"><i class="fa fa-check"></i><b>8.1</b> Source types and file systems</a><ul>
<li class="chapter" data-level="8.1.1" data-path="data.html"><a href="data.html#default-packages"><i class="fa fa-check"></i><b>8.1.1</b> Default packages</a></li>
<li class="chapter" data-level="8.1.2" data-path="data.html"><a href="data.html#source-types"><i class="fa fa-check"></i><b>8.1.2</b> Source types</a></li>
<li class="chapter" data-level="8.1.3" data-path="data.html"><a href="data.html#file-systems"><i class="fa fa-check"></i><b>8.1.3</b> File systems</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="data.html"><a href="data.html#reading-data"><i class="fa fa-check"></i><b>8.2</b> Reading data</a><ul>
<li class="chapter" data-level="8.2.1" data-path="data.html"><a href="data.html#folders-as-a-table"><i class="fa fa-check"></i><b>8.2.1</b> Folders as a table</a></li>
<li class="chapter" data-level="8.2.2" data-path="data.html"><a href="data.html#file-layout"><i class="fa fa-check"></i><b>8.2.2</b> File layout</a></li>
<li class="chapter" data-level="8.2.3" data-path="data.html"><a href="data.html#spark-memory"><i class="fa fa-check"></i><b>8.2.3</b> Spark memory</a></li>
<li class="chapter" data-level="8.2.4" data-path="data.html"><a href="data.html#column-names"><i class="fa fa-check"></i><b>8.2.4</b> Column Names</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="data.html"><a href="data.html#writing-data"><i class="fa fa-check"></i><b>8.3</b> Writing Data</a><ul>
<li class="chapter" data-level="8.3.1" data-path="data.html"><a href="data.html#spark-not-r-as-pass-through"><i class="fa fa-check"></i><b>8.3.1</b> Spark, not R, as pass-through</a></li>
<li class="chapter" data-level="8.3.2" data-path="data.html"><a href="data.html#practical-approach"><i class="fa fa-check"></i><b>8.3.2</b> Practical approach</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="data.html"><a href="data.html#date-time"><i class="fa fa-check"></i><b>8.4</b> Date &amp; time</a></li>
<li class="chapter" data-level="8.5" data-path="data.html"><a href="data.html#specific-types-and-protocols"><i class="fa fa-check"></i><b>8.5</b> Specific types and protocols</a><ul>
<li class="chapter" data-level="8.5.1" data-path="data.html"><a href="data.html#amazon-s3"><i class="fa fa-check"></i><b>8.5.1</b> Amazon S3</a></li>
<li class="chapter" data-level="8.5.2" data-path="data.html"><a href="data.html#sql"><i class="fa fa-check"></i><b>8.5.2</b> SQL</a></li>
<li class="chapter" data-level="8.5.3" data-path="data.html"><a href="data.html#hive"><i class="fa fa-check"></i><b>8.5.3</b> Hive</a></li>
<li class="chapter" data-level="8.5.4" data-path="data.html"><a href="data.html#comma-delimited-values-csv"><i class="fa fa-check"></i><b>8.5.4</b> Comma Delimited Values (CSV)</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="data.html"><a href="data.html#recap-3"><i class="fa fa-check"></i><b>8.6</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="tuning.html"><a href="tuning.html"><i class="fa fa-check"></i><b>9</b> Tuning</a><ul>
<li class="chapter" data-level="9.1" data-path="tuning.html"><a href="tuning.html#overview-1"><i class="fa fa-check"></i><b>9.1</b> Overview</a><ul>
<li class="chapter" data-level="9.1.1" data-path="tuning.html"><a href="tuning.html#tuning-graph-visualization"><i class="fa fa-check"></i><b>9.1.1</b> Graph</a></li>
<li class="chapter" data-level="9.1.2" data-path="tuning.html"><a href="tuning.html#tuning-event-timeline"><i class="fa fa-check"></i><b>9.1.2</b> Timeline</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="tuning.html"><a href="tuning.html#tuning-configuring"><i class="fa fa-check"></i><b>9.2</b> Configuring</a><ul>
<li class="chapter" data-level="9.2.1" data-path="tuning.html"><a href="tuning.html#connect-settings"><i class="fa fa-check"></i><b>9.2.1</b> Connect Settings</a></li>
<li class="chapter" data-level="9.2.2" data-path="tuning.html"><a href="tuning.html#submit-settings"><i class="fa fa-check"></i><b>9.2.2</b> Submit Settings</a></li>
<li class="chapter" data-level="9.2.3" data-path="tuning.html"><a href="tuning.html#runtime-settings"><i class="fa fa-check"></i><b>9.2.3</b> Runtime Settings</a></li>
<li class="chapter" data-level="9.2.4" data-path="tuning.html"><a href="tuning.html#sparklyr-settings"><i class="fa fa-check"></i><b>9.2.4</b> sparklyr Settings</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="tuning.html"><a href="tuning.html#tuning-partitioning"><i class="fa fa-check"></i><b>9.3</b> Partitioning</a><ul>
<li class="chapter" data-level="9.3.1" data-path="tuning.html"><a href="tuning.html#implicit"><i class="fa fa-check"></i><b>9.3.1</b> Implicit</a></li>
<li class="chapter" data-level="9.3.2" data-path="tuning.html"><a href="tuning.html#explicit"><i class="fa fa-check"></i><b>9.3.2</b> Explicit</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="tuning.html"><a href="tuning.html#tuning-caching"><i class="fa fa-check"></i><b>9.4</b> Caching</a><ul>
<li class="chapter" data-level="9.4.1" data-path="tuning.html"><a href="tuning.html#checkpointing"><i class="fa fa-check"></i><b>9.4.1</b> Checkpointing</a></li>
<li class="chapter" data-level="9.4.2" data-path="tuning.html"><a href="tuning.html#tuning-memory"><i class="fa fa-check"></i><b>9.4.2</b> Memory</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="tuning.html"><a href="tuning.html#tuning-shuffling"><i class="fa fa-check"></i><b>9.5</b> Shuffling</a></li>
<li class="chapter" data-level="9.6" data-path="tuning.html"><a href="tuning.html#tuning-serialization"><i class="fa fa-check"></i><b>9.6</b> Serialization</a></li>
<li class="chapter" data-level="9.7" data-path="tuning.html"><a href="tuning.html#configuration-files"><i class="fa fa-check"></i><b>9.7</b> Configuration Files</a></li>
<li class="chapter" data-level="9.8" data-path="tuning.html"><a href="tuning.html#recap-4"><i class="fa fa-check"></i><b>9.8</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="extensions.html"><a href="extensions.html"><i class="fa fa-check"></i><b>10</b> Extensions</a><ul>
<li class="chapter" data-level="10.1" data-path="extensions.html"><a href="extensions.html#overview-2"><i class="fa fa-check"></i><b>10.1</b> Overview</a></li>
<li class="chapter" data-level="10.2" data-path="extensions.html"><a href="extensions.html#h2o"><i class="fa fa-check"></i><b>10.2</b> H2O</a></li>
<li class="chapter" data-level="10.3" data-path="extensions.html"><a href="extensions.html#graphs"><i class="fa fa-check"></i><b>10.3</b> Graphs</a></li>
<li class="chapter" data-level="10.4" data-path="extensions.html"><a href="extensions.html#xgboost"><i class="fa fa-check"></i><b>10.4</b> XGBoost</a></li>
<li class="chapter" data-level="10.5" data-path="extensions.html"><a href="extensions.html#deep-learning"><i class="fa fa-check"></i><b>10.5</b> Deep Learning</a></li>
<li class="chapter" data-level="10.6" data-path="extensions.html"><a href="extensions.html#genomics"><i class="fa fa-check"></i><b>10.6</b> Genomics</a></li>
<li class="chapter" data-level="10.7" data-path="extensions.html"><a href="extensions.html#spatial"><i class="fa fa-check"></i><b>10.7</b> Spatial</a></li>
<li class="chapter" data-level="10.8" data-path="extensions.html"><a href="extensions.html#troubleshooting"><i class="fa fa-check"></i><b>10.8</b> Troubleshooting</a></li>
<li class="chapter" data-level="10.9" data-path="extensions.html"><a href="extensions.html#recap-5"><i class="fa fa-check"></i><b>10.9</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="distributed.html"><a href="distributed.html"><i class="fa fa-check"></i><b>11</b> Distributed R</a><ul>
<li class="chapter" data-level="11.1" data-path="distributed.html"><a href="distributed.html#overview-3"><i class="fa fa-check"></i><b>11.1</b> Overview</a></li>
<li class="chapter" data-level="11.2" data-path="distributed.html"><a href="distributed.html#use-cases"><i class="fa fa-check"></i><b>11.2</b> Use Cases</a><ul>
<li class="chapter" data-level="11.2.1" data-path="distributed.html"><a href="distributed.html#custom-parsers"><i class="fa fa-check"></i><b>11.2.1</b> Custom Parsers</a></li>
<li class="chapter" data-level="11.2.2" data-path="distributed.html"><a href="distributed.html#partitioned-modeling"><i class="fa fa-check"></i><b>11.2.2</b> Partitioned Modeling</a></li>
<li class="chapter" data-level="11.2.3" data-path="distributed.html"><a href="distributed.html#distributed-grid-search"><i class="fa fa-check"></i><b>11.2.3</b> Grid Search</a></li>
<li class="chapter" data-level="11.2.4" data-path="distributed.html"><a href="distributed.html#web-apis"><i class="fa fa-check"></i><b>11.2.4</b> Web APIs</a></li>
<li class="chapter" data-level="11.2.5" data-path="distributed.html"><a href="distributed.html#distributed-rendering"><i class="fa fa-check"></i><b>11.2.5</b> Distributed Rendering</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="distributed.html"><a href="distributed.html#partitions"><i class="fa fa-check"></i><b>11.3</b> Partitions</a></li>
<li class="chapter" data-level="11.4" data-path="distributed.html"><a href="distributed.html#grouping"><i class="fa fa-check"></i><b>11.4</b> Grouping</a></li>
<li class="chapter" data-level="11.5" data-path="distributed.html"><a href="distributed.html#columns"><i class="fa fa-check"></i><b>11.5</b> Columns</a></li>
<li class="chapter" data-level="11.6" data-path="distributed.html"><a href="distributed.html#context"><i class="fa fa-check"></i><b>11.6</b> Context</a></li>
<li class="chapter" data-level="11.7" data-path="distributed.html"><a href="distributed.html#functions"><i class="fa fa-check"></i><b>11.7</b> Functions</a></li>
<li class="chapter" data-level="11.8" data-path="distributed.html"><a href="distributed.html#packages"><i class="fa fa-check"></i><b>11.8</b> Packages</a></li>
<li class="chapter" data-level="11.9" data-path="distributed.html"><a href="distributed.html#cluster-requirements"><i class="fa fa-check"></i><b>11.9</b> Cluster Requirements</a><ul>
<li class="chapter" data-level="11.9.1" data-path="distributed.html"><a href="distributed.html#installing-r"><i class="fa fa-check"></i><b>11.9.1</b> Installing R</a></li>
<li class="chapter" data-level="11.9.2" data-path="distributed.html"><a href="distributed.html#apache-arrow"><i class="fa fa-check"></i><b>11.9.2</b> Apache Arrow</a></li>
</ul></li>
<li class="chapter" data-level="11.10" data-path="distributed.html"><a href="distributed.html#troubleshooting-1"><i class="fa fa-check"></i><b>11.10</b> Troubleshooting</a><ul>
<li class="chapter" data-level="11.10.1" data-path="distributed.html"><a href="distributed.html#worker-logs"><i class="fa fa-check"></i><b>11.10.1</b> Worker Logs</a></li>
<li class="chapter" data-level="11.10.2" data-path="distributed.html"><a href="distributed.html#resolving-timeouts"><i class="fa fa-check"></i><b>11.10.2</b> Resolving Timeouts</a></li>
<li class="chapter" data-level="11.10.3" data-path="distributed.html"><a href="distributed.html#inspecting-partition"><i class="fa fa-check"></i><b>11.10.3</b> Inspecting Partition</a></li>
<li class="chapter" data-level="11.10.4" data-path="distributed.html"><a href="distributed.html#debugging-workers"><i class="fa fa-check"></i><b>11.10.4</b> Debugging Workers</a></li>
</ul></li>
<li class="chapter" data-level="11.11" data-path="distributed.html"><a href="distributed.html#recap-6"><i class="fa fa-check"></i><b>11.11</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="streaming.html"><a href="streaming.html"><i class="fa fa-check"></i><b>12</b> Streaming</a><ul>
<li class="chapter" data-level="12.1" data-path="streaming.html"><a href="streaming.html#spark-streaming"><i class="fa fa-check"></i><b>12.1</b> Spark Streaming</a></li>
<li class="chapter" data-level="12.2" data-path="streaming.html"><a href="streaming.html#working-with-spark-streams"><i class="fa fa-check"></i><b>12.2</b> Working with Spark Streams</a></li>
<li class="chapter" data-level="12.3" data-path="streaming.html"><a href="streaming.html#sparklyr-extras"><i class="fa fa-check"></i><b>12.3</b> <code>sparklyr</code> extras</a><ul>
<li class="chapter" data-level="12.3.1" data-path="streaming.html"><a href="streaming.html#stream-monitor"><i class="fa fa-check"></i><b>12.3.1</b> Stream monitor</a></li>
<li class="chapter" data-level="12.3.2" data-path="streaming.html"><a href="streaming.html#stream-generator"><i class="fa fa-check"></i><b>12.3.2</b> Stream generator</a></li>
<li class="chapter" data-level="12.3.3" data-path="streaming.html"><a href="streaming.html#shiny-reactive"><i class="fa fa-check"></i><b>12.3.3</b> Shiny reactive</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="streaming.html"><a href="streaming.html#intro-example"><i class="fa fa-check"></i><b>12.4</b> Intro example</a></li>
<li class="chapter" data-level="12.5" data-path="streaming.html"><a href="streaming.html#transformations"><i class="fa fa-check"></i><b>12.5</b> Transformations</a><ul>
<li class="chapter" data-level="12.5.1" data-path="streaming.html"><a href="streaming.html#dplyr"><i class="fa fa-check"></i><b>12.5.1</b> dplyr</a></li>
<li class="chapter" data-level="12.5.2" data-path="streaming.html"><a href="streaming.html#transformer-functions"><i class="fa fa-check"></i><b>12.5.2</b> Transformer functions</a></li>
<li class="chapter" data-level="12.5.3" data-path="streaming.html"><a href="streaming.html#r-code"><i class="fa fa-check"></i><b>12.5.3</b> R code</a></li>
<li class="chapter" data-level="12.5.4" data-path="streaming.html"><a href="streaming.html#ml-pipelines"><i class="fa fa-check"></i><b>12.5.4</b> ML Pipelines</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="streaming.html"><a href="streaming.html#shiny-integration"><i class="fa fa-check"></i><b>12.6</b> Shiny integration</a></li>
<li class="chapter" data-level="12.7" data-path="streaming.html"><a href="streaming.html#kafka"><i class="fa fa-check"></i><b>12.7</b> Kafka</a><ul>
<li class="chapter" data-level="12.7.1" data-path="streaming.html"><a href="streaming.html#workflow"><i class="fa fa-check"></i><b>12.7.1</b> Workflow</a></li>
<li class="chapter" data-level="12.7.2" data-path="streaming.html"><a href="streaming.html#spark-integration"><i class="fa fa-check"></i><b>12.7.2</b> Spark integration</a></li>
<li class="chapter" data-level="12.7.3" data-path="streaming.html"><a href="streaming.html#r-integration"><i class="fa fa-check"></i><b>12.7.3</b> R integration</a></li>
<li class="chapter" data-level="12.7.4" data-path="streaming.html"><a href="streaming.html#example"><i class="fa fa-check"></i><b>12.7.4</b> Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="contributing.html"><a href="contributing.html"><i class="fa fa-check"></i><b>13</b> Contributing</a><ul>
<li class="chapter" data-level="13.1" data-path="contributing.html"><a href="contributing.html#contributing-overview"><i class="fa fa-check"></i><b>13.1</b> Overview</a></li>
<li class="chapter" data-level="13.2" data-path="contributing.html"><a href="contributing.html#contributing-spark-api"><i class="fa fa-check"></i><b>13.2</b> Spark API</a></li>
<li class="chapter" data-level="13.3" data-path="contributing.html"><a href="contributing.html#spark-extensions"><i class="fa fa-check"></i><b>13.3</b> Spark Extensions</a></li>
<li class="chapter" data-level="13.4" data-path="contributing.html"><a href="contributing.html#scala-code"><i class="fa fa-check"></i><b>13.4</b> Scala Code</a></li>
<li class="chapter" data-level="13.5" data-path="contributing.html"><a href="contributing.html#recap-7"><i class="fa fa-check"></i><b>13.5</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>14</b> Appendix</a><ul>
<li class="chapter" data-level="14.1" data-path="appendix.html"><a href="appendix.html#appendix-prerequisites"><i class="fa fa-check"></i><b>14.1</b> Prerequisites</a><ul>
<li class="chapter" data-level="14.1.1" data-path="appendix.html"><a href="appendix.html#appendix-install-r"><i class="fa fa-check"></i><b>14.1.1</b> Installing R</a></li>
<li class="chapter" data-level="14.1.2" data-path="appendix.html"><a href="appendix.html#appendix-install-java"><i class="fa fa-check"></i><b>14.1.2</b> Installing Java</a></li>
<li class="chapter" data-level="14.1.3" data-path="appendix.html"><a href="appendix.html#appendix-install-rstudio"><i class="fa fa-check"></i><b>14.1.3</b> Installing RStudio</a></li>
<li class="chapter" data-level="14.1.4" data-path="appendix.html"><a href="appendix.html#appendix-using-rstudio"><i class="fa fa-check"></i><b>14.1.4</b> Using RStudio</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="appendix.html"><a href="appendix.html#diagrams"><i class="fa fa-check"></i><b>14.2</b> Diagrams</a><ul>
<li class="chapter" data-level="14.2.1" data-path="appendix.html"><a href="appendix.html#appendix-storage-capacity"><i class="fa fa-check"></i><b>14.2.1</b> Worlds Store Capacity</a></li>
<li class="chapter" data-level="14.2.2" data-path="appendix.html"><a href="appendix.html#appendix-cran-downloads"><i class="fa fa-check"></i><b>14.2.2</b> Daily downloads of CRAN packages</a></li>
<li class="chapter" data-level="14.2.3" data-path="appendix.html"><a href="appendix.html#appendix-cluster-trends"><i class="fa fa-check"></i><b>14.2.3</b> Google trends for mainframes, cloud computing and kubernetes</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="appendix.html"><a href="appendix.html#appendix-ggplot2-theme"><i class="fa fa-check"></i><b>14.3</b> Formatting</a></li>
<li class="chapter" data-level="14.4" data-path="appendix.html"><a href="appendix.html#ml-functionlist"><i class="fa fa-check"></i><b>14.4</b> List of ML Functions</a><ul>
<li class="chapter" data-level="14.4.1" data-path="appendix.html"><a href="appendix.html#classification"><i class="fa fa-check"></i><b>14.4.1</b> Classification</a></li>
<li class="chapter" data-level="14.4.2" data-path="appendix.html"><a href="appendix.html#regression"><i class="fa fa-check"></i><b>14.4.2</b> Regression</a></li>
<li class="chapter" data-level="14.4.3" data-path="appendix.html"><a href="appendix.html#clustering"><i class="fa fa-check"></i><b>14.4.3</b> Clustering</a></li>
<li class="chapter" data-level="14.4.4" data-path="appendix.html"><a href="appendix.html#recommendation"><i class="fa fa-check"></i><b>14.4.4</b> Recommendation</a></li>
<li class="chapter" data-level="14.4.5" data-path="appendix.html"><a href="appendix.html#frequent-pattern-mining"><i class="fa fa-check"></i><b>14.4.5</b> Frequent Pattern Mining</a></li>
<li class="chapter" data-level="14.4.6" data-path="appendix.html"><a href="appendix.html#feature-transformers"><i class="fa fa-check"></i><b>14.4.6</b> Feature Transformers</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="appendix.html"><a href="appendix.html#kafka-1"><i class="fa fa-check"></i><b>14.5</b> Kafka</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>15</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Mastering Apache Spark with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="clusters" class="section level1">
<h1><span class="header-section-number">Chapter 6</span> Clusters</h1>
<p>Previous chapters focused on using Spark over a single computing instance, your personal computer. In this chapter we will introduce techniques to run Spark over multiple computing instances, also known as a computing cluster. This chapter and subsequent ones will introduce and make use of concepts applicable to computing clusters; however, it’s not required to use a computing cluster to follow along, you can still use your personal computer. It’s worth mentioning that while previous chapters focused on single computing instances, all the data analysis and modeling techniques we presented, can also be used in a computing cluster without changing any code.</p>
<p>For those of you who already have a Spark cluster in your organization, you could consider skipping to the next chapter, <a href="connections.html#connections">Connections</a>, which will teach you how to connect to an existing cluster. Otherwise, if you don’t have a cluster or are considering improvements to your existing infrastructure, this chapter will introduce the cluster trends, managers, and providers available today.</p>
<div id="clusters-overview" class="section level2">
<h2><span class="header-section-number">6.1</span> Overview</h2>
<p>There are three major trends in cluster computing worth discussing: <strong>On-Premise</strong>, <strong>Cloud</strong> computing, and <strong>Kubernetes</strong>. Framing these trends over time will help us understand how they came to be, what they are, and what their future might be. To illustrate this, Figure <a href="clusters.html#fig:clusters-trends">6.1</a> <a href="appendix.html#appendix-cluster-trends">plots these trends over time</a> using data from Google trends.</p>
<div class="figure" style="text-align: center"><span id="fig:clusters-trends"></span>
<img src="images/clusters-trends-resized.png" alt="Google trends for on-premise (mainframe), cloud computing and Kubernetes" width="100%" />
<p class="caption">
FIGURE 6.1: Google trends for on-premise (mainframe), cloud computing and Kubernetes
</p>
</div>
<p>For <strong>on-premise</strong> clusters, yourself or someone in your organization purchased physical computers that were intended to be used for cluster computing. The computers in this cluster are made of <em>off-the-shelf</em> hardware, meaning that someone placed an order to purchase computers usually found in stores shelves or, <em>high-performance</em> hardware, meaning that a computing vendor provided highly customized computing hardware which also comes optimized for high-performance network connectivity, power consumption, etc. When purchasing hundreds or thousands of computing instances, it doesn’t make sense to keep them in the usual computing case that we are all familiar with, instead, it makes sense to stack them as efficiently as possible on top of each other to minimize the space the use. This group of efficiently stacked computing instances is known as a <a href="https://en.wikipedia.org/wiki/Rack_unit">rack</a>. Once a cluster grows to thousands of computers, you will also need to host hundreds of racks of computing devices, at this scale, you would also need significant physical space to hosts those racks. A building that provides racks of computing instances is usually known as a <em>data center</em>. At the scale of a data center, you would also need to find ways to make the building more efficient, specially the cooling system, power supplies, network connectivity, and so on. Since this is time consuming, a few organization have come together to open source their infrastructure under the <a href="http://www.opencompute.org/">Open Compute Project</a> initiative, which provides a set of data center blueprints free for anyone to use.</p>
<p>There is nothing preventing you from building our own data center and in fact, many organizations have followed this path. For instance, Amazon started as an online book store, over the years Amazon grew to sell much more than just books and, with its online store growth, their data centers also grew in size. In 2002, Amazon considered <a href="https://en.wikipedia.org/wiki/Amazon_Web_Services#History">renting servers in their data centers to the public</a>, two year laters, Amazon Web Services launched as a way to let anyone rent servers in their data centers on-demand, meaning that, one did not have to purchase, configure, maintain nor teardown it’s own clusters but could rather rent them from Amazon directly.</p>
<p>This on-demand compute model is what we know today as <strong>Cloud Computing</strong>. In the cloud, the cluster you use is not owned by you and it’s neither in your physical building, but rather, it’s a data center owned and managed by someone else. Today, there are many cloud providers in this space ranging from Amazon, Databricks, IBM, Google, Microsoft and many others. Most cloud computing platforms provide a user interface either through a web application and command line to request and manage resources.</p>
<p>While the benefits of processing data in the <em>cloud</em> were obvious for many years, picking a cloud provider had the unintended side-effect of locking organizations with one particular provider, making it hard to switch between providers or back to on-premise clusters. <strong>Kubernetes</strong>, announced by Google in 2014, is an <a href="https://github.com/kubernetes/kubernetes/">open source system for managing containerized applications across multiple hosts</a>. In practice, it makes it easier to deploy across multiple cloud providers and on-premise as well.</p>
<p>In summary, we have seen a transition from on-premise, to cloud computing and more recently Kubernetes which have been also described as the <strong>private cloud</strong>, the <strong>public cloud</strong> and the <strong>hybrid cloud</strong> respectevely. This chapter will walk you through each cluster computing trend in the context of Spark and R.</p>
</div>
<div id="on-premise" class="section level2">
<h2><span class="header-section-number">6.2</span> On-Premise</h2>
<p>As mentioned in the overview section, on-premise clusters represent a set of computing instances procured and managed by staff members from your organization. These clusters can be highly customized and controlled; however, they can also incur higher initial expenses and maintenance costs.</p>
<p>When using On-Premise Spark clusters, there are two concepts you should consider:</p>
<ul>
<li><strong>Cluster Manager</strong>: In a similar way as to how an Operating Systems (like Windows os OS X) allows you to run multiple applications in the same computer; a cluster manager allows multiple applications to be run in the same cluster. You will have to choose one yourself when working with On-Premise clusters.</li>
<li><strong>Spark Distribution</strong>: While you can install Spark from the Apache Spark site, many companies partner with companies that can provide support and enhancements to Apache Spark which we often reffer as, Spark distributions.</li>
</ul>
<div id="clusters-manager" class="section level3">
<h3><span class="header-section-number">6.2.1</span> Managers</h3>
<p>In order to run Spark within a computing cluster, you will need to run software capable of initializing Spark over each pyshical machine and register all the available computing nodes, this software is known as a <a href="https://en.wikipedia.org/wiki/Cluster_manager">cluster manager</a>. The available cluster managers in Spark are: <strong>Spark Standalone</strong>, <strong>YARN</strong>, <strong>Mesos</strong> and <strong>Kubernetes</strong>.</p>
<p><strong>Note:</strong> In distributed systems and clusters literature, we often refer to each physical machine as a compute instance, compute node, instance or node.</p>
<div id="clusters-standalone" class="section level4">
<h4><span class="header-section-number">6.2.1.1</span> Standalone</h4>
<p>In <strong>Spark Standalone</strong>, Spark uses itself as its own cluster manager, which allows you to use Spark without installing additional software in your cluster. This can be useful if you are planning to use your cluster to only run Spark applications; if this cluster is not dedicated to Spark, a generic cluster manager like YARN, Mesos or Kubernetes would be more suitable. The Spark Standalone documentation is available under <a href="https://spark.apache.org/docs/latest/spark-standalone.html">spark.apache.org</a> <span class="citation">(“Spark Standalone Mode” <a href="#ref-clusters-spark-standalone-mode">2019</a>)</span> and contains detailed information on configuring, launching, monitoring and enabling high-availability, see Figure <a href="clusters.html#fig:clusters-spark-standalone">6.2</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:clusters-spark-standalone"></span>
<img src="images/clusters-spark-standalone-resized.png" alt="Spark Standalone Site" width="1500" />
<p class="caption">
FIGURE 6.2: Spark Standalone Site
</p>
</div>
<p>However, since Spark Standalone is contained within a Spark installation; then, by completing the <a href="starting.html#starting">Getting Started</a> chapter, you have now a Spark installation available that you can use to initialize a local Spark Standalone cluster in your own machine. In practice, you would want to start the worker nodes in different machines but, for simplicity, we will present the code to start a standalone cluster in a single machine.</p>
<p>First, retrieve the <code>SPARK_HOME</code> directory by running <code>spark_home_dir()</code> then, run <code>start-master.sh</code> and <code>start-slave.sh</code> as follows:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Retrieve the Spark installation directory</span>
spark_home &lt;-<span class="st"> </span><span class="kw">spark_home_dir</span>()

<span class="co"># Build path to start-master.sh</span>
start_master &lt;-<span class="st"> </span><span class="kw">file.path</span>(spark_home, <span class="st">&quot;sbin&quot;</span>, <span class="st">&quot;start-master.sh&quot;</span>)

<span class="co"># Execute start-master.sh to start the cluster manager master node</span>
<span class="kw">system2</span>(start_master)

<span class="co"># Build path to start-slave</span>
start_slave &lt;-<span class="st"> </span><span class="kw">file.path</span>(spark_home, <span class="st">&quot;sbin&quot;</span>, <span class="st">&quot;start-slave.sh&quot;</span>)

<span class="co"># Execute start-slave.sh to start a worker and register in master node</span>
<span class="kw">system2</span>(start_slave, <span class="kw">paste0</span>(<span class="st">&quot;spark://&quot;</span>, <span class="kw">system2</span>(<span class="st">&quot;hostname&quot;</span>, <span class="dt">stdout =</span> <span class="ot">TRUE</span>), <span class="st">&quot;:7077&quot;</span>))</code></pre>
<p>The previous command initialized the master node and a worker node, the master node interface can be accessed under <a href="http://localhost:8080">localhost:8080</a> as captured in Figure <a href="clusters.html#fig:clusters-spark-standalone-web">6.3</a>:</p>
<div class="figure" style="text-align: center"><span id="fig:clusters-spark-standalone-web"></span>
<img src="images/clusters-spark-standalone-web-ui-resized.png" alt="Spark Standalone Web Interface." width="1500" />
<p class="caption">
FIGURE 6.3: Spark Standalone Web Interface.
</p>
</div>
<p>Notice that there is one worker register in Spark standalone, you can follow the link to this worker node to see, Figure <a href="clusters.html#fig:clusters-spark-standalone-webui">6.4</a>, details for this particular worker like available memory and cores.</p>
<div class="figure" style="text-align: center"><span id="fig:clusters-spark-standalone-webui"></span>
<img src="images/clusters-spark-standalone-web-ui-worker.png" alt="Spark Standalone Worker Web Interface" width="992" />
<p class="caption">
FIGURE 6.4: Spark Standalone Worker Web Interface
</p>
</div>
<p>Once you are done performing computations in this cluster, you can simply stop all the running nodes in this local cluster by running:</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Build path to stop-all</span>
stop_all &lt;-<span class="st"> </span><span class="kw">file.path</span>(spark_home, <span class="st">&quot;sbin&quot;</span>, <span class="st">&quot;stop-all.sh&quot;</span>)

<span class="co"># Execute stop-all.sh to stop the workers and master nodes</span>
<span class="kw">system2</span>(stop_all)</code></pre>
<p>A similar approach can be followed to configure a cluster by running each <code>start-slave.sh</code> command over each machine in the cluster.</p>
<p><strong>Note:</strong> When running on a Mac, if you hit: <code>ssh: connect to host localhost port 22: Connection refused</code>, you will need to manually turn off the workers using <code>system2("jps")</code> to list the running Java process and then, <code>system2("kill", c("-9", "&lt;process id&gt;"))</code> to stop the specific workers.</p>
</div>
<div id="yarn" class="section level4">
<h4><span class="header-section-number">6.2.1.2</span> Yarn</h4>
<p>YARN for short, or Hadoop YARN, is the resource manager of the Hadoop project. It was originally developed in the Hadoop project but, refactored into it’s own project in Hadoop 2. As we mentioned in in the <a href="#introduction">introduction</a> chapter, Spark was built to speed up computation over Hadoop and therefore, it’s very common to find Spark intalled on Hadoop clusters.</p>
<p>One advantage of YARN, is that it is likely to be already installed in many existing clusters that support Hadoop; which means that you can easily use Spark with many existing Hadoop clusters without requesting any major changes to the existing cluster infrastructure. It is also very common to find Spark deployed in YARN clusters since many started out as Hadoop clusters that were eventually upgraded to also support Spark.</p>
<p>YARN applications can be submitted in two modes: <strong>yarn-client</strong> and <strong>yarn-cluster</strong>. In yarn-cluster mode the driver is running remotely (potentially), while in yarn-client mode, the driver is running locally, both modes are supported and are explained further in the <a href="connections.html#connections">connections</a> chapter.</p>
<p>YARN provides a resource management user interface useful to access logs, monitor available resources, terminate applications, etc. Once connecting to Spark from R, you will be able to manage the running application in YARN, this is shown in Figure <a href="clusters.html#fig:clusters-hadoop-yarn-site">6.5</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:clusters-hadoop-yarn-site"></span>
<img src="images/clusters-yarn-resource-manager-resized.png" alt="YARN's Resource Manager running sparklyr application" width="1500" />
<p class="caption">
FIGURE 6.5: YARN’s Resource Manager running sparklyr application
</p>
</div>
<p>Since YARN is the cluster manager from the Hadoop project, YARN’s documentation can be found under the <a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html">hadoop.apache.org</a> <span class="citation">(“Apache Hadoop Yarn” <a href="#ref-clusters-hadoop-yarn">2019</a>)</span>, you can also reference the “Running Spark on YARN” guide from <a href="https://spark.apache.org/docs/latest/running-on-yarn.html">spark.apache.org</a> <span class="citation">(<span class="citeproc-not-found" data-reference-id="clusters-spark-on-yarn"><strong>???</strong></span>)</span>.</p>
</div>
<div id="mesos" class="section level4">
<h4><span class="header-section-number">6.2.1.3</span> Mesos</h4>
<p>Apache Mesos is an open-source project to manage computer clusters. Mesos began as a research project in the UC Berkeley RAD Lab and makes use of Linux <a href="https://en.wikipedia.org/wiki/Cgroups">Cgroups</a> to provide isolation for CPU, memory, I/O and file system access.</p>
<p>Mesos, like YARN, supports executing many cluster frameworks, including Spark. However, one advantage particular to Mesos is that, it allows cluster framework like Spark to implement custom task schedulers. An scheduler is the component that coordinates in a cluster which applications get execution time and which resources are assign to them. Spark uses a coarse-grained scheduler <span class="citation">(“Spark on Mesos” <a href="#ref-clusters-spark-mesos-run-modes">2018</a>)</span> which schedules resources for the duration of the application; however, other frameworks might use Mesos’ fine-grained scheduler, which can increase the overall efficiency in the cluster by scheduling tasks in shorter intervals allowing them to share resources between them.</p>
<p>Mesos provides a web interface to manage your running applications, resources, and so on. After connecting to Spark from R, your application will be registered like any other application running in Mesos, Figure <a href="clusters.html#fig:clusters-mesos-webui">6.6</a> shows a successful connection to Spark from R.</p>
<div class="figure" style="text-align: center"><span id="fig:clusters-mesos-webui"></span>
<img src="images/clusters-mesos-webui.png" alt="Mesos web interface running Spark from R" width="1437" />
<p class="caption">
FIGURE 6.6: Mesos web interface running Spark from R
</p>
</div>
<p>Mesos is an Apache project with its documentation available under <a href="https://mesos.apache.org/">mesos.apache.org</a>. The “Running Spark on Mesos” guide from <a href="https://spark.apache.org/docs/latest/running-on-mesos.html">spark.apache.org</a> is also a great resource if you choose to use Mesos as your cluster manager.</p>
</div>
</div>
<div id="distributions" class="section level3">
<h3><span class="header-section-number">6.2.2</span> Distributions</h3>
<p>One can use a cluster manager in on-premise clusters as described in the previous section; however, many organizations choose to partner with companies providing additional management software, services and resources to help manage applications in their cluster; including, but not limited to, Apache Spark. Some of the on-premise cluster providers include: <strong>Cloudera</strong>, <strong>Hortonworks</strong> and <strong>MapR</strong> to mention a few which we will be briefly introduce next.</p>
<p><strong>Cloudera</strong>, Inc. is a United States-based software company that provides Apache Hadoop and Apache Spark-based software, support and services, and training to business customers. Cloudera’s hybrid open-source Apache Hadoop distribution, CDH (Cloudera Distribution Including Apache Hadoop), targets enterprise-class deployments of that technology. Cloudera donates more than 50% of its engineering output to the various Apache-licensed open source projects (Apache Hive, Apache Avro, Apache HBase, and so on) that combine to form the Apache Hadoop platform. Cloudera is also a sponsor of the Apache Software Foundation <span class="citation">(“Cloudera Wikipedia” <a href="#ref-clusters-cloudera-wikipedia">2018</a>)</span>.</p>
<p>Cloudera clusters make use of <strong>parcels</strong>, which are binary distributions containing program files and metadata <span class="citation">(“Cloudera Documentation” <a href="#ref-clusters-cloudera-parcel">2018</a>)</span>, Spark happens to be installed as a parcel in Cloudera. It’s beyond the scope of this book to present how to configure Cloudera clusters, resources and documentation can be found under <a href="https://www.cloudera.com/products/open-source/apache-hadoop/apache-spark.html">cloudera.com</a>, and <a href="https://blog.cloudera.com/blog/2016/09/introducing-sparklyr-an-r-interface-for-apache-spark/">“Introducing sparklyr, an R Interface for Apache Spark”</a> <span class="citation">(“Cloudera Engineering” <a href="#ref-clusters-cloudera-engineering-blog">2016</a>)</span> under Cloudera’s Engineering Blog.</p>
<p>Cloudera provides the Cloudera Manager web interface to manage resource, services, parcels, diagnostics, etc. Figure <a href="clusters.html#fig:clusters-cloudera-manager-spark">6.7</a> shows a Spark parcel running in Cloduera Manager which you can later use to connect from R.</p>
<div class="figure" style="text-align: center"><span id="fig:clusters-cloudera-manager-spark"></span>
<img src="images/clusters-cloudera-manager-resized.png" alt="Cloudera Manager running Spark parcel" width="1500" />
<p class="caption">
FIGURE 6.7: Cloudera Manager running Spark parcel
</p>
</div>
<p><code>sparklyr</code> is certified with Cloudera <span class="citation">(“Cloudera Partners” <a href="#ref-clusters-cloudera-sparklyr-certification">2017</a>)</span>; meaning that, Cloudera’s support is aware of <code>sparklyr</code> and can be effective helping organizations that are using Spark and R, the following table summarizes the versions currently certified.</p>
<table>
<thead>
<tr class="header">
<th>Cloudera Version</th>
<th>Product</th>
<th>Version</th>
<th>Components</th>
<th>Kerberos</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>CDH5.9</td>
<td>sparklyr</td>
<td>0.5</td>
<td>HDFS, Spark</td>
<td>Yes</td>
</tr>
<tr class="even">
<td>CDH5.9</td>
<td>sparklyr</td>
<td>0.6</td>
<td>HDFS, Spark</td>
<td>Yes</td>
</tr>
<tr class="odd">
<td>CDH5.9</td>
<td>sparklyr</td>
<td>0.7</td>
<td>HDFS, Spark</td>
<td>Yes</td>
</tr>
</tbody>
</table>
<p><strong>Hortonworks</strong> is a big data software company based in Santa Clara, California. The company develops, supports, and provides expertise on an expansive set of entirely open source software designed to manage data and processing for everything from IOT, to advanced analytics and machine learning. Hortonworks believes it is a data management company bridging the cloud and the datacenter <span class="citation">(“Hortonworks Wikipedia” <a href="#ref-clusters-hortonworks-wikipedia">2018</a>)</span>.</p>
<p>Hortonworks partnered with Microsoft <span class="citation">(“Hortonworks Microsoft” <a href="#ref-clusters-hortonworks-microsoft">2018</a>)</span> to improve support in Microsoft Windows for Hadoop and Spark, this used to be a differentiation point; however, comparing Hortonworks and Cloudera is less relevant today since both companies are merging in 2019 <span class="citation">(“Hortonworks Cloudera” <a href="#ref-clusters-hortonworks-cloudera">2018</a>)</span>. While the companies are merging, support for the Cloudera and Hortonworks Spark distributions are still available. Additional resources to configure Spark under Hortonworks are available under <a href="https://hortonworks.com/apache/spark/">hortonworks.com</a>.</p>
<p><strong>MapR</strong> is a business software company headquartered in Santa Clara, California. MapR provides access to a variety of data sources from a single computer cluster, including big data workloads such as Apache Hadoop and Apache Spark, a distributed file system, a multi-model database management system, and event stream processing, combining analytics in real-time with operational applications. Its technology runs on both commodity hardware and public cloud computing services <span class="citation">(“MapR Wikipedia” <a href="#ref-clusters-mapr-wikipedia">2018</a>)</span>.</p>
</div>
</div>
<div id="cloud" class="section level2">
<h2><span class="header-section-number">6.3</span> Cloud</h2>
<p>If you don’t have an on-prem cluster nor spare machines to reuse, starting with a cloud cluster can be quite convenient since it will allow you to access a proper cluster in a matter of minutes. This section will briefly mention some of the major cloud infrastructure providers and give you resources to help you get started if you choose to use a cloud provider.</p>
<p>In cloud services, the compute instances are billed for as long the Spark cluster runs; you start getting billed when the cluster launches and stops when the cluster stops. This cost needs to be multiplied by the number of instances reserved for your cluster. SO for instance, if a cloud provider chargets $1.00USD per compute instance per hour and you start a three node cluster that you use for one hour and 10 minutes; it is likely that you’ll get billed <code>$1.00 * 2 hours * 3 nodes = $6.00</code>. Some cloud providers charge per minute but, at least, you can rely on all of them charging per compute hour.</p>
<p>Please be aware that, while compute costs can be quite low for small clusters, accidentally leaving a cluster running can cause significant billing expenses. Therefore, is is worth taking the extra time to check twice that your cluster is terminated when you no longer need it. It’s also a good practice to monitor costs daily while using clusters to make sure your expectations match the daily bill.</p>
<p>From past experience, you should also plan to request compute resources in advance while dealing with large-scale projects; various cloud providers will not allow you to start a cluster with hundreds of machines before requesting them explicitly through a support request. While this can be cumbersome, it’s also a way to help you controll costs in your organization.</p>
<p>Since the cluster size is flexible, it is a good practice to start with small clusters and scale compute resources as needed. Even if you know in advance that a cluster of significant size will be required, starting small provides an opportunity to troubleshoot issues at a lower cost since it’s unlikely that your data analysis will run at scale flawlessly on the first try. As a rule oh thumb, grow the instances exponentially; if you need to run a computation over an eight node cluster, start with one node and an eighth of the entire dataset, then two nodes with a fourth, then four nodes with a half the dataset and then, finally, eight nodes and the entire dataset. As you become more experienced, you’ll develop a good sense of how to troubleshoot issues, the size of the required cluster and you’ll be able to skip intermediate steps, but for starters, this is a good practice to follow.</p>
<p>One can also use a cloud provider to acquire bare computing resources and then, install the on-premise distributions presented in the previous section yourself; for instance, you can run the Cloudera distribution on Amazon Elastic Compute Cloud (EC2). This model would avoid procuring colocated hardware, but still allow you to closely manage and customize your cluster. This book presents an overview of only the fully-managed Spark services available by cloud providers; however, you can usually find with ease instructions online on how to install on-premise distributions in the cloud.</p>
<p>Some of the major providers of cloud computing infrastructure are: Amazon, Databricks, Google, IBM and Microsoft that this section will briefly introduce.</p>
<div id="clusters-amazon-emr" class="section level3">
<h3><span class="header-section-number">6.3.1</span> Amazon</h3>
<p>Amazon provides cloud services through <a href="https://aws.amazon.com/">Amazon Web Services</a>(Amazon AWS); more specifically, provides an on-demand Spark cluster through <a href="https://aws.amazon.com/emr/">Amazon Elastic MapReduce</a> or EMR for short,</p>
<p>Detailed instructions on using R with Amazon EMR was published under Amazon’s Big Data Blog: <a href="https://aws.amazon.com/blogs/big-data/running-sparklyr-rstudios-r-interface-to-spark-on-amazon-emr/">“Running sparklyr on Amazon EMR”</a> <span class="citation">(“AWS Blog” <a href="#ref-clusters-amazon-emr-sparklyr-blog">2016</a>)</span>, this post introduced the launch of <code>sparklyr</code> and instructions to configure EMR clusters with <code>sparklyr</code>. For instance, it suggests you can use the <a href="https://aws.amazon.com/cli/">Amazon Command Line Interface</a> to launch a cluster with three nodes as follows:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="ex">aws</span> emr create-cluster --applications Name=Hadoop Name=Spark Name=Hive \
  --release-label emr-5.8.0 --service-role EMR_DefaultRole --instance-groups \
  InstanceGroupType=MASTER,InstanceCount=1,InstanceType=m3.2xlarge \
  InstanceGroupType=CORE,InstanceCount=2,InstanceType=m3.2xlarge <span class="dt">\ </span>
  <span class="ex">--bootstrap-action</span> Path=s3://aws-bigdata-blog/artifacts/aws-blog-emr-\
rstudio-sparklyr/rstudio_sparklyr_emr5.sh,Args=[<span class="st">&quot;--user-pw&quot;</span>, <span class="st">&quot;&lt;password&gt;&quot;</span>, \
  <span class="st">&quot;--rstudio&quot;</span>, <span class="st">&quot;--arrow&quot;</span>] --ec2-attributes InstanceProfile=EMR_EC2_DefaultRole</code></pre>
<p>You can then see the cluster launching, and eventually running under the AWS portal, see Figure <a href="clusters.html#fig:clusters-amazon-emr-launching">6.8</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:clusters-amazon-emr-launching"></span>
<img src="images/clusters-amazon-emr-launching-resized.png" alt="Launching an Amazon EMR Cluster" width="1500" />
<p class="caption">
FIGURE 6.8: Launching an Amazon EMR Cluster
</p>
</div>
<p>You can then navigate to the Master Public DNS and find RStudio under port 8787, for instance: <code>ec2-12-34-567-890.us-west-1.compute.amazonaws.com:8787</code>, and then login with user <code>hadoop</code> and password <code>&lt;password&gt;</code>.</p>
<p>It is also possible to launch the EMR cluster using the web interface, the same introductory post contains additional details and walkthroughs specifically designed for EMR.</p>
<p>Please remember to turn off your cluster to avoid unnecessary charges and use appropriate security restrictions when starting EMR clusters for sensitive data analysis.</p>
<p>Regarding cost, the most up to date information can be found under <a href="https://aws.amazon.com/emr/pricing/">aws.amazon.com/emr/pricing</a>. As of this writing, these are some of the instance types available in the <code>us-west-1</code> region, it is meant to provide a glimpse of the resources and costs associated with cloud processing. Notice that the “EMR price is in addition to the Amazon EC2 price (the price for the underlying servers)”.</p>
<table>
<thead>
<tr class="header">
<th>Instance</th>
<th>CPUs</th>
<th>Memory</th>
<th>Storage</th>
<th>EC2 Cost</th>
<th>EMR Cost</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>c1.medium</td>
<td>2</td>
<td>1.7GB</td>
<td>350GB</td>
<td>$0.148 USD/hr</td>
<td>$0.030 USD/hr</td>
</tr>
<tr class="even">
<td>m3.2xlarge</td>
<td>8</td>
<td>30GB</td>
<td>160GB</td>
<td>$0.616 USD/hr</td>
<td>$0.140 USD/hr</td>
</tr>
<tr class="odd">
<td>i2.8xlarge</td>
<td>32</td>
<td>244GB</td>
<td>6400GB</td>
<td>$7.502 USD/hr</td>
<td>$0.270 USD/hr</td>
</tr>
</tbody>
</table>
<p><strong>Note:</strong> We are only presentring a subset of the available compute instances for Amazon and subsequent cloud providers during 2019; however, please note that hardware (CPU speed, hard drive speed, etc.) varies between vendors and locations; therefore, you can’t use these hardware tables as an accurate price comparison, an accurate comparison would require running your particular workloads and considering other aspects beyond compute instance cost.</p>
</div>
<div id="databricks" class="section level3">
<h3><span class="header-section-number">6.3.2</span> Databricks</h3>
<p><a href="http://databricks.com">Databricks</a> is a company founded by the creators of Apache Spark, that aims to help clients with cloud-based big data processing using Spark. Databricks grew out of the <a href="https://amplab.cs.berkeley.edu/">AMPLab</a> project at University of California, Berkeley <span class="citation">(“Databricks Wikipedia” <a href="#ref-clusters-databricks-wikipedia">2018</a>)</span>.</p>
<p>Databricks provides enterprise-level cluster computing plans, while also providing a free/community tear to explore functionality and get familiar with their environment.</p>
<p>Once a cluster is launched, R and <code>sparklyr</code> can be used from Databricks notebooks following the steps from the <a href="starting.html#starting">Getting Started</a> chapter or, by installing <a href="https://docs.databricks.com/spark/latest/sparkr/rstudio.html">RStudio on Databricks</a> <span class="citation">(“Databricks Rstudio” <a href="#ref-clusters-databricks-rstudio">2018</a>)</span>. Figure <a href="clusters.html#fig:clusters-databricks-notebook">6.9</a> shows a Databricks notebook using Spark through <code>sparkylr</code>.</p>
<div class="figure" style="text-align: center"><span id="fig:clusters-databricks-notebook"></span>
<img src="images/clusters-databricks-sparklyr.png" alt="Databricks community notebook running sparklyr" width="1432" />
<p class="caption">
FIGURE 6.9: Databricks community notebook running sparklyr
</p>
</div>
<p>Additional resources are available under the Databricks Engineering Blog post: <a href="https://databricks.com/blog/2017/05/25/using-sparklyr-databricks.html">“Using sparklyr in Databricks”</a> <span class="citation">(“Databricks Blog” <a href="#ref-clusters-databricks-blog-sparklyr">2017</a>)</span> and the <a href="https://docs.databricks.com/spark/latest/sparkr/sparklyr.html">“Databricks Documentation for sparklyr”</a> <span class="citation">(“Databricks Documentation” <a href="#ref-clusters-databricks-documentation-sparklyr">2018</a>)</span>.</p>
<p>The latest pricing information can be found under <a href="https://databricks.com/product/pricing">databricks.com/product/pricing</a>, as of this writing, available plans</p>
<table>
<thead>
<tr class="header">
<th>Plan</th>
<th>Basic</th>
<th>Data Engineering</th>
<th>Data Analytics</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>AWS Standard</td>
<td>$0.07 USD/DBU</td>
<td>$0.20 USD/DBU</td>
<td>$0.40 USD/DBU</td>
</tr>
<tr class="even">
<td>Azure Standard</td>
<td></td>
<td>$0.20 USD/DBU</td>
<td>$0.40 USD/DBU</td>
</tr>
<tr class="odd">
<td>Azure Premium</td>
<td></td>
<td>$0.35 USD/DBU</td>
<td>$0.55 USD/DBU</td>
</tr>
</tbody>
</table>
<p>Notice that pricing is based on cost of DBU/hr. From Databricks, “A Databricks Unit (DBU) is a unit of Apache Spark processing capability per hour. For a varied set of instances, DBUs are a more transparent way to view usage instead of the node-hour” <span class="citation">(“Databricks Units” <a href="#ref-clusters-databricks-dbu">2018</a>)</span>.</p>
</div>
<div id="google" class="section level3">
<h3><span class="header-section-number">6.3.3</span> Google</h3>
<p>Google provides Gooble Cloud Dataproc as a cloud-based managed Spark and Hadoop service offered on Google Cloud Platform. Dataproc utilizes many Google Cloud Platform technologies such as Google Compute Engine and Google Cloud Storage to offer fully managed clusters running popular data processing frameworks such as Apache Hadoop and Apache Spark <span class="citation">(“Dataproc Wikipedia” <a href="#ref-clusters-dataproc-wikipedia">2018</a>)</span>.</p>
<p>A cluster can be easily created from the Google Cloud console or the Google Cloud command line interface as illustrated in Figure <a href="clusters.html#fig:clusters-google-dataproc-launch">6.10</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:clusters-google-dataproc-launch"></span>
<img src="images/clusters-dataproc-launching-resized.png" alt="Launching a Dataproc cluster" width="1500" />
<p class="caption">
FIGURE 6.10: Launching a Dataproc cluster
</p>
</div>
<p>Once created, ports can be forwarded to allow you to access this cluster from your machine; for instance, by launching Chrome to make use of this proxy and securely connect to the Dataproc cluster. Configuring this connection looks as follows:</p>
<pre class="sourceCode bash"><code class="sourceCode bash"><span class="ex">gcloud</span> compute ssh sparklyr-m --project=<span class="op">&lt;</span>project<span class="op">&gt;</span> --zone=<span class="op">&lt;</span>region<span class="op">&gt;</span> -- -D 1080 \
  -N <span class="st">&quot;&lt;path to chrome&gt;&quot;</span> --proxy-server=<span class="st">&quot;socks5://localhost:1080&quot;</span> \
  --user-data-dir=<span class="st">&quot;/tmp/sparklyr-m&quot;</span> http://sparklyr-m:8088</code></pre>
<p>There are various tutorials available under <a href="https://cloud.google.com/dataproc/docs/tutorials">cloud.google.com/dataproc/docs/tutorials</a>, including, a comprehensive tutorial to configure RStudio and <code>sparklyr</code> <span class="citation">(“Dataproc Sparklyr” <a href="#ref-clusters-dataproc-rstudio">2018</a>)</span>.</p>
<p>The latest pricing information can be found under <a href="https://cloud.google.com/dataproc/pricing">cloud.google.com/dataproc/pricing</a>. Notice that the cost is split between Compute Engine and a Dataproc Premium.</p>
<table>
<thead>
<tr class="header">
<th>Instance</th>
<th>CPUs</th>
<th>Memory</th>
<th>Compute Engine</th>
<th>Dataproc Premium</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>n1-standard-1</td>
<td>1</td>
<td>3.75GB</td>
<td>$0.0475 USD/hr</td>
<td>$0.010 USD/hr</td>
</tr>
<tr class="even">
<td>n1-standard-8</td>
<td>8</td>
<td>30GB</td>
<td>$0.3800 USD/hr</td>
<td>$0.080 USD/hr</td>
</tr>
<tr class="odd">
<td>n1-standard-64</td>
<td>64</td>
<td>244GB</td>
<td>$3.0400 USD/hr</td>
<td>$0.640 USD/hr</td>
</tr>
</tbody>
</table>
</div>
<div id="ibm" class="section level3">
<h3><span class="header-section-number">6.3.4</span> IBM</h3>
<p>IBM cloud computing is a set of cloud computing services for business offered by the information technology company IBM. IBM cloud includes infrastructure as a service (IaaS), software as a service (SaaS) and platform as a service (PaaS) offered through public, private and hybrid cloud delivery models, in addition to the components that make up those clouds <span class="citation">(“IBM Cloud Wikipedia” <a href="#ref-clusters-ibm-wikipedia">2018</a>)</span>.</p>
<p>From within IBM Cloud, open Watson Studio and create a Data Science project, add a Spark cluster under the project settings and launch RStudio from the Launch IDE menu. Please note that, as of this writting, the provided version of <code>sparklyr</code> was not the latest version available in CRAN, since <code>sparklyr</code> was modified to run under the IBM Cloud. In any case, please follow IBMs documentation as an authoritative reference to run R and Spark on the IBM Cloud and particularily, on how to upgrade <code>sparklyr</code> appropiately. Figure <a href="clusters.html#fig:clusters-ibm-portal">6.11</a> captures IBM’s Cloud portal launching a Spark cluster.</p>
<div class="figure" style="text-align: center"><span id="fig:clusters-ibm-portal"></span>
<img src="images/clusters-ibm-sparklyr-resized.png" alt="IBM Watson Studio launching Spark with R support" width="1500" />
<p class="caption">
FIGURE 6.11: IBM Watson Studio launching Spark with R support
</p>
</div>
<p>The most up to date pricing information is available under <a href="https://www.ibm.com/cloud/pricing">ibm.com/cloud/pricing</a>. In the following table, compute cost was normalized using 31 days from the per-month costs.</p>
<table>
<thead>
<tr class="header">
<th>Instance</th>
<th>CPUs</th>
<th>Memory</th>
<th>Storage</th>
<th>Cost</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>C1.1x1x25</td>
<td>1</td>
<td>1GB</td>
<td>25GB</td>
<td>$0.033 USD/hr</td>
</tr>
<tr class="even">
<td>C1.4x4x25</td>
<td>4</td>
<td>4GB</td>
<td>25GB</td>
<td>$0.133 USD/hr</td>
</tr>
<tr class="odd">
<td>C1.32x32x25</td>
<td>32</td>
<td>25GB</td>
<td>25GB</td>
<td>$0.962 USD/hr</td>
</tr>
</tbody>
</table>
</div>
<div id="microsoft" class="section level3">
<h3><span class="header-section-number">6.3.5</span> Microsoft</h3>
<p>Microsoft Azure is a cloud computing service created by Microsoft for building, testing, deploying, and managing applications and services through a global network of Microsoft-managed data centers. It provides software as a service (SaaS), platform as a service (PaaS) and infrastructure as a service (IaaS) and supports many different programming languages, tools and frameworks, including both Microsoft-specific and third-party software and systems <span class="citation">(“Azure Wikipedia” <a href="#ref-clusters-azure-wikipedia">2018</a>)</span>.</p>
<p>From the Azure portal, the Azure HDInsight service provides support for on-demand Spark clusters. An HDInsight cluster with support for Spark and RStudio can be easily created by selecting the ML Services cluster type. Please note that the provided version of <code>sparklyr</code> might not be the latest version available in CRAN since the default package repo seems to be initialized using an MRAN (Microsoft R Application Network) snapshot, not directly from CRAN. Figure <a href="clusters.html#fig:clusters-azure-hdinsight-mlservices">6.12</a> shows the Azure portal launching an Spark cluster with support for R.</p>
<div class="figure" style="text-align: center"><span id="fig:clusters-azure-hdinsight-mlservices"></span>
<img src="images/clusters-azure-mlservices-resized.png" alt="Creating an Azure HDInsight Spark Cluster" width="1500" />
<p class="caption">
FIGURE 6.12: Creating an Azure HDInsight Spark Cluster
</p>
</div>
<p>Up to date pricing for HDInsight is available under <a href="https://azure.microsoft.com/en-us/pricing/details/hdinsight/">azure.microsoft.com/en-us/pricing/details/hdinsight</a>.</p>
<table>
<thead>
<tr class="header">
<th>Instance</th>
<th>CPUs</th>
<th>Memory</th>
<th>Total Cost</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>D1 v2</td>
<td>1</td>
<td>3.5 GB</td>
<td>$0.074/hour</td>
</tr>
<tr class="even">
<td>D4 v2</td>
<td>8</td>
<td>28 GB</td>
<td>$0.59/hour</td>
</tr>
<tr class="odd">
<td>G5</td>
<td>64</td>
<td>448 GB</td>
<td>$9.298/hour</td>
</tr>
</tbody>
</table>
</div>
</div>
<div id="kubernetes" class="section level2">
<h2><span class="header-section-number">6.4</span> Kubernetes</h2>
<p>Kubernetes is an open-source container-orchestration system for automating deployment, scaling and management of containerized applications that was originally designed by Google and now maintained by the <a href="https://www.cncf.io/">Cloud Native Computing Foundation</a>. Kubernetes was originally based on <a href="https://www.docker.com/">Docker</a> while, like Mesos, it’s also based on Linux Cgroups.</p>
<p>Kubernetes can execute many cluster applications and frameworks that can be highly customized by using container images with specific resources and libraries. This allows a single Kubernetes cluster to be used for many different purposes beyond data analysis, which in turn helps organizations manage their compute resources with ease. One trade off from using custom images is that they add additional configuration overhead but make kubernetes clusters extremely flexible. Nevertheless, this flexibility has proven to be instrumental to administrate with ease cluster resources in many organizations and, as shown in the <a href="clusters.html#clusters-overview">overview</a> section, it’s becoming a very popular cluster framework.</p>
<p>Kubernetes is supported across all major cloud providers. They all provide extensive documentation as to how to launch, manage and teard down Kubernetes clusters; Figure <a href="clusters.html#fig:clusters-kubernetes-google-console">6.13</a> shows Google Gloud’s console while creating a Kubernetes cluster. Spark can be deployed over any Kubernetes cluster and R used to connect, analyze, model and so on.</p>
<div class="figure" style="text-align: center"><span id="fig:clusters-kubernetes-google-console"></span>
<img src="images/clusters-kubernetes-google-console-resized.png" alt="Creating a Kubernetes cluster for Spark and R using Google Cloud" width="1500" />
<p class="caption">
FIGURE 6.13: Creating a Kubernetes cluster for Spark and R using Google Cloud
</p>
</div>
<p>You can learn more about <a href="https://kubernetes.io/">kubernetes.io</a>, and the “Running Spark on Kubernetes” guide from <a href="https://spark.apache.org/docs/latest/running-on-kubernetes.html">spark.apache.org</a>.</p>
<p>Strictly speaking, Kubernetes is a cluster technology not an specific cluster architecture. However, Kubernetes represents a larger trend often refered as a hybrid cloud. A hybrid cloud is a computing environment that makes use of on-premises and public cloud services with orchestration between the various platforms. It’s still early to precesily categorize the leading technologies that will form a hybrid approach to cluster computing; while Kubernetes is the leading one, many more are likely to form to complement or even replace existing technologies.</p>
</div>
<div id="tools" class="section level2">
<h2><span class="header-section-number">6.5</span> Tools</h2>
<p>While using only R and Spark can be sufficient for some clusters, it is common to install complementary tools in your cluster to improve: monitoring, sql analysis, workflow coordination, etc. with applications like <a href="http://ganglia.info/">Ganglia</a>, <a href="http://gethue.com/">Hue</a> and <a href="https://oozie.apache.org">Oozie</a> respectively. This section is not meant to cover all, but rather mention the ones that are commonly to use.</p>
<div id="rstudio" class="section level3">
<h3><span class="header-section-number">6.5.1</span> RStudio</h3>
<p>From reading the Introduction chapter, you are aware that RStudio is a well known, free, desktop development environment for R; therefore, it is likely that you are following the examples in this book using RStudio Desktop; however, you might not be aware that RStudio can also be run as a web service inside an Spark cluster, this version of RStudio is known as RStudio Server. You can see RStudio Server running in Figure <a href="clusters.html#fig:clusters-rstudio-server">6.14</a>. In the same way that the Spark UI runs in the cluster, RStudio Server can be installed inside the cluster, then you can connect to RStudio Server and use RStudio in exactly the same way you use RStudio Desktop but with the ability to run code against the Spark cluster. As you can see on the following image, RStudio Server is running on a web browser inside a Spark cluster; it looks and feels just like RStudio Desktop, but adds support to run commands efficiently by being located within the cluster.</p>
<div class="figure" style="text-align: center"><span id="fig:clusters-rstudio-server"></span>
<img src="images/clusters-rstudio-server-resized.png" alt="RStudio Server Pro running inside Apache Spark" width="1500" />
<p class="caption">
FIGURE 6.14: RStudio Server Pro running inside Apache Spark
</p>
</div>
<p>For those familiar with R, Shiny is a very popular tool for building interactive web applications from R; which it is also recommended you install directly in your Spark cluster.</p>
<p>RStudio Server and Shiny Server are a free and open source; however, RStudio also provides professional producs, like: RStudio Server, <a href="https://www.rstudio.com/products/rstudio-server-pro/">RStudio Server Pro</a> <span class="citation">(“RStudio Server Pro” <a href="#ref-clusters-rstudio-server-pro">2019</a>)</span>, <a href="https://www.rstudio.com/products/shiny-server-pro/">Shiny Server Pro</a> <span class="citation">(“Shiny Server Pro” <a href="#ref-clusters-shiny-server-pro">2019</a>)</span> and <a href="https://www.rstudio.com/products/connect/">RStudio Connect</a> <span class="citation">(“RStudio Connect” <a href="#ref-clusters-rstudio-connect">2019</a>)</span> which can be installed within the cluster to support additional R workflows, while <code>sparklyr</code> does not require any additional tools, they provide significant productivity gains worth considering. You can learn more about them at <a href="https://www.rstudio.com/products/">rstudio.com/products/</a>.</p>
</div>
<div id="jupyter" class="section level3">
<h3><span class="header-section-number">6.5.2</span> Jupyter</h3>
<p>Project <a href="http://jupyter.org/">Jupyter</a> exists to develop open-source software, open-standards, and services for interactive computing across dozens of programming languages. A Jupyter notebook, provide support for various programming languages, including R. <code>sparklyr</code> can be used with Jupyter notebooks using the R Kernel. Figure <a href="clusters.html#fig:clusters-jupyter-sparklyr">6.15</a> shows <code>sparklyr</code> running inside a local Jupyter notebook.</p>
<div class="figure" style="text-align: center"><span id="fig:clusters-jupyter-sparklyr"></span>
<img src="images/clusters-jupyter-resized.png" alt="Jupyter notebook running sparklyr" width="1500" />
<p class="caption">
FIGURE 6.15: Jupyter notebook running sparklyr
</p>
</div>
</div>
<div id="clusters-livy" class="section level3">
<h3><span class="header-section-number">6.5.3</span> Livy</h3>
<p><a href="https://livy.incubator.apache.org/">Apache Livy</a> is an incubation project in Apache providing support to use Spark clusters remotely through a web interface. It is ideal to connect directly into the Spark cluster; however, there are times where connecting directly to the cluster is not feasible. When facing those constraints, one can consider installing Livy in their cluster and secure it properly to enable remote use over web protocols. However, there is a significant performance overhead from using Livy in <code>sparklyr</code>.</p>
<p>To help test Livy locally, <code>sparklyr</code> provides support to list, install, start and stop a local Livy instance by executing:</p>
<pre><code>##    livy
## 1 0.2.0
## 2 0.3.0
## 3 0.4.0
## 4 0.5.0</code></pre>
<p>Which lists the versions that you can install, we recommend installing the latest version and verifying the installed version as follows</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Install default Livy version</span>
<span class="kw">livy_install</span>()

<span class="co"># List installed Livy services</span>
<span class="kw">livy_installed_versions</span>()

<span class="co"># Start the Livy service</span>
<span class="kw">livy_service_start</span>()</code></pre>
<p>You can then navigate to this local Livy session under <a href="http://localhost:8998" class="uri">http://localhost:8998</a>, the <a href="connections.html#connections-livy">Livy Connections</a> section will detail how to connect to this local instance and also proper clusters with Livy enabled, once connected, you can navigate to the Livy web application as captured by Figure <a href="clusters.html#fig:clusters-livy-local">6.16</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:clusters-livy-local"></span>
<img src="images/clusters-livy-local.png" alt="Apache Livy running as a local service" width="1440" />
<p class="caption">
FIGURE 6.16: Apache Livy running as a local service
</p>
</div>
<p>Make sure you also stop the Livy service when working with local Livy instances, for proper Livy services running in a cluster, you won’t have to.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="co"># Stops the Livy service</span>
<span class="kw">livy_service_stop</span>()</code></pre>
</div>
</div>
<div id="recap-1" class="section level2">
<h2><span class="header-section-number">6.6</span> Recap</h2>
<p>This chapter explained the history and tradeoffs of on-premise, cloud computing and presented Kubernetes as a promising framework to provide flexibility across on-premise and cloud providers. It also introduced cluster managers (Spark Standalone, YARN, Mesos and Kubernetes) as the software needed to run Spark as a cluster application. This chapter briefly mentioned on-premise cluster providers like Cloudera, Hortonworks and MapR as well as the major cloud providers: Amazon, Google and Microsoft.</p>
<p>While this chapter provided a solid foundation to understand current cluster computing trends, tools and providers useful to perform data science at scale; it did not provide a comprehensive framework to decide which cluster technologies to choose. Instead, use this chapter as an overview and a starting point to reach out to additional resources to help you find the cluster stack that best fits your organization needs.</p>
<p>The next chapter, <a href="connections.html#connections">connections</a>, will focus on understanding how to connect to existing clusters; therefore, it assumes a Spark cluster like the ones presented in this chapter, is already available to you.</p>

</div>
</div>
<h3> References</h3>
<div id="refs" class="references">
<div id="ref-clusters-spark-standalone-mode">
<p>“Spark Standalone Mode.” 2019. <a href="https://spark.apache.org/docs/latest/spark-standalone.html">https://spark.apache.org/docs/latest/spark-standalone.html</a>.</p>
</div>
<div id="ref-clusters-hadoop-yarn">
<p>“Apache Hadoop Yarn.” 2019. <a href="https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html">https://hadoop.apache.org/docs/current/hadoop-yarn/hadoop-yarn-site/YARN.html</a>.</p>
</div>
<div id="ref-clusters-spark-mesos-run-modes">
<p>“Spark on Mesos.” 2018. <a href="https://spark.apache.org/docs/2.4.0/running-on-mesos.html#mesos-run-modes">https://spark.apache.org/docs/2.4.0/running-on-mesos.html#mesos-run-modes</a>.</p>
</div>
<div id="ref-clusters-cloudera-wikipedia">
<p>“Cloudera Wikipedia.” 2018. <a href="https://en.wikipedia.org/wiki/Cloudera">https://en.wikipedia.org/wiki/Cloudera</a>.</p>
</div>
<div id="ref-clusters-cloudera-parcel">
<p>“Cloudera Documentation.” 2018. <a href="https://www.cloudera.com/documentation/enterprise/5-7-x/topics/cm_ig_parcels.html">https://www.cloudera.com/documentation/enterprise/5-7-x/topics/cm_ig_parcels.html</a>.</p>
</div>
<div id="ref-clusters-cloudera-engineering-blog">
<p>“Cloudera Engineering.” 2016. <a href="https://blog.cloudera.com/blog/2016/09/introducing-sparklyr-an-r-interface-for-apache-spark/">https://blog.cloudera.com/blog/2016/09/introducing-sparklyr-an-r-interface-for-apache-spark/</a>.</p>
</div>
<div id="ref-clusters-cloudera-sparklyr-certification">
<p>“Cloudera Partners.” 2017. <a href="https://www.cloudera.com/partners/partners-listing.html?q=rstudio">https://www.cloudera.com/partners/partners-listing.html?q=rstudio</a>.</p>
</div>
<div id="ref-clusters-hortonworks-wikipedia">
<p>“Hortonworks Wikipedia.” 2018. <a href="https://en.wikipedia.org/wiki/Hortonworks">https://en.wikipedia.org/wiki/Hortonworks</a>.</p>
</div>
<div id="ref-clusters-hortonworks-microsoft">
<p>“Hortonworks Microsoft.” 2018. <a href="https://hortonworks.com/partner/microsoft/">https://hortonworks.com/partner/microsoft/</a>.</p>
</div>
<div id="ref-clusters-hortonworks-cloudera">
<p>“Hortonworks Cloudera.” 2018. <a href="https://www.cloudera.com/more/news-and-blogs/press-releases/2018-10-03-cloudera-and-hortonworks-announce-merger-to-create-worlds-leading-next-generation-data-platform-and-deliver-industrys-first-enterprise-data-cloud.html">https://www.cloudera.com/more/news-and-blogs/press-releases/2018-10-03-cloudera-and-hortonworks-announce-merger-to-create-worlds-leading-next-generation-data-platform-and-deliver-industrys-first-enterprise-data-cloud.html</a>.</p>
</div>
<div id="ref-clusters-mapr-wikipedia">
<p>“MapR Wikipedia.” 2018. <a href="https://en.wikipedia.org/wiki/MapR">https://en.wikipedia.org/wiki/MapR</a>.</p>
</div>
<div id="ref-clusters-amazon-emr-sparklyr-blog">
<p>“AWS Blog.” 2016. <a href="https://aws.amazon.com/blogs/big-data/running-sparklyr-rstudios-r-interface-to-spark-on-amazon-emr/">https://aws.amazon.com/blogs/big-data/running-sparklyr-rstudios-r-interface-to-spark-on-amazon-emr/</a>.</p>
</div>
<div id="ref-clusters-databricks-wikipedia">
<p>“Databricks Wikipedia.” 2018. <a href="https://en.wikipedia.org/wiki/Databricks">https://en.wikipedia.org/wiki/Databricks</a>.</p>
</div>
<div id="ref-clusters-databricks-rstudio">
<p>“Databricks Rstudio.” 2018. <a href="https://docs.databricks.com/spark/latest/sparkr/rstudio.html">https://docs.databricks.com/spark/latest/sparkr/rstudio.html</a>.</p>
</div>
<div id="ref-clusters-databricks-blog-sparklyr">
<p>“Databricks Blog.” 2017. <a href="https://databricks.com/blog/2017/05/25/using-sparklyr-databricks.html">https://databricks.com/blog/2017/05/25/using-sparklyr-databricks.html</a>.</p>
</div>
<div id="ref-clusters-databricks-documentation-sparklyr">
<p>“Databricks Documentation.” 2018. <a href="https://docs.databricks.com/spark/latest/sparkr/sparklyr.html">https://docs.databricks.com/spark/latest/sparkr/sparklyr.html</a>.</p>
</div>
<div id="ref-clusters-databricks-dbu">
<p>“Databricks Units.” 2018. <a href="https://docs.databricks.com/release-notes/product/2.24.html#databricks-units">https://docs.databricks.com/release-notes/product/2.24.html#databricks-units</a>.</p>
</div>
<div id="ref-clusters-dataproc-wikipedia">
<p>“Dataproc Wikipedia.” 2018. <a href="https://en.wikipedia.org/wiki/Google_Cloud_Dataproc">https://en.wikipedia.org/wiki/Google_Cloud_Dataproc</a>.</p>
</div>
<div id="ref-clusters-dataproc-rstudio">
<p>“Dataproc Sparklyr.” 2018. <a href="https://cloud.google.com/solutions/running-rstudio-server-on-a-cloud-dataproc-cluster">https://cloud.google.com/solutions/running-rstudio-server-on-a-cloud-dataproc-cluster</a>.</p>
</div>
<div id="ref-clusters-ibm-wikipedia">
<p>“IBM Cloud Wikipedia.” 2018. <a href="https://en.wikipedia.org/wiki/IBM_cloud_computing">https://en.wikipedia.org/wiki/IBM_cloud_computing</a>.</p>
</div>
<div id="ref-clusters-azure-wikipedia">
<p>“Azure Wikipedia.” 2018. <a href="https://en.wikipedia.org/wiki/Microsoft_Azure">https://en.wikipedia.org/wiki/Microsoft_Azure</a>.</p>
</div>
<div id="ref-clusters-rstudio-server-pro">
<p>“RStudio Server Pro.” 2019. <a href="https://www.rstudio.com/products/rstudio-server-pro/">https://www.rstudio.com/products/rstudio-server-pro/</a>.</p>
</div>
<div id="ref-clusters-shiny-server-pro">
<p>“Shiny Server Pro.” 2019. <a href="https://www.rstudio.com/products/shiny-server-pro/">https://www.rstudio.com/products/shiny-server-pro/</a>.</p>
</div>
<div id="ref-clusters-rstudio-connect">
<p>“RStudio Connect.” 2019. <a href="https://www.rstudio.com/products/connect/">https://www.rstudio.com/products/connect/</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="pipelines.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="connections.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
