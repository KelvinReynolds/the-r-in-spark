<!DOCTYPE html>
<html >

<head>

  <meta charset="UTF-8">
  <meta http-equiv="X-UA-Compatible" content="IE=edge">
  <title>Chapter 12 Streaming | Mastering Apache Spark with R</title>
  <meta name="description" content="The complete guide to large-scale analysis and modeling.">
  <meta name="generator" content="bookdown  and GitBook 2.6.7">

  <meta property="og:title" content="Chapter 12 Streaming | Mastering Apache Spark with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The complete guide to large-scale analysis and modeling." />
  <meta name="github-repo" content="javierluraschi/the-r-in-spark" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 12 Streaming | Mastering Apache Spark with R" />
  
  <meta name="twitter:description" content="The complete guide to large-scale analysis and modeling." />
  

<meta name="author" content="Javier Luraschi, Kevin Kuo, Edgar Ruiz">


<meta name="date" content="2019-06-10">

  <meta name="viewport" content="width=device-width, initial-scale=1">
  <meta name="apple-mobile-web-app-capable" content="yes">
  <meta name="apple-mobile-web-app-status-bar-style" content="black">
  
  
<link rel="prev" href="distributed.html">
<link rel="next" href="contributing.html">
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<script src="libs/htmlwidgets-1.3/htmlwidgets.js"></script>
<script src="libs/dagre-0.0.1/dagre.min.js"></script>
<script src="libs/lodash-3.7.0/lodash.js"></script>
<script src="libs/nomnoml-0.2.0/nomnoml.js"></script>
<script src="libs/nomnoml-binding-0.1.0/nomnoml.js"></script>
<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-119986300-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-119986300-1');
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./">Learning Apache Spark with R</a></strong></li>

<li class="divider"></li>
<li class="chapter" data-level="" data-path="index.html"><a href="index.html"><i class="fa fa-check"></i>Welcome</a></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#intro-background"><i class="fa fa-check"></i><b>1.1</b> Information</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#intro-hadoop"><i class="fa fa-check"></i><b>1.2</b> Hadoop</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#intro-spark"><i class="fa fa-check"></i><b>1.3</b> Spark</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#intro-r"><i class="fa fa-check"></i><b>1.4</b> R</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#intro-sparklyr"><i class="fa fa-check"></i><b>1.5</b> sparklyr</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#intro-recap"><i class="fa fa-check"></i><b>1.6</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="starting.html"><a href="starting.html"><i class="fa fa-check"></i><b>2</b> Getting Started</a><ul>
<li class="chapter" data-level="2.1" data-path="starting.html"><a href="starting.html#starting-prerequisites"><i class="fa fa-check"></i><b>2.1</b> Prerequisites</a></li>
<li class="chapter" data-level="2.2" data-path="starting.html"><a href="starting.html#starting-install-sparklyr"><i class="fa fa-check"></i><b>2.2</b> Installing sparklyr</a></li>
<li class="chapter" data-level="2.3" data-path="starting.html"><a href="starting.html#starting-installing-spark"><i class="fa fa-check"></i><b>2.3</b> Installing Spark</a></li>
<li class="chapter" data-level="2.4" data-path="starting.html"><a href="starting.html#starting-connect-to-spark"><i class="fa fa-check"></i><b>2.4</b> Connecting to Spark</a></li>
<li class="chapter" data-level="2.5" data-path="starting.html"><a href="starting.html#starting-sparklyr-hello-world"><i class="fa fa-check"></i><b>2.5</b> Using Spark</a><ul>
<li class="chapter" data-level="2.5.1" data-path="starting.html"><a href="starting.html#starting-spark-web-interface"><i class="fa fa-check"></i><b>2.5.1</b> Web Interface</a></li>
<li class="chapter" data-level="2.5.2" data-path="starting.html"><a href="starting.html#starting-analysis"><i class="fa fa-check"></i><b>2.5.2</b> Analysis</a></li>
<li class="chapter" data-level="2.5.3" data-path="starting.html"><a href="starting.html#starting-modeling"><i class="fa fa-check"></i><b>2.5.3</b> Modeling</a></li>
<li class="chapter" data-level="2.5.4" data-path="starting.html"><a href="starting.html#starting-data"><i class="fa fa-check"></i><b>2.5.4</b> Data</a></li>
<li class="chapter" data-level="2.5.5" data-path="starting.html"><a href="starting.html#starting-extensions"><i class="fa fa-check"></i><b>2.5.5</b> Extensions</a></li>
<li class="chapter" data-level="2.5.6" data-path="starting.html"><a href="starting.html#starting-distributed-r"><i class="fa fa-check"></i><b>2.5.6</b> Distributed R</a></li>
<li class="chapter" data-level="2.5.7" data-path="starting.html"><a href="starting.html#starting-streaming"><i class="fa fa-check"></i><b>2.5.7</b> Streaming</a></li>
<li class="chapter" data-level="2.5.8" data-path="starting.html"><a href="starting.html#starting-logs"><i class="fa fa-check"></i><b>2.5.8</b> Logs</a></li>
</ul></li>
<li class="chapter" data-level="2.6" data-path="starting.html"><a href="starting.html#starting-disconnecting"><i class="fa fa-check"></i><b>2.6</b> Disconnecting</a></li>
<li class="chapter" data-level="2.7" data-path="starting.html"><a href="starting.html#starting-using-spark-from-rstudio"><i class="fa fa-check"></i><b>2.7</b> Using RStudio</a></li>
<li class="chapter" data-level="2.8" data-path="starting.html"><a href="starting.html#starting-resources"><i class="fa fa-check"></i><b>2.8</b> Resources</a></li>
<li class="chapter" data-level="2.9" data-path="starting.html"><a href="starting.html#starting-recap"><i class="fa fa-check"></i><b>2.9</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="analysis.html"><a href="analysis.html"><i class="fa fa-check"></i><b>3</b> Analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="analysis.html"><a href="analysis.html#r-as-an-interface-to-spark"><i class="fa fa-check"></i><b>3.1</b> R as an Interface to Spark</a><ul>
<li class="chapter" data-level="3.1.1" data-path="analysis.html"><a href="analysis.html#exercise"><i class="fa fa-check"></i><b>3.1.1</b> Exercise</a></li>
</ul></li>
<li class="chapter" data-level="3.2" data-path="analysis.html"><a href="analysis.html#import-access"><i class="fa fa-check"></i><b>3.2</b> Import / Access</a></li>
<li class="chapter" data-level="3.3" data-path="analysis.html"><a href="analysis.html#wrangle"><i class="fa fa-check"></i><b>3.3</b> Wrangle</a><ul>
<li class="chapter" data-level="3.3.1" data-path="analysis.html"><a href="analysis.html#correlations"><i class="fa fa-check"></i><b>3.3.1</b> Correlations</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="analysis.html"><a href="analysis.html#visualize"><i class="fa fa-check"></i><b>3.4</b> Visualize</a><ul>
<li class="chapter" data-level="3.4.1" data-path="analysis.html"><a href="analysis.html#recommended-approach"><i class="fa fa-check"></i><b>3.4.1</b> Recommended approach</a></li>
<li class="chapter" data-level="3.4.2" data-path="analysis.html"><a href="analysis.html#simple-plots"><i class="fa fa-check"></i><b>3.4.2</b> Simple Plots</a></li>
<li class="chapter" data-level="3.4.3" data-path="analysis.html"><a href="analysis.html#histograms"><i class="fa fa-check"></i><b>3.4.3</b> Histograms</a></li>
<li class="chapter" data-level="3.4.4" data-path="analysis.html"><a href="analysis.html#scatter-vs-raster-plots"><i class="fa fa-check"></i><b>3.4.4</b> Scatter vs Raster Plots</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="analysis.html"><a href="analysis.html#model"><i class="fa fa-check"></i><b>3.5</b> Model</a><ul>
<li class="chapter" data-level="3.5.1" data-path="analysis.html"><a href="analysis.html#cache-model-data"><i class="fa fa-check"></i><b>3.5.1</b> Cache model data</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="analysis.html"><a href="analysis.html#communicate"><i class="fa fa-check"></i><b>3.6</b> Communicate</a><ul>
<li class="chapter" data-level="3.6.1" data-path="analysis.html"><a href="analysis.html#reports"><i class="fa fa-check"></i><b>3.6.1</b> Reports</a></li>
<li class="chapter" data-level="3.6.2" data-path="analysis.html"><a href="analysis.html#presentation-decks"><i class="fa fa-check"></i><b>3.6.2</b> Presentation decks</a></li>
</ul></li>
<li class="chapter" data-level="3.7" data-path="analysis.html"><a href="analysis.html#recap"><i class="fa fa-check"></i><b>3.7</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="modeling.html"><a href="modeling.html"><i class="fa fa-check"></i><b>4</b> Modeling</a><ul>
<li class="chapter" data-level="4.1" data-path="modeling.html"><a href="modeling.html#overview"><i class="fa fa-check"></i><b>4.1</b> Overview</a></li>
<li class="chapter" data-level="4.2" data-path="modeling.html"><a href="modeling.html#the-data"><i class="fa fa-check"></i><b>4.2</b> The Data</a></li>
<li class="chapter" data-level="4.3" data-path="modeling.html"><a href="modeling.html#exploratory-data-analysis"><i class="fa fa-check"></i><b>4.3</b> Exploratory Data Analysis</a></li>
<li class="chapter" data-level="4.4" data-path="modeling.html"><a href="modeling.html#feature-engineering"><i class="fa fa-check"></i><b>4.4</b> Feature Engineering</a></li>
<li class="chapter" data-level="4.5" data-path="modeling.html"><a href="modeling.html#model-building"><i class="fa fa-check"></i><b>4.5</b> Model Building</a><ul>
<li class="chapter" data-level="4.5.1" data-path="modeling.html"><a href="modeling.html#logistic-regression-as-a-generalized-linear-regression"><i class="fa fa-check"></i><b>4.5.1</b> Logistic Regression as a Generalized Linear Regression</a></li>
<li class="chapter" data-level="4.5.2" data-path="modeling.html"><a href="modeling.html#more-machine-learning-algorithms"><i class="fa fa-check"></i><b>4.5.2</b> More Machine Learning Algorithms</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="modeling.html"><a href="modeling.html#working-with-textual-data"><i class="fa fa-check"></i><b>4.6</b> Working with Textual Data</a><ul>
<li class="chapter" data-level="4.6.1" data-path="modeling.html"><a href="modeling.html#data-prep"><i class="fa fa-check"></i><b>4.6.1</b> Data Prep</a></li>
<li class="chapter" data-level="4.6.2" data-path="modeling.html"><a href="modeling.html#topic-modeling"><i class="fa fa-check"></i><b>4.6.2</b> Topic Modeling</a></li>
</ul></li>
<li class="chapter" data-level="4.7" data-path="modeling.html"><a href="modeling.html#conclusion"><i class="fa fa-check"></i><b>4.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="pipelines.html"><a href="pipelines.html"><i class="fa fa-check"></i><b>5</b> Pipelines</a><ul>
<li class="chapter" data-level="5.1" data-path="pipelines.html"><a href="pipelines.html#estimators-and-transformers"><i class="fa fa-check"></i><b>5.1</b> Estimators and Transformers</a></li>
<li class="chapter" data-level="5.2" data-path="pipelines.html"><a href="pipelines.html#pipelines-and-pipeline-models"><i class="fa fa-check"></i><b>5.2</b> Pipelines and Pipeline Models</a></li>
<li class="chapter" data-level="5.3" data-path="pipelines.html"><a href="pipelines.html#applying-pipelines-to-okcupid-data"><i class="fa fa-check"></i><b>5.3</b> Applying Pipelines to OKCupid Data</a><ul>
<li class="chapter" data-level="5.3.1" data-path="pipelines.html"><a href="pipelines.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>5.3.1</b> Hyperparameter Tuning</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="pipelines.html"><a href="pipelines.html#operating-modes-of-pipelines-functions"><i class="fa fa-check"></i><b>5.4</b> Operating Modes of Pipelines Functions</a></li>
<li class="chapter" data-level="5.5" data-path="pipelines.html"><a href="pipelines.html#model-persistence-and-interoperability"><i class="fa fa-check"></i><b>5.5</b> Model Persistence and Interoperability</a></li>
<li class="chapter" data-level="5.6" data-path="pipelines.html"><a href="pipelines.html#model-deployment"><i class="fa fa-check"></i><b>5.6</b> Model Deployment</a><ul>
<li class="chapter" data-level="5.6.1" data-path="pipelines.html"><a href="pipelines.html#batch-scoring-with-ml-pipelines"><i class="fa fa-check"></i><b>5.6.1</b> Batch Scoring With ML Pipelines</a></li>
<li class="chapter" data-level="5.6.2" data-path="pipelines.html"><a href="pipelines.html#real-time-scoring-with-mleap"><i class="fa fa-check"></i><b>5.6.2</b> Real-Time Scoring with MLeap</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="pipelines.html"><a href="pipelines.html#conclusion-1"><i class="fa fa-check"></i><b>5.7</b> Conclusion</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="clusters.html"><a href="clusters.html"><i class="fa fa-check"></i><b>6</b> Clusters</a><ul>
<li class="chapter" data-level="6.1" data-path="clusters.html"><a href="clusters.html#clusters-overview"><i class="fa fa-check"></i><b>6.1</b> Overview</a></li>
<li class="chapter" data-level="6.2" data-path="clusters.html"><a href="clusters.html#on-premise"><i class="fa fa-check"></i><b>6.2</b> On-Premise</a><ul>
<li class="chapter" data-level="6.2.1" data-path="clusters.html"><a href="clusters.html#clusters-manager"><i class="fa fa-check"></i><b>6.2.1</b> Managers</a></li>
<li class="chapter" data-level="6.2.2" data-path="clusters.html"><a href="clusters.html#distributions"><i class="fa fa-check"></i><b>6.2.2</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="clusters.html"><a href="clusters.html#cloud"><i class="fa fa-check"></i><b>6.3</b> Cloud</a><ul>
<li class="chapter" data-level="6.3.1" data-path="clusters.html"><a href="clusters.html#clusters-amazon-emr"><i class="fa fa-check"></i><b>6.3.1</b> Amazon</a></li>
<li class="chapter" data-level="6.3.2" data-path="clusters.html"><a href="clusters.html#databricks"><i class="fa fa-check"></i><b>6.3.2</b> Databricks</a></li>
<li class="chapter" data-level="6.3.3" data-path="clusters.html"><a href="clusters.html#google"><i class="fa fa-check"></i><b>6.3.3</b> Google</a></li>
<li class="chapter" data-level="6.3.4" data-path="clusters.html"><a href="clusters.html#ibm"><i class="fa fa-check"></i><b>6.3.4</b> IBM</a></li>
<li class="chapter" data-level="6.3.5" data-path="clusters.html"><a href="clusters.html#microsoft"><i class="fa fa-check"></i><b>6.3.5</b> Microsoft</a></li>
<li class="chapter" data-level="6.3.6" data-path="clusters.html"><a href="clusters.html#qubole"><i class="fa fa-check"></i><b>6.3.6</b> Qubole</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="clusters.html"><a href="clusters.html#kubernetes"><i class="fa fa-check"></i><b>6.4</b> Kubernetes</a></li>
<li class="chapter" data-level="6.5" data-path="clusters.html"><a href="clusters.html#tools"><i class="fa fa-check"></i><b>6.5</b> Tools</a><ul>
<li class="chapter" data-level="6.5.1" data-path="clusters.html"><a href="clusters.html#rstudio"><i class="fa fa-check"></i><b>6.5.1</b> RStudio</a></li>
<li class="chapter" data-level="6.5.2" data-path="clusters.html"><a href="clusters.html#jupyter"><i class="fa fa-check"></i><b>6.5.2</b> Jupyter</a></li>
<li class="chapter" data-level="6.5.3" data-path="clusters.html"><a href="clusters.html#clusters-livy"><i class="fa fa-check"></i><b>6.5.3</b> Livy</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="clusters.html"><a href="clusters.html#recap-1"><i class="fa fa-check"></i><b>6.6</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="connections.html"><a href="connections.html"><i class="fa fa-check"></i><b>7</b> Connections</a><ul>
<li class="chapter" data-level="7.1" data-path="connections.html"><a href="connections.html#connections-overview"><i class="fa fa-check"></i><b>7.1</b> Overview</a><ul>
<li class="chapter" data-level="7.1.1" data-path="connections.html"><a href="connections.html#connections-spark-edge-nodes"><i class="fa fa-check"></i><b>7.1.1</b> Edge Nodes</a></li>
<li class="chapter" data-level="7.1.2" data-path="connections.html"><a href="connections.html#connections-spark-home"><i class="fa fa-check"></i><b>7.1.2</b> Spark Home</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="connections.html"><a href="connections.html#connections-local"><i class="fa fa-check"></i><b>7.2</b> Local</a></li>
<li class="chapter" data-level="7.3" data-path="connections.html"><a href="connections.html#connections-standalone"><i class="fa fa-check"></i><b>7.3</b> Standalone</a></li>
<li class="chapter" data-level="7.4" data-path="connections.html"><a href="connections.html#connections-yarn"><i class="fa fa-check"></i><b>7.4</b> Yarn</a><ul>
<li class="chapter" data-level="7.4.1" data-path="connections.html"><a href="connections.html#connections-yarn-client"><i class="fa fa-check"></i><b>7.4.1</b> Yarn Client</a></li>
<li class="chapter" data-level="7.4.2" data-path="connections.html"><a href="connections.html#connections-yarn-cluster"><i class="fa fa-check"></i><b>7.4.2</b> Yarn Cluster</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="connections.html"><a href="connections.html#connections-livy"><i class="fa fa-check"></i><b>7.5</b> Livy</a></li>
<li class="chapter" data-level="7.6" data-path="connections.html"><a href="connections.html#connections-mesos"><i class="fa fa-check"></i><b>7.6</b> Mesos</a></li>
<li class="chapter" data-level="7.7" data-path="connections.html"><a href="connections.html#connections-kubernetes"><i class="fa fa-check"></i><b>7.7</b> Kubernetes</a></li>
<li class="chapter" data-level="7.8" data-path="connections.html"><a href="connections.html#cloud-1"><i class="fa fa-check"></i><b>7.8</b> Cloud</a></li>
<li class="chapter" data-level="7.9" data-path="connections.html"><a href="connections.html#batches"><i class="fa fa-check"></i><b>7.9</b> Batches</a></li>
<li class="chapter" data-level="7.10" data-path="connections.html"><a href="connections.html#tools-1"><i class="fa fa-check"></i><b>7.10</b> Tools</a></li>
<li class="chapter" data-level="7.11" data-path="connections.html"><a href="connections.html#multiple"><i class="fa fa-check"></i><b>7.11</b> Multiple</a></li>
<li class="chapter" data-level="7.12" data-path="connections.html"><a href="connections.html#connections-troubleshooting"><i class="fa fa-check"></i><b>7.12</b> Troubleshooting</a><ul>
<li class="chapter" data-level="7.12.1" data-path="connections.html"><a href="connections.html#logging"><i class="fa fa-check"></i><b>7.12.1</b> Logging</a></li>
<li class="chapter" data-level="7.12.2" data-path="connections.html"><a href="connections.html#troubleshoot-spark-submit"><i class="fa fa-check"></i><b>7.12.2</b> Spark Submit</a></li>
<li class="chapter" data-level="7.12.3" data-path="connections.html"><a href="connections.html#windows"><i class="fa fa-check"></i><b>7.12.3</b> Windows</a></li>
</ul></li>
<li class="chapter" data-level="7.13" data-path="connections.html"><a href="connections.html#recap-2"><i class="fa fa-check"></i><b>7.13</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>8</b> Data</a><ul>
<li class="chapter" data-level="8.1" data-path="data.html"><a href="data.html#source-types-and-file-systems"><i class="fa fa-check"></i><b>8.1</b> Source types and file systems</a><ul>
<li class="chapter" data-level="8.1.1" data-path="data.html"><a href="data.html#default-packages"><i class="fa fa-check"></i><b>8.1.1</b> Default packages</a></li>
<li class="chapter" data-level="8.1.2" data-path="data.html"><a href="data.html#source-types"><i class="fa fa-check"></i><b>8.1.2</b> Source types</a></li>
<li class="chapter" data-level="8.1.3" data-path="data.html"><a href="data.html#file-systems"><i class="fa fa-check"></i><b>8.1.3</b> File systems</a></li>
</ul></li>
<li class="chapter" data-level="8.2" data-path="data.html"><a href="data.html#reading-data"><i class="fa fa-check"></i><b>8.2</b> Reading data</a><ul>
<li class="chapter" data-level="8.2.1" data-path="data.html"><a href="data.html#folders-as-a-table"><i class="fa fa-check"></i><b>8.2.1</b> Folders as a table</a></li>
<li class="chapter" data-level="8.2.2" data-path="data.html"><a href="data.html#file-layout"><i class="fa fa-check"></i><b>8.2.2</b> File layout</a></li>
<li class="chapter" data-level="8.2.3" data-path="data.html"><a href="data.html#spark-memory"><i class="fa fa-check"></i><b>8.2.3</b> Spark memory</a></li>
<li class="chapter" data-level="8.2.4" data-path="data.html"><a href="data.html#column-names"><i class="fa fa-check"></i><b>8.2.4</b> Column Names</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="data.html"><a href="data.html#writing-data"><i class="fa fa-check"></i><b>8.3</b> Writing Data</a><ul>
<li class="chapter" data-level="8.3.1" data-path="data.html"><a href="data.html#spark-not-r-as-pass-through"><i class="fa fa-check"></i><b>8.3.1</b> Spark, not R, as pass-through</a></li>
<li class="chapter" data-level="8.3.2" data-path="data.html"><a href="data.html#practical-approach"><i class="fa fa-check"></i><b>8.3.2</b> Practical approach</a></li>
</ul></li>
<li class="chapter" data-level="8.4" data-path="data.html"><a href="data.html#date-time"><i class="fa fa-check"></i><b>8.4</b> Date &amp; time</a></li>
<li class="chapter" data-level="8.5" data-path="data.html"><a href="data.html#specific-types-and-protocols"><i class="fa fa-check"></i><b>8.5</b> Specific types and protocols</a><ul>
<li class="chapter" data-level="8.5.1" data-path="data.html"><a href="data.html#amazon-s3"><i class="fa fa-check"></i><b>8.5.1</b> Amazon S3</a></li>
<li class="chapter" data-level="8.5.2" data-path="data.html"><a href="data.html#sql"><i class="fa fa-check"></i><b>8.5.2</b> SQL</a></li>
<li class="chapter" data-level="8.5.3" data-path="data.html"><a href="data.html#hive"><i class="fa fa-check"></i><b>8.5.3</b> Hive</a></li>
<li class="chapter" data-level="8.5.4" data-path="data.html"><a href="data.html#comma-delimited-values-csv"><i class="fa fa-check"></i><b>8.5.4</b> Comma Delimited Values (CSV)</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="data.html"><a href="data.html#recap-3"><i class="fa fa-check"></i><b>8.6</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="tuning.html"><a href="tuning.html"><i class="fa fa-check"></i><b>9</b> Tuning</a><ul>
<li class="chapter" data-level="9.1" data-path="tuning.html"><a href="tuning.html#overview-1"><i class="fa fa-check"></i><b>9.1</b> Overview</a><ul>
<li class="chapter" data-level="9.1.1" data-path="tuning.html"><a href="tuning.html#tuning-graph-visualization"><i class="fa fa-check"></i><b>9.1.1</b> Graph</a></li>
<li class="chapter" data-level="9.1.2" data-path="tuning.html"><a href="tuning.html#tuning-event-timeline"><i class="fa fa-check"></i><b>9.1.2</b> Timeline</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="tuning.html"><a href="tuning.html#tuning-configuring"><i class="fa fa-check"></i><b>9.2</b> Configuring</a><ul>
<li class="chapter" data-level="9.2.1" data-path="tuning.html"><a href="tuning.html#connect-settings"><i class="fa fa-check"></i><b>9.2.1</b> Connect Settings</a></li>
<li class="chapter" data-level="9.2.2" data-path="tuning.html"><a href="tuning.html#submit-settings"><i class="fa fa-check"></i><b>9.2.2</b> Submit Settings</a></li>
<li class="chapter" data-level="9.2.3" data-path="tuning.html"><a href="tuning.html#runtime-settings"><i class="fa fa-check"></i><b>9.2.3</b> Runtime Settings</a></li>
<li class="chapter" data-level="9.2.4" data-path="tuning.html"><a href="tuning.html#sparklyr-settings"><i class="fa fa-check"></i><b>9.2.4</b> sparklyr Settings</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="tuning.html"><a href="tuning.html#tuning-partitioning"><i class="fa fa-check"></i><b>9.3</b> Partitioning</a><ul>
<li class="chapter" data-level="9.3.1" data-path="tuning.html"><a href="tuning.html#implicit"><i class="fa fa-check"></i><b>9.3.1</b> Implicit</a></li>
<li class="chapter" data-level="9.3.2" data-path="tuning.html"><a href="tuning.html#explicit"><i class="fa fa-check"></i><b>9.3.2</b> Explicit</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="tuning.html"><a href="tuning.html#tuning-caching"><i class="fa fa-check"></i><b>9.4</b> Caching</a><ul>
<li class="chapter" data-level="9.4.1" data-path="tuning.html"><a href="tuning.html#checkpointing"><i class="fa fa-check"></i><b>9.4.1</b> Checkpointing</a></li>
<li class="chapter" data-level="9.4.2" data-path="tuning.html"><a href="tuning.html#tuning-memory"><i class="fa fa-check"></i><b>9.4.2</b> Memory</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="tuning.html"><a href="tuning.html#tuning-shuffling"><i class="fa fa-check"></i><b>9.5</b> Shuffling</a></li>
<li class="chapter" data-level="9.6" data-path="tuning.html"><a href="tuning.html#tuning-serialization"><i class="fa fa-check"></i><b>9.6</b> Serialization</a></li>
<li class="chapter" data-level="9.7" data-path="tuning.html"><a href="tuning.html#configuration-files"><i class="fa fa-check"></i><b>9.7</b> Configuration Files</a></li>
<li class="chapter" data-level="9.8" data-path="tuning.html"><a href="tuning.html#recap-4"><i class="fa fa-check"></i><b>9.8</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="extensions.html"><a href="extensions.html"><i class="fa fa-check"></i><b>10</b> Extensions</a><ul>
<li class="chapter" data-level="10.1" data-path="extensions.html"><a href="extensions.html#overview-2"><i class="fa fa-check"></i><b>10.1</b> Overview</a></li>
<li class="chapter" data-level="10.2" data-path="extensions.html"><a href="extensions.html#h2o"><i class="fa fa-check"></i><b>10.2</b> H2O</a></li>
<li class="chapter" data-level="10.3" data-path="extensions.html"><a href="extensions.html#graphs"><i class="fa fa-check"></i><b>10.3</b> Graphs</a></li>
<li class="chapter" data-level="10.4" data-path="extensions.html"><a href="extensions.html#xgboost"><i class="fa fa-check"></i><b>10.4</b> XGBoost</a></li>
<li class="chapter" data-level="10.5" data-path="extensions.html"><a href="extensions.html#deep-learning"><i class="fa fa-check"></i><b>10.5</b> Deep Learning</a></li>
<li class="chapter" data-level="10.6" data-path="extensions.html"><a href="extensions.html#genomics"><i class="fa fa-check"></i><b>10.6</b> Genomics</a></li>
<li class="chapter" data-level="10.7" data-path="extensions.html"><a href="extensions.html#spatial"><i class="fa fa-check"></i><b>10.7</b> Spatial</a></li>
<li class="chapter" data-level="10.8" data-path="extensions.html"><a href="extensions.html#troubleshooting"><i class="fa fa-check"></i><b>10.8</b> Troubleshooting</a></li>
<li class="chapter" data-level="10.9" data-path="extensions.html"><a href="extensions.html#recap-5"><i class="fa fa-check"></i><b>10.9</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="distributed.html"><a href="distributed.html"><i class="fa fa-check"></i><b>11</b> Distributed R</a><ul>
<li class="chapter" data-level="11.1" data-path="distributed.html"><a href="distributed.html#overview-3"><i class="fa fa-check"></i><b>11.1</b> Overview</a></li>
<li class="chapter" data-level="11.2" data-path="distributed.html"><a href="distributed.html#use-cases"><i class="fa fa-check"></i><b>11.2</b> Use Cases</a><ul>
<li class="chapter" data-level="11.2.1" data-path="distributed.html"><a href="distributed.html#custom-parsers"><i class="fa fa-check"></i><b>11.2.1</b> Custom Parsers</a></li>
<li class="chapter" data-level="11.2.2" data-path="distributed.html"><a href="distributed.html#partitioned-modeling"><i class="fa fa-check"></i><b>11.2.2</b> Partitioned Modeling</a></li>
<li class="chapter" data-level="11.2.3" data-path="distributed.html"><a href="distributed.html#distributed-grid-search"><i class="fa fa-check"></i><b>11.2.3</b> Grid Search</a></li>
<li class="chapter" data-level="11.2.4" data-path="distributed.html"><a href="distributed.html#web-apis"><i class="fa fa-check"></i><b>11.2.4</b> Web APIs</a></li>
<li class="chapter" data-level="11.2.5" data-path="distributed.html"><a href="distributed.html#intensive-computations"><i class="fa fa-check"></i><b>11.2.5</b> Intensive Computations</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="distributed.html"><a href="distributed.html#partitions"><i class="fa fa-check"></i><b>11.3</b> Partitions</a></li>
<li class="chapter" data-level="11.4" data-path="distributed.html"><a href="distributed.html#grouping"><i class="fa fa-check"></i><b>11.4</b> Grouping</a></li>
<li class="chapter" data-level="11.5" data-path="distributed.html"><a href="distributed.html#columns"><i class="fa fa-check"></i><b>11.5</b> Columns</a></li>
<li class="chapter" data-level="11.6" data-path="distributed.html"><a href="distributed.html#context"><i class="fa fa-check"></i><b>11.6</b> Context</a></li>
<li class="chapter" data-level="11.7" data-path="distributed.html"><a href="distributed.html#functions"><i class="fa fa-check"></i><b>11.7</b> Functions</a></li>
<li class="chapter" data-level="11.8" data-path="distributed.html"><a href="distributed.html#packages"><i class="fa fa-check"></i><b>11.8</b> Packages</a></li>
<li class="chapter" data-level="11.9" data-path="distributed.html"><a href="distributed.html#cluster-requirements"><i class="fa fa-check"></i><b>11.9</b> Cluster Requirements</a><ul>
<li class="chapter" data-level="11.9.1" data-path="distributed.html"><a href="distributed.html#installing-r"><i class="fa fa-check"></i><b>11.9.1</b> Installing R</a></li>
<li class="chapter" data-level="11.9.2" data-path="distributed.html"><a href="distributed.html#apache-arrow"><i class="fa fa-check"></i><b>11.9.2</b> Apache Arrow</a></li>
</ul></li>
<li class="chapter" data-level="11.10" data-path="distributed.html"><a href="distributed.html#troubleshooting-1"><i class="fa fa-check"></i><b>11.10</b> Troubleshooting</a><ul>
<li class="chapter" data-level="11.10.1" data-path="distributed.html"><a href="distributed.html#worker-logs"><i class="fa fa-check"></i><b>11.10.1</b> Worker Logs</a></li>
<li class="chapter" data-level="11.10.2" data-path="distributed.html"><a href="distributed.html#resolving-timeouts"><i class="fa fa-check"></i><b>11.10.2</b> Resolving Timeouts</a></li>
<li class="chapter" data-level="11.10.3" data-path="distributed.html"><a href="distributed.html#inspecting-partition"><i class="fa fa-check"></i><b>11.10.3</b> Inspecting Partition</a></li>
<li class="chapter" data-level="11.10.4" data-path="distributed.html"><a href="distributed.html#debugging-workers"><i class="fa fa-check"></i><b>11.10.4</b> Debugging Workers</a></li>
</ul></li>
<li class="chapter" data-level="11.11" data-path="distributed.html"><a href="distributed.html#recap-6"><i class="fa fa-check"></i><b>11.11</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="streaming.html"><a href="streaming.html"><i class="fa fa-check"></i><b>12</b> Streaming</a><ul>
<li class="chapter" data-level="12.1" data-path="streaming.html"><a href="streaming.html#spark-streaming"><i class="fa fa-check"></i><b>12.1</b> Spark Streaming</a></li>
<li class="chapter" data-level="12.2" data-path="streaming.html"><a href="streaming.html#working-with-spark-streams"><i class="fa fa-check"></i><b>12.2</b> Working with Spark Streams</a></li>
<li class="chapter" data-level="12.3" data-path="streaming.html"><a href="streaming.html#sparklyr-extras"><i class="fa fa-check"></i><b>12.3</b> <code>sparklyr</code> extras</a><ul>
<li class="chapter" data-level="12.3.1" data-path="streaming.html"><a href="streaming.html#stream-monitor"><i class="fa fa-check"></i><b>12.3.1</b> Stream monitor</a></li>
<li class="chapter" data-level="12.3.2" data-path="streaming.html"><a href="streaming.html#stream-generator"><i class="fa fa-check"></i><b>12.3.2</b> Stream generator</a></li>
<li class="chapter" data-level="12.3.3" data-path="streaming.html"><a href="streaming.html#shiny-reactive"><i class="fa fa-check"></i><b>12.3.3</b> Shiny reactive</a></li>
</ul></li>
<li class="chapter" data-level="12.4" data-path="streaming.html"><a href="streaming.html#intro-example"><i class="fa fa-check"></i><b>12.4</b> Intro example</a></li>
<li class="chapter" data-level="12.5" data-path="streaming.html"><a href="streaming.html#transformations"><i class="fa fa-check"></i><b>12.5</b> Transformations</a><ul>
<li class="chapter" data-level="12.5.1" data-path="streaming.html"><a href="streaming.html#dplyr"><i class="fa fa-check"></i><b>12.5.1</b> dplyr</a></li>
<li class="chapter" data-level="12.5.2" data-path="streaming.html"><a href="streaming.html#transformer-functions"><i class="fa fa-check"></i><b>12.5.2</b> Transformer functions</a></li>
<li class="chapter" data-level="12.5.3" data-path="streaming.html"><a href="streaming.html#r-code"><i class="fa fa-check"></i><b>12.5.3</b> R code</a></li>
<li class="chapter" data-level="12.5.4" data-path="streaming.html"><a href="streaming.html#ml-pipelines"><i class="fa fa-check"></i><b>12.5.4</b> ML Pipelines</a></li>
</ul></li>
<li class="chapter" data-level="12.6" data-path="streaming.html"><a href="streaming.html#shiny-integration"><i class="fa fa-check"></i><b>12.6</b> Shiny integration</a></li>
<li class="chapter" data-level="12.7" data-path="streaming.html"><a href="streaming.html#kafka"><i class="fa fa-check"></i><b>12.7</b> Kafka</a><ul>
<li class="chapter" data-level="12.7.1" data-path="streaming.html"><a href="streaming.html#workflow"><i class="fa fa-check"></i><b>12.7.1</b> Workflow</a></li>
<li class="chapter" data-level="12.7.2" data-path="streaming.html"><a href="streaming.html#spark-integration"><i class="fa fa-check"></i><b>12.7.2</b> Spark integration</a></li>
<li class="chapter" data-level="12.7.3" data-path="streaming.html"><a href="streaming.html#r-integration"><i class="fa fa-check"></i><b>12.7.3</b> R integration</a></li>
<li class="chapter" data-level="12.7.4" data-path="streaming.html"><a href="streaming.html#example"><i class="fa fa-check"></i><b>12.7.4</b> Example</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="13" data-path="contributing.html"><a href="contributing.html"><i class="fa fa-check"></i><b>13</b> Contributing</a><ul>
<li class="chapter" data-level="13.1" data-path="contributing.html"><a href="contributing.html#contributing-overview"><i class="fa fa-check"></i><b>13.1</b> Overview</a></li>
<li class="chapter" data-level="13.2" data-path="contributing.html"><a href="contributing.html#contributing-spark-api"><i class="fa fa-check"></i><b>13.2</b> Spark API</a></li>
<li class="chapter" data-level="13.3" data-path="contributing.html"><a href="contributing.html#spark-extensions"><i class="fa fa-check"></i><b>13.3</b> Spark Extensions</a></li>
<li class="chapter" data-level="13.4" data-path="contributing.html"><a href="contributing.html#scala-code"><i class="fa fa-check"></i><b>13.4</b> Scala Code</a></li>
<li class="chapter" data-level="13.5" data-path="contributing.html"><a href="contributing.html#recap-7"><i class="fa fa-check"></i><b>13.5</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>14</b> Appendix</a><ul>
<li class="chapter" data-level="14.1" data-path="appendix.html"><a href="appendix.html#appendix-prerequisites"><i class="fa fa-check"></i><b>14.1</b> Prerequisites</a><ul>
<li class="chapter" data-level="14.1.1" data-path="appendix.html"><a href="appendix.html#appendix-install-r"><i class="fa fa-check"></i><b>14.1.1</b> Installing R</a></li>
<li class="chapter" data-level="14.1.2" data-path="appendix.html"><a href="appendix.html#appendix-install-java"><i class="fa fa-check"></i><b>14.1.2</b> Installing Java</a></li>
<li class="chapter" data-level="14.1.3" data-path="appendix.html"><a href="appendix.html#appendix-install-rstudio"><i class="fa fa-check"></i><b>14.1.3</b> Installing RStudio</a></li>
<li class="chapter" data-level="14.1.4" data-path="appendix.html"><a href="appendix.html#appendix-using-rstudio"><i class="fa fa-check"></i><b>14.1.4</b> Using RStudio</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="appendix.html"><a href="appendix.html#diagrams"><i class="fa fa-check"></i><b>14.2</b> Diagrams</a><ul>
<li class="chapter" data-level="14.2.1" data-path="appendix.html"><a href="appendix.html#appendix-storage-capacity"><i class="fa fa-check"></i><b>14.2.1</b> Worlds Store Capacity</a></li>
<li class="chapter" data-level="14.2.2" data-path="appendix.html"><a href="appendix.html#appendix-cran-downloads"><i class="fa fa-check"></i><b>14.2.2</b> Daily downloads of CRAN packages</a></li>
<li class="chapter" data-level="14.2.3" data-path="appendix.html"><a href="appendix.html#appendix-cluster-trends"><i class="fa fa-check"></i><b>14.2.3</b> Google trends for mainframes, cloud computing and kubernetes</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="appendix.html"><a href="appendix.html#appendix-ggplot2-theme"><i class="fa fa-check"></i><b>14.3</b> Formatting</a></li>
<li class="chapter" data-level="14.4" data-path="appendix.html"><a href="appendix.html#ml-functionlist"><i class="fa fa-check"></i><b>14.4</b> List of ML Functions</a><ul>
<li class="chapter" data-level="14.4.1" data-path="appendix.html"><a href="appendix.html#classification"><i class="fa fa-check"></i><b>14.4.1</b> Classification</a></li>
<li class="chapter" data-level="14.4.2" data-path="appendix.html"><a href="appendix.html#regression"><i class="fa fa-check"></i><b>14.4.2</b> Regression</a></li>
<li class="chapter" data-level="14.4.3" data-path="appendix.html"><a href="appendix.html#clustering"><i class="fa fa-check"></i><b>14.4.3</b> Clustering</a></li>
<li class="chapter" data-level="14.4.4" data-path="appendix.html"><a href="appendix.html#recommendation"><i class="fa fa-check"></i><b>14.4.4</b> Recommendation</a></li>
<li class="chapter" data-level="14.4.5" data-path="appendix.html"><a href="appendix.html#frequent-pattern-mining"><i class="fa fa-check"></i><b>14.4.5</b> Frequent Pattern Mining</a></li>
<li class="chapter" data-level="14.4.6" data-path="appendix.html"><a href="appendix.html#feature-transformers"><i class="fa fa-check"></i><b>14.4.6</b> Feature Transformers</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="appendix.html"><a href="appendix.html#kafka-1"><i class="fa fa-check"></i><b>14.5</b> Kafka</a></li>
</ul></li>
<li class="chapter" data-level="15" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>15</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Mastering Apache Spark with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="streaming" class="section level1">
<h1><span class="header-section-number">Chapter 12</span> Streaming</h1>
<p>As the velocity of the generation of data increases, so does the need to real-time analysis. Real-time refers to the ability to continuously analyze data from a constantly updating data feed. This is usually called Stream Analytics. In Stream Analytics, data is analyzed as its generated, not retroactively as it happens with “everyday” analysis.</p>
<p>This chapter will cover how to analyze a stream of data using R and Spark. It will also cover basics about stream analysis, along with how to implement these using <code>sparklyr</code> and other R packages.</p>
<div id="spark-streaming" class="section level2">
<h2><span class="header-section-number">12.1</span> Spark Streaming</h2>
<p>Spark Streaming is an extension of the core Spark API. It is used for processing live streams of data. It does this in a scalable, high-throughput, and fault tolerant way. It also allows for the current data to be joined with the historical data.</p>
<p>Spark Streaming works by splitting the live input into small batches. Each batch is processed by Spark individually. The output from Spark is also in small batches. This process is not visible to the user. Spark displays streams as a DStream. The name stands for “discretize stream”. DStream represents the small batches as one continuous stream. Inside Spark, the DStream is represented as a sequence of Resilient Distributed Datasets (RDD).</p>
<p>The best resource to learn how Spark analyzes streams is the Apache Spark’s Official site <span class="citation">(“Spark Streaming Programming Guide” <a href="#ref-streaming-programming-guide">2018</a>)</span>. This chapter will cover just enough Spark Streaming concepts to help you understand the mechanics of what the R code is doing. It is recommended to read the official resource, specially if you need to implement solutions based on Spark Streaming.</p>
</div>
<div id="working-with-spark-streams" class="section level2">
<h2><span class="header-section-number">12.2</span> Working with Spark Streams</h2>
<p>In practice, a Spark Stream update is a three stage operation. This is the breakdown of the three stages:</p>
<ol style="list-style-type: decimal">
<li><p><strong>Read</strong> - The stream is expected to append new files in a specified folder. Those new files contain the most recent information from the stream. Spark monitors the folder, and reads the data from the files. The following file formats are supported: CSV, text, JSON, parquet, and orc.</p></li>
<li><p><strong>Transform</strong> - Spark applies the desired operations on top of the data. No special <code>sparklyr</code> functions are needed to transform stream data. You can use same <code>dplyr</code> verbs, Spark transformers and even native R code (via <code>spark_apply()</code>).</p></li>
<li><p><strong>Write</strong> - The results of the transformed input are saved in a different folder. The following file formats are supported: CSV, text, JSON, parquet, and orc.</p></li>
</ol>
<p>Figure <a href="streaming.html#fig:streaming-working">12.1</a> provides a visual aid to what each stage does and how they connect:</p>
<div class="figure" style="text-align: center"><span id="fig:streaming-working"></span>
<div id="htmlwidget-8c9f02e3832f7d8a826a" style="width:100%;height:220pt;" class="nomnoml html-widget"></div>
<script type="application/json" data-for="htmlwidget-8c9f02e3832f7d8a826a">{"x":{"code":"\n  #fill: #FEFEFF\n  #lineWidth: 2\n  #zoom: 4\n  #direction: right\n   \n\n#direction: right\n#padding: 10\n#fontSize: 14\n#leading: 2\n[<note> Input\nFolder] -> [<transceiver> Spark reads data \nfrom files in folder] \n[Spark reads data \nfrom files in folder] -> [<transceiver> Spark applies \nthe operations] \n[Spark applies \nthe operations] -> [<transceiver> Spark writes \nresults to folder]\n [<transceiver> Spark writes \nresults to folder] -> [<note> Output \nFolder] \n[Spark reads data \nfrom files in folder] -/- [<label> 2. Transform]\n[Input\nFolder] -/- [<label> 1. Read]\n[Spark applies \nthe operations] -/- [<label> 3. Write]\n","className":null,"svg":false},"evals":[],"jsHooks":[]}</script>
<p class="caption">
FIGURE 12.1: Working with Spark Streams
</p>
</div>
<p>Here is the breakdown of the available <code>sparklyr</code> functions for reading and writing:</p>
<table>
<thead>
<tr class="header">
<th>Format</th>
<th>Read</th>
<th>Write</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td>CSV</td>
<td>stream_read_csv</td>
<td>stream_write_csv</td>
</tr>
<tr class="even">
<td>JSON</td>
<td>stream_read_json</td>
<td>stream_write_json</td>
</tr>
<tr class="odd">
<td>Kafka</td>
<td>stream_read_kafka</td>
<td>stream_write_kafka</td>
</tr>
<tr class="even">
<td>ORC</td>
<td>stream_read_orc</td>
<td>stream_write_orc</td>
</tr>
<tr class="odd">
<td>Parquet</td>
<td>stream_read_parquet</td>
<td>stream_write_parquet</td>
</tr>
<tr class="even">
<td>Text</td>
<td>stream_read_text</td>
<td>stream_write_text</td>
</tr>
<tr class="odd">
<td>Memory</td>
<td></td>
<td>stream_write_memory</td>
</tr>
</tbody>
</table>
<p>In the same way all of the read and write operations in <code>sparklyr</code> for Spark Standalone, or in <code>sparklyr</code>’s local mode, the input and output folders are actual Operating System file system folders. For YARN managed clusters, these will be folder locations inside the Hadoop File System (HDFS).</p>
<p>Non-file driven applications are also supported, such as Kafka. Kafka will be discussed in the last part of the chapter.</p>
</div>
<div id="sparklyr-extras" class="section level2">
<h2><span class="header-section-number">12.3</span> <code>sparklyr</code> extras</h2>
<p>The <code>sparklyr</code> package goes beyond providing an easy-to-use-interface to Spark Streaming. The R package includes features which provide a more complete integration with R:</p>
<ol style="list-style-type: decimal">
<li><p>An out-of-the box graph visualization to monitor the stream.</p></li>
<li><p>Stream generator for testing and learning purposes.</p></li>
<li><p>A Shiny reactive function. It allows Shiny apps to read the contents of a steam.</p></li>
</ol>
<div id="stream-monitor" class="section level3">
<h3><span class="header-section-number">12.3.1</span> Stream monitor</h3>
<p>The <code>stream_view()</code> function will generate a Shiny app which displays the current state, as well as the history, of the stream. An example of how to use it is available in the <strong>Intro Example</strong> section.</p>
</div>
<div id="stream-generator" class="section level3">
<h3><span class="header-section-number">12.3.2</span> Stream generator</h3>
<p>The <code>stream_generate_test()</code> function creates a local test stream. This function works independently from a Spark connection. The following example will create five files in sub-folder called “source”. The files will be created one second apart from the previous file’s creation.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(sparklyr)

<span class="kw">stream_generate_test</span>(<span class="dt">iterations =</span> <span class="dv">5</span>, <span class="dt">path =</span> <span class="st">&quot;source&quot;</span>, <span class="dt">interval =</span> <span class="dv">1</span>)</code></pre>
<p>After the function completes, all of the files should show up in the “source” folder. Notice that the file size vary. This is so that it simulates what a true stream would do.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">file.info</span>(<span class="kw">file.path</span>(<span class="st">&quot;source&quot;</span>, <span class="kw">list.files</span>(<span class="st">&quot;source&quot;</span>)))[<span class="dv">1</span>] </code></pre>
<pre><code>##                     size
## source/stream_1.csv   44
## source/stream_2.csv  121
## source/stream_3.csv  540
## source/stream_4.csv 2370
## source/stream_5.csv 7236</code></pre>
<p>The <code>stream_generate_test()</code> by default will create a single numeric variable data frame.</p>
<pre class="sourceCode r"><code class="sourceCode r">readr<span class="op">::</span><span class="kw">read_csv</span>(<span class="st">&quot;source/stream_5.csv&quot;</span>)</code></pre>
<pre><code>## # A tibble: 1,489 x 1
##        x
##    &lt;dbl&gt;
##  1   630
##  2   631
##  3   632
##  4   633
##  5   634
##  6   635
##  7   636
##  8   637
##  9   638
## 10   639
## # ... with 1,479 more rows</code></pre>
</div>
<div id="shiny-reactive" class="section level3">
<h3><span class="header-section-number">12.3.3</span> Shiny reactive</h3>
<p>Shiny’s reactive framework is well suited to support streaming information. The idea is that your Shiny app can automatically display the latest results as fast as Spark can process them. The <code>reactiveSpark()</code> function provides that integration.</p>
</div>
</div>
<div id="intro-example" class="section level2">
<h2><span class="header-section-number">12.4</span> Intro example</h2>
<p>This section will use a very simple example to introduce the mechanics of Spark Streaming, and how <code>sparklyr</code> interacts with it. This is very simple example. It will only move the input contents to the output contents without any transformations being done to it.</p>
<ol style="list-style-type: decimal">
<li><p>Open a local Spark session</p>
<pre class="sourceCode r"><code class="sourceCode r">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>)</code></pre></li>
<li><p>Remove the “source” and “destination” folders. This step ensures a clean slate if you try to run the example again.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="cf">if</span>(<span class="kw">file.exists</span>(<span class="st">&quot;source&quot;</span>)) <span class="kw">unlink</span>(<span class="st">&quot;source&quot;</span>, <span class="ot">TRUE</span>)
<span class="cf">if</span>(<span class="kw">file.exists</span>(<span class="st">&quot;destination&quot;</span>)) <span class="kw">unlink</span>(<span class="st">&quot;destination&quot;</span>, <span class="ot">TRUE</span>)</code></pre></li>
<li><p>Just like with <code>read_csv()</code>, <code>stream_read_csv()</code> needs a file specification. To save ourselves from providing one, a single test file is generated.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">stream_generate_test</span>(<span class="dt">iterations =</span> <span class="dv">1</span>)</code></pre></li>
<li><p><code>stream_read_csv()</code> starts the ingestion part of the job. It corresponds to the <strong>1. Read</strong> stage described in the previous section.</p>
<pre class="sourceCode r"><code class="sourceCode r">read_folder &lt;-<span class="st"> </span><span class="kw">stream_read_csv</span>(sc, <span class="st">&quot;source&quot;</span>)</code></pre></li>
<li><p>Set the output of the job to read the incoming data. That is done by passing the <em>read_folder</em> variable, set in the previous step. It corresponds to the <strong>3. Read</strong> stage described in the previous section.</p>
<pre class="sourceCode r"><code class="sourceCode r">write_output &lt;-<span class="st"> </span><span class="kw">stream_write_csv</span>(read_folder, <span class="st">&quot;destination&quot;</span>)</code></pre></li>
<li><p>The library<code>future</code> will allow the test generation to run in a asynchronous fashion. This is needed because the next step, <code>stream_view()</code> will start a Shiny app which takes over the R session.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(future)
<span class="kw">invisible</span>(<span class="kw">future</span>(<span class="kw">stream_generate_test</span>(<span class="dt">interval =</span> <span class="fl">0.3</span>)))</code></pre></li>
<li><p><code>stream_view()</code> is the out-of-the box graph visualization to monitor the stream that was mentioned in the <em>sparklyr Interface</em> section.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">stream_view</span>(write_output)</code></pre></li>
</ol>
<p>The Shiny app shows up in the Viewer pane. The column bars will slowly accumulate in the app’s plot After the test generator completes, the plot should look like what Figure <a href="streaming.html#fig:streaming-stream-view">12.2</a> shows.</p>
<div class="figure" style="text-align: center"><span id="fig:streaming-stream-view"></span>
<img src="images/streaming-stream-view-resized.png" alt="stream_view() output" width="1500" />
<p class="caption">
FIGURE 12.2: stream_view() output
</p>
</div>
<p>The final step is to clean up the stream and Spark connection</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">stream_stop</span>(write_output)
<span class="kw">spark_disconnect</span>(sc)</code></pre>
</div>
<div id="transformations" class="section level2">
<h2><span class="header-section-number">12.5</span> Transformations</h2>
<p>Streams can be transformed using <code>dplyr</code>, SQL queries, ML Pipelines or R code. We can use as many transformations as needed in the same way that Spark data frames can be transformed with <code>sparklyr</code>. The transformation source can be streams or data frames but the output is always a stream. If needed, one can always take a snapshot from the destination stream and save the output as a data frame, which is what <code>sparklyr</code> will do for you if a destination stream is not specified.</p>
<div id="dplyr" class="section level3">
<h3><span class="header-section-number">12.5.1</span> dplyr</h3>
<p>The same <code>dplyr</code> verbs can be used on top of a Spark Stream. The following example shows how easy it is to filter rows and add columns to data from an input folder.</p>
<pre class="sourceCode r"><code class="sourceCode r">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>)
<span class="cf">if</span>(<span class="kw">file.exists</span>(<span class="st">&quot;source&quot;</span>)) <span class="kw">unlink</span>(<span class="st">&quot;source&quot;</span>, <span class="ot">TRUE</span>)

<span class="kw">stream_generate_test</span>(<span class="dt">iterations =</span> <span class="dv">5</span>)

<span class="kw">stream_read_csv</span>(sc, <span class="st">&quot;source&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(x <span class="op">&gt;</span><span class="st"> </span><span class="dv">700</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y =</span> <span class="kw">round</span>(x <span class="op">/</span><span class="st"> </span><span class="dv">100</span>))</code></pre>
<pre><code>## # Source: spark&lt;?&gt; [inf x 2]
##        x     y
##    &lt;int&gt; &lt;dbl&gt;
##  1   701     7
##  2   702     7
##  3   703     7
##  4   704     7
##  5   705     7
##  6   706     7
##  7   707     7
##  8   708     7
##  9   709     7
## 10   710     7
## # ... with more rows</code></pre>
<p>It also is possible to perform aggregations over the entire history of the stream. The history could be filtered or not.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">stream_read_csv</span>(sc, <span class="st">&quot;source&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">filter</span>(x <span class="op">&gt;</span><span class="st"> </span><span class="dv">700</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y =</span> <span class="kw">round</span>(x <span class="op">/</span><span class="st"> </span><span class="dv">100</span>)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">count</span>(y) </code></pre>
<pre><code>## # Source: spark&lt;?&gt; [inf x 2]
##       y     n
##   &lt;dbl&gt; &lt;dbl&gt;
## 1     8   200
## 2     9   200
## 3    10   102
## 4     7    98</code></pre>
<p>Grouped aggregations of the latest data in the stream require a time stamp. The time stamp will be of when reading function, in this case <code>stream_read_csv()</code> , first “saw” that specific record. In Spark stream terms, that time stamp is called a “watermark”. The <code>spark_watermark()</code> function is used to add the time stamp. For this exercise, the watermark will be the same for all records. That is because the 5 files were read by the stream after they were created. Please note that only Kafka and memory <em>outputs</em> support watermarks.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">stream_read_csv</span>(sc, <span class="st">&quot;source&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">stream_watermark</span>()</code></pre>
<pre><code>## # Source: spark&lt;?&gt; [inf x 2]
##        x timestamp          
##    &lt;int&gt; &lt;dttm&gt;             
##  1   630 2019-04-07 15:44:50
##  2   631 2019-04-07 15:44:50
##  3   632 2019-04-07 15:44:50
##  4   633 2019-04-07 15:44:50
##  5   634 2019-04-07 15:44:50
##  6   635 2019-04-07 15:44:50
##  7   636 2019-04-07 15:44:50
##  8   637 2019-04-07 15:44:50
##  9   638 2019-04-07 15:44:50
## 10   639 2019-04-07 15:44:50
## # ... with more rows</code></pre>
<p>After the watermark is created, it can be used in the <code>group_by()</code> verb. It can then be piped into a <code>summarise()</code> function to get some stats of the stream.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">stream_read_csv</span>(sc, <span class="st">&quot;source&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">stream_watermark</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">group_by</span>(timestamp) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">summarise</span>(
    <span class="dt">max_x =</span> <span class="kw">max</span>(x, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>),
    <span class="dt">min_x =</span> <span class="kw">min</span>(x, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>),
    <span class="dt">count =</span> <span class="kw">n</span>()
  ) </code></pre>
<pre><code>## # Source: spark&lt;?&gt; [inf x 4]
##   timestamp           max_x min_x count
##   &lt;dttm&gt;              &lt;int&gt; &lt;int&gt; &lt;dbl&gt;
## 1 2019-04-07 15:45:59  1000     1  2122</code></pre>
</div>
<div id="transformer-functions" class="section level3">
<h3><span class="header-section-number">12.5.2</span> Transformer functions</h3>
<p>Transformer functions can also be used to modify a stream. They can also be combined with the regular <code>dplyr</code> functions.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">stream_read_csv</span>(sc, <span class="st">&quot;source&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">x =</span> <span class="kw">as.numeric</span>(x)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ft_bucketizer</span>(<span class="st">&quot;x&quot;</span>, <span class="st">&quot;buckets&quot;</span>, <span class="dt">splits =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">10</span> <span class="op">*</span><span class="st"> </span><span class="dv">100</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">count</span>(buckets)  <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">arrange</span>(buckets)</code></pre>
<pre><code>## # Source:     spark&lt;?&gt; [inf x 2]
## # Ordered by: buckets
##    buckets     n
##      &lt;dbl&gt; &lt;dbl&gt;
##  1       0   299
##  2       1   220
##  3       2   200
##  4       3   200
##  5       4   200
##  6       5   200
##  7       6   201
##  8       7   200
##  9       8   200
## 10       9   202</code></pre>
</div>
<div id="r-code" class="section level3">
<h3><span class="header-section-number">12.5.3</span> R code</h3>
<p>Arbitrary R code can also be used to transform a stream with the use of <code>spark_apply()</code>. Following the same principles from executing R code over Spark data frames, for structured streams, <code>spark_apply()</code> runs R code over each executor in the cluster where data is available, this enables processing high-throughput streams and fulfill low-latency requirements.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">stream_read_csv</span>(sc, <span class="st">&quot;source&quot;</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">spark_apply</span>(<span class="op">~</span><span class="st"> </span><span class="kw">nrow</span>(.x), <span class="kw">list</span>(<span class="dt">n=</span><span class="st">&quot;integer&quot;</span>))</code></pre>
<pre><code>## # Source: spark&lt;?&gt; [inf x 1]
##       n
##   &lt;int&gt;
## 1  1962
## 2   148
## 3    12</code></pre>
</div>
<div id="ml-pipelines" class="section level3">
<h3><span class="header-section-number">12.5.4</span> ML Pipelines</h3>
<p>Spark pipelines can be used for scoring streams, but not to train over streaming data. The former is fully supported while the latter is a feature under active development by the Spark community.</p>
<ol style="list-style-type: decimal">
<li><p>In order to try scoring data in a stream, it is necessary to first create a Pipeline Model. The following build, fits and saves a simple pipeline. It also opens and closes the Spark connection.</p>
<pre class="sourceCode r"><code class="sourceCode r">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>)
cars &lt;-<span class="st"> </span><span class="kw">copy_to</span>(sc, mtcars, <span class="st">&quot;mtcars_remote&quot;</span>)
sc <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ml_pipeline</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ft_binarizer</span>(<span class="st">&quot;mpg&quot;</span>, <span class="st">&quot;over_30&quot;</span>,<span class="dv">30</span>) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ft_r_formula</span>(over_<span class="dv">30</span> <span class="op">~</span><span class="st"> </span>wt) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ml_logistic_regression</span>() <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ml_fit</span>(cars) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ml_save</span>(<span class="st">&quot;cars_model&quot;</span>)
<span class="kw">spark_disconnect</span>(sc)</code></pre></li>
<li><p>A new connection of Spark is opened. The saved model is loaded into the new connection.</p>
<pre class="sourceCode r"><code class="sourceCode r">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>)
model &lt;-<span class="st"> </span><span class="kw">ml_load</span>(sc, <span class="st">&quot;cars_model&quot;</span>)</code></pre></li>
<li><p>Data that can be used for predictions is needed. The <code>stream_generate_test()</code> can be used for this as well. Instead of relying on the default output, the <em>mtcars</em> variable is passed to it.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="cf">if</span>(<span class="kw">file.exists</span>(<span class="st">&quot;source&quot;</span>)) <span class="kw">unlink</span>(<span class="st">&quot;source&quot;</span>, <span class="ot">TRUE</span>)
<span class="kw">stream_generate_test</span>(mtcars, <span class="dt">iterations =</span> <span class="dv">5</span>)</code></pre></li>
<li><p>The <code>ml_transform()</code> function can now be used on top of the stream. Because the function expects the model as the first function, the piping works a little different. Instead of starting with reading the stream, we start with the model, and use the stream input as the argument on <code>ml_transform()</code></p>
<pre class="sourceCode r"><code class="sourceCode r">model <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">ml_transform</span>(<span class="kw">stream_read_csv</span>(sc, <span class="st">&quot;source&quot;</span>))</code></pre>
<pre><code>## # Source: spark&lt;?&gt; [inf x 17]
##      mpg   cyl  disp    hp  drat    wt  qsec    vs    am
##    &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt;
##  1  15.5     8 318     150  2.76  3.52  16.9     0     0
##  2  15.2     8 304     150  3.15  3.44  17.3     0     0
##  3  13.3     8 350     245  3.73  3.84  15.4     0     0
##  4  19.2     8 400     175  3.08  3.84  17.0     0     0
##  5  27.3     4  79      66  4.08  1.94  18.9     1     1
##  6  26       4 120.     91  4.43  2.14  16.7     0     1
##  7  30.4     4  95.1   113  3.77  1.51  16.9     1     1
##  8  15.8     8 351     264  4.22  3.17  14.5     0     1
##  9  19.7     6 145     175  3.62  2.77  15.5     0     1
## 10  15       8 301     335  3.54  3.57  14.6     0     1
## # ... with more rows, and 8 more variables: gear &lt;int&gt;,
## #   carb &lt;int&gt;, over_30 &lt;dbl&gt;, features &lt;list&gt;,
## #   label &lt;dbl&gt;, rawPrediction &lt;list&gt;,
## #   probability &lt;list&gt;, prediction &lt;dbl&gt;</code></pre></li>
</ol>
</div>
</div>
<div id="shiny-integration" class="section level2">
<h2><span class="header-section-number">12.6</span> Shiny integration</h2>
<p>The <code>reactiveSpark()</code> provides a mechanism to process the transformations on a stream. It allows you to circumvent the need for writing an output. Also, because it does not depend on the stream writing functions, it is possible to to use watermark groups.</p>
<p>This section’s example will result in a Shiny app. It will start to accumulate and display the current and historical results. The app’s output is shown on Figure <a href="streaming.html#fig:streaming-shiny-app">12.3</a>.</p>
<ol style="list-style-type: decimal">
<li><p>Start by opening a Spark connection and begin a test generation.</p>
<pre class="sourceCode r"><code class="sourceCode r">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>)
<span class="cf">if</span>(<span class="kw">file.exists</span>(<span class="st">&quot;source&quot;</span>)) <span class="kw">unlink</span>(<span class="st">&quot;source&quot;</span>, <span class="ot">TRUE</span>)
<span class="kw">invisible</span>(<span class="kw">future</span>(<span class="kw">stream_generate_test</span>(<span class="dt">interval =</span> <span class="fl">0.2</span>, <span class="dt">iterations =</span> <span class="dv">10</span>)))</code></pre></li>
<li><p>Load the <code>shiny</code> library and create a simple <em>UI</em> function with one table output.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(shiny)
ui &lt;-<span class="st"> </span><span class="cf">function</span>() <span class="kw">tableOutput</span>(<span class="st">&quot;table&quot;</span>)</code></pre></li>
<li><p>The <em>server</em> function contains a <code>reactiveSpark()</code> function. This function reads the stream, adds the watermark and then performs the aggregation. The results are then rendered via the <em>table</em> output.</p>
<pre class="sourceCode r"><code class="sourceCode r">server &lt;-<span class="st"> </span><span class="cf">function</span>(input, output, session){
  ps &lt;-<span class="st"> </span><span class="kw">stream_read_csv</span>(sc, <span class="st">&quot;source&quot;</span>)  <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">stream_watermark</span>() <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">group_by</span>(timestamp) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">summarise</span>(
      <span class="dt">max_x =</span> <span class="kw">max</span>(x, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>),
      <span class="dt">min_x =</span> <span class="kw">min</span>(x, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>),
      <span class="dt">count =</span> <span class="kw">n</span>()) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">reactiveSpark</span>()  <span class="co"># Spark stream reactive</span>
  output<span class="op">$</span>table &lt;-<span class="st"> </span><span class="kw">renderTable</span>(
    <span class="kw">ps</span>() <span class="op">%&gt;%</span>
<span class="st">      </span><span class="kw">mutate</span>(<span class="dt">timestamp =</span> <span class="kw">as.character</span>(timestamp))
  )}</code></pre></li>
<li><p>The Shiny app can be activated with <code>runGadget()</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">runGadget</span>(ui, server)</code></pre></li>
</ol>
<div class="figure" style="text-align: center"><span id="fig:streaming-shiny-app"></span>
<img src="images/streaming-shiny-app-resized.png" alt="Shiny reactive" width="1500" />
<p class="caption">
FIGURE 12.3: Shiny reactive
</p>
</div>
</div>
<div id="kafka" class="section level2">
<h2><span class="header-section-number">12.7</span> Kafka</h2>
<p>Apache Kafka is to streaming as what Hadoop is to data storage/retrieval. Hadoop provides a a distributed, resilient and reliable architecture for large-scale data storage. Kafka does the same, but for large-scale streaming applications.</p>
<div id="workflow" class="section level3">
<h3><span class="header-section-number">12.7.1</span> Workflow</h3>
<p>A most basic Kafka workflow is illustrated on Figure <a href="streaming.html#fig:streaming-kafka-apis">12.4</a>. An application streams data into Kafka, called a <strong>Producer</strong>. Kafka stores the stream as records. Each record has a key, a value and a timestamp. Kafka can handle multiple streams that contain different information. To properly categorize each stream, Kafka uses a mechanism called topic. A topic is a alpha-numeric identifier of the stream. A <strong>Consumer</strong> is an app that is external to Kafka. It reads what is stored in Kafka for a given topic. Because the Consumer app is constantly monitoring the topic, the term used for its integration with Kafka is subscribe.</p>
<div class="figure" style="text-align: center"><span id="fig:streaming-kafka-apis"></span>
<div id="htmlwidget-eeacf23e70bf42e63bef" style="width:100%;height:220pt;" class="nomnoml html-widget"></div>
<script type="application/json" data-for="htmlwidget-eeacf23e70bf42e63bef">{"x":{"code":"\n  #fill: #FEFEFF\n  #lineWidth: 2\n  #zoom: 4\n  #direction: right\n   \n\n#direction: right\n#padding: 10\n#fontSize: 14\n#leading: 2\n[Producer | Stream topic A] -> [Kafka | Store topic A]\n[Kafka] -> [Consumer | Subscribe to topic A]\n","className":null,"svg":false},"evals":[],"jsHooks":[]}</script>
<p class="caption">
FIGURE 12.4: Basic workflow
</p>
</div>
<p>Kafka also allows for an application to read from one topic, process its data, and then write the results to a different topic. That is called a <em>Stream Processor</em>. In Figure <a href="streaming.html#fig:streaming-kafka-two-outputs">12.5</a>, the Stream Processor reads topic A, and then writes results to topic B. This allows for a given <em>Consumer</em> application to read results instead of “raw” feed data.</p>
<div class="figure" style="text-align: center"><span id="fig:streaming-kafka-two-outputs"></span>
<div id="htmlwidget-64a59c423b4cb23e8da0" style="width:100%;height:220pt;" class="nomnoml html-widget"></div>
<script type="application/json" data-for="htmlwidget-64a59c423b4cb23e8da0">{"x":{"code":"\n  #fill: #FEFEFF\n  #lineWidth: 2\n  #zoom: 4\n  #direction: right\n   \n\n#direction: right\n#padding: 10\n#fontSize: 14\n#leading: 2\n\n[Producer | Stream topic A] -> [Kafka | Store topics A & B]\n[Kafka] <-> [Stream Processor | Subscribes to topic A\nResults to topic B]\n[Kafka] -> [Consumer | Subscribe to topic B]\n\n","className":null,"svg":false},"evals":[],"jsHooks":[]}</script>
<p class="caption">
FIGURE 12.5: Kafka workflow
</p>
</div>
</div>
<div id="spark-integration" class="section level3">
<h3><span class="header-section-number">12.7.2</span> Spark integration</h3>
<p>Spark Streaming enables the integration of Spark with Kafka. Spark is able to both, read from and write into Kafka topics. This means that Spark could be a Consumer, Stream Processor or Producer application of a Kafka implementation.</p>
<p>Unless there is a very specific need, using Spark as a Producer does not make much sense. That is because Spark Streaming reacts to a stream, it doesn’t generate it. Theoretically, there could be cases where Spark reads from one stream (Kafka or not) and streams to a Kafka cluster. To the target, Spark would look as a pure Producer, even though that’s not really the case.</p>
<p>The more likely use case for this integration, is to use Spark to read (Consumer) from one, or several, topics and then reactively write (Producer) to a different topic with the results of the analysis. All within the same Kafka cluster. This effectively makes Spark a Stream Processor.</p>
<p>There are nuances on how the Spark-to-Kafka write-back modes works. It is important to offer some clarification. There are three modes available: <strong>complete</strong>, <strong>update</strong> and <strong>append</strong>. The <strong>complete</strong> mode will provide the totals for every group every time there is a new batch. The <strong>update</strong> mode will provide totals for only the groups that have updates in the latest batch. The <strong>append</strong> mode is able to add raw records to the target topic. This mode is not meant for aggregates, but works well for passing a filtered subset to the target topic.</p>
</div>
<div id="r-integration" class="section level3">
<h3><span class="header-section-number">12.7.3</span> R integration</h3>
<p>The R integration is delivered via the <code>sparklyr</code> interface. There are a couple of things to keep in mind:</p>
<ul>
<li><p><strong>The Kafka integration Spark package is required </strong> - The name should be <code>org.apache.spark:spark-sql-kafka</code> followed by the Kafka version, then the Scala version, and lastly the Spark package version. For example: <code>org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0</code>, requests the version 2.4.0 of the Spark package that supports Scala 2.11 and Kafka 10.</p>
<pre class="sourceCode r"><code class="sourceCode r">config &lt;-<span class="st"> </span><span class="kw">spark_config</span>()
config<span class="op">$</span>sparklyr.shell.packages &lt;-<span class="st"> &quot;org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0&quot;</span>
sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>, <span class="dt">config =</span> config)</code></pre></li>
<li><p><strong>The Kafka writing and reading function in <code>sparklyr</code> rely on additional parameters</strong> - These parameters are passed via the <code>options</code> argument. The contents of the <code>options</code> argument are passed down to Kafka as-is. This means that the same Kafka options used in your other applications can be reused here. There are three basic Kafka options to keep in mind: <code>kafka.bootstrap.server</code>, <code>topic</code> and <code>subscribe</code>. The former expects a list of the of one or more hosts from the Kafka cluster. The other two set the topic that the function is either reading from or writing to. One is used at the exclusion of the other. For reading the <code>subscribe</code> option is used, and for writing, <code>topic</code> is used.</p></li>
</ul>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">stream_read_kafka</span>(
  sc, 
  <span class="dt">options =</span> <span class="kw">list</span>(
    <span class="dt">kafka.bootstrap.server =</span> <span class="st">&quot;host1:9092, host2:9092&quot;</span>, 
    <span class="dt">subscribe =</span> <span class="st">&quot;topic&quot;</span>
    )
  ) </code></pre>
</div>
<div id="example" class="section level3">
<h3><span class="header-section-number">12.7.4</span> Example</h3>
<p>For the example, the <strong>Producer</strong> will stream random, single letters of the alphabet into Kafka. The topic will be called “letters”. A <strong>Stream Processor</strong> will be built in Spark by having it read the “letters” topic, and then produce the count by unique letter passed throught the stream. The count will be passed back to Kafka in separate topic called “totals”. To see the results, the same Spark connection will be used to setup a <strong>Consumer</strong> that reads the “totals” topic. Figure <a href="streaming.html#fig:streaming-kafka-example">12.6</a> is a diagram of how this example will work.</p>
<div class="figure" style="text-align: center"><span id="fig:streaming-kafka-example"></span>
<div id="htmlwidget-956c1a53c8b9c3d32277" style="width:100%;height:220pt;" class="nomnoml html-widget"></div>
<script type="application/json" data-for="htmlwidget-956c1a53c8b9c3d32277">{"x":{"code":"\n  #fill: #FEFEFF\n  #lineWidth: 2\n  #zoom: 4\n  #direction: right\n   \n\n#direction: right\n#padding: 10\n#fontSize: 14\n#leading: 2\n\n[Producer | Streams 'letters'] -> [Kafka | Stores 'letters' & 'totals']\n[Kafka] <-> [Stream Processor \n(Spark)| Subscribes to 'letters'\nWrites results to 'totals']\n[Kafka] -> [Consumer\n(Spark) | Subscribes to 'totals']\n\n","className":null,"svg":false},"evals":[],"jsHooks":[]}</script>
<p class="caption">
FIGURE 12.6: Kafka example
</p>
</div>
<p>The example will use the <code>update</code> mode for writing back into Kafka. This means that only the totals of the letters that changed will be sent to the “totals” topic. The change in totals is determined after each batch from the “letters” topic is received. Figure <a href="streaming.html#fig:streaming-kafka-processor">12.7</a> offers an deeper look of what the <strong>Stream Processor (Spark)</strong> process is supposed to do.</p>
<div class="figure" style="text-align: center"><span id="fig:streaming-kafka-processor"></span>
<div id="htmlwidget-5f2c3e89255607a070ca" style="width:100%;height:220pt;" class="nomnoml html-widget"></div>
<script type="application/json" data-for="htmlwidget-5f2c3e89255607a070ca">{"x":{"code":"\n  #fill: #FEFEFF\n  #lineWidth: 2\n  #zoom: 4\n  #direction: right\n   \n\n#direction: right\n#padding: 10\n#fontSize: 14\n#leading: 2\n\n['totlas' topic |\n[timestamp 1 | A B A A B C ]\n[timestamp 2 | B B C C B C ]\n] -> [Stream Processor | Spark Streaming]\n[Stream Processor] -> ['totals' topic | \n[timestamp 1 | A=3 B=2 C=1 ] \n[timestamp 2 | B=5 C=4 ]\n]\n","className":null,"svg":false},"evals":[],"jsHooks":[]}</script>
<p class="caption">
FIGURE 12.7: Stream Processor - Update mode
</p>
</div>
<p>The <strong>infrastructure</strong> used for this example was a local, single node Kafka cluster. The external Producer uses the Kafka CLI to send the stream of letters. The installation instructions that were used can be found in the Appendix under the Kafka section.</p>
<ol style="list-style-type: decimal">
<li><p>Load libraries, and open the connection. Remember to load the Kafka integration Spark package.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(sparklyr)
<span class="kw">library</span>(dplyr)

config &lt;-<span class="st"> </span><span class="kw">spark_config</span>()
config<span class="op">$</span>sparklyr.shell.packages &lt;-<span class="st"> &quot;org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0&quot;</span>
sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>, <span class="dt">config =</span> config)</code></pre></li>
<li><p>The local Kafka cluster is served on port 9092, by default. In order to keep the read and write calls a little cleaner, a couple of variables will contain the Kafka options. Notice that the read option has <code>subscribe</code>, while the write option contains <code>topic</code>.</p>
<pre class="sourceCode r"><code class="sourceCode r">hosts   &lt;-<span class="st"> &quot;localhost:9092&quot;</span>
read_options  &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">kafka.bootstrap.servers =</span> hosts, <span class="dt">subscribe =</span> <span class="st">&quot;letters&quot;</span>)
write_options &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">kafka.bootstrap.servers =</span> hosts, <span class="dt">topic =</span> <span class="st">&quot;totals&quot;</span>)</code></pre></li>
<li><p>Typically, the following steps would be written together using a pipe. So that explanations can be shared, they have been broken into individual step variables. What is unique about this setup is the use of the <code>read_options</code> variable as the <code>options</code> argument.</p>
<pre class="sourceCode r"><code class="sourceCode r">step_<span class="dv">1</span> &lt;-<span class="st"> </span><span class="kw">stream_read_kafka</span>(sc, <span class="dt">options =</span> read_options)</code></pre></li>
<li><p>This steps coerces the <code>value</code> field into a character. The resulting content of the field is a single letter entry from the stream. The letters are grouped and counted. A single field is permitted by Kafka to send back as results. It also expect that the results are in a field called <code>value</code>. The new <code>value</code> field is a concatenated field with the letter and the count.</p>
<pre class="sourceCode r"><code class="sourceCode r">step_<span class="dv">2</span> &lt;-<span class="st"> </span>step_<span class="dv">1</span> <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">value =</span> <span class="kw">as.character</span>(value)) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">count</span>(value) <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">mutate</span>(<span class="dt">value =</span> <span class="kw">paste0</span>(value, <span class="st">&quot;=&quot;</span>, n))</code></pre></li>
<li><p>The results are written to the “totals” topic. The<code>mode</code> argument is set to “update”. This sets the count behavior illustrated in Figure <a href="streaming.html#fig:streaming-shiny-kafka">12.8</a></p>
<pre class="sourceCode r"><code class="sourceCode r">step_<span class="dv">3</span> &lt;-<span class="st"> </span>step_<span class="dv">2</span> <span class="op">%&gt;%</span>
<span class="st">  </span><span class="kw">stream_write_kafka</span>(<span class="dt">mode =</span> <span class="st">&quot;update&quot;</span>, <span class="dt">options =</span> write_options)</code></pre></li>
</ol>
<p>The last step starts a Spark job. The job will remain active until stopped or until Spark disconnects. At this point, there is no visible output. Even if there was an active Producer sending letters over.</p>
<p>A simple Shiny routine can be used as a <em>Consumer</em> app. It will read the “totals” topic, select the latest count for a given letter, and then display the results on a table.</p>
<pre class="sourceCode r"><code class="sourceCode r"><span class="kw">library</span>(shiny)
ui &lt;-<span class="st"> </span><span class="cf">function</span>() <span class="kw">tableOutput</span>(<span class="st">&quot;table&quot;</span>)
server &lt;-<span class="st"> </span><span class="cf">function</span>(input, output, session){
  totals_options  &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">kafka.bootstrap.servers =</span> hosts, <span class="dt">subscribe =</span> <span class="st">&quot;totals&quot;</span>)
  ps &lt;-<span class="st"> </span><span class="kw">stream_read_kafka</span>(sc, <span class="dt">options =</span> totals_options) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">mutate</span>(<span class="dt">value =</span> <span class="kw">as.character</span>(value),
           <span class="dt">letter =</span> <span class="kw">substr</span>(value, <span class="dv">1</span>,<span class="dv">1</span>),
           <span class="dt">total =</span> <span class="kw">as.numeric</span>(<span class="kw">substr</span>(value, <span class="dv">3</span>, <span class="dv">100</span>))
           ) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">group_by</span>(letter) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">summarise</span>(<span class="dt">total =</span> <span class="kw">max</span>(total, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>)) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">arrange</span>(letter) <span class="op">%&gt;%</span>
<span class="st">    </span><span class="kw">reactiveSpark</span>()  
  output<span class="op">$</span>table &lt;-<span class="st"> </span><span class="kw">renderTable</span>(<span class="kw">ps</span>())
}
<span class="kw">runGadget</span>(ui, server)</code></pre>
<p>A new terminal session is started. Kafka’s CLI provides a simple Producer program that runs in the console. Using that program, we can manually write a single letter, and then press enter.</p>
<pre><code>user@laptop:~/kafka_2.12-2.2.0$ bin/kafka-console-producer.sh --broker-list localhost:9092 --topic letters
&gt;A
&gt;B
&gt;C
&gt;A
&gt;A
&gt;C
&gt;D</code></pre>
<p>The Shiny reactive function will poll and refresh the results as the letters are being entered. This is shown in figure 11.8.</p>
<div class="figure" style="text-align: center"><span id="fig:streaming-shiny-kafka"></span>
<img src="images/streaming-shiny-kafka-resized.png" alt="Shiny with Kafka" width="1500" />
<p class="caption">
FIGURE 12.8: Shiny with Kafka
</p>
</div>

</div>
</div>
</div>
<h3> References</h3>
<div id="refs" class="references">
<div id="ref-streaming-programming-guide">
<p>“Spark Streaming Programming Guide.” 2018. <a href="https://spark.apache.org/docs/latest/streaming-programming-guide.html">https://spark.apache.org/docs/latest/streaming-programming-guide.html</a>.</p>
</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="distributed.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="contributing.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:" && /^https?:/.test(src))
      src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
