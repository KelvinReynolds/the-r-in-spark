<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="" xml:lang="">
<head>

  <meta charset="utf-8" />
  <meta http-equiv="X-UA-Compatible" content="IE=edge" />
  <title>Chapter 12 Streaming | Mastering Spark with R</title>
  <meta name="description" content="The Complete Guide to Large-Scale Analysis and Modeling." />
  <meta name="generator" content="bookdown 0.13 and GitBook 2.6.7" />

  <meta property="og:title" content="Chapter 12 Streaming | Mastering Spark with R" />
  <meta property="og:type" content="book" />
  
  
  <meta property="og:description" content="The Complete Guide to Large-Scale Analysis and Modeling." />
  <meta name="github-repo" content="r-spark/the-r-in-spark" />

  <meta name="twitter:card" content="summary" />
  <meta name="twitter:title" content="Chapter 12 Streaming | Mastering Spark with R" />
  
  <meta name="twitter:description" content="The Complete Guide to Large-Scale Analysis and Modeling." />
  

<meta name="author" content="Javier Luraschi, Kevin Kuo, Edgar Ruiz" />



  <meta name="viewport" content="width=device-width, initial-scale=1" />
  <meta name="apple-mobile-web-app-capable" content="yes" />
  <meta name="apple-mobile-web-app-status-bar-style" content="black" />
  
  
<link rel="prev" href="distributed.html"/>
<link rel="next" href="contributing.html"/>
<script src="libs/jquery-2.2.3/jquery.min.js"></script>
<link href="libs/gitbook-2.6.7/css/style.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-table.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-bookdown.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-highlight.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-search.css" rel="stylesheet" />
<link href="libs/gitbook-2.6.7/css/plugin-fontsettings.css" rel="stylesheet" />







<!-- Global site tag (gtag.js) - Google Analytics -->
<script async src="https://www.googletagmanager.com/gtag/js?id=UA-119986300-1"></script>
<script>
  window.dataLayer = window.dataLayer || [];
  function gtag(){dataLayer.push(arguments);}
  gtag('js', new Date());

  gtag('config', 'UA-119986300-1');
</script>


<style type="text/css">
a.sourceLine { display: inline-block; line-height: 1.25; }
a.sourceLine { pointer-events: none; color: inherit; text-decoration: inherit; }
a.sourceLine:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode { white-space: pre; position: relative; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
code.sourceCode { white-space: pre-wrap; }
a.sourceLine { text-indent: -1em; padding-left: 1em; }
}
pre.numberSource a.sourceLine
  { position: relative; left: -4em; }
pre.numberSource a.sourceLine::before
  { content: attr(title);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; pointer-events: all; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
    color: #aaaaaa;
  }
pre.numberSource { margin-left: 3em; border-left: 1px solid #aaaaaa;  padding-left: 4px; }
div.sourceCode
  {  }
@media screen {
a.sourceLine::before { text-decoration: underline; }
}
code span.al { color: #ff0000; font-weight: bold; } /* Alert */
code span.an { color: #60a0b0; font-weight: bold; font-style: italic; } /* Annotation */
code span.at { color: #7d9029; } /* Attribute */
code span.bn { color: #40a070; } /* BaseN */
code span.bu { } /* BuiltIn */
code span.cf { color: #007020; font-weight: bold; } /* ControlFlow */
code span.ch { color: #4070a0; } /* Char */
code span.cn { color: #880000; } /* Constant */
code span.co { color: #60a0b0; font-style: italic; } /* Comment */
code span.cv { color: #60a0b0; font-weight: bold; font-style: italic; } /* CommentVar */
code span.do { color: #ba2121; font-style: italic; } /* Documentation */
code span.dt { color: #902000; } /* DataType */
code span.dv { color: #40a070; } /* DecVal */
code span.er { color: #ff0000; font-weight: bold; } /* Error */
code span.ex { } /* Extension */
code span.fl { color: #40a070; } /* Float */
code span.fu { color: #06287e; } /* Function */
code span.im { } /* Import */
code span.in { color: #60a0b0; font-weight: bold; font-style: italic; } /* Information */
code span.kw { color: #007020; font-weight: bold; } /* Keyword */
code span.op { color: #666666; } /* Operator */
code span.ot { color: #007020; } /* Other */
code span.pp { color: #bc7a00; } /* Preprocessor */
code span.sc { color: #4070a0; } /* SpecialChar */
code span.ss { color: #bb6688; } /* SpecialString */
code span.st { color: #4070a0; } /* String */
code span.va { color: #19177c; } /* Variable */
code span.vs { color: #4070a0; } /* VerbatimString */
code span.wa { color: #60a0b0; font-weight: bold; font-style: italic; } /* Warning */
</style>

<link rel="stylesheet" href="style.css" type="text/css" />
</head>

<body>



  <div class="book without-animation with-summary font-size-2 font-family-1" data-basepath=".">

    <div class="book-summary">
      <nav role="navigation">

<ul class="summary">
<li><strong><a href="./">Mastering Spark with R</a></strong></li>

<li class="divider"></li>
<li class="chapter" data-level="1" data-path="intro.html"><a href="intro.html"><i class="fa fa-check"></i><b>1</b> Introduction</a><ul>
<li class="chapter" data-level="1.1" data-path="intro.html"><a href="intro.html#intro-background"><i class="fa fa-check"></i><b>1.1</b> Overview</a></li>
<li class="chapter" data-level="1.2" data-path="intro.html"><a href="intro.html#intro-hadoop"><i class="fa fa-check"></i><b>1.2</b> Hadoop</a></li>
<li class="chapter" data-level="1.3" data-path="intro.html"><a href="intro.html#intro-spark"><i class="fa fa-check"></i><b>1.3</b> Spark</a></li>
<li class="chapter" data-level="1.4" data-path="intro.html"><a href="intro.html#intro-r"><i class="fa fa-check"></i><b>1.4</b> R</a></li>
<li class="chapter" data-level="1.5" data-path="intro.html"><a href="intro.html#intro-sparklyr"><i class="fa fa-check"></i><b>1.5</b> sparklyr</a></li>
<li class="chapter" data-level="1.6" data-path="intro.html"><a href="intro.html#intro-recap"><i class="fa fa-check"></i><b>1.6</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="2" data-path="starting.html"><a href="starting.html"><i class="fa fa-check"></i><b>2</b> Getting Started</a><ul>
<li class="chapter" data-level="2.1" data-path="starting.html"><a href="starting.html#overview"><i class="fa fa-check"></i><b>2.1</b> Overview</a></li>
<li class="chapter" data-level="2.2" data-path="starting.html"><a href="starting.html#starting-prerequisites"><i class="fa fa-check"></i><b>2.2</b> Prerequisites</a><ul>
<li class="chapter" data-level="2.2.1" data-path="starting.html"><a href="starting.html#starting-install-sparklyr"><i class="fa fa-check"></i><b>2.2.1</b> Installing sparklyr</a></li>
<li class="chapter" data-level="2.2.2" data-path="starting.html"><a href="starting.html#starting-installing-spark"><i class="fa fa-check"></i><b>2.2.2</b> Installing Spark</a></li>
</ul></li>
<li class="chapter" data-level="2.3" data-path="starting.html"><a href="starting.html#starting-connect-to-spark"><i class="fa fa-check"></i><b>2.3</b> Connecting</a></li>
<li class="chapter" data-level="2.4" data-path="starting.html"><a href="starting.html#starting-sparklyr-hello-world"><i class="fa fa-check"></i><b>2.4</b> Using Spark</a><ul>
<li class="chapter" data-level="2.4.1" data-path="starting.html"><a href="starting.html#starting-spark-web-interface"><i class="fa fa-check"></i><b>2.4.1</b> Web Interface</a></li>
<li class="chapter" data-level="2.4.2" data-path="starting.html"><a href="starting.html#starting-analysis"><i class="fa fa-check"></i><b>2.4.2</b> Analysis</a></li>
<li class="chapter" data-level="2.4.3" data-path="starting.html"><a href="starting.html#starting-modeling"><i class="fa fa-check"></i><b>2.4.3</b> Modeling</a></li>
<li class="chapter" data-level="2.4.4" data-path="starting.html"><a href="starting.html#starting-data"><i class="fa fa-check"></i><b>2.4.4</b> Data</a></li>
<li class="chapter" data-level="2.4.5" data-path="starting.html"><a href="starting.html#starting-extensions"><i class="fa fa-check"></i><b>2.4.5</b> Extensions</a></li>
<li class="chapter" data-level="2.4.6" data-path="starting.html"><a href="starting.html#starting-distributed-r"><i class="fa fa-check"></i><b>2.4.6</b> Distributed R</a></li>
<li class="chapter" data-level="2.4.7" data-path="starting.html"><a href="starting.html#starting-streaming"><i class="fa fa-check"></i><b>2.4.7</b> Streaming</a></li>
<li class="chapter" data-level="2.4.8" data-path="starting.html"><a href="starting.html#starting-logs"><i class="fa fa-check"></i><b>2.4.8</b> Logs</a></li>
</ul></li>
<li class="chapter" data-level="2.5" data-path="starting.html"><a href="starting.html#starting-disconnecting"><i class="fa fa-check"></i><b>2.5</b> Disconnecting</a></li>
<li class="chapter" data-level="2.6" data-path="starting.html"><a href="starting.html#starting-using-spark-from-rstudio"><i class="fa fa-check"></i><b>2.6</b> Using RStudio</a></li>
<li class="chapter" data-level="2.7" data-path="starting.html"><a href="starting.html#starting-resources"><i class="fa fa-check"></i><b>2.7</b> Resources</a></li>
<li class="chapter" data-level="2.8" data-path="starting.html"><a href="starting.html#starting-recap"><i class="fa fa-check"></i><b>2.8</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="3" data-path="analysis.html"><a href="analysis.html"><i class="fa fa-check"></i><b>3</b> Analysis</a><ul>
<li class="chapter" data-level="3.1" data-path="analysis.html"><a href="analysis.html#analysis-overview"><i class="fa fa-check"></i><b>3.1</b> Overview</a></li>
<li class="chapter" data-level="3.2" data-path="analysis.html"><a href="analysis.html#import"><i class="fa fa-check"></i><b>3.2</b> Import</a></li>
<li class="chapter" data-level="3.3" data-path="analysis.html"><a href="analysis.html#wrangle"><i class="fa fa-check"></i><b>3.3</b> Wrangle</a><ul>
<li class="chapter" data-level="3.3.1" data-path="analysis.html"><a href="analysis.html#built-in-functions"><i class="fa fa-check"></i><b>3.3.1</b> Built-in Functions</a></li>
<li class="chapter" data-level="3.3.2" data-path="analysis.html"><a href="analysis.html#correlations"><i class="fa fa-check"></i><b>3.3.2</b> Correlations</a></li>
</ul></li>
<li class="chapter" data-level="3.4" data-path="analysis.html"><a href="analysis.html#visualize"><i class="fa fa-check"></i><b>3.4</b> Visualize</a><ul>
<li class="chapter" data-level="3.4.1" data-path="analysis.html"><a href="analysis.html#using-ggplot2"><i class="fa fa-check"></i><b>3.4.1</b> Using ggplot2</a></li>
<li class="chapter" data-level="3.4.2" data-path="analysis.html"><a href="analysis.html#using-dbplot"><i class="fa fa-check"></i><b>3.4.2</b> Using dbplot</a></li>
</ul></li>
<li class="chapter" data-level="3.5" data-path="analysis.html"><a href="analysis.html#model"><i class="fa fa-check"></i><b>3.5</b> Model</a><ul>
<li class="chapter" data-level="3.5.1" data-path="analysis.html"><a href="analysis.html#caching"><i class="fa fa-check"></i><b>3.5.1</b> Caching</a></li>
</ul></li>
<li class="chapter" data-level="3.6" data-path="analysis.html"><a href="analysis.html#communicate"><i class="fa fa-check"></i><b>3.6</b> Communicate</a></li>
<li class="chapter" data-level="3.7" data-path="analysis.html"><a href="analysis.html#recap"><i class="fa fa-check"></i><b>3.7</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="4" data-path="modeling.html"><a href="modeling.html"><i class="fa fa-check"></i><b>4</b> Modeling</a><ul>
<li class="chapter" data-level="4.1" data-path="modeling.html"><a href="modeling.html#overview-1"><i class="fa fa-check"></i><b>4.1</b> Overview</a></li>
<li class="chapter" data-level="4.2" data-path="modeling.html"><a href="modeling.html#exploratory-data-analysis"><i class="fa fa-check"></i><b>4.2</b> Exploratory Data Analysis</a></li>
<li class="chapter" data-level="4.3" data-path="modeling.html"><a href="modeling.html#feature-engineering"><i class="fa fa-check"></i><b>4.3</b> Feature Engineering</a></li>
<li class="chapter" data-level="4.4" data-path="modeling.html"><a href="modeling.html#supervised-learning"><i class="fa fa-check"></i><b>4.4</b> Supervised Learning</a><ul>
<li class="chapter" data-level="4.4.1" data-path="modeling.html"><a href="modeling.html#generalized-linear-regression"><i class="fa fa-check"></i><b>4.4.1</b> Generalized Linear Regression</a></li>
<li class="chapter" data-level="4.4.2" data-path="modeling.html"><a href="modeling.html#other-models"><i class="fa fa-check"></i><b>4.4.2</b> Other Models</a></li>
</ul></li>
<li class="chapter" data-level="4.5" data-path="modeling.html"><a href="modeling.html#unsupervised-learning"><i class="fa fa-check"></i><b>4.5</b> Unsupervised Learning</a><ul>
<li class="chapter" data-level="4.5.1" data-path="modeling.html"><a href="modeling.html#data-preparation"><i class="fa fa-check"></i><b>4.5.1</b> Data Preparation</a></li>
<li class="chapter" data-level="4.5.2" data-path="modeling.html"><a href="modeling.html#topic-modeling"><i class="fa fa-check"></i><b>4.5.2</b> Topic Modeling</a></li>
</ul></li>
<li class="chapter" data-level="4.6" data-path="modeling.html"><a href="modeling.html#recap-1"><i class="fa fa-check"></i><b>4.6</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="5" data-path="pipelines.html"><a href="pipelines.html"><i class="fa fa-check"></i><b>5</b> Pipelines</a><ul>
<li class="chapter" data-level="5.1" data-path="pipelines.html"><a href="pipelines.html#overview-2"><i class="fa fa-check"></i><b>5.1</b> Overview</a></li>
<li class="chapter" data-level="5.2" data-path="pipelines.html"><a href="pipelines.html#creation"><i class="fa fa-check"></i><b>5.2</b> Creation</a></li>
<li class="chapter" data-level="5.3" data-path="pipelines.html"><a href="pipelines.html#use-cases"><i class="fa fa-check"></i><b>5.3</b> Use Cases</a><ul>
<li class="chapter" data-level="5.3.1" data-path="pipelines.html"><a href="pipelines.html#hyperparameter-tuning"><i class="fa fa-check"></i><b>5.3.1</b> Hyperparameter Tuning</a></li>
</ul></li>
<li class="chapter" data-level="5.4" data-path="pipelines.html"><a href="pipelines.html#operating-modes"><i class="fa fa-check"></i><b>5.4</b> Operating Modes</a></li>
<li class="chapter" data-level="5.5" data-path="pipelines.html"><a href="pipelines.html#interoperability"><i class="fa fa-check"></i><b>5.5</b> Interoperability</a></li>
<li class="chapter" data-level="5.6" data-path="pipelines.html"><a href="pipelines.html#deployment"><i class="fa fa-check"></i><b>5.6</b> Deployment</a><ul>
<li class="chapter" data-level="5.6.1" data-path="pipelines.html"><a href="pipelines.html#batch-scoring"><i class="fa fa-check"></i><b>5.6.1</b> Batch Scoring</a></li>
<li class="chapter" data-level="5.6.2" data-path="pipelines.html"><a href="pipelines.html#real-time-scoring"><i class="fa fa-check"></i><b>5.6.2</b> Real-Time Scoring</a></li>
</ul></li>
<li class="chapter" data-level="5.7" data-path="pipelines.html"><a href="pipelines.html#recap-2"><i class="fa fa-check"></i><b>5.7</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="6" data-path="clusters.html"><a href="clusters.html"><i class="fa fa-check"></i><b>6</b> Clusters</a><ul>
<li class="chapter" data-level="6.1" data-path="clusters.html"><a href="clusters.html#clusters-overview"><i class="fa fa-check"></i><b>6.1</b> Overview</a></li>
<li class="chapter" data-level="6.2" data-path="clusters.html"><a href="clusters.html#on-premise"><i class="fa fa-check"></i><b>6.2</b> On-Premise</a><ul>
<li class="chapter" data-level="6.2.1" data-path="clusters.html"><a href="clusters.html#clusters-manager"><i class="fa fa-check"></i><b>6.2.1</b> Managers</a></li>
<li class="chapter" data-level="6.2.2" data-path="clusters.html"><a href="clusters.html#distributions"><i class="fa fa-check"></i><b>6.2.2</b> Distributions</a></li>
</ul></li>
<li class="chapter" data-level="6.3" data-path="clusters.html"><a href="clusters.html#cloud"><i class="fa fa-check"></i><b>6.3</b> Cloud</a><ul>
<li class="chapter" data-level="6.3.1" data-path="clusters.html"><a href="clusters.html#clusters-amazon-emr"><i class="fa fa-check"></i><b>6.3.1</b> Amazon</a></li>
<li class="chapter" data-level="6.3.2" data-path="clusters.html"><a href="clusters.html#databricks"><i class="fa fa-check"></i><b>6.3.2</b> Databricks</a></li>
<li class="chapter" data-level="6.3.3" data-path="clusters.html"><a href="clusters.html#google"><i class="fa fa-check"></i><b>6.3.3</b> Google</a></li>
<li class="chapter" data-level="6.3.4" data-path="clusters.html"><a href="clusters.html#ibm"><i class="fa fa-check"></i><b>6.3.4</b> IBM</a></li>
<li class="chapter" data-level="6.3.5" data-path="clusters.html"><a href="clusters.html#microsoft"><i class="fa fa-check"></i><b>6.3.5</b> Microsoft</a></li>
<li class="chapter" data-level="6.3.6" data-path="clusters.html"><a href="clusters.html#qubole"><i class="fa fa-check"></i><b>6.3.6</b> Qubole</a></li>
</ul></li>
<li class="chapter" data-level="6.4" data-path="clusters.html"><a href="clusters.html#kubernetes"><i class="fa fa-check"></i><b>6.4</b> Kubernetes</a></li>
<li class="chapter" data-level="6.5" data-path="clusters.html"><a href="clusters.html#tools"><i class="fa fa-check"></i><b>6.5</b> Tools</a><ul>
<li class="chapter" data-level="6.5.1" data-path="clusters.html"><a href="clusters.html#rstudio"><i class="fa fa-check"></i><b>6.5.1</b> RStudio</a></li>
<li class="chapter" data-level="6.5.2" data-path="clusters.html"><a href="clusters.html#jupyter"><i class="fa fa-check"></i><b>6.5.2</b> Jupyter</a></li>
<li class="chapter" data-level="6.5.3" data-path="clusters.html"><a href="clusters.html#clusters-livy"><i class="fa fa-check"></i><b>6.5.3</b> Livy</a></li>
</ul></li>
<li class="chapter" data-level="6.6" data-path="clusters.html"><a href="clusters.html#recap-3"><i class="fa fa-check"></i><b>6.6</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="7" data-path="connections.html"><a href="connections.html"><i class="fa fa-check"></i><b>7</b> Connections</a><ul>
<li class="chapter" data-level="7.1" data-path="connections.html"><a href="connections.html#connections-overview"><i class="fa fa-check"></i><b>7.1</b> Overview</a><ul>
<li class="chapter" data-level="7.1.1" data-path="connections.html"><a href="connections.html#connections-spark-edge-nodes"><i class="fa fa-check"></i><b>7.1.1</b> Edge Nodes</a></li>
<li class="chapter" data-level="7.1.2" data-path="connections.html"><a href="connections.html#connections-spark-home"><i class="fa fa-check"></i><b>7.1.2</b> Spark Home</a></li>
</ul></li>
<li class="chapter" data-level="7.2" data-path="connections.html"><a href="connections.html#connections-local"><i class="fa fa-check"></i><b>7.2</b> Local</a></li>
<li class="chapter" data-level="7.3" data-path="connections.html"><a href="connections.html#connections-standalone"><i class="fa fa-check"></i><b>7.3</b> Standalone</a></li>
<li class="chapter" data-level="7.4" data-path="connections.html"><a href="connections.html#connections-yarn"><i class="fa fa-check"></i><b>7.4</b> Yarn</a><ul>
<li class="chapter" data-level="7.4.1" data-path="connections.html"><a href="connections.html#connections-yarn-client"><i class="fa fa-check"></i><b>7.4.1</b> Yarn Client</a></li>
<li class="chapter" data-level="7.4.2" data-path="connections.html"><a href="connections.html#connections-yarn-cluster"><i class="fa fa-check"></i><b>7.4.2</b> Yarn Cluster</a></li>
</ul></li>
<li class="chapter" data-level="7.5" data-path="connections.html"><a href="connections.html#connections-livy"><i class="fa fa-check"></i><b>7.5</b> Livy</a></li>
<li class="chapter" data-level="7.6" data-path="connections.html"><a href="connections.html#connections-mesos"><i class="fa fa-check"></i><b>7.6</b> Mesos</a></li>
<li class="chapter" data-level="7.7" data-path="connections.html"><a href="connections.html#connections-kubernetes"><i class="fa fa-check"></i><b>7.7</b> Kubernetes</a></li>
<li class="chapter" data-level="7.8" data-path="connections.html"><a href="connections.html#cloud-1"><i class="fa fa-check"></i><b>7.8</b> Cloud</a></li>
<li class="chapter" data-level="7.9" data-path="connections.html"><a href="connections.html#batches"><i class="fa fa-check"></i><b>7.9</b> Batches</a></li>
<li class="chapter" data-level="7.10" data-path="connections.html"><a href="connections.html#tools-1"><i class="fa fa-check"></i><b>7.10</b> Tools</a></li>
<li class="chapter" data-level="7.11" data-path="connections.html"><a href="connections.html#multiple"><i class="fa fa-check"></i><b>7.11</b> Multiple</a></li>
<li class="chapter" data-level="7.12" data-path="connections.html"><a href="connections.html#connections-troubleshooting"><i class="fa fa-check"></i><b>7.12</b> Troubleshooting</a><ul>
<li class="chapter" data-level="7.12.1" data-path="connections.html"><a href="connections.html#logging"><i class="fa fa-check"></i><b>7.12.1</b> Logging</a></li>
<li class="chapter" data-level="7.12.2" data-path="connections.html"><a href="connections.html#connections-troubleshoot-spark-submit"><i class="fa fa-check"></i><b>7.12.2</b> Spark Submit</a></li>
<li class="chapter" data-level="7.12.3" data-path="connections.html"><a href="connections.html#windows"><i class="fa fa-check"></i><b>7.12.3</b> Windows</a></li>
</ul></li>
<li class="chapter" data-level="7.13" data-path="connections.html"><a href="connections.html#recap-4"><i class="fa fa-check"></i><b>7.13</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="8" data-path="data.html"><a href="data.html"><i class="fa fa-check"></i><b>8</b> Data</a><ul>
<li class="chapter" data-level="8.1" data-path="data.html"><a href="data.html#overview-3"><i class="fa fa-check"></i><b>8.1</b> Overview</a></li>
<li class="chapter" data-level="8.2" data-path="data.html"><a href="data.html#reading-data"><i class="fa fa-check"></i><b>8.2</b> Reading Data</a><ul>
<li class="chapter" data-level="8.2.1" data-path="data.html"><a href="data.html#paths"><i class="fa fa-check"></i><b>8.2.1</b> Paths</a></li>
<li class="chapter" data-level="8.2.2" data-path="data.html"><a href="data.html#schema"><i class="fa fa-check"></i><b>8.2.2</b> Schema</a></li>
<li class="chapter" data-level="8.2.3" data-path="data.html"><a href="data.html#memory"><i class="fa fa-check"></i><b>8.2.3</b> Memory</a></li>
<li class="chapter" data-level="8.2.4" data-path="data.html"><a href="data.html#columns"><i class="fa fa-check"></i><b>8.2.4</b> Columns</a></li>
</ul></li>
<li class="chapter" data-level="8.3" data-path="data.html"><a href="data.html#writing-data"><i class="fa fa-check"></i><b>8.3</b> Writing Data</a></li>
<li class="chapter" data-level="8.4" data-path="data.html"><a href="data.html#copy"><i class="fa fa-check"></i><b>8.4</b> Copy</a></li>
<li class="chapter" data-level="8.5" data-path="data.html"><a href="data.html#data-file-formats"><i class="fa fa-check"></i><b>8.5</b> File Formats</a><ul>
<li class="chapter" data-level="8.5.1" data-path="data.html"><a href="data.html#csv"><i class="fa fa-check"></i><b>8.5.1</b> CSV</a></li>
<li class="chapter" data-level="8.5.2" data-path="data.html"><a href="data.html#json"><i class="fa fa-check"></i><b>8.5.2</b> JSON</a></li>
<li class="chapter" data-level="8.5.3" data-path="data.html"><a href="data.html#parquet"><i class="fa fa-check"></i><b>8.5.3</b> Parquet</a></li>
<li class="chapter" data-level="8.5.4" data-path="data.html"><a href="data.html#others"><i class="fa fa-check"></i><b>8.5.4</b> Others</a></li>
</ul></li>
<li class="chapter" data-level="8.6" data-path="data.html"><a href="data.html#data-file-systems"><i class="fa fa-check"></i><b>8.6</b> File Systems</a></li>
<li class="chapter" data-level="8.7" data-path="data.html"><a href="data.html#data-storage-systems"><i class="fa fa-check"></i><b>8.7</b> Storage Systems</a><ul>
<li class="chapter" data-level="8.7.1" data-path="data.html"><a href="data.html#hive"><i class="fa fa-check"></i><b>8.7.1</b> Hive</a></li>
<li class="chapter" data-level="8.7.2" data-path="data.html"><a href="data.html#cassandra"><i class="fa fa-check"></i><b>8.7.2</b> Cassandra</a></li>
<li class="chapter" data-level="8.7.3" data-path="data.html"><a href="data.html#jdbc"><i class="fa fa-check"></i><b>8.7.3</b> JDBC</a></li>
</ul></li>
<li class="chapter" data-level="8.8" data-path="data.html"><a href="data.html#recap-5"><i class="fa fa-check"></i><b>8.8</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="9" data-path="tuning.html"><a href="tuning.html"><i class="fa fa-check"></i><b>9</b> Tuning</a><ul>
<li class="chapter" data-level="9.1" data-path="tuning.html"><a href="tuning.html#overview-4"><i class="fa fa-check"></i><b>9.1</b> Overview</a><ul>
<li class="chapter" data-level="9.1.1" data-path="tuning.html"><a href="tuning.html#tuning-graph-visualization"><i class="fa fa-check"></i><b>9.1.1</b> Graph</a></li>
<li class="chapter" data-level="9.1.2" data-path="tuning.html"><a href="tuning.html#tuning-event-timeline"><i class="fa fa-check"></i><b>9.1.2</b> Timeline</a></li>
</ul></li>
<li class="chapter" data-level="9.2" data-path="tuning.html"><a href="tuning.html#tuning-configuring"><i class="fa fa-check"></i><b>9.2</b> Configuring</a><ul>
<li class="chapter" data-level="9.2.1" data-path="tuning.html"><a href="tuning.html#connect-settings"><i class="fa fa-check"></i><b>9.2.1</b> Connect Settings</a></li>
<li class="chapter" data-level="9.2.2" data-path="tuning.html"><a href="tuning.html#submit-settings"><i class="fa fa-check"></i><b>9.2.2</b> Submit Settings</a></li>
<li class="chapter" data-level="9.2.3" data-path="tuning.html"><a href="tuning.html#runtime-settings"><i class="fa fa-check"></i><b>9.2.3</b> Runtime Settings</a></li>
<li class="chapter" data-level="9.2.4" data-path="tuning.html"><a href="tuning.html#sparklyr-settings"><i class="fa fa-check"></i><b>9.2.4</b> sparklyr Settings</a></li>
</ul></li>
<li class="chapter" data-level="9.3" data-path="tuning.html"><a href="tuning.html#tuning-partitioning"><i class="fa fa-check"></i><b>9.3</b> Partitioning</a><ul>
<li class="chapter" data-level="9.3.1" data-path="tuning.html"><a href="tuning.html#implicit-partitions"><i class="fa fa-check"></i><b>9.3.1</b> Implicit Partitions</a></li>
<li class="chapter" data-level="9.3.2" data-path="tuning.html"><a href="tuning.html#explicit-partitions"><i class="fa fa-check"></i><b>9.3.2</b> Explicit Partitions</a></li>
</ul></li>
<li class="chapter" data-level="9.4" data-path="tuning.html"><a href="tuning.html#tuning-caching"><i class="fa fa-check"></i><b>9.4</b> Caching</a><ul>
<li class="chapter" data-level="9.4.1" data-path="tuning.html"><a href="tuning.html#checkpointing"><i class="fa fa-check"></i><b>9.4.1</b> Checkpointing</a></li>
<li class="chapter" data-level="9.4.2" data-path="tuning.html"><a href="tuning.html#tuning-memory"><i class="fa fa-check"></i><b>9.4.2</b> Memory</a></li>
</ul></li>
<li class="chapter" data-level="9.5" data-path="tuning.html"><a href="tuning.html#tuning-shuffling"><i class="fa fa-check"></i><b>9.5</b> Shuffling</a></li>
<li class="chapter" data-level="9.6" data-path="tuning.html"><a href="tuning.html#tuning-serialization"><i class="fa fa-check"></i><b>9.6</b> Serialization</a></li>
<li class="chapter" data-level="9.7" data-path="tuning.html"><a href="tuning.html#configuration-files"><i class="fa fa-check"></i><b>9.7</b> Configuration Files</a></li>
<li class="chapter" data-level="9.8" data-path="tuning.html"><a href="tuning.html#recap-6"><i class="fa fa-check"></i><b>9.8</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="10" data-path="extensions.html"><a href="extensions.html"><i class="fa fa-check"></i><b>10</b> Extensions</a><ul>
<li class="chapter" data-level="10.1" data-path="extensions.html"><a href="extensions.html#overview-5"><i class="fa fa-check"></i><b>10.1</b> Overview</a></li>
<li class="chapter" data-level="10.2" data-path="extensions.html"><a href="extensions.html#h2o"><i class="fa fa-check"></i><b>10.2</b> H2O</a></li>
<li class="chapter" data-level="10.3" data-path="extensions.html"><a href="extensions.html#graphs"><i class="fa fa-check"></i><b>10.3</b> Graphs</a></li>
<li class="chapter" data-level="10.4" data-path="extensions.html"><a href="extensions.html#xgboost"><i class="fa fa-check"></i><b>10.4</b> XGBoost</a></li>
<li class="chapter" data-level="10.5" data-path="extensions.html"><a href="extensions.html#deep-learning"><i class="fa fa-check"></i><b>10.5</b> Deep Learning</a></li>
<li class="chapter" data-level="10.6" data-path="extensions.html"><a href="extensions.html#genomics"><i class="fa fa-check"></i><b>10.6</b> Genomics</a></li>
<li class="chapter" data-level="10.7" data-path="extensions.html"><a href="extensions.html#spatial"><i class="fa fa-check"></i><b>10.7</b> Spatial</a></li>
<li class="chapter" data-level="10.8" data-path="extensions.html"><a href="extensions.html#troubleshooting"><i class="fa fa-check"></i><b>10.8</b> Troubleshooting</a></li>
<li class="chapter" data-level="10.9" data-path="extensions.html"><a href="extensions.html#recap-7"><i class="fa fa-check"></i><b>10.9</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="11" data-path="distributed.html"><a href="distributed.html"><i class="fa fa-check"></i><b>11</b> Distributed R</a><ul>
<li class="chapter" data-level="11.1" data-path="distributed.html"><a href="distributed.html#overview-6"><i class="fa fa-check"></i><b>11.1</b> Overview</a></li>
<li class="chapter" data-level="11.2" data-path="distributed.html"><a href="distributed.html#use-cases-1"><i class="fa fa-check"></i><b>11.2</b> Use Cases</a><ul>
<li class="chapter" data-level="11.2.1" data-path="distributed.html"><a href="distributed.html#custom-parsers"><i class="fa fa-check"></i><b>11.2.1</b> Custom Parsers</a></li>
<li class="chapter" data-level="11.2.2" data-path="distributed.html"><a href="distributed.html#partitioned-modeling"><i class="fa fa-check"></i><b>11.2.2</b> Partitioned Modeling</a></li>
<li class="chapter" data-level="11.2.3" data-path="distributed.html"><a href="distributed.html#distributed-grid-search"><i class="fa fa-check"></i><b>11.2.3</b> Grid Search</a></li>
<li class="chapter" data-level="11.2.4" data-path="distributed.html"><a href="distributed.html#web-apis"><i class="fa fa-check"></i><b>11.2.4</b> Web APIs</a></li>
<li class="chapter" data-level="11.2.5" data-path="distributed.html"><a href="distributed.html#simulations"><i class="fa fa-check"></i><b>11.2.5</b> Simulations</a></li>
</ul></li>
<li class="chapter" data-level="11.3" data-path="distributed.html"><a href="distributed.html#partitions"><i class="fa fa-check"></i><b>11.3</b> Partitions</a></li>
<li class="chapter" data-level="11.4" data-path="distributed.html"><a href="distributed.html#grouping"><i class="fa fa-check"></i><b>11.4</b> Grouping</a></li>
<li class="chapter" data-level="11.5" data-path="distributed.html"><a href="distributed.html#columns-1"><i class="fa fa-check"></i><b>11.5</b> Columns</a></li>
<li class="chapter" data-level="11.6" data-path="distributed.html"><a href="distributed.html#context"><i class="fa fa-check"></i><b>11.6</b> Context</a></li>
<li class="chapter" data-level="11.7" data-path="distributed.html"><a href="distributed.html#functions"><i class="fa fa-check"></i><b>11.7</b> Functions</a></li>
<li class="chapter" data-level="11.8" data-path="distributed.html"><a href="distributed.html#packages"><i class="fa fa-check"></i><b>11.8</b> Packages</a></li>
<li class="chapter" data-level="11.9" data-path="distributed.html"><a href="distributed.html#cluster-requirements"><i class="fa fa-check"></i><b>11.9</b> Cluster Requirements</a><ul>
<li class="chapter" data-level="11.9.1" data-path="distributed.html"><a href="distributed.html#installing-r"><i class="fa fa-check"></i><b>11.9.1</b> Installing R</a></li>
<li class="chapter" data-level="11.9.2" data-path="distributed.html"><a href="distributed.html#apache-arrow"><i class="fa fa-check"></i><b>11.9.2</b> Apache Arrow</a></li>
</ul></li>
<li class="chapter" data-level="11.10" data-path="distributed.html"><a href="distributed.html#troubleshooting-1"><i class="fa fa-check"></i><b>11.10</b> Troubleshooting</a><ul>
<li class="chapter" data-level="11.10.1" data-path="distributed.html"><a href="distributed.html#worker-logs"><i class="fa fa-check"></i><b>11.10.1</b> Worker Logs</a></li>
<li class="chapter" data-level="11.10.2" data-path="distributed.html"><a href="distributed.html#resolving-timeouts"><i class="fa fa-check"></i><b>11.10.2</b> Resolving Timeouts</a></li>
<li class="chapter" data-level="11.10.3" data-path="distributed.html"><a href="distributed.html#inspecting-partitions"><i class="fa fa-check"></i><b>11.10.3</b> Inspecting Partitions</a></li>
<li class="chapter" data-level="11.10.4" data-path="distributed.html"><a href="distributed.html#debugging-workers"><i class="fa fa-check"></i><b>11.10.4</b> Debugging Workers</a></li>
</ul></li>
<li class="chapter" data-level="11.11" data-path="distributed.html"><a href="distributed.html#recap-8"><i class="fa fa-check"></i><b>11.11</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="12" data-path="streaming.html"><a href="streaming.html"><i class="fa fa-check"></i><b>12</b> Streaming</a><ul>
<li class="chapter" data-level="12.1" data-path="streaming.html"><a href="streaming.html#overview-7"><i class="fa fa-check"></i><b>12.1</b> Overview</a></li>
<li class="chapter" data-level="12.2" data-path="streaming.html"><a href="streaming.html#transformations"><i class="fa fa-check"></i><b>12.2</b> Transformations</a><ul>
<li class="chapter" data-level="12.2.1" data-path="streaming.html"><a href="streaming.html#analysis-1"><i class="fa fa-check"></i><b>12.2.1</b> Analysis</a></li>
<li class="chapter" data-level="12.2.2" data-path="streaming.html"><a href="streaming.html#modeling-1"><i class="fa fa-check"></i><b>12.2.2</b> Modeling</a></li>
<li class="chapter" data-level="12.2.3" data-path="streaming.html"><a href="streaming.html#pipelines-1"><i class="fa fa-check"></i><b>12.2.3</b> Pipelines</a></li>
<li class="chapter" data-level="12.2.4" data-path="streaming.html"><a href="streaming.html#distributed-r-streaming-r-code"><i class="fa fa-check"></i><b>12.2.4</b> Distributed R {streaming-r-code}</a></li>
</ul></li>
<li class="chapter" data-level="12.3" data-path="streaming.html"><a href="streaming.html#kafka"><i class="fa fa-check"></i><b>12.3</b> Kafka</a></li>
<li class="chapter" data-level="12.4" data-path="streaming.html"><a href="streaming.html#shiny"><i class="fa fa-check"></i><b>12.4</b> Shiny</a></li>
<li class="chapter" data-level="12.5" data-path="streaming.html"><a href="streaming.html#recap-9"><i class="fa fa-check"></i><b>12.5</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="13" data-path="contributing.html"><a href="contributing.html"><i class="fa fa-check"></i><b>13</b> Contributing</a><ul>
<li class="chapter" data-level="13.1" data-path="contributing.html"><a href="contributing.html#contributing-overview"><i class="fa fa-check"></i><b>13.1</b> Overview</a></li>
<li class="chapter" data-level="13.2" data-path="contributing.html"><a href="contributing.html#contributing-spark-api"><i class="fa fa-check"></i><b>13.2</b> Spark API</a></li>
<li class="chapter" data-level="13.3" data-path="contributing.html"><a href="contributing.html#spark-extensions"><i class="fa fa-check"></i><b>13.3</b> Spark Extensions</a></li>
<li class="chapter" data-level="13.4" data-path="contributing.html"><a href="contributing.html#scala-code"><i class="fa fa-check"></i><b>13.4</b> Scala Code</a></li>
<li class="chapter" data-level="13.5" data-path="contributing.html"><a href="contributing.html#recap-10"><i class="fa fa-check"></i><b>13.5</b> Recap</a></li>
</ul></li>
<li class="chapter" data-level="14" data-path="appendix.html"><a href="appendix.html"><i class="fa fa-check"></i><b>14</b> Appendix</a><ul>
<li class="chapter" data-level="14.1" data-path="appendix.html"><a href="appendix.html#appendix-preface"><i class="fa fa-check"></i><b>14.1</b> Preface</a><ul>
<li class="chapter" data-level="14.1.1" data-path="appendix.html"><a href="appendix.html#appendix-ggplot2-theme"><i class="fa fa-check"></i><b>14.1.1</b> Formatting</a></li>
</ul></li>
<li class="chapter" data-level="14.2" data-path="appendix.html"><a href="appendix.html#appendix-intro"><i class="fa fa-check"></i><b>14.2</b> Introduction</a><ul>
<li class="chapter" data-level="14.2.1" data-path="appendix.html"><a href="appendix.html#appendix-storage-capacity"><i class="fa fa-check"></i><b>14.2.1</b> Worlds Store Capacity</a></li>
<li class="chapter" data-level="14.2.2" data-path="appendix.html"><a href="appendix.html#appendix-cran-downloads"><i class="fa fa-check"></i><b>14.2.2</b> Daily downloads of CRAN packages</a></li>
</ul></li>
<li class="chapter" data-level="14.3" data-path="appendix.html"><a href="appendix.html#appendix-starting"><i class="fa fa-check"></i><b>14.3</b> Getting Started</a><ul>
<li class="chapter" data-level="14.3.1" data-path="appendix.html"><a href="appendix.html#appendix-prerequisites"><i class="fa fa-check"></i><b>14.3.1</b> Prerequisites</a></li>
</ul></li>
<li class="chapter" data-level="14.4" data-path="appendix.html"><a href="appendix.html#appendix-analysis"><i class="fa fa-check"></i><b>14.4</b> Analysis</a><ul>
<li class="chapter" data-level="14.4.1" data-path="appendix.html"><a href="appendix.html#hive-functions"><i class="fa fa-check"></i><b>14.4.1</b> Hive Functions</a></li>
</ul></li>
<li class="chapter" data-level="14.5" data-path="appendix.html"><a href="appendix.html#appendix-modeling"><i class="fa fa-check"></i><b>14.5</b> Modeling</a><ul>
<li class="chapter" data-level="14.5.1" data-path="appendix.html"><a href="appendix.html#appendix-ml-functionlist"><i class="fa fa-check"></i><b>14.5.1</b> MLlib Functions</a></li>
</ul></li>
<li class="chapter" data-level="14.6" data-path="appendix.html"><a href="appendix.html#appendix-clusters"><i class="fa fa-check"></i><b>14.6</b> Clusters</a><ul>
<li class="chapter" data-level="14.6.1" data-path="appendix.html"><a href="appendix.html#appendix-cluster-trends"><i class="fa fa-check"></i><b>14.6.1</b> Google trends for mainframes, cloud computing and kubernetes</a></li>
</ul></li>
<li class="chapter" data-level="14.7" data-path="appendix.html"><a href="appendix.html#appendix-streaming"><i class="fa fa-check"></i><b>14.7</b> Streaming</a><ul>
<li class="chapter" data-level="14.7.1" data-path="appendix.html"><a href="appendix.html#appendix-streaming-generator"><i class="fa fa-check"></i><b>14.7.1</b> Stream Generator</a></li>
<li class="chapter" data-level="14.7.2" data-path="appendix.html"><a href="appendix.html#appendix-streaming-kafka"><i class="fa fa-check"></i><b>14.7.2</b> Installing Kafka</a></li>
</ul></li>
</ul></li>
<li class="chapter" data-level="15" data-path="references.html"><a href="references.html"><i class="fa fa-check"></i><b>15</b> References</a></li>
</ul>

      </nav>
    </div>

    <div class="book-body">
      <div class="body-inner">
        <div class="book-header" role="navigation">
          <h1>
            <i class="fa fa-circle-o-notch fa-spin"></i><a href="./">Mastering Spark with R</a>
          </h1>
        </div>

        <div class="page-wrapper" tabindex="-1" role="main">
          <div class="page-inner">

            <section class="normal" id="section-">
<div id="streaming" class="section level1">
<h1><span class="header-section-number">Chapter 12</span> Streaming</h1>
<blockquote>
<p>Our stories aren’t over yet.</p>
<p>— Arya Stark</p>
</blockquote>
<p>Looking back at the previous chapters, we’ve covered a good deal, but not everything. We’ve analyzed tabular datasets, performed unsupervised learning over raw text, analyzed graphs and geographic datasets, and even transformed data with custom R code! So now what?</p>
<p>Though we weren’t explicit about this, we’ve assumed until this point that your data is static, and didn’t change over time. But suppose for a moment your job is to analyze traffic patterns to give recommendations to the department of transportation. A reasonable approach would be to analyze historical data and then design predictive models that compute forecasts overnight. Overnight? That’s very useful, but traffic patterns change by the hour and even by the minute. You could try to preprocess and predict faster and faster, but eventually this model breaks—you can’t load large-scale datasets, transform them, score them, unload them, and repeat this process by the second.</p>
<p>Instead, we need to introduce a different kind of dataset—one that is not static but rather dynamic, one that is like a table but is growing constantly. We will refer to such datasets as <em>streams</em>.</p>
<div id="overview-7" class="section level2">
<h2><span class="header-section-number">12.1</span> Overview</h2>
<p>We<!--((("streaming", "overview of")))--> know how to work with large-scale static datasets, but how can we reason about large-scale real-time datasets? Datasets with an infinite amount of entries are known as <strong>streams</strong>.</p>
<p>For static datasets, if we were to do real-time scoring using a pretrained topic model, the entries would be lines of text; for real-time datasets, we would perform the same scoring over an infinite number of lines of text. Now, in practice, you will never process an infinite number of records. You will eventually stop the stream—or this universe might end, whichever comes first. Regardless, thinking of the datasets as infinite makes it much easier to reason about them.</p>
<p>Streams are most relevant when processing real-time data—for example, when analyzing a Twitter feed or stock prices. Both examples have well-defined columns, like “tweet” or “price,” but there are always new rows of data to be analyzed.</p>
<p><em>Spark Streaming</em> provides<!--((("Spark Streaming")))--> scalable and fault-tolerant data processing over streams of data. That means you can use many machines to process multiple streaming sources, perform joins with other streams or static sources, and recover from failures with at-least-once guarantees (each message is certain to be delivered, but may do so multiple times).</p>
<p>In Spark, you<!--((("source")))((("sink")))--> create streams by defining a <em>source</em>, a <em>transformation</em>, and a <em>sink</em>; you can think of these steps as reading, transforming, and writing a stream, as shown in Figure <a href="streaming.html#fig:streaming-working">12.1</a> describes.</p>
<div class="figure" style="text-align: center"><span id="fig:streaming-working"></span>
<img src="the-r-in-spark_files/figure-html/streaming-working-1.png" alt="Working with Spark Streaming" width="auto" height="280pt" />
<p class="caption">
FIGURE 12.1: Working with Spark Streaming
</p>
</div>
<p>Let’s take a look at each of these a little more closely:</p>
<dl>
<dt>Reading</dt>
<dd>Streams read data using any of the <code>stream_read_*()</code> functions; the read operation defines the <em>source</em> of the stream. You can define one or multiple sources from which to read.
</dd>
<dt>Transforming</dt>
<dd>A stream can perform one or multiple transformations using <code>dplyr</code>, <code>SQL</code>, feature transformers, scoring pipelines, or distributed R code. Transformations can not only be applied to one or more streams, but can also use a combination of streams and static data sources; for instance, those loaded into Spark with <code>spark_read_()</code> functions—this means that you can combine static data and real-time data sources with ease.
</dd>
<dt>Writing</dt>
<dd>The write operations are performed with the family of <code>stream_write_*()</code> functions, while the read operation defined the sink of the stream. You can specify a single sink or multiple ones to write data to.
</dd>
</dl>
<p>You can read and write to streams in several different file formats: CSV, JSON, Parquet, Optimized Row Columnar (ORC), and text (see Table <a href="streaming.html#tab:streaming-functions-table">12.1</a>). You<!--((("Kafka", "reading and writing from")))--> also can read and write from and to Kafka, which we will introduce later on.</p>
<table>
<caption><span id="tab:streaming-functions-table">TABLE 12.1: </span>Spark functions to read and write streams</caption>
<thead>
<tr class="header">
<th align="left">Format</th>
<th align="left">Read</th>
<th align="left">Write</th>
</tr>
</thead>
<tbody>
<tr class="odd">
<td align="left">CSV</td>
<td align="left">stream_read_csv</td>
<td align="left">stream_write_csv</td>
</tr>
<tr class="even">
<td align="left">JSON</td>
<td align="left">stream_read_json</td>
<td align="left">stream_write_json</td>
</tr>
<tr class="odd">
<td align="left">Kafka</td>
<td align="left">stream_read_kafka</td>
<td align="left">stream_write_kafka</td>
</tr>
<tr class="even">
<td align="left">ORC</td>
<td align="left">stream_read_orc</td>
<td align="left">stream_write_orc</td>
</tr>
<tr class="odd">
<td align="left">Parquet</td>
<td align="left">stream_read_parquet</td>
<td align="left">stream_write_parquet</td>
</tr>
<tr class="even">
<td align="left">Text</td>
<td align="left">stream_read_text</td>
<td align="left">stream_write_text</td>
</tr>
<tr class="odd">
<td align="left">Memory</td>
<td align="left"></td>
<td align="left">stream_write_memory</td>
</tr>
</tbody>
</table>
<p>Since the transformation step is optional, the simplest stream we can define is one that continuously copies text files between source and destination.</p>
<p>First, install the <code>future</code> package using <code>install.packages("future")</code> and connect to Spark.</p>
<p>Since a stream requires the source to exist, create a <code>source</code> folder:</p>
<div class="sourceCode" id="cb463"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb463-1" title="1"><span class="kw">dir.create</span>(<span class="st">&quot;source&quot;</span>)</a></code></pre></div>
<p>We are now ready to define our first stream!</p>
<div class="sourceCode" id="cb464"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb464-1" title="1">stream &lt;-<span class="st"> </span><span class="kw">stream_read_text</span>(sc, <span class="st">&quot;source/&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb464-2" title="2"><span class="st">  </span><span class="kw">stream_write_text</span>(<span class="st">&quot;destination/&quot;</span>)</a></code></pre></div>
<p>The<!--((("commands", "stream_write_*()")))--> streams starts running with <code>stream_write_*()</code>; once executed, the stream will monitor the <em><code>source</code></em> path and process data into the <em>++destination /++</em> path as it arrives.</p>
<p>We can use <code>stream_generate_test()</code> to produce a file every second containing lines of text that follow a given distribution; you can read more about this in <a href="appendix.html#appendix">Appendix</a>. In practice, you would connect to existing sources without having to generate data artificially. We can then use <code>view_stream()</code> to track the rows per second (rps) being processed in the source, and in the destination, and their latest values over time:</p>
<div class="sourceCode" id="cb465"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb465-1" title="1">future<span class="op">::</span><span class="kw">future</span>(<span class="kw">stream_generate_test</span>(<span class="dt">interval =</span> <span class="fl">0.5</span>))</a></code></pre></div>
<div class="sourceCode" id="cb466"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb466-1" title="1"><span class="kw">stream_view</span>(stream)</a></code></pre></div>
<p>The result is shown in Figure <a href="streaming.html#fig:streaming-view-stream">12.2</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:streaming-view-stream"></span>
<img src="images/streaming-stream-view-resized.png" alt="Monitoring a stream generating rows following a binomial distribution" width="1500" />
<p class="caption">
FIGURE 12.2: Monitoring a stream generating rows following a binomial distribution
</p>
</div>
<p>Notice that the rps rate in the destination stream is higher than that in the source stream. This is expected and desirable since Spark measures incoming rates from the source stream, but also actual row-processing times in the destination stream. For example, if 10 rows per second are written to the <em>source/</em> path, the incoming rate is 10 rps. However, if it takes Spark only 0.01 seconds to write all those 10 rows, the output rate is 100 rps.</p>
<p>Use <code>stream_stop()</code> to properly stop processing data from this stream:</p>
<div class="sourceCode" id="cb467"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb467-1" title="1"><span class="kw">stream_stop</span>(stream)</a></code></pre></div>
<p>This exercise introduced how we can easily start a Spark stream that reads and writes data based on a simulated stream. Let’s do something more interesting than just copying data with proper transformations.</p>
</div>
<div id="transformations" class="section level2">
<h2><span class="header-section-number">12.2</span> Transformations</h2>
<p>In<!--((("streaming", "transformations", id="Strans12")))((("transformations", "overview of")))--> a real-life scenario, the incoming data from a stream would not be written as is to the output. The Spark Streaming job would make transformations to the data, and then write the transformed data.</p>
<p>Streams<!--((("DataFrames", "transforming")))--> can be transformed using <code>dplyr</code>, SQL queries, ML pipelines, or R code. We can use as many transformations as needed in the same way that Spark DataFrames can be transformed with <code>sparklyr</code>.</p>
<p>The source of the transformation can be a stream or DataFrame, but the output is always a stream. If needed, you can always take a snapshot from the destination stream and then save the output as a DataFrame. That is what <code>sparklyr</code> will do for you if a destination stream is not specified.</p>
<p>Each of the following subsections covers an option provided by <code>sparklyr</code> to perform transformations on a stream.</p>
<div id="analysis-1" class="section level3">
<h3><span class="header-section-number">12.2.1</span> Analysis</h3>
<p>You<!--((("transformations", "analysis")))--> can analyze streams with <code>dplyr</code> verbs and SQL using <code>DBI</code>. As a quick example, we will filter rows and add columns over a stream. We won’t explicitly call <code>stream_generate_test()</code>, but you can call it on your own through the <code>later</code> package if you feel the urge to verify that data is being processed continuously:</p>
<div class="sourceCode" id="cb468"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb468-1" title="1"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb468-2" title="2"></a>
<a class="sourceLine" id="cb468-3" title="3"><span class="kw">stream_read_csv</span>(sc, <span class="st">&quot;source&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb468-4" title="4"><span class="st">  </span><span class="kw">filter</span>(x <span class="op">&gt;</span><span class="st"> </span><span class="dv">700</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb468-5" title="5"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y =</span> <span class="kw">round</span>(x <span class="op">/</span><span class="st"> </span><span class="dv">100</span>))</a></code></pre></div>
<pre><code># Source: spark&lt;?&gt; [inf x 2]
       x     y
   &lt;int&gt; &lt;dbl&gt;
 1   701     7
 2   702     7
 3   703     7
 4   704     7
 5   705     7
 6   706     7
 7   707     7
 8   708     7
 9   709     7
10   710     7
# … with more rows</code></pre>
<p>It’s also possible to perform aggregations over the entire history of the stream. The history could be filtered or not:</p>
<div class="sourceCode" id="cb470"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb470-1" title="1"><span class="kw">stream_read_csv</span>(sc, <span class="st">&quot;source&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb470-2" title="2"><span class="st">  </span><span class="kw">filter</span>(x <span class="op">&gt;</span><span class="st"> </span><span class="dv">700</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb470-3" title="3"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">y =</span> <span class="kw">round</span>(x <span class="op">/</span><span class="st"> </span><span class="dv">100</span>)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb470-4" title="4"><span class="st">  </span><span class="kw">count</span>(y) </a></code></pre></div>
<pre><code># Source: spark&lt;?&gt; [inf x 2]
      y     n
  &lt;dbl&gt; &lt;dbl&gt;
1     8 25902
2     9 25902
3    10 13210
4     7 12692</code></pre>
<p>Grouped aggregations of the latest data in the stream require a timestamp. The timestamp will note when the reading function (in this case <code>stream_read_csv()</code>) first “saw” that specific record. In Spark Streaming terminology, the timestamp<!--((("watermarks")))--> is called a <em>watermark</em>. The <code>spark_watermark()</code> function adds the timestamp. In this example, the watermark will be the same for all records, since the five files were read by the stream after they were created. Note that only<!--((("Kafka", "reading and writing from")))--> Kafka and memory <em>outputs</em> support watermarks:</p>
<div class="sourceCode" id="cb472"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb472-1" title="1"><span class="kw">stream_read_csv</span>(sc, <span class="st">&quot;source&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb472-2" title="2"><span class="st">  </span><span class="kw">stream_watermark</span>()</a></code></pre></div>
<pre><code># Source: spark&lt;?&gt; [inf x 2]
       x timestamp          
   &lt;int&gt; &lt;dttm&gt;             
 1   276 2019-06-30 07:14:21
 2   277 2019-06-30 07:14:21
 3   278 2019-06-30 07:14:21
 4   279 2019-06-30 07:14:21
 5   280 2019-06-30 07:14:21
 6   281 2019-06-30 07:14:21
 7   282 2019-06-30 07:14:21
 8   283 2019-06-30 07:14:21
 9   284 2019-06-30 07:14:21
10   285 2019-06-30 07:14:21
# … with more rows</code></pre>
<p>After the watermark is created, you can use it in the <code>group_by()</code> verb. You can then pipe it into a <code>summarise()</code> function to get some stats of the stream:</p>
<div class="sourceCode" id="cb474"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb474-1" title="1"><span class="kw">stream_read_csv</span>(sc, <span class="st">&quot;source&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb474-2" title="2"><span class="st">  </span><span class="kw">stream_watermark</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb474-3" title="3"><span class="st">  </span><span class="kw">group_by</span>(timestamp) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb474-4" title="4"><span class="st">  </span><span class="kw">summarise</span>(</a>
<a class="sourceLine" id="cb474-5" title="5">    <span class="dt">max_x =</span> <span class="kw">max</span>(x, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>),</a>
<a class="sourceLine" id="cb474-6" title="6">    <span class="dt">min_x =</span> <span class="kw">min</span>(x, <span class="dt">na.rm =</span> <span class="ot">TRUE</span>),</a>
<a class="sourceLine" id="cb474-7" title="7">    <span class="dt">count =</span> <span class="kw">n</span>()</a>
<a class="sourceLine" id="cb474-8" title="8">  ) </a></code></pre></div>
<pre><code># Source: spark&lt;?&gt; [inf x 4]
  timestamp           max_x min_x  count
  &lt;dttm&gt;              &lt;int&gt; &lt;int&gt;  &lt;dbl&gt;
1 2019-06-30 07:14:55  1000     1 259332</code></pre>
</div>
<div id="modeling-1" class="section level3">
<h3><span class="header-section-number">12.2.2</span> Modeling</h3>
<p>Spark<!--((("transformations", "modeling")))((("online learning")))((("modeling", "Spark streams")))--> streams currently don’t support training on real-time datasets. Aside from the technical challenges, even if it were possible, it would be quite difficult to train models since the model itself would need to adapt over time. Known as <em>online learning</em>, this is perhaps something that Spark will support in the future.</p>
<p>That said, there are other modeling concepts we can use with streams, like feature transformers and scoring. Let’s try out a feature transformer with streams, and leave scoring for the next section, since we will need to train a model.</p>
<p>The<!--((("commands", "ft_bucketizer()")))--> next example uses the <code>ft_bucketizer()</code> feature transformer to modify the stream followed by regular <code>dplyr</code> functions, which you can use just as you would with static datasets:</p>
<div class="sourceCode" id="cb476"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb476-1" title="1"><span class="kw">stream_read_csv</span>(sc, <span class="st">&quot;source&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb476-2" title="2"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">x =</span> <span class="kw">as.numeric</span>(x)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb476-3" title="3"><span class="st">  </span><span class="kw">ft_bucketizer</span>(<span class="st">&quot;x&quot;</span>, <span class="st">&quot;buckets&quot;</span>, <span class="dt">splits =</span> <span class="dv">0</span><span class="op">:</span><span class="dv">10</span> <span class="op">*</span><span class="st"> </span><span class="dv">100</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb476-4" title="4"><span class="st">  </span><span class="kw">count</span>(buckets)  <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb476-5" title="5"><span class="st">  </span><span class="kw">arrange</span>(buckets)</a></code></pre></div>
<pre><code># Source:     spark&lt;?&gt; [inf x 2]
# Ordered by: buckets
   buckets     n
     &lt;dbl&gt; &lt;dbl&gt;
 1       0 25747
 2       1 26008
 3       2 25992
 4       3 25908
 5       4 25905
 6       5 25903
 7       6 25904
 8       7 25901
 9       8 25902
10       9 26162</code></pre>
</div>
<div id="pipelines-1" class="section level3">
<h3><span class="header-section-number">12.2.3</span> Pipelines</h3>
<p>Spark<!--((("transformations", "pipelines")))((("pipelines", "Spark pipelines")))--> pipelines can be used for scoring streams, but not to train over streaming data. The former is fully supported, while the latter is a feature under active development by the Spark community.</p>
<p>To score a stream, it’s necessary to first create our model. So let’s build, fit, and save a simple pipeline:</p>
<div class="sourceCode" id="cb478"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb478-1" title="1">cars &lt;-<span class="st"> </span><span class="kw">copy_to</span>(sc, mtcars)</a>
<a class="sourceLine" id="cb478-2" title="2"></a>
<a class="sourceLine" id="cb478-3" title="3">model &lt;-<span class="st"> </span><span class="kw">ml_pipeline</span>(sc) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb478-4" title="4"><span class="st">  </span><span class="kw">ft_binarizer</span>(<span class="st">&quot;mpg&quot;</span>, <span class="st">&quot;over_30&quot;</span>, <span class="dv">30</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb478-5" title="5"><span class="st">  </span><span class="kw">ft_r_formula</span>(over_<span class="dv">30</span> <span class="op">~</span><span class="st"> </span>wt) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb478-6" title="6"><span class="st">  </span><span class="kw">ml_logistic_regression</span>() <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb478-7" title="7"><span class="st">  </span><span class="kw">ml_fit</span>(cars)</a></code></pre></div>
<p><strong>Tip:</strong> If you choose to, you can make use of other concepts presented in <a href="pipelines.html#pipelines">Chapter 5</a>, like saving and reloading pipelines through <code>ml_save()</code> and <code>ml_load()</code> before scoring streams.</p>
<p>We can then generate a stream based on <code>mtcars</code> using <code>stream_generate_test()</code>, and score the model using <code>ml_transform()</code>:</p>
<div class="sourceCode" id="cb479"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb479-1" title="1">future<span class="op">::</span><span class="kw">future</span>(<span class="kw">stream_generate_test</span>(mtcars, <span class="st">&quot;cars-stream&quot;</span>, <span class="dt">iterations =</span> <span class="dv">5</span>))</a>
<a class="sourceLine" id="cb479-2" title="2"></a>
<a class="sourceLine" id="cb479-3" title="3"><span class="kw">ml_transform</span>(model, <span class="kw">stream_read_csv</span>(sc, <span class="st">&quot;cars-stream&quot;</span>))</a></code></pre></div>
<pre><code># Source: spark&lt;?&gt; [inf x 17]
     mpg   cyl  disp    hp  drat    wt  qsec    vs    am  gear  carb over_30
   &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;int&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;dbl&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt; &lt;int&gt;   &lt;dbl&gt;
 1  15.5     8 318     150  2.76  3.52  16.9     0     0     3     2       0
 2  15.2     8 304     150  3.15  3.44  17.3     0     0     3     2       0
 3  13.3     8 350     245  3.73  3.84  15.4     0     0     3     4       0
 4  19.2     8 400     175  3.08  3.84  17.0     0     0     3     2       0
 5  27.3     4  79      66  4.08  1.94  18.9     1     1     4     1       0
 6  26       4 120.     91  4.43  2.14  16.7     0     1     5     2       0
 7  30.4     4  95.1   113  3.77  1.51  16.9     1     1     5     2       1
 8  15.8     8 351     264  4.22  3.17  14.5     0     1     5     4       0
 9  19.7     6 145     175  3.62  2.77  15.5     0     1     5     6       0
10  15       8 301     335  3.54  3.57  14.6     0     1     5     8       0
# … with more rows, and 5 more variables: features &lt;list&gt;, label &lt;dbl&gt;,
#   rawPrediction &lt;list&gt;, probability &lt;list&gt;, prediction &lt;dbl&gt;</code></pre>
<p>Though this example was put together with a few lines of code, what we just accomplished is actually quite impressive. You copied data into Spark, performed feature engineering, trained a model, and scored the model over a real-time dataset, with just seven lines of code! Let’s try now to use custom transformations, in real time.</p>
</div>
<div id="distributed-r-streaming-r-code" class="section level3">
<h3><span class="header-section-number">12.2.4</span> Distributed R {streaming-r-code}</h3>
<p>Arbitrary<!--((("transformations", "distributed R")))((("distributed R", "streaming")))--> R code can also be used to transform a stream with the use of <code>spark_apply()</code>. This approach follows the same principles discussed in <a href="distributed.html#distributed">Chapter 11</a>, where <code>spark_apply()</code> runs R code over each executor in the cluster where data is available. This enables processing high-throughput streams and fulfills low-latency requirements:</p>
<div class="sourceCode" id="cb481"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb481-1" title="1"><span class="kw">stream_read_csv</span>(sc, <span class="st">&quot;cars-stream&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb481-2" title="2"><span class="st">  </span><span class="kw">select</span>(mpg) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb481-3" title="3"><span class="st">  </span><span class="kw">spark_apply</span>(<span class="op">~</span><span class="st"> </span><span class="kw">round</span>(.x), <span class="dt">mpg =</span> <span class="st">&quot;integer&quot;</span>) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb481-4" title="4"><span class="st">  </span><span class="kw">stream_write_csv</span>(<span class="st">&quot;cars-round&quot;</span>)</a></code></pre></div>
<p>which, as you would expect, processes data from <code>cars-stream</code> into <code>cars-round</code> by running the custom <code>round()</code> R function. Let’s peek into the output sink:</p>
<div class="sourceCode" id="cb482"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb482-1" title="1"><span class="kw">spark_read_csv</span>(sc, <span class="st">&quot;cars-round&quot;</span>)</a></code></pre></div>
<pre><code># Source: spark&lt;carsround&gt; [?? x 1]
     mpg
   &lt;dbl&gt;
 1    16
 2    15
 3    13
 4    19
 5    27
 6    26
 7    30
 8    16
 9    20
10    15
# … with more rows</code></pre>
<p>Again, make sure you apply the concepts you already know about <code>spark_apply()</code> when using streams; for instance, you should consider using <code>arrow</code> to significantly improve performance. Before we move on, disconnect from Spark:</p>
<p>This was our last transformation for streams. We’ll now learn how to use Spark Streaming with Kafka.<!--((("", startref="Strans12")))--></p>
<div class="sourceCode" id="cb484"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb484-1" title="1"><span class="kw">spark_disconnect</span>(sc)</a></code></pre></div>
</div>
</div>
<div id="kafka" class="section level2">
<h2><span class="header-section-number">12.3</span> Kafka</h2>
<p>Apache Kafka<!--((("Kafka", "streaming")))((("Apache Kafka", "streaming")))((("streaming", "Kafka")))--> is an open source stream-processing software platform developed by LinkedIn and donated to the Apache Software Foundation. It is written in Scala and Java. To describe it using an analogy, Kafka is to real-time storage what Hadoop is to static storage.</p>
<p>Kafka stores the stream as records, which consist of a key, a value, and a timestamp. It can handle multiple streams that contain different information, by categorizing them by topic. Kafka is commonly used to connect multiple real-time applications. A <em>producer</em> is<!--((("producers")))((("consumers")))((("subscribers")))--> an application that streams data into Kafka, while a <em>consumer</em> is the one that reads from Kafka; in Kafka terminology, a consumer application <em>subscribes</em> to topics. Therefore, the most basic workflow we can accomplish with Kafka is one with a single producer and a single consumer; this is illustrated in Figure <a href="streaming.html#fig:streaming-kafka-apis">12.3</a>.</p>
<div class="figure" style="text-align: center"><span id="fig:streaming-kafka-apis"></span>
<img src="the-r-in-spark_files/figure-html/streaming-kafka-apis-1.png" alt="A basic Kafka workflow" width="auto" height="100pt" />
<p class="caption">
FIGURE 12.3: A basic Kafka workflow
</p>
</div>
<p>If you are new to Kafka, we don’t recommend you run the code from this section. However, if you’re really motivated to follow along, you will first need to install Kafka as explained in <a href="appendix.html#appendix">Appendix</a> or deploy it in your cluster.</p>
<p>Using Kafka also requires you to have the Kafka package when connecting to Spark. Make sure this is specified in your connection <code>config</code>:</p>
<div class="sourceCode" id="cb485"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb485-1" title="1"><span class="kw">library</span>(sparklyr)</a>
<a class="sourceLine" id="cb485-2" title="2"><span class="kw">library</span>(dplyr)</a>
<a class="sourceLine" id="cb485-3" title="3"></a>
<a class="sourceLine" id="cb485-4" title="4">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(<span class="dt">master =</span> <span class="st">&quot;local&quot;</span>, <span class="dt">config =</span> <span class="kw">list</span>(</a>
<a class="sourceLine" id="cb485-5" title="5">  <span class="dt">sparklyr.shell.packages =</span> <span class="st">&quot;org.apache.spark:spark-sql-kafka-0-10_2.11:2.4.0&quot;</span></a>
<a class="sourceLine" id="cb485-6" title="6">))</a></code></pre></div>
<p>Once connected, it’s straightforward to read data from a stream:</p>
<div class="sourceCode" id="cb486"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb486-1" title="1"><span class="kw">stream_read_kafka</span>(</a>
<a class="sourceLine" id="cb486-2" title="2">  sc, </a>
<a class="sourceLine" id="cb486-3" title="3">  <span class="dt">options =</span> <span class="kw">list</span>(</a>
<a class="sourceLine" id="cb486-4" title="4">    <span class="dt">kafka.bootstrap.server =</span> <span class="st">&quot;host1:9092, host2:9092&quot;</span>, </a>
<a class="sourceLine" id="cb486-5" title="5">    <span class="dt">subscribe =</span> <span class="st">&quot;&lt;topic-name&gt;&quot;</span></a>
<a class="sourceLine" id="cb486-6" title="6">    )</a>
<a class="sourceLine" id="cb486-7" title="7">  ) </a></code></pre></div>
<p>However, notice that you need to properly configure the <code>options</code> list; <code>kafka.bootstrap.server</code> expects a list of Kafka hosts, while <code>topic</code> and <code>subscribe</code> define which topic should be used when writing or reading from Kafka, respectively.</p>
<p>Though we’ve started by presenting a simple single-producer and single-consumer use case, Kafka also allows much more complex interactions. We will next read from one topic, process its data, and then write the results to a different topic. Systems that are producers and consumers from the same topic are referred to as <em>stream processors</em>. In Figure <a href="streaming.html#fig:streaming-kafka-two-outputs">12.4</a>, the stream processor reads topic A and then writes results to topic B. This allows for a given consumer application to read results instead of “raw” feed data.</p>
<div class="figure" style="text-align: center"><span id="fig:streaming-kafka-two-outputs"></span>
<img src="the-r-in-spark_files/figure-html/streaming-kafka-two-outputs-1.png" alt="A Kafka workflow using stream processors" width="auto" height="260pt" />
<p class="caption">
FIGURE 12.4: A Kafka workflow using stream processors
</p>
</div>
<p>Three modes are available when processing Kafka streams in Spark: <em>complete</em>, <em>update</em>, and <em>append</em>. The <code>complete</code> mode provides the totals for every group every time there is a new batch; <code>update</code> provides totals for only the groups that have updates in the latest batch; and <code>append</code> adds raw records to the target topic. The <code>append</code> mode is not meant for aggregates, but works well for passing a filtered subset to the target topic.</p>
<p>In our next example, the producer streams random letters into Kafka under a <code>letters</code> topic. Then, Spark will act as the stream processor, reading the <code>letters</code> topic and computing unique letters, which are then written back to Kafka under the <code>totals</code> topic. We’ll use the <code>update</code> mode when writing back into Kafka; that is, only the totals that changed will be sent to Kafka. This change is determined after each batch from the <code>letters</code> topic:</p>
<div class="sourceCode" id="cb487"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb487-1" title="1">hosts  &lt;-<span class="st"> &quot;localhost:9092&quot;</span></a>
<a class="sourceLine" id="cb487-2" title="2"></a>
<a class="sourceLine" id="cb487-3" title="3">read_options &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">kafka.bootstrap.servers =</span> hosts, <span class="dt">subscribe =</span> <span class="st">&quot;letters&quot;</span>)</a>
<a class="sourceLine" id="cb487-4" title="4">write_options &lt;-<span class="st"> </span><span class="kw">list</span>(<span class="dt">kafka.bootstrap.servers =</span> hosts, <span class="dt">topic =</span> <span class="st">&quot;totals&quot;</span>)</a>
<a class="sourceLine" id="cb487-5" title="5"></a>
<a class="sourceLine" id="cb487-6" title="6"><span class="kw">stream_read_kafka</span>(sc, <span class="dt">options =</span> read_options) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb487-7" title="7"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">value =</span> <span class="kw">as.character</span>(value)) <span class="op">%&gt;%</span><span class="st">         </span><span class="co"># coerce into a character</span></a>
<a class="sourceLine" id="cb487-8" title="8"><span class="st">  </span><span class="kw">count</span>(value) <span class="op">%&gt;%</span><span class="st">                                </span><span class="co"># group and count letters</span></a>
<a class="sourceLine" id="cb487-9" title="9"><span class="st">  </span><span class="kw">mutate</span>(<span class="dt">value =</span> <span class="kw">paste0</span>(value, <span class="st">&quot;=&quot;</span>, n)) <span class="op">%&gt;%</span><span class="st">       </span><span class="co"># kafka expects a value field</span></a>
<a class="sourceLine" id="cb487-10" title="10"><span class="st">  </span><span class="kw">stream_write_kafka</span>(<span class="dt">mode =</span> <span class="st">&quot;update&quot;</span>,</a>
<a class="sourceLine" id="cb487-11" title="11">                     <span class="dt">options =</span> write_options)</a></code></pre></div>
<p>You can take a quick look at totals by reading from Kafka:</p>
<div class="sourceCode" id="cb488"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb488-1" title="1"><span class="kw">stream_read_kafka</span>(sc, <span class="dt">options =</span> totals_options)</a></code></pre></div>
<p>Using a new terminal session, use Kafka’s command-line tool to manually add single letters into the <code>letters</code> topic:</p>
<pre><code>kafka-console-producer.sh --broker-list localhost:9092 --topic letters
&gt;A
&gt;B
&gt;C</code></pre>
<p>The letters that you input are pushed to Kafka, read by Spark, aggregated within Spark, and pushed back into Kafka, Then, finally, they are consumed by Spark again to give you a glimpse into the <code>totals</code> topic. This was quite a setup, but also a realistic configuration commonly found in real-time processing projects.</p>
<p>Next, we will use the Shiny framework to visualize streams, in real time!</p>
</div>
<div id="shiny" class="section level2">
<h2><span class="header-section-number">12.4</span> Shiny</h2>
<p>Shiny’s reactive<!--((("Shiny Server")))((("streaming", "Shiny")))--> framework is well suited to support streaming information, which you can use to display real-time data from Spark using <code>reactiveSpark()</code>. There is far more to learn about Shiny than we could possibly present here. However, if you’re already familiar with Shiny, this example should be quite easy to understand.</p>
<p>We have a modified version of the <em>k</em>-means Shiny example that, instead of getting the data from the static <code>iris</code> dataset, is generated with <code>stream_generate_test()</code>, consumed by Spark, retrieved to Shiny through <code>reactiveSpark()</code>, and then displayed as shown in Figure <a href="streaming.html#fig:streaming-shiny-app">12.5</a>.</p>
<p>To run this example, store the following Shiny app under <code>shiny/shiny-stream.R</code>:</p>
<div class="sourceCode" id="cb490"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb490-1" title="1"><span class="kw">library</span>(sparklyr)</a>
<a class="sourceLine" id="cb490-2" title="2"><span class="kw">library</span>(shiny)</a>
<a class="sourceLine" id="cb490-3" title="3"></a>
<a class="sourceLine" id="cb490-4" title="4"><span class="kw">unlink</span>(<span class="st">&quot;shiny-stream&quot;</span>, <span class="dt">recursive =</span> <span class="ot">TRUE</span>)</a>
<a class="sourceLine" id="cb490-5" title="5"><span class="kw">dir.create</span>(<span class="st">&quot;shiny-stream&quot;</span>, <span class="dt">showWarnings =</span> <span class="ot">FALSE</span>)</a>
<a class="sourceLine" id="cb490-6" title="6"></a>
<a class="sourceLine" id="cb490-7" title="7">sc &lt;-<span class="st"> </span><span class="kw">spark_connect</span>(</a>
<a class="sourceLine" id="cb490-8" title="8">  <span class="dt">master =</span> <span class="st">&quot;local&quot;</span>, <span class="dt">version =</span> <span class="st">&quot;2.3&quot;</span>,</a>
<a class="sourceLine" id="cb490-9" title="9">  <span class="dt">config =</span> <span class="kw">list</span>(<span class="dt">sparklyr.sanitize.column.names =</span> <span class="ot">FALSE</span>))</a>
<a class="sourceLine" id="cb490-10" title="10">  </a>
<a class="sourceLine" id="cb490-11" title="11">ui &lt;-<span class="st"> </span><span class="kw">pageWithSidebar</span>(</a>
<a class="sourceLine" id="cb490-12" title="12">  <span class="kw">headerPanel</span>(<span class="st">&#39;Iris k-means clustering from Spark stream&#39;</span>),</a>
<a class="sourceLine" id="cb490-13" title="13">  <span class="kw">sidebarPanel</span>(</a>
<a class="sourceLine" id="cb490-14" title="14">    <span class="kw">selectInput</span>(<span class="st">&#39;xcol&#39;</span>, <span class="st">&#39;X Variable&#39;</span>, <span class="kw">names</span>(iris)),</a>
<a class="sourceLine" id="cb490-15" title="15">    <span class="kw">selectInput</span>(<span class="st">&#39;ycol&#39;</span>, <span class="st">&#39;Y Variable&#39;</span>, <span class="kw">names</span>(iris),</a>
<a class="sourceLine" id="cb490-16" title="16">                <span class="dt">selected=</span><span class="kw">names</span>(iris)[[<span class="dv">2</span>]]),</a>
<a class="sourceLine" id="cb490-17" title="17">    <span class="kw">numericInput</span>(<span class="st">&#39;clusters&#39;</span>, <span class="st">&#39;Cluster count&#39;</span>, <span class="dv">3</span>,</a>
<a class="sourceLine" id="cb490-18" title="18">                 <span class="dt">min =</span> <span class="dv">1</span>, <span class="dt">max =</span> <span class="dv">9</span>)</a>
<a class="sourceLine" id="cb490-19" title="19">  ),</a>
<a class="sourceLine" id="cb490-20" title="20">  <span class="kw">mainPanel</span>(<span class="kw">plotOutput</span>(<span class="st">&#39;plot1&#39;</span>))</a>
<a class="sourceLine" id="cb490-21" title="21">)</a>
<a class="sourceLine" id="cb490-22" title="22"></a>
<a class="sourceLine" id="cb490-23" title="23">server &lt;-<span class="st"> </span><span class="cf">function</span>(input, output, session) {</a>
<a class="sourceLine" id="cb490-24" title="24">  iris &lt;-<span class="st"> </span><span class="kw">stream_read_csv</span>(sc, <span class="st">&quot;shiny-stream&quot;</span>,</a>
<a class="sourceLine" id="cb490-25" title="25">                          <span class="dt">columns =</span> <span class="kw">sapply</span>(datasets<span class="op">::</span>iris, class)) <span class="op">%&gt;%</span></a>
<a class="sourceLine" id="cb490-26" title="26"><span class="st">    </span><span class="kw">reactiveSpark</span>()</a>
<a class="sourceLine" id="cb490-27" title="27">  </a>
<a class="sourceLine" id="cb490-28" title="28">  selectedData &lt;-<span class="st"> </span><span class="kw">reactive</span>(<span class="kw">iris</span>()[, <span class="kw">c</span>(input<span class="op">$</span>xcol, input<span class="op">$</span>ycol)])</a>
<a class="sourceLine" id="cb490-29" title="29">  clusters &lt;-<span class="st"> </span><span class="kw">reactive</span>(<span class="kw">kmeans</span>(<span class="kw">selectedData</span>(), input<span class="op">$</span>clusters))</a>
<a class="sourceLine" id="cb490-30" title="30">  </a>
<a class="sourceLine" id="cb490-31" title="31">  output<span class="op">$</span>plot1 &lt;-<span class="st"> </span><span class="kw">renderPlot</span>({</a>
<a class="sourceLine" id="cb490-32" title="32">    <span class="kw">par</span>(<span class="dt">mar =</span> <span class="kw">c</span>(<span class="fl">5.1</span>, <span class="fl">4.1</span>, <span class="dv">0</span>, <span class="dv">1</span>))</a>
<a class="sourceLine" id="cb490-33" title="33">    <span class="kw">plot</span>(<span class="kw">selectedData</span>(), <span class="dt">col =</span> <span class="kw">clusters</span>()<span class="op">$</span>cluster, <span class="dt">pch =</span> <span class="dv">20</span>, <span class="dt">cex =</span> <span class="dv">3</span>)</a>
<a class="sourceLine" id="cb490-34" title="34">    <span class="kw">points</span>(<span class="kw">clusters</span>()<span class="op">$</span>centers, <span class="dt">pch =</span> <span class="dv">4</span>, <span class="dt">cex =</span> <span class="dv">4</span>, <span class="dt">lwd =</span> <span class="dv">4</span>)</a>
<a class="sourceLine" id="cb490-35" title="35">  })</a>
<a class="sourceLine" id="cb490-36" title="36">}</a>
<a class="sourceLine" id="cb490-37" title="37"></a>
<a class="sourceLine" id="cb490-38" title="38"><span class="kw">shinyApp</span>(ui, server)</a></code></pre></div>
<p>This Shiny application can then be launched with <code>runApp()</code>, like so:</p>
<div class="sourceCode" id="cb491"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb491-1" title="1">shiny<span class="op">::</span><span class="kw">runApp</span>(<span class="st">&quot;shiny/shiny-stream.R&quot;</span>)</a></code></pre></div>
<p>While the Shiny app is running, launch a new R session from the same directory and create a test stream with <code>stream_generate_test()</code>. This will generate a stream of continuous data that Spark can process and Shiny can visualize (as illustrated in Figure <a href="streaming.html#fig:streaming-shiny-app">12.5</a>):</p>
<div class="sourceCode" id="cb492"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb492-1" title="1">sparklyr<span class="op">::</span><span class="kw">stream_generate_test</span>(datasets<span class="op">::</span>iris, <span class="st">&quot;shiny/shiny-stream&quot;</span>,</a>
<a class="sourceLine" id="cb492-2" title="2">                               <span class="kw">rep</span>(<span class="dv">5</span>, <span class="dv">10</span><span class="op">^</span><span class="dv">3</span>))</a></code></pre></div>
<div class="figure" style="text-align: center"><span id="fig:streaming-shiny-app"></span>
<img src="images/streaming-shiny-app-resized.png" alt="Progression of Spark reactive loading data into the Shiny app" width="1500" />
<p class="caption">
FIGURE 12.5: Progression of Spark reactive loading data into the Shiny app
</p>
</div>
<p>In this section you learned how easy it is to create a Shiny app that can be used for several different purposes, such as monitoring and dashboarding.</p>
<p>In a more complex implementation, the source would more likely be a Kafka stream.</p>
<p>Before we transition, disconnect from Spark and clear the folders that we used:</p>
<div class="sourceCode" id="cb493"><pre class="sourceCode r"><code class="sourceCode r"><a class="sourceLine" id="cb493-1" title="1"><span class="kw">spark_disconnect</span>(sc)</a>
<a class="sourceLine" id="cb493-2" title="2"></a>
<a class="sourceLine" id="cb493-3" title="3"><span class="kw">unlink</span>(<span class="kw">c</span>(<span class="st">&quot;source&quot;</span>, <span class="st">&quot;destination&quot;</span>, <span class="st">&quot;cars-stream&quot;</span>,</a>
<a class="sourceLine" id="cb493-4" title="4">         <span class="st">&quot;car-round&quot;</span>, <span class="st">&quot;shiny/shiny-stream&quot;</span>), <span class="dt">recursive =</span> <span class="ot">TRUE</span>)</a></code></pre></div>
</div>
<div id="recap-9" class="section level2">
<h2><span class="header-section-number">12.5</span> Recap</h2>
<p>From static datasets to real-time datasets, you’ve truly mastered many of the large-scale computing techniques. Specifically, in this chapter, you learned how static data can be generalized to real time if we think of it as an infinite table. We were then able to create a simple stream, without any data transformations, that copies data from point A to point B.</p>
<p>This humble start became quite useful when you learned about the several different transformations you can apply to streaming data—from data analysis transformations using the <code>dplyr</code> and <code>DBI</code> packages, to feature transformers introduced while modeling, to fully fledged pipelines capable of scoring in real time, to, last but not least, transforming datasets with custom R code. This was a lot to digest, for sure.</p>
<p>We then presented Apache Kafka as a reliable and scalable solution for real-time data. We showed you how a real-time system could be structured by introducing you to consumers, producers, and topics. These, when properly combined, create powerful abstractions to process real-time data.</p>
<p>Then we closed with “a cherry on top of the sundae”: presenting how to use Spark Streaming in Shiny. Since a stream can be transformed into a reactive (which is the lingua franca of the world of reactivity), the ease of this approach was a nice surprise.</p>
<p>It’s time now to move on to our very last (and quite short) chapter, <a href="contributing.html#contributing">Chapter 13</a>; there we’ll try to persuade you to use your newly acquired knowledge for the benefit of the Spark and R communities at large.</p>

</div>
</div>
            </section>

          </div>
        </div>
      </div>
<a href="distributed.html" class="navigation navigation-prev " aria-label="Previous page"><i class="fa fa-angle-left"></i></a>
<a href="contributing.html" class="navigation navigation-next " aria-label="Next page"><i class="fa fa-angle-right"></i></a>
    </div>
  </div>
<script src="libs/gitbook-2.6.7/js/app.min.js"></script>
<script src="libs/gitbook-2.6.7/js/lunr.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-search.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-sharing.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-fontsettings.js"></script>
<script src="libs/gitbook-2.6.7/js/plugin-bookdown.js"></script>
<script src="libs/gitbook-2.6.7/js/jquery.highlight.js"></script>
<script>
gitbook.require(["gitbook"], function(gitbook) {
gitbook.start({
"sharing": {
"github": true,
"facebook": false,
"twitter": true,
"google": false,
"linkedin": false,
"weibo": false,
"instapaper": false,
"vk": false,
"all": ["facebook", "google", "twitter", "linkedin", "weibo", "instapaper"]
},
"fontsettings": {
"theme": "white",
"family": "sans",
"size": 2
},
"edit": {
"link": null,
"text": null
},
"history": {
"link": null,
"text": null
},
"download": null,
"toc": {
"collapse": "subsection"
}
});
});
</script>

<!-- dynamically load mathjax for compatibility with self-contained -->
<script>
  (function () {
    var script = document.createElement("script");
    script.type = "text/javascript";
    var src = "true";
    if (src === "" || src === "true") src = "https://mathjax.rstudio.com/latest/MathJax.js?config=TeX-MML-AM_CHTML";
    if (location.protocol !== "file:")
      if (/^https?:/.test(src))
        src = src.replace(/^https?:/, '');
    script.src = src;
    document.getElementsByTagName("head")[0].appendChild(script);
  })();
</script>
</body>

</html>
