```{r include=FALSE, eval=TRUE}
knitr::opts_chunk$set(eval = FALSE)
source("r/render.R")

styles <- "
#padding: 16
#fontSize: 18
#direction: right
#lineWidth:2
#leading:1
#spacing: 20
"
```

# Distributed R {#distributed}

Previous chapters presented how to perform many operations in Spark, the previous [Extensions](#extensions) chapter, described how to use functionality provided not in Spark itself, but in extensions developed by the Spark community. In most cases, the combination of Spark and Spark extensions is more than enough to scale almost any computation. However, for those few cases where functionality is lacking, you can consider using `spark_apply()` to perform custom mapping operations using in R and R packages.

## Overview

The [Introduction](#intro) chapter introduced MapReduce as a technique capable of processing large scale datasets; it also described how Apache Spark provided a superset of operations to perform MapReduce computations with ease and more efficiently. The [Tuning](#tuning) chapter presented insights into how Spark works by applying custom transformation over each partition of the distributed datasets. For instance, if we were to multiply by ten each element of a distributed numeric dataset, Spark would apply a mapping operation over each partition through multiple workers, conceptually similar to:

```{r distributed-times-ten, echo=FALSE, eval=TRUE, fig.cap='Conceptual mapping operation when multiplying by ten.', out.width='580pt', out.height='220pt', fig.align='center'}
render_nomnoml("
[Data |
  [1]
  [2]
  [3]
]->[Worker |
  [1] -> [f(x) = 10 * x]
  [2] -> [f(x) = 10 * x]
  [3] -> [f(x) = 10 * x]
  [f(x) = 10 * x] -> [10]
  [f(x) = 10 * x] -> [20]
  [f(x) = 10 * x] -> [30]
]", "images/distributed-times-ten.png", "Conceptual mapping operation when multiplying by ten.", styles)
```

This chapter presents how to define custom `f(x)` mapping operations using `spark_apply()`; for the example above, `spark_apply()` provides support to define `x + 1` as follows:

```{r distributed-increment-one-code}
sdf_len(sc, 3) %>% spark_apply(~ 10 * .x)
```
```
# Source: spark<?> [?? x 1]
     id
* <dbl>
1     2
2     3
3     4
```

Notice that `~ 10 * .x` is plain R code executed across all worker nodes; the `~` character is defined in the `rlang` package and provides a compact definition of a function equivalent to `function(.x) 10 * .x` or what is also known as an anonymous function or lambda expression.

Now, the first thing to notice is that `f(x)` takes an R data frame as input and must also produce an R data frame as output, conceptually this looks as follows:

```{r distributed-spark-apply-input-output, echo=FALSE, eval=TRUE, fig.cap='Expected function signature in spark_apply() mappings.', out.width='580pt', out.height='120pt', fig.align='center'}
render_nomnoml("
[DataFrame]->[f(x)]
[f(x)]->[DataFrame']
", "images/distributed-spark-apply-input-output.png", "Expected function signature in spark_apply() mappings.", styles)
```

We can use the orginal MapReduce example from the [Introduction](#intro) chapter where, the map operations was defined to split sentences into words and then, the total unique words were counted as the reduce operation. We can compute this using knowledge from previous chapters as follows:

```{r}
sentences <- data_frame(text = c(
  "I like apples",
  "I like bananas"
))

sentences_tbl <- copy_to(sc, sentences)

sentences_tbl %>%
  ft_tokenizer("text", "words") %>%
  transmute(word = explode(words)) %>%
  group_by(word) %>%
  summarise(count = count())
```
```
# Source: spark<?> [?? x 2]
  word    count
* <chr>   <dbl>
1 i           2
2 apples      1
3 like        2
4 bananas     1
```

In this example, the operation that provides the mapping from text into words is a combination of `ft_tokenizer()` and `explode()`; however, if those operations were not available, we could use the `tidytext` R package to provide support spliting sentonces into words, this would look as follows without using Spark:

```{r}
tidytext::unnest_tokens(sentences, word, text)
```
```
# A tibble: 6 x 1
  word   
  <chr>  
1 i      
2 like   
3 apples 
4 i      
5 like   
6 bananas
```

We can use `unnest_tokens()` as a custom mapping operation in Spark using `spark_apply()` as follows:

```{r}
sentences_tbl %>%
  spark_apply(~tidytext::unnest_tokens(., word, text))
```
```
# Source: spark<?> [?? x 1]
  word   
* <chr>  
1 i      
2 like   
3 apples 
4 i      
5 like   
6 bananas
```

Finally, we can reduce this dataset using `dplyr` to compute this original MapReduce word-count example using `dplyr` as follows:

```{r}
sentences_tbl %>%
  spark_apply(~tidytext::unnest_tokens(., word, text)) %>%
  group_by(word) %>%
  summarise(count = count())
```
```
# Source: spark<?> [?? x 2]
  word    count
* <chr>   <dbl>
1 i           2
2 apples      1
3 like        2
4 bananas     1
```

The rest of this chapter will explain in detail additional features, caveats, considerations and troubleshooting techniques required when defining custom mappings through `spark_apply()`

**Warning:** The proffient R reader can be tempted to use this approach for all Spark operations since it enables using all the functionality from many familiar R packages; however, this is NOT the recommended use of `spark_apply()` since it introduces additional cognitive overhead to the reader, additional troubleshooting steps, performance degradation and, in general, additional complexity that should be avoided.

## Partitions

## Columns

Inference vs Excplicit

```{r distributed-columns-explicit}
iris_tbl <- spark_apply(
  I,
  columns = lapply(iris, class)
)
```

## Grouping

```{r distributed-parallel-grouping}
sdf_len(sc, 10, repartition = 1) %>%
  transmute(groups = floor(id / 2)) %>%
  spark_apply(~nrow(.x))
```

```
# Source: spark<?> [?? x 1]
  result
*  <int>
1      5
2      5
```

```{r distributed-grouping-repartition, eval=FALSE}
sdf_len(sc, 10, repartition = 1) %>%
  transmute(groups = floor(id / 2)) %>%
  sdf_repartition(partition_by = "groups") %>%
  spark_apply(~nrow(.x))
```
```
# Source: spark<?> [?? x 1]
  result
*  <int>
1      4
2      1
3      4
4      1
```

```{r distributed-grouping-transmute}
sdf_len(sc, 10, repartition = 1) %>%
  transmute(groups = floor(id / 2)) %>%
  sdf_repartition(partition_by = "groups") %>%
  spark_apply(~nrow(.x), group_by = "groups")
```
```
# Source: spark<?> [?? x 2]
  groups result
*  <dbl>  <int>
1      1      2
2      2      2
3      5      1
4      3      2
5      4      2
6      0      1
```

Notice that `spark_apply()` does not repartition data automatically, so optimizing how data is repartitioned mus be considered using `sdf_repartition()`.

## Packages

## Context

```{r distributed-context}
sdf_len(sc, 3, repartition = 3) %>%
  spark_apply(function(data, context) context, context = data.frame(something = c("foo", "bar")))
```
```
# Source: spark<?> [?? x 1]
  something
* <chr>    
1 foo      
2 bar      
3 foo      
4 bar      
5 foo      
6 bar    
```

```{r distributed-context-list}
sdf_len(sc, 3, repartition = 3) %>%
  spark_apply(
    function(data, context) context$numbers * context$constant,
    context = list(
      numbers = c(2, 3, 5),
      constant = 10
    )
  )
```
```
# Source: spark<?> [?? x 1]
  result
*  <dbl>
1     20
2     30
3     50
4     20
5     30
6     50
7     20
8     30
9     50
```

## Restrictions


### Troubleshooting

There are a couple common troubleshooting techniquest in `spark_apply()` 

#### Worker Logs

Whenever `spark_apply()` is executed, information regarding execution is written over each worker node. You can use this log to write custom messages o help you diagnose and fine-tune your code.

For instance, suppose that you don't know what the first column name of `df` is, we can write a custom log message executed from the worker nodes using `worker_log()` as follows:

```{r distributed-logs}
sdf_len(sc, 1) %>% spark_apply(function(df) {
  worker_log("the first column in the data frame is named ", names(df)[[1]])
  df
})
```
```
# Source: spark<?> [?? x 1]
     id
* <int>
1     1
```

When running locally, we can filter the log entries for the worker as follows:

```{r distributed-logs-filter}
spark_log(sc, filter = "sparklyr: RScript")
```

```
18/12/18 11:33:47 INFO sparklyr: RScript (3513) the first column in the dataframe is named id 
18/12/18 11:33:47 INFO sparklyr: RScript (3513) computed closure 
18/12/18 11:33:47 INFO sparklyr: RScript (3513) updating 1 rows 
18/12/18 11:33:47 INFO sparklyr: RScript (3513) updated 1 rows 
18/12/18 11:33:47 INFO sparklyr: RScript (3513) finished apply 
18/12/18 11:33:47 INFO sparklyr: RScript (3513) finished 
```

Notice that the logs show out custom log entry showing that `id` is the name of the first column in the given data frame.

This functionality is useful when troubleshooting errors, for instance, if we force an error using the `stop()` function:

```{r distributed-force-error}
sdf_len(sc, 1) %>% spark_apply(function(df) {
  stop("force an error")
})
```

You will get an error similar to,

```
 Error in force(code) : 
  sparklyr worker rscript failure, check worker logs for details
```

As suggested in the error, we can look in the worker logs for the specific errors as follows:

```{r distributed-force-error-log}
spark_log(sc)
```

This will show an entry containing the error and the callstack:

```
18/12/18 11:26:47 INFO sparklyr: RScript (1860) computing closure 
18/12/18 11:26:47 ERROR sparklyr: RScript (1860) terminated unexpectedly: force an error 
18/12/18 11:26:47 ERROR sparklyr: RScript (1860) collected callstack: 
11: stop("force and error")
10: (function (df) 
{
    stop("force and error")
})(structure(list(id = 1L), class = "data.frame", row.names = c(NA, 
-1L)))
```

Notice that, spark_log(sc) only retrieves the worker logs when using local clusters, when running in proper clusters with multiple machines, you will have to use the tools and user interface provided by the cluster manager to find these log entries.

#### Worker Error

```{r distributed-worker-error}
sdf_len(sc, 1) %>% spark_apply(function(df) {
    tryCatch({
        stop("an error")
    }, error = function(e) {
        e$message
    })
})
```

#### Worker Partitions

If a particular partition fails, you can detect the broken partition by computing a digest, and then retrieving that particular partition as follows:

```{r distributed-worker-partitions}
sdf_len(sc, 3) %>% spark_apply(function(x) {
    worker_log("processing ", digest::digest(x), " partition")
    # your code
})
```

This will add an entry similar to:

```
18/11/03 14:48:32 INFO sparklyr: RScript (2566) processing f35b1c321df0162e3f914adfb70b5416 partition 
```

When executing this in your cluster, you will have to look in the logs for the task that is not finishing, once you have that digest, you can cancel the job.

Then you can use that digest to retrieve that specific data frame to R with something like:

```{r distributed-worker-partitions-collect}
broken_partition <- sdf_len(sc, 3) %>% spark_apply(function(x) {
    if (identical(digest::digest(x), "f35b1c321df0162e3f914adfb70b5416")) x else x[0,]
}) %>% collect()
```

WHich you can then run in R to troubleshoot further.

#### Worker Debugger

```{r distributed-debugger}
sdf_len(sc, 1) %>% spark_apply(function() {
  stop("Error!")
}, debug = TRUE)
```
```
Debugging spark_apply(), connect to worker debugging session as follows:
  1. Find the workers <sessionid> and <port> in the worker logs, from RStudio click
     'Log' under the connection, look for the last entry with contents:
     'Session (<sessionid>) is waiting for sparklyr client to connect to port <port>'
  2. From a new R session run:
     debugonce(sparklyr:::spark_worker_main)
     sparklyr:::spark_worker_main(<sessionid>, <port>)
```

## Clusters

When using `spark_apply()`, R needs to be properly installed in each worker node. Different cluster managers, distributions and services, proivide different solutions to install additional software; those instructions should be followed when installing R over each worker node. To mention a few,

- **Spark Standalone**: Requires connecting to each machine and installing R; there are tools like `pssh` that allow you to run a single installation command against multiple machines.
- **Cloudera**: Provides an R parcel, see ["How to Distribute your R code with sparklyr and Cloudera Data Science Workbench"](https://blog.cloudera.com/blog/2017/09/how-to-distribute-your-r-code-with-sparklyr-and-cdsw/)[@cloudera-sparklyr-parcel], which enables R over each worker node.
- **Amazon EMR**: R is pre-installed when starting an EMR cluster as mentioned in the [Amazon EMR](#clusters-amazon-emr) section.
- **Microsoft HDInsight**: R is pre-installed and no additional steps are needed.

## Apache Arrow

Apache Arrow is strongly adviced while working spark_apply().

## Recap
