# (PART) Advanced {-}

# Distributed R {#distributed}

While **this chatper has not been written.**, use [spark.rstudio.com/guides/distributed-r](http://spark.rstudio.com/guides/distributed-r/) to learn how to use R directly over each worker node.

## Use Cases

### Embarrassingly Parallel

```{r eval=FALSE}
sdf_len(sc, total_executors, repartition = total_executors) %>%
  spark_apply(~ data.frame(pi = 3.1416), columns = c(pi = "character")) %>%
  summarize(mean = mean(pi))
```

## Columns

### Inference

### Excplicit

```{r eval=FALSE}
iris_tbl <- spark_apply(
  I,
  columns = lapply(iris, class)
)
```

## Grouping

## Packages

## Context

```{r eval=F}
sdf_len(sc, 3, repartition = 3) %>%
  spark_apply(function(data, context) context, context = data.frame(something = c("foo", "bar")))
```
```
# Source: spark<?> [?? x 1]
      a
* <dbl>
1     1
2     2
3     3
4     1
5     2
6     3
```

```{r eval=F}
sdf_len(sc, 3, repartition = 3) %>%
  spark_apply(
    function(data, context) context$numbers * context$constant,
    context = list(
      numbers = c(2, 3, 5),
      constant = 10
    )
  )
```
```
# Source: spark<?> [?? x 1]
  result
*  <dbl>
1     20
2     30
3     50
4     20
5     30
6     50
7     20
8     30
9     50
```

## Restrictions

## Troubleshooting

### Tips

```{r eval=FALSE}
odbc_logs %>% head() %>% spark_apply(function(df) {
    tryCatch({
        webreadr::read_s3(df[[1]])
        ""
    }, error = function(e) {
        e$message
    })
})
```

### Logs

### Debugging

If a particular partition fails, you can detect the broken partition by computing a digest, and then retrieving that particular partition as follows:

```{r eval=FALSE}
sdf_len(sc, 3) %>% spark_apply(function(x) {
    worker_log("processing ", digest::digest(x), " partition")
    # your code
})
```

This will add an entry similar to:

```
18/11/03 14:48:32 INFO sparklyr: RScript (2566) processing f35b1c321df0162e3f914adfb70b5416 partition 
```

When executing this in your cluster, you will have to look in the logs for the task that is not finishing, once you have that digest, you can cancel the job.

Then you can use that digest to retrieve that specific data frame to R with something like:

```{r eval=FALSE}
broken_partition <- sdf_len(sc, 3) %>% spark_apply(function(x) {
    if (identical(digest::digest(x), "f35b1c321df0162e3f914adfb70b5416")) x else x[0,]
}) %>% collect()
```

WHich you can then run in R to troubleshoot further.
