# Distributed R {#distributed}

While **this chatper has not been written.**, use [spark.rstudio.com/guides/distributed-r](http://spark.rstudio.com/guides/distributed-r/) to learn how to use R directly over each worker node.

## Use Cases

### Embarrassingly Parallel

```{r eval=FALSE}
sdf_len(sc, total_executors, repartition = total_executors) %>%
  spark_apply(~ data.frame(pi = 3.1416), columns = c(pi = "character")) %>%
  summarize(mean = mean(pi))
```

## Columns

### Inference

### Excplicit

```{r eval=FALSE}
iris_tbl <- spark_apply(
  I,
  columns = lapply(iris, class)
)
```

## Grouping

```{r eval=FALSE}
sdf_len(sc, 10, repartition = 1) %>%
  transmute(groups = floor(id / 2)) %>%
  spark_apply(~nrow(.x))
```

```
# Source: spark<?> [?? x 1]
  result
*  <int>
1      5
2      5
```

```{r eval=FALSE}
sdf_len(sc, 10, repartition = 1) %>%
  transmute(groups = floor(id / 2)) %>%
  sdf_repartition(partition_by = "groups") %>%
  spark_apply(~nrow(.x))
```
```
# Source: spark<?> [?? x 1]
  result
*  <int>
1      4
2      1
3      4
4      1
```

```{r eval=FALSE}
sdf_len(sc, 10, repartition = 1) %>%
  transmute(groups = floor(id / 2)) %>%
  sdf_repartition(partition_by = "groups") %>%
  spark_apply(~nrow(.x), group_by = "groups")
```
```
# Source: spark<?> [?? x 2]
  groups result
*  <dbl>  <int>
1      1      2
2      2      2
3      5      1
4      3      2
5      4      2
6      0      1
```

Notice that `spark_apply()` does not repartition data automatically, so optimizing how data is repartitioned mus be considered using `sdf_repartition()`.

## Packages

## Context

```{r eval=F}
sdf_len(sc, 3, repartition = 3) %>%
  spark_apply(function(data, context) context, context = data.frame(something = c("foo", "bar")))
```
```
# Source: spark<?> [?? x 1]
      a
* <dbl>
1     1
2     2
3     3
4     1
5     2
6     3
```

```{r eval=F}
sdf_len(sc, 3, repartition = 3) %>%
  spark_apply(
    function(data, context) context$numbers * context$constant,
    context = list(
      numbers = c(2, 3, 5),
      constant = 10
    )
  )
```
```
# Source: spark<?> [?? x 1]
  result
*  <dbl>
1     20
2     30
3     50
4     20
5     30
6     50
7     20
8     30
9     50
```

## Restrictions


### Troubleshooting

There are a couple common troubleshooting techniquest in `spark_apply()` 

#### Worker Logs

Whenever `spark_apply()` is executed, information regarding execution is written over each worker node. You can use this log to write custom messages o help you diagnose and fine-tune your code.

For instance, suppose that you don't know what the first column name of `df` is, we can write a custom log message executed from the worker nodes using `worker_log()` as follows:

```{r eval=FALSE}
sdf_len(sc, 1) %>% spark_apply(function(df) {
  worker_log("the first column in the dataframe is named ", names(df)[[1]])
  df
})
```
```
# Source: spark<?> [?? x 1]
     id
* <int>
1     1
```

When running locally, we can filter the log entries for the worker as follows:

```{r eval=FALSE}
spark_log(sc, filter = "sparklyr: RScript")
```

```
18/12/18 11:33:47 INFO sparklyr: RScript (3513) the first column in the dataframe is named id 
18/12/18 11:33:47 INFO sparklyr: RScript (3513) computed closure 
18/12/18 11:33:47 INFO sparklyr: RScript (3513) updating 1 rows 
18/12/18 11:33:47 INFO sparklyr: RScript (3513) updated 1 rows 
18/12/18 11:33:47 INFO sparklyr: RScript (3513) finished apply 
18/12/18 11:33:47 INFO sparklyr: RScript (3513) finished 
```

Notice that the logs show out custom log entry showing that `id` is the name of the first column in the given data frame.

This functionality is useful when troubleshooting errors, for instance, if we force an error using the `stop()` function:

```{ eval=FALSE}
sdf_len(sc, 1) %>% spark_apply(function(df) {
  stop("force an error")
})
```

You will get an error similar to,

```
 Error in force(code) : 
  sparklyr worker rscript failure, check worker logs for details
```

As suggested in the error, we can look in the worker logs for the specific errors as follows:

```
spark_log(sc)
```

This will show an entry containing the error and the callstack:

```
18/12/18 11:26:47 INFO sparklyr: RScript (1860) computing closure 
18/12/18 11:26:47 ERROR sparklyr: RScript (1860) terminated unexpectedly: force an error 
18/12/18 11:26:47 ERROR sparklyr: RScript (1860) collected callstack: 
11: stop("force and error")
10: (function (df) 
{
    stop("force and error")
})(structure(list(id = 1L), class = "data.frame", row.names = c(NA, 
-1L)))
```

Notice that, spark_log(sc) only retrieves the worker logs when using local clusters, when running in proper clusters with multiple machines, you will have to use the tools and user interface provided by the cluster manager to find these log entries.

#### Worker Error

```{r eval=FALSE}
sdf_len(sc, 1) %>% spark_apply(function(df) {
    tryCatch({
        stop("an error")
    }, error = function(e) {
        e$message
    })
})
```

#### Worker Partitions

If a particular partition fails, you can detect the broken partition by computing a digest, and then retrieving that particular partition as follows:

```{r eval=FALSE}
sdf_len(sc, 3) %>% spark_apply(function(x) {
    worker_log("processing ", digest::digest(x), " partition")
    # your code
})
```

This will add an entry similar to:

```
18/11/03 14:48:32 INFO sparklyr: RScript (2566) processing f35b1c321df0162e3f914adfb70b5416 partition 
```

When executing this in your cluster, you will have to look in the logs for the task that is not finishing, once you have that digest, you can cancel the job.

Then you can use that digest to retrieve that specific data frame to R with something like:

```{r eval=FALSE}
broken_partition <- sdf_len(sc, 3) %>% spark_apply(function(x) {
    if (identical(digest::digest(x), "f35b1c321df0162e3f914adfb70b5416")) x else x[0,]
}) %>% collect()
```

WHich you can then run in R to troubleshoot further.

#### Worker Debugger

```{r eval=FALSE}
  stop("Error!")
}, debug = TRUE)
```

## Clusters

When using `spark_apply()`, R needs to be properly installed in each worker node. Different cluster managers, distributions and services, proivide different solutions to install additional software; those instructions should be followed when installing R over each worker node. To mention a few,

- **Spark Standalone**: Requires connecting to each machine and installing R; there are tools like `pssh` that allow you to run a single installation command against multiple machines.
- **Cloudera**: Provides an R parcel, see ["How to Distribute your R code with sparklyr and Cloudera Data Science Workbench"](https://blog.cloudera.com/blog/2017/09/how-to-distribute-your-r-code-with-sparklyr-and-cdsw/)[@cloudera-sparklyr-parcel], which enables R over each worker node.
- **Amazon EMR**: R is pre-installed when starting an EMR cluster through: ["Running sparklyr from the AWS Big Data Blog"](https://aws.amazon.com/blogs/big-data/running-sparklyr-rstudios-r-interface-to-spark-on-amazon-emr/)[@amazon-emr-sparklyr-blog].
- **Microsoft HDInsight**: R is pre-installed and no additional steps are needed.

## Recap
