```{r analysis-setup, include=FALSE}
knitr::opts_chunk$set(eval = FALSE)
```

# Analysis {#analysis}

## Typical analysis

As illustrated in *Figure 3.1*, a typical Data Science project is made up of multiple steps. Because its capabilities, every single step can be completed inside R.

```{r echo=FALSE, out.width='100%', out.height='220pt', fig.cap='Typical Data Science Project', fig.align = 'center', eval = TRUE}
nomnoml::nomnoml("
#direction: right
#edgeMargin: 4
#padding: 25
#fontSize: 18

[R |
  [Import] -> [Tidy]
  [Tidy] -> [Understand |
      [Transform] -> [Visualize] 
      [Visualize] -> [Model]
      [Model] -> [Transform]
  ] 
  [Understand] -> [Share]
]
")
```

In addition, the large universe of R packages allow analyst to extend R capabilities in each of the steps.  One good example is the `ggplot2` package which enables the creation of effective and professional looking plots. 

## Working with Big Data

R developers are used to a very specific cadence: Import the data into R, and analyze the data in-memory.  But what happens when dealing with Big Data?

For the purposes of this book, let's define Big Data as data that is too big to fit into RAM.  A secondary definition of Big Data, is data located in a remote machine, and can only be accessed through a small conduit. An example would be a remote database that can only be accessed via a network connection.  

Given these definitions, it is actually very common for an R developer to encounter Big Data in their day-to-day work.  Almost instinctively, the developer opts for one of the following strategies to handle such an encounter with Big Data:

1. **Sampling** - Download into R, what the analyst hopes is, a representative sample of the data being analyzed. Most remote sources do not offer a true randomize way of selecting which records will be downloaded, so any alternative would not provide a data set from which inferences can safely be made.

1. **In parts** - This may be the most common method for general data analysis.  The data is downloaded in segments, and then saved into the local disk.  The analyst then imports into memory the files recursively, and often importing only some of the columns from each file. An example of this would be an R developer using one R script to download one-day's worth of transactions at a time, and saving them into a CSV file, and then using a second R script to compile and analyze the data.  This approach is preferred because it allows the analyst to not have to go "back to the well" every time a new angle needs to be looked at, or the analyst needs to re-run the existing analysis. The main downside of this approach is that the copies saved in the local machine will eventually go stale because they won't receive get any updates made in the original source data. 

1. **Whole** - It is surprising how many choose this option. The idea is to download all of the data, or at least as much as it is physically possible for the local machine.  This usually means having to wait hours before the data is imported.  Once in memory, each step of the Data Science project is painfully slow because the local machine struggles with processing complex calculations over vast amounts of local data.

It is obvious that none of the three strategies are ideal, and this is more likely why an analyst will look into Spark as a way to scale the analysis.

## Avoid running R inside Spark

As it was personally, the usual first reaction is to try to run existing R code in a Spark cluster.  The thought is that somehow Spark will take the R code and packages and divide the job between all of the executors in parallel.  Unfortunately, that is not the case.  

Yes, it is true that `sparklyr` offers a way to run R code inside Spark.  But that is reserved for a very specific use case: embarrassingly parallel jobs.  In other words, if the data to be analyzed needs to be first divided into segments, and then the R code is applied to each segment.  

This is not due to anything lacking on `sparklyr`, or Spark for that matter.  It boils down to how Spark divides its operations.  When running R code, Spark can run one segment per Spark Executor. In the example on *Figure 3.2*, the data is split by the company's customer data, and then a model is fitted for each customer.


```{r echo=FALSE, out.width='100%', out.height='220pt', fig.cap='Works for embarassing parallel jobs', fig.align = 'center', eval = TRUE}
nomnoml::nomnoml("
#direction: down
#padding: 20
#fontSize: 18
#edgeMargin: 10
[Spark |
	[Executor 1 | [R| Model for customer A]] 
  [Executor 2 | [R| Model for customer B]] 
  [Executor 3 | [R| Model for customer C]] 
] 
")
```

The ability to split a job into multiple executors is a core strength of Spark.  It allows the job to run faster because theoretically, each executor has its own discrete amount of machine resources it can use to complete its assigned part of the job.  This means that if the data is not split, or segmented, then the entire R job will run on a single executor, and thus only using that executor's limited resources will be used. Additionally, the job will take much longer to complete, if it completes at all, because the are no other executors helping to complete the job in parallel, see *Figure 3.3*.


```{r echo=FALSE, out.width='100%', out.height='220pt', fig.cap='Running a single large job', fig.align = 'center', eval = TRUE}
nomnoml::nomnoml("
#direction: down
#padding: 20
#fontSize: 18
#edgeMargin: 10
[Spark |
	[Executor 1 | [R | Entire;model]]
  [Executor 2 | [Nothing]] 
  [Executor 3 | [Nothing]] 
] 
")
```

The authors of this book recommend that this approach should only be considered by advanced R users and experienced Spark/Big Data engineers.  There are several infrastructure considerations that need to be taken into account when using this method in an enterprise cluster, please see the chapter [Distributed R](#distributed) for more information.

## Use R as an interface to Spark

Understanding and applying the concepts in this section are going to be key to a successful analysis using R and Spark. 

First, lets mentally separate R the language, from R as an analysis engine.  In reality, R does not have a calculation engine, it actually depends on other languages to run these operations.  Under the covers, there are C++ and FORTRAN routines that R simply interfaces with in order to run a given calculation.  

What drives data analysts to R, is the fact that all of the complicated lower level code is now behind easy to use and understand R language functions. 

R is not limited to interacting with the languages mentioned above.  There are R packages that allow data analysts to directly interact languages such as python (`reticulate`) and D3 (`r2d3`), or interact indirectly with other languages, such as how the `shiny` package interfaces with JavaScript and HTML on behalf of the developer.

Given this pattern, running R inside Spark, is akin to thinking in terms of running R code using the browser's HTML interpreter, it would not make much sense.  With `shiny`, R writes HTML and JavaScript code to allow the browser do what its good at.  

A better approach then, is to let Spark do what its good at. Which is to be a escalable and parallel computation engine.  Spark goes beyond offering generic calculations. Out of the box, Spark includes libraries that actually can do a lot of what analysts usually do in R, but for large amounts of data. *Figure 3.4* paraphrases the four main capabilities available to data analysts in Spark.


```{r echo=FALSE, out.width='100%', out.height='220pt', fig.cap='Spark capabilities', fig.align = 'center', eval = TRUE}
nomnoml::nomnoml("
#direction: down
#fontSize: 18
#padding: 30
[Apache Spark |
  [SQL Engine]
  [Machine Learning]
  [Graph analysis]
  [Streaming]
]
")
```




## Cut here

```{r echo=FALSE, out.width='100%', out.height='220pt', fig.cap='R as an interface for Spark', fig.align = 'center', eval = TRUE}
nomnoml::nomnoml("
[R code |
	[sparklyr::ml_linear_regression() |
    	[Wraps Scala code |
        	var lr = new LinearRegression()
            var lrModel = lr.fit(training)
        ]
    ]
] 
")
```


```{r echo=FALSE, out.width='100%', out.height='220pt', fig.cap='R as an interface for Spark', fig.align = 'center', eval = TRUE}
nomnoml::nomnoml("
#direction: right
#padding: 40
#spacing: 140
#arrowSize:0.2
#.tag: visual=none
[<tag>R]Collect results->Push compute[Big;Data]
[Big;Data]->[R]
")
```



```{r echo=FALSE, out.width='100%', out.height='300pt', fig.cap='Working with Big Data', fig.align = 'center', eval = TRUE}
nomnoml::nomnoml("
#direction: right
#spacing: 100
#arrowSize: 0.3
#leading: 5
#padding: 30
#fontSize: 15
#.tag: visual=none
[Big Data] -> Import[<tag>R]
")
```
