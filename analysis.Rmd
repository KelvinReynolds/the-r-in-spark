```{r analysis-setup, include=FALSE}
knitr::opts_chunk$set(eval = FALSE)
source("r/render.R")
```

# Analysis {#analysis}

## Introduction

Previous chapters focused on introducing Spark, R and helping you get started with the tools you need throughout this book. In this chapter you will learn how to do data analysis in Spark from R. Data analysis may become the most common task you will do when working with Spark. This chapter will serve as a foundation to later chapters because concepts from this chapter will apply to properly prepare data when doing modeling, graph processing, streaming and other related topics that might not be strictly considered data analysis.

For those who are new to R, this might be a good time to consider complementing this chapter with R for Data Science [@intro-r-for-data-science] and other online resources to learn R. This chapter will try to briefly introduce all the concepts it presents.

### Searching for insights

In a data analysis project, the main goal is to search for insights that can be derived from the data.  It is the results of the data transformations identified during the data analysis phase that can later be formalized into artifacts such as dashboard or model pipelines. The output of the data transformations could be models, aggregations or visualizations. Most data analysis projects follow a set of steps outlined in *Figure 3.1*. 

```{r echo=FALSE, out.width='100%', out.height='220pt', fig.cap='General steps of a data analysis', fig.align = 'center', eval = TRUE}
render_nomnoml("
#direction: right
#edgeMargin: 4
#padding: 25
#fontSize: 18

[Import] -> [Understand]
[Understand |
  [Wrangle] -> [Visualize] 
  [Visualize] -> [Model]
  [Model] -> [Wrangle]
]
[Understand] -> [Communicate]
", "images/analysis-overview-diagram.png", "General steps of a data analysis")
```

### R as an interface to Spark

For data analysis, the ideal approach  is to let Spark do what its good at. It excels at being a parallel computation engine that works at a large scale.  Spark goes beyond offering generic calculations. Out of the box, Spark includes libraries that actually can do a lot of what analysts usually do in R, but for large amounts of data. *Figure 3.2* paraphrases the four main capabilities available to data analysts in Spark.

```{r echo=FALSE, out.width='100%', out.height='220pt', fig.cap='Spark capabilities', fig.align = 'center', eval = TRUE}
render_nomnoml("
#direction: down
#fontSize: 18
#padding: 30
[Apache Spark |
  [SQL Engine]
  [Machine Learning]
  [Graph analysis]
  [Streaming]
]
", "images/analysis-spark-capabilities.png", "Spark capabilities")
```

Thanks to Spark's libraries, most of the Data Science project steps can be completed inside Spark.  For example, selecting, transforming and modeling can all be done by Spark.  The idea is to use R to tell Spark what data operations to run (import, wrangle, model), and then focus on only bringing back into R the results of the operation.  

```{r echo=FALSE, out.width='100%', out.height='220pt', fig.cap='R as an interface for Spark', fig.align = 'center', eval = TRUE}
render_nomnoml("
#direction: right
#padding: 40
#spacing: 140
#arrowSize:0.2
#.tag: visual=none
[<tag>R]Collect results->Push compute[Spark]
[Spark]->[R]
", "images/analysis-r-interface-to-spark-sql.png", "R as an interface for Spark")
```

The `sparklyr` package focuses on implementing the principle mentioned in the previous section.  Most of its functions are mainly wrappers on top of Spark API calls.

The idea is take advantage of Spark's analysis components instead of R's.  For example, if the analyst needs to fit a Linear Regression model, instead of using the familiar `lm()` function, for data available via Spark, the analyst would use the `ml_linear_regression()` function.  The R function will actually run Scala code that runs the Spark's API model.  

```{r echo=FALSE, out.width='100%', out.height='220pt', fig.cap='R as an interface for Spark', fig.align = 'center', eval = TRUE}
render_nomnoml("
[R code |
	[ml_linear_regression() |
    	[Wraps Scala code |
        	var lr = new LinearRegression()
            var lrModel = lr.fit(training)
        ]
    ]
] 
", "images/analysis-r-interface-to-spark-scala.png", "R as an interface for Spark")
```

For more common data manipulation tasks, `sparklyr` provides a back-end for `dplyr`.  This means that already familiar `dplyr` verbs can be used in R, and then `sparklyr` and `dplyr` will translate those actions into Spark SQL statements, see *figure 3.8*.

```{r echo=FALSE, out.width='100%', out.height='220pt', fig.cap='dplyr-to-SQL translation', fig.align = 'center', eval = TRUE}
render_nomnoml("
[R code |
	[filter(y == 1) |
    	[Wraps Spark SQL |
        	... WHERE y = 1 ...
        ]
    ]
] 
", "images/analysis-dplyr-sql-translation.png", "dplyr-to-SQL translation")
```


## Exercise

In order to practice as you learn, the rest of this chapter's code will use a single exercise that runs in the **local** Spark master, this allows the code to be replicated in your laptop. Please, make sure to already have `sparklyr` and a local copy of Spark installed. The installation of Spark can be done by using the utility that comes with the package.  For more information on how to do that please see the Local section in the [Connections](conections/#connections-local) chapter.

First, load the `sparklyr` and `dplyr` packages, and open a new **local** connection.
```{r}
library(sparklyr)
sc <- spark_connect(master = "local")
```



## Import / Access

The step of importing data is to be approached differently when using Spark with R, as oppose to R alone.  When used alone, importing data means that R will read files and import the information into memory  But when used with Spark, it is important to only focus in importing results into R.  The data is either imported or accessed by Spark. This way, the actual analysis takes place inside the Spark session. 
```{r echo=FALSE, out.width='100%', out.height='220pt', fig.cap='Import Data to Spark not R', fig.align = 'center', eval = TRUE}
render_nomnoml("
#direction: right
#padding: 40
#spacing: 140
#arrowSize:0.2
#.tag: visual=none
[<tag>R]Collect results->Push compute[Spark]
[Spark]->[R]
[Spark]<-Import/Access[Data;Source]
", "images/analysis-import-data-to-spark.png", "Import Data to Spark not R")
```

An example of accessing versus importing is found in an enterprise environment.  More likely, Spark sessions are created on top of Hadoop clusters, so data would already be available to be accessed directly by Spark via either a Hive table, or through the Hadoop File System (HDFS). 

The decision of having Spark either access the data source or to import data into memory is mostly a decisions based on speed and performance.  That will be covered in the [Tuning](tuning) chapter.

The exercise's Spark session does not have any data.  So the next step is to prime the session with data, in this case `mtcars`. The `copy_to()` command from `dplyr` can be used for that. The mechanics of this operation is explained in the [Getting Started](starting.html#starting-sparklyr-hello-world) chapter. 

```{r}
library(dplyr)
cars <- copy_to(sc, mtcars, "mtcars_remote")
```

In an enterprise setting, `copy_to()` should only be used to transfer small tables from R, such as a look up value table.

## Wrangle

### Transformations using `dplyr`

The goal is to keep is much of the data transformations in R code as possible. To accomplish that, instead of writing SQL statements to perform data exploration, it is better to take advantage of `sparklyr`'s `dplyr` back end interface.  

In the R environment, *cars* can be treated as if it is a local data frame, so `dplyr` verbs can be used, and in a piped fashion.

```{r}
cars %>%
  group_by(am) %>%
  summarise(mpg_mean = mean(mpg, na.rm = TRUE))
```
```
## # Source: spark<?> [?? x 2]
##      am mpg_mean
##   <dbl>    <dbl>
## 1     0     17.1
## 2     1     24.4
```

Instead of importing the *mtcars_remote* data set from Spark, and then performing the aggregation, `dplyr` converts the verbs into SQL statements that are then sent to Spark.  The `show_query()` command makes it possible to peer into the SQL statement that `sparklyr` and `dplyr` created and sent to Spark.

```{r}
cars %>%
  group_by(am) %>%
  summarise(mpg_mean = mean(mpg, na.rm = TRUE)) %>%
  show_query()
```
```
## <SQL>
## SELECT `am`, AVG(`mpg`) AS `mpg_mean`
## FROM `mtcars_remote`
## GROUP BY `am`
```
As it is evident, it will not be necessary to have to see the resulting query every time `dplyr` verbs are being used. The focus can remain on obtaining insights from the data, as opposed to figuring out how to express a given set of transformation in SQL.

```{r}
cars %>%
  group_by(am) %>%
  summarise(
    wt_mean = mean(wt, na.rm = TRUE),
    mpg_mean = mean(mpg, na.rm = TRUE)
  )
```
```
## # Source: spark<?> [?? x 3]
##      am wt_mean mpg_mean
##   <dbl>   <dbl>    <dbl>
## 1     0    3.77     17.1
## 2     1    2.41     24.4
```

Most of the data transformation made available by `dplyr` to work with local data frames are also available to use with a Spark connection.  This means that a general approach to learning `dplyr` can be taken in order to gain more proficiency with data exploration and preparation with Spark.  The chapter on Data Transformation in the R for Data Science [@intro-r-for-data-science] book should be a great help with this.  If proficiency with `dplyr` is not an issue for the reader, please take some time to experiment with different `dplyr` functions against the *cars* table.

### Correlations

A very common exploration technique is to calculate and visualize correlations.  The Spark API provides an internal function that calculates correlations across the entire data set.  The results are then returned to R as a `data.frame` object. 

```{r}
ml_corr(cars) %>%
  as_tibble()
```

```
## # A tibble: 11 x 11
##       mpg    cyl   disp     hp    drat     wt    qsec
##     <dbl>  <dbl>  <dbl>  <dbl>   <dbl>  <dbl>   <dbl>
##  1  1     -0.852 -0.848 -0.776  0.681  -0.868  0.419 
##  2 -0.852  1      0.902  0.832 -0.700   0.782 -0.591 
##  3 -0.848  0.902  1      0.791 -0.710   0.888 -0.434 
##  4 -0.776  0.832  0.791  1     -0.449   0.659 -0.708 
##  5  0.681 -0.700 -0.710 -0.449  1      -0.712  0.0912
##  6 -0.868  0.782  0.888  0.659 -0.712   1     -0.175 
##  7  0.419 -0.591 -0.434 -0.708  0.0912 -0.175  1     
##  8  0.664 -0.811 -0.710 -0.723  0.440  -0.555  0.745 
##  9  0.600 -0.523 -0.591 -0.243  0.713  -0.692 -0.230 
## 10  0.480 -0.493 -0.556 -0.126  0.700  -0.583 -0.213 
## 11 -0.551  0.527  0.395  0.750 -0.0908  0.428 -0.656 
## # ... with 4 more variables: vs <dbl>, am <dbl>,
## #   gear <dbl>, carb <dbl>
```

The `corrr` R package specializes in correlations.  It contains friendly functions to prepare and visualize the results.  Included inside the package is a back-end for `sparklyr` table objects, so it will not return an error if a non local table is passed to it.  In the background, the `correlate()` function runs `ml_corr()`, so there is no need to collect any data into R prior running the command.

```{r}
library(corrr)

cars %>%
  correlate(use = "pairwise.complete.obs", method = "pearson") 
```

```
## # A tibble: 11 x 12
##    rowname     mpg     cyl    disp      hp     drat      wt
##    <chr>     <dbl>   <dbl>   <dbl>   <dbl>    <dbl>   <dbl>
##  1 mpg      NA      -0.852  -0.848  -0.776   0.681   -0.868
##  2 cyl      -0.852  NA       0.902   0.832  -0.700    0.782
##  3 disp     -0.848   0.902  NA       0.791  -0.710    0.888
##  4 hp       -0.776   0.832   0.791  NA      -0.449    0.659
##  5 drat      0.681  -0.700  -0.710  -0.449  NA       -0.712
##  6 wt       -0.868   0.782   0.888   0.659  -0.712   NA    
##  7 qsec      0.419  -0.591  -0.434  -0.708   0.0912  -0.175
##  8 vs        0.664  -0.811  -0.710  -0.723   0.440   -0.555
##  9 am        0.600  -0.523  -0.591  -0.243   0.713   -0.692
## 10 gear      0.480  -0.493  -0.556  -0.126   0.700   -0.583
## 11 carb     -0.551   0.527   0.395   0.750  -0.0908   0.428
## # ... with 5 more variables: qsec <dbl>, vs <dbl>,
## #   am <dbl>, gear <dbl>, carb <dbl>
```

The `correlate()` function returns a local R object that `corrr` recognizes. This way, it is easy to perform more functions on top of the results. In this case, the `shave()` command turns all of the duplicated results into `NA`'s

```{r}
cars %>%
  correlate(use = "pairwise.complete.obs", method = "pearson") %>%
  shave()
```

```
## # A tibble: 11 x 12
##    rowname     mpg     cyl    disp      hp     drat      wt
##    <chr>     <dbl>   <dbl>   <dbl>   <dbl>    <dbl>   <dbl>
##  1 mpg      NA      NA      NA      NA      NA       NA    
##  2 cyl      -0.852  NA      NA      NA      NA       NA    
##  3 disp     -0.848   0.902  NA      NA      NA       NA    
##  4 hp       -0.776   0.832   0.791  NA      NA       NA    
##  5 drat      0.681  -0.700  -0.710  -0.449  NA       NA    
##  6 wt       -0.868   0.782   0.888   0.659  -0.712   NA    
##  7 qsec      0.419  -0.591  -0.434  -0.708   0.0912  -0.175
##  8 vs        0.664  -0.811  -0.710  -0.723   0.440   -0.555
##  9 am        0.600  -0.523  -0.591  -0.243   0.713   -0.692
## 10 gear      0.480  -0.493  -0.556  -0.126   0.700   -0.583
## 11 carb     -0.551   0.527   0.395   0.750  -0.0908   0.428
## # ... with 5 more variables: qsec <dbl>, vs <dbl>,
## #   am <dbl>, gear <dbl>, carb <dbl>
```
Finally, the results can be easily visualized using `rplot()`.  This function returns a `ggplot` object.

```{r}
cars %>%
  correlate(use = "pairwise.complete.obs", method = "pearson") %>%
  shave() %>%
  rplot()
```

```{r, eval = TRUE, fig.align = 'center', echo = FALSE}
render_image("images/analysis-wrangling-1.png", "")
```

## Visualize

Visualizations are fundamentally a human task.  They are a vital tool to help us find patterns from data.  It is easier for us to identify outliers in a data set of 1,000 observations when plotted then when reading them from a list.

R is great at data visualizations. Its capabilities for creating plots is extended by the many R packages that focus on this analysis step.   Unfortunately, the vast majority of R functions that create plots depend on the data already being in local memory within R, so they fail when using a remote table inside Spark. 

It is possible to create visualizations in R from data source from Spark. To understand how to do this, let's first break down how computer programs build plots:

```{r echo=FALSE, out.width='100%', out.height='100pt', fig.cap='Stages of a plot', fig.align = 'center', eval = TRUE}
render_nomnoml("
#direction: right
#edgeMargin: 4
#padding: 25
#fontSize: 18

[Raw data] -> [Transformed Data]
[Transformed Data] -> [Map data to coordinates]
[Map data to coordinates] -> [Draw plot]
", "images/analysis-stages-of-a-plot.png", "Stages of a plot")
```

For example, to create a bar plot in R, we simply call a function:

```{r, eval = TRUE, out.width='500pt', out.height='400pt', fig.cap='Plotting inside R', fig.align = 'center', eval = TRUE}
ggplot(aes(as.factor(cyl), mpg), data = mtcars) + geom_col()
```

In this case, the `mtcars` raw data was **automatically** transformed into three discrete aggregated numbers, then each result was mapped into an `x` and `y` plane, and then the plot was drawn, see *figure 3.8*. As R users, all of the stages of building the plot are conveniently abstracted for us.

```{r echo=FALSE, out.width='100%', out.height='200pt', fig.cap='R plotting function', fig.align = 'center', eval = TRUE}
render_nomnoml("
#direction: right
#edgeMargin: 4
#padding: 25
#fontSize: 18

[R |
  [mtcars] -> [Total mpg for each cyl]
  [Total mpg for each cyl] -> [x = cyl, y = mpg]
  [x = cyl, y = mpg] -> [Draw plot]
]
", "images/analysis-r-plotting-function.png", "R plotting function")
```

### Transform remotely, plot locally

In essence, the approach for visualizing is the same as in wrangling.  Push the computation to Spark, and then collect the results in R for plotting.  The heavy lifting of preparing the data, such as in aggregating the data by groups or bins, can be done inside Spark, and then collect the much smaller data set into R.  Inside R, the plot becomes a more basic operation.  For example, to plot a histogram, the bins are calculated in Spark, and then in R, the plot that will be used is a simple column plot, as opposed to a histogram plot, because there is no need for R to re-calculate the bins.


```{r echo=FALSE, out.width='100%', out.height='200pt', fig.cap='Plotting with Spark and R', fig.align = 'center', eval = TRUE}
render_nomnoml("
#direction: right
#edgeMargin: 4
#padding: 25
#fontSize: 18
[Spark |
  [Raw data] -> [Transformed Data]
]
[R |
  [Map to coordinates] -> [Draw plot]
]
[Spark] -> [R]
", "images/analysis-plotting-with-spark-and-r.png", "Plotting with Spark and R")
```

### Simple plots

There are a couple of key steps when codifying the "Transform remotely, plot locally" approach.  First, ensure that the transformation operations happen inside Spark. In the example below, `group_by()` and `summarise()` will run as SQL inside the Spark session.  The second is to bring the results back into R after the data has been transformed.  Make sure to transform and then collect, in that order, because if `collect()` is run first, then all R will try to ingest the entire data set from Spark. Depending on the size of the data, collecting all of the data will slow down or may even bring down your system.

```{r}
car_group <- cars %>%
  group_by(cyl) %>%
  summarise(mpg = sum(mpg, na.rm = TRUE)) %>%
  collect()

car_group
```

```
## # A tibble: 3 x 2
##     cyl   mpg
##   <dbl> <dbl>
## 1     6  138.
## 2     4  293.
## 3     8  211.
```

In the example, now that the data has been pre-aggregated and collected into R, only three records are passed to the plotting function.

```{r}
ggplot(aes(as.factor(cyl), mpg), data = car_group) + geom_col()
```

```{r, eval = TRUE, fig.cap='Plot from Spark', fig.align = 'center', echo = FALSE}
render_image("images/analysis-visualizations-1.png", "Plot from Spark")
```

Thanks to the consistency among the `tidyverse` packages, the entire operation can be written in a single piped code segment:

```{r}
cars %>%
  group_by(cyl) %>%
  summarise(mpg = sum(mpg, na.rm = TRUE)) %>%
  collect() %>%
  ggplot(aes(as.factor(cyl), mpg)) + 
  geom_col()
```

```{r, eval = TRUE, fig.align = 'center', echo = FALSE}
render_image("images/analysis-visualizations-1.png", "")
```

Using this approach, most visualizations can be easily produced. 

### Complex plots

There are plots that are both useful and commonly used, but their calculations are not easily reproducible. This makes producing a histogram that runs over the entire large data set has not been typically feasible, because the data needs to be imported into R first. 

Calculating bins of the *mpg* variable would involve the following formula:

```r
(3 * ifelse(
  as.integer(floor(((mpg) - min(mpg, na.rm = TRUE))/3)) == 
    as.integer((max(mpg, na.rm = TRUE) - min(mpg, na.rm = TRUE))/3), 
  as.integer(floor(((mpg) - min(mpg, na.rm = TRUE))/3)) - 1,
  as.integer(floor(((mpg) - min(mpg, na.rm = TRUE))/3)))
) + min(mpg, na.rm = TRUE)
```

The formula breaks down the creation of bins 3 mpg wide into a combination of the most basic aggregation functions.

The formula can run easily inside the `mutate()` command.  It creates as many discrete bins size 3 mpg, and passes the minimum value as the "label" of that bin.


```{r}
mtcars %>%
  mutate(
    bins =
      (3 * ifelse(
        as.integer(floor(((mpg) - min(mpg, na.rm = TRUE))/3)) == 
          as.integer((max(mpg, na.rm = TRUE) - min(mpg, na.rm = TRUE))/3), 
        as.integer(floor(((mpg) - min(mpg, na.rm = TRUE))/3)) - 1,
        as.integer(floor(((mpg) - min(mpg, na.rm = TRUE))/3)))
      ) + min(mpg, na.rm = TRUE)
  ) %>%
  select(mpg, bins) %>%
  head()
```

```
##    mpg bins
## 1 21.0 19.4
## 2 21.0 19.4
## 3 22.8 22.4
## 4 21.4 19.4
## 5 18.7 16.4
## 6 18.1 16.4
```

The same R formula can run inside Spark, `dplyr` will translate the R code into a SQL statement.  The results can then be collected into R for visualizing. 

```{r}
bins <- cars %>%
  mutate(
    bins =
      (3 * ifelse(
        as.integer(floor(((mpg) - min(mpg, na.rm = TRUE))/3)) == 
          as.integer((max(mpg, na.rm = TRUE) - min(mpg, na.rm = TRUE))/3), 
        as.integer(floor(((mpg) - min(mpg, na.rm = TRUE))/3)) - 1,
        as.integer(floor(((mpg) - min(mpg, na.rm = TRUE))/3)))
      ) + min(mpg, na.rm = TRUE)
  ) %>%
  count(bins) %>%
  collect()

bins
```

```
## # A tibble: 7 x 2
##    bins     n
##   <dbl> <dbl>
## 1  19.4     6
## 2  22.4     3
## 3  13.4     8
## 4  10.4     3
## 5  16.4     6
## 6  28.4     4
## 7  25.4     2
```

Since the bins and counts have been pre-calculated, a simple column plot is used.  

```{r}
bins %>%
  ggplot() +
  geom_col(aes(bins, n))
```

```{r, eval = TRUE, fig.align = 'center', echo = FALSE}
render_image("images/analysis-visualizations-2.png", "")
```

## Model


## Communicate



## Later review

### Background

Understanding and applying the concepts in this section are going to be key to a successful analysis using R and Spark. It provides the background information about why the methods used in later sections should be applied in practice. Please keep in mind that this chapter focuses on interactive analysis, not Production pipelines.  The line between the two are sometimes blurred, because often there is an expectation for the analyst to use the insights gathered from the Data Science project, and implement a solution that is to be used in Production, such as Shiny application, or a Model Pipeline.  

As illustrated in *Figure 3.1*, a typical Data Science project is made up of multiple steps. Because its capabilities, every single step for most analyses can be completed inside R.

```{r echo=FALSE, out.width='100%', out.height='220pt', fig.cap='Typical Data Science Project', fig.align = 'center', eval = TRUE}
render_nomnoml("
#direction: right
#edgeMargin: 4
#padding: 25
#fontSize: 18

[R |
  [Import] -> [Tidy]
  [Tidy] -> [Understand |
      [Transform] -> [Visualize] 
      [Visualize] -> [Model]
      [Model] -> [Transform]
  ] 
  [Understand] -> [Share]
]
", "images/analysis-typical-data-science-project.png", "Typical Data Science Project")
```

In addition, the large universe of R packages allow analyst to extend R capabilities in each of the steps.  One good example is the `ggplot2` package which enables the creation of effective and professional looking plots. 

### Working with Big Data

R developers are used to a very specific cadence: Import the data into R, and analyze the data in-memory.  But what happens when dealing with Big Data?

```{r echo=FALSE, out.width='80%', out.height='220pt', fig.cap='Working with Big Data', fig.align = 'center', eval = TRUE}
render_nomnoml("
#direction: right
#spacing: 100
#arrowSize: 0.3
#leading: 5
#padding: 30
#fontSize: 15
#.tag: visual=none
[Big Data] -> Import[<tag>R]
", "images/analysis-working-with-big-data.png", "Working with Big Data")
```


For the purposes of this book, let's define Big Data as data that is too big to fit into RAM.  A secondary definition of Big Data, is data located in a remote machine, and can only be accessed through a small conduit. An example would be a remote database that can only be accessed via a network connection.  

Given these definitions, it is actually very common for an R developer to encounter Big Data in their day-to-day work.  Almost instinctively, the developer opts for one of the following strategies to handle such an encounter with Big Data:

1. **Sampling** - Download into R, what the analyst hopes is, a representative sample of the data being analyzed. Most remote sources do not offer a true randomize way of selecting which records will be downloaded, so any alternative would not provide a data set from which inferences can safely be made.

1. **In parts** - This may be the most common method.  The data is downloaded in segments, and then saved into the local disk.  The analyst then imports into memory the files recursively, and often importing only some of the columns from each file. An example of this would be an R developer using one R script to download one-day's worth of transactions at a time, and saving them into a CSV file, and then using a second R script to compile and analyze the data.  This approach is preferred because it allows the analyst to not have to go "back to the well" every time a new angle needs to be looked at, or the analyst needs to re-run the existing analysis. The main downside of this approach is that the copies saved in the local machine will eventually go stale because they won't receive get any updates made in the original source data. 

1. **Whole** - It is surprising how many choose this option. The idea is to download all of the data, or at least as much as it is physically possible for the local machine.  This usually means having to wait hours before the data is imported.  Once in memory, each step of the Data Science project is painfully slow because the local machine struggles with processing complex calculations over vast amounts of local data.

It is obvious that none of the three strategies are ideal, and this is more likely why an analyst will look into Spark as a way to scale the analysis.

### Avoid running R inside Spark

As it was personally, the usual first reaction is to try to run existing R code in a Spark cluster.  The thought is that somehow Spark will take the R code and packages and divide the job between all of the executors in parallel.  Unfortunately, that is not the case.  

Yes, it is true that `sparklyr` offers a way to run R code inside Spark.  But that is reserved for a very specific use case: embarrassingly parallel jobs.  In other words, if the data to be analyzed needs to be first divided into segments, and then the R code is applied to each segment.  

This is not due to anything lacking on `sparklyr`, or Spark for that matter.  It boils down to how Spark divides its operations.  When running R code, Spark can run one segment per Spark Executor. In the example on *Figure 3.2*, the data is split by the company's customer data, and then a model is fitted for each customer.


```{r echo=FALSE, out.width='100%', out.height='220pt', fig.cap='Works for embarassing parallel jobs', fig.align = 'center', eval = TRUE}
render_nomnoml("
#direction: down
#padding: 20
#fontSize: 18
#edgeMargin: 10
[Spark |
	[Executor 1 | [R| Model for customer A]] 
  [Executor 2 | [R| Model for customer B]] 
  [Executor 3 | [R| Model for customer C]] 
] 
", "images/analysis-embarassing-parallel-jobs.png", "Works for embarassing parallel jobs")
```

The ability to split a job into multiple executors is a core strength of Spark.  It allows the job to run faster because theoretically, each executor has its own discrete amount of machine resources it can use to complete its assigned part of the job.  This means that if the data is not split, or segmented, then the entire R job will run on a single executor, and thus only using that executor's limited resources will be used. Additionally, the job will take much longer to complete, if it completes at all, because the are no other executors helping to complete the job in parallel, see *Figure 3.3*.


```{r echo=FALSE, out.width='100%', out.height='220pt', fig.cap='Running a single large job', fig.align = 'center', eval = TRUE}
render_nomnoml("
#direction: down
#padding: 20
#fontSize: 18
#edgeMargin: 10
[Spark |
	[Executor 1 | [R | Entire;model]]
  [Executor 2 | [Nothing]] 
  [Executor 3 | [Nothing]] 
] 
", "images/analysis-running-single-large-job.png", "Running a single large job")
```

The authors of this book recommend that this approach should only be considered by advanced R users and experienced Spark/Big Data engineers.  There are several infrastructure considerations that need to be taken into account when using this method in an enterprise cluster, please see the chapter [Distributed R](#distributed) for more information.

### R, under the hood

First, lets mentally separate R the language, from R as an analysis engine.  In reality, R does not have a calculation engine, it actually depends on other languages to run these operations.  Under the hood, there are C++ and FORTRAN routines that R simply interfaces with in order to run a given calculation.  

What drives data analysts to R, is the fact that all of the complicated lower level code is now behind easy to use and understand R language functions. 

R is not limited to interacting with the languages mentioned above.  There are R packages that allow data analysts to directly interact languages such as python (`reticulate`) and D3 (`r2d3`), or interact indirectly with other languages, such as how the `shiny` package interfaces with JavaScript and HTML on behalf of the developer.

Given this pattern, running R inside Spark, is akin to thinking in terms of running R code using the browser's HTML interpreter, it would not make much sense.  With `shiny`, R writes HTML and JavaScript code to allow the browser do what its good at.  
